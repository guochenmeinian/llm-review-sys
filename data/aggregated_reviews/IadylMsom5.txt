ID: IadylMsom5
Title: Beyond Detection: A Defend-and-Summarize Strategy for Robust and Interpretable Rumor Analysis on Social Media
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a defend-and-summarize strategy for robust and interpretable fake news detection, leveraging both abstractive and extractive summarization for interpretability and employing bidirectional graph neural networks for text encoding and classification. The authors propose a rumor detection model called Defend-And-Summarize (DAS), which aims to enhance robustness against critical responses and improve interpretability through multi-perspective explanations. Experiments on three datasets demonstrate that DAS outperforms existing methods and provides valuable insights into rumor detection.

### Strengths and Weaknesses
Strengths:  
- The paper addresses significant challenges in rumor detection, specifically robustness and interpretability.  
- Experimental results are comprehensive and support the claims made, showing that the proposed method outperforms existing approaches.  
- The ablation study highlights the necessity of the GCN complex for performance enhancement.  

Weaknesses:  
- The assumption that "responses with similar stances or viewpoints should lie closer in the embedding space" lacks literature and experimental support.  
- Certain claims, such as the robustness of transformer encoders, are made without justification.  
- The complexity of the system is oversold, affecting readability, and the captions for figures are not sufficiently informative.  
- The evaluation of non-graph approaches as baselines is not considered, which may affect the fairness of comparisons.  

### Suggestions for Improvement
We recommend that the authors improve the connection between summarization and interpretability in Section 4.3, specifically clarifying how summaries of social media posts provide interpretable evidence of model decision-making. Additionally, we suggest including more recent fake news datasets from ACL conferences beyond 2019 to reflect the evolving landscape of misinformation. It would be beneficial to provide more details about the human evaluation process in the appendix, including the number of annotators, agreement metrics, and compensation schemes. We also recommend averaging results over multiple random seeds and conducting statistical significance tests for Table 4, as performance gains are sometimes marginal. Lastly, we urge the authors to consider ethical implications regarding the presentation of specific social media posts with non-anonymized user IDs.