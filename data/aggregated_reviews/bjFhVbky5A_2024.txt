ID: bjFhVbky5A
Title: LSH-MoE: Communication-efficient MoE Training via Locality-Sensitive Hashing
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LSH-MoE, a communication-efficient training framework for Mixture-of-Experts (MoE) models utilizing Locality-Sensitive Hashing (LSH). The authors address inefficiencies in existing MoE training methods, particularly high communication costs from all-to-all communications among GPUs. By leveraging token similarity for data compression, the proposed method significantly reduces communication overhead and achieves substantial speedups in training time while maintaining model quality. Evaluations are conducted across various architectures in both language and vision tasks, achieving speedups of 1.28-2.2Ã—.

### Strengths and Weaknesses
Strengths:
1. The use of Locality-Sensitive Hashing for compressing communication data in MoE training is novel and addresses a significant bottleneck in distributed training systems.
2. The authors conduct extensive experiments on various language and vision models, demonstrating the effectiveness of their method across different tasks and datasets.
3. The paper is well-written, with a clear presentation of the algorithm and impressive experimental results.

Weaknesses:
1. The authors may provide a more detailed analysis of the communication overhead, including factors influencing it, such as the relationship between communication overhead and the scale of training servers and models.
2. There is a lack of background on LSH algorithms and related works that improve MoE training efficiency, such as DeepSpeed MoE and SCoMoE, which should be discussed and compared.
3. The paper lacks a comprehensive framework figure for the proposed LSH-MoE method and deeper analysis in the ablation study regarding the effects of different hash functions.

### Suggestions for Improvement
We recommend that the authors improve the analysis of communication overhead to enhance the work's significance. Additionally, providing background on LSH algorithms and discussing related works would strengthen the paper. Including a framework figure for LSH-MoE and conducting a more in-depth ablation study would also benefit readers' understanding. Furthermore, clarifying the difference between compression rates and compression ratios in Figure 6 is essential, and exploring other hashing types or learning to hash methods could enhance the proposed approach's effectiveness.