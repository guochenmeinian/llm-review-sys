ID: Cyn1PvuZsB
Title: Neural Priming for Sample-Efficient Adaptation
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 7, 4, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Neural Priming, a retrieval-based approach for adapting large pretrained models, specifically CLIP, to downstream tasks with few or no labeled examples. The method constructs a priming pool from relevant pretraining data using class names, which is then filtered at test time based on cosine similarity to the test samples. The authors demonstrate that Neural Priming outperforms existing prompt-tuning and test-time adaptation methods across several classification datasets, showing improvements in both zero-shot and few-shot settings.

### Strengths and Weaknesses
Strengths:  
1. The retrieval-based approach is a novel and intuitive method for adapting pretrained classifiers, which is of significant interest to the ML community.  
2. The paper is well-written and presents a clear evaluation, showing consistent improvements over existing methods.  
3. The open questions posed in the discussion section may inspire future research in transfer learning.

Weaknesses:  
1. The paper lacks differentiation from related works, particularly in the related works section, which does not adequately address test-time training/adaptation and few-shot learning.  
2. There is insufficient description of the baselines, making it challenging to assess the performance of Neural Priming relative to state-of-the-art methods.  
3. The method's simplicity raises questions about its originality and effectiveness compared to similar approaches like REACT.

### Suggestions for Improvement
We recommend that the authors improve the related works section by clearly differentiating Neural Priming from test-time training/adaptation and few-shot learning. Additionally, including relevant baselines such as CLIP LP or WiSE for comparison would strengthen the evaluation. We suggest conducting experiments to compare the effectiveness of string retrieval versus semantic retrieval methods, as well as exploring the integration of CLIP and pixel similarity in the filtering process. Clarifying the choice of the linear head and discussing the potential for tuning other parts of the CLIP model would enhance the paper's arguments. Finally, we encourage the authors to report error bars in their experiments to better illustrate the significance of their results.