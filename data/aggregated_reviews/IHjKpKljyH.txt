ID: IHjKpKljyH
Title: Consistency Models for Scalable and Fast Simulation-Based Inference
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel application of consistency models for simulation-based inference (SBI), introducing the consistency model posterior estimation (CMPE) method. The authors demonstrate that CMPE outperforms Neural Posterior Estimation (NPE) and is competitive with Flow Matching Posterior Estimation (FMPE) methods regarding amortized inference quality and speed, achieving similar or better results with fewer sampling steps. The paper includes a thorough experimental analysis across diverse datasets, showcasing the method's efficiency and flexibility in architecture. However, concerns are raised regarding the clarity of the presentation, the fairness of speed comparisons, and the justification of design decisions.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, with clear descriptions of methods and results.
- It provides a comprehensive experimental analysis, demonstrating high-quality results and reproducibility through shared code.
- The authors acknowledge and address the reviewers' critiques, showing a willingness to improve clarity and incorporate historical context.
- CMPE is recognized as competitive with FMPE, particularly in terms of speed and accuracy in specific experiments.

Weaknesses:
- The contribution appears relatively modest, primarily applying existing models to SBI without significant novelty.
- Presentation issues hinder clarity, making it difficult to follow the main arguments, particularly in Sections 1-3.
- Speed comparisons between CMPE and FMPE may be unfair due to differing sampling steps, leading to potential misinterpretations of performance.
- Some methodological choices lack justification, and the experiments are based on relatively simple datasets, which may not fully demonstrate the model's capabilities in complex scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, particularly in Sections 1-3, to better articulate the significance of their contributions. It is essential to provide explicit justifications for design decisions in the text to guide users on adjusting hyperparameters effectively. We suggest clarifying the statement regarding "overcoming sampling inefficiency at [simulator-based] inference time" to avoid misinterpretation. Additionally, consider including all evaluation metrics in one table for easier comparison and expanding the section on SBI methods to include fast and high-accuracy alternatives for lower-dimensional data. We encourage the authors to evaluate their method on more complex datasets to demonstrate its robustness in real-world applications and to address the training time and stability issues associated with consistency models. Lastly, incorporating references to earlier seminal papers on likelihood-free inference would provide a more comprehensive historical context.