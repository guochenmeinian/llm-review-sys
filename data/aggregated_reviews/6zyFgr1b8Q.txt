ID: 6zyFgr1b8Q
Title: Causes and Effects of Unanticipated Numerical Deviations in Neural Network Inference Frameworks
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extensive empirical study on the numerical instabilities encountered during inference across various hardware platforms, including CPUs and GPUs. The authors identify key factors contributing to these discrepancies, such as precision issues and convolution algorithms, and propose mitigation strategies. The study quantifies numerical deviations using metrics like EQCs and Remaining Precisions, aiming to enhance understanding of reproducibility in machine learning. Additionally, the authors provide experimental measurements for initialized weights without training, as shown in Figures R3 and R4, which further supports their conclusions.

### Strengths and Weaknesses
Strengths:
- The authors have conducted a thorough analysis across a diverse range of 75 hardware platforms, effectively quantifying deviations.
- The identification of causes for numerical deviations is clearly articulated, particularly regarding the impact of precision and convolution algorithms.
- The paper addresses a critical issue of reproducibility in the ML/DL community, providing valuable insights.
- The authors' ability to conduct extensive experiments quickly enhances the robustness of their findings and supports their conclusions effectively.

Weaknesses:
- Many numerical deviations discussed are already well-known in the industry, limiting the novelty of the findings. The non-determinism attributed to runtime variance could be mitigated by enforcing determinism in frameworks like TensorFlow or PyTorch.
- The metric "Remaining Precisions" lacks clarity in its relevance to final inference accuracy, as ML applications are generally robust to numerical deviations.
- The paper does not effectively communicate its broader significance to the NeurIPS audience, lacking methodologies for benchmarking numerical precision across new hardware.
- The study is limited to single CPU/GPU configurations and single precision, overlooking the impact of quantized versions and distributed systems.
  
### Suggestions for Improvement
We recommend that the authors improve the introduction by summarizing key results and outlining the journey of the analysis. Additionally, incorporating related work, such as "A Siren Song of Open Source Reproducibility," would enhance scholarly completeness. The authors should also clarify how "Remaining Precisions" translates to inference accuracy and consider exploring the effects of quantized models and distributed systems. To strengthen the paper, we suggest evaluating the effectiveness of proposed mitigation strategies and providing more detailed descriptions of related work and real-world scenarios. Finally, including an appendix with CPU flags and their corresponding clusters could aid reader comprehension. Furthermore, we recommend that the authors ensure all experimental results are presented in a manner that is easily interpretable for the readers.