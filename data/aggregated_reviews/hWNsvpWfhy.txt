ID: hWNsvpWfhy
Title: Localizing Active Objects from Egocentric Vision with Symbolic World Knowledge
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for actively grounding task instructions from an egocentric view, focusing on detecting and tracking objects undergoing change. The authors propose leveraging textual information from LLMs to enhance an open-vocabulary object detector (GLIP) and evaluate their approach on the Ego4D and Epic-Kitchens datasets. The contributions include a prompting pipeline for knowledge extraction and a joint inference framework for object state-change detection.

### Strengths and Weaknesses
Strengths:
- The integration of LLMs to improve object detection and tracking is an interesting approach.
- The proposed method can be applied to further multimodal or vision-based tasks.
- The results demonstrate that incorporating textual information enhances performance over purely visual methods.
- The paper is well-written, with detailed results and analyses, and a thorough limitations section.

Weaknesses:
- The presentation, particularly of figures and tables, requires significant improvement for clarity and descriptiveness.
- The novelty of the approach is questioned, as similar ideas have been previously explored.
- There are concerns regarding the rigor of the reported results, particularly in terms of execution counts and evaluator details.
- The paper lacks comparisons with existing active object detection methods and sufficient details about the datasets used.

### Suggestions for Improvement
We recommend that the authors improve the clarity of figures and tables, ensuring captions are descriptive and informative. Specifically, the caption of Figure 1 should describe the figure's content rather than repeat the abstract, and Table 3 should be split for better readability. We suggest that the authors address the novelty of their approach by discussing related works more comprehensively. Additionally, we recommend providing details on the number of executions for results in Table 3 and clarifying the human evaluation process in Table 2. Finally, including comparisons with existing methods and enhancing the dataset descriptions would strengthen the paper.