ID: 69dAz94zPv
Title: Logarithmic-Regret Quantum Learning Algorithms for Zero-Sum Games
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an improved quantum online learning algorithm for approximating the Nash equilibrium of zero-sum games, achieving $\tilde{O}(1)$ regret with quantum speedup. The authors propose a quantization of classical algorithms based on the MMW method and introduce a fast quantum multi-sampling procedure for Gibbs sampling. The algorithm also serves as a fast quantum linear programming solver, marking a significant advancement in quantum learning theory.

### Strengths and Weaknesses
Strengths:
- The paper addresses a timely topic, proposing the first online quantum algorithm for zero-sum games with near-optimal regret.
- It achieves a quadratic improvement over classical algorithms for computing the approximate Nash equilibrium.
- The work is well-written, with multiple novel contributions, including speedups for multi-Gibbs sampling and logarithmic regret approximation.

Weaknesses:
- The necessity of QRAM is a minor weakness, as its physical realizability remains uncertain.
- The main algorithm's derivation from classical methods lacks clarity regarding the unique contributions of quantum computing.
- The paper could benefit from a clearer example demonstrating the algorithm's practical implications.
- The novelty of the techniques used is questioned, as they do not significantly depart from prior work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions of online learning and regret, particularly in the context of Nash equilibrium. Additionally, we suggest including a well-motivated example to illustrate the practical applications of the proposed algorithm. The authors should also clarify the unique contributions of their quantum approach compared to classical methods, particularly regarding the Gibbâ€™s sampling algorithm and the implications of low regret. Finally, addressing the concerns about QRAM's necessity and its implications for the algorithm's applicability would strengthen the paper.