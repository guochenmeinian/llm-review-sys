ID: T3SstRu5fq
Title: PUCA: Patch-Unshuffle and Channel Attention for Enhanced Self-Supervised Image Denoising
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 7, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel network architecture for self-supervised image denoising, utilizing a patch-unshuffle operation and dilated attention blocks to enhance the J-invariance property. The proposed method, referred to as PUCA, integrates these components into a U-Net-like architecture, allowing for multi-scale representation and improved performance on real-world denoising datasets. Experimental results indicate that PUCA outperforms existing methods, demonstrating its effectiveness in self-supervised image denoising.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow.
2. The introduction of patch-unshuffle and dilated attention blocks is interesting and contributes to preserving J-invariance.
3. Extensive experiments show that the proposed method significantly outperforms prior art on benchmark datasets.

Weaknesses:
1. The overall novelty is limited, as the architecture is built upon existing frameworks, with the main contribution being the flexibility of the network design.
2. The ablation study is incomplete, lacking comparisons with existing multi-scale BSN architectures.
3. Competing methods are not adequately addressed, as several recent self-supervised denoising methods are ignored in the evaluation.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contributions by providing clearer justifications for the significance of patch-unshuffle compared to existing methods. Additionally, we suggest completing the ablation study to include comparisons with existing multi-scale BSN architectures. It would also be beneficial to incorporate evaluations against the latest self-supervised denoising methods to strengthen the competitive analysis. Lastly, enhancing clarity in the explanations of the dilated attention block and pixel-shuffle downsampling would improve the overall presentation of the paper.