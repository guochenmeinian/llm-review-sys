ID: wvQHQgnpGN
Title: Solving Zero-Sum Markov Games with Continuous State via Spectral Dynamic Embedding
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 4, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm, Spectral Dynamic Embedding Policy Optimization (SDEPO), aimed at solving two-player zero-sum stochastic Markov games (2p0s-MGs) with continuous state spaces. The authors demonstrate that SDEPO achieves optimal convergence rates and provides theoretical guarantees for its performance, marking a significant advancement in handling continuous state spaces in reinforcement learning. The effectiveness of the algorithm is validated through empirical experiments against baseline methods.

### Strengths and Weaknesses
Strengths:  
- The function approximation approach is essential for large or infinite state cardinality problems, and SDEPO is the first to effectively utilize problem dynamics.
- The paper provides a clear motivation for adapting spectral dynamic embedding to stochastic nonlinear control problems.
- The theoretical results are well-supported by empirical experiments, showcasing the algorithm's efficiency.

Weaknesses:  
- The evaluation of SDEPO is limited to its success rate against baseline algorithms, failing to fully validate its convergence properties.
- Sections 3-4 require clearer explanations, while sections 5 and 6 appear rushed and diverge from the analytical results.
- Assumption 5 is notably strong and warrants further discussion regarding its implications.
- The paper does not adequately address existing algorithms that incorporate dynamics and function approximation in single-agent settings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of sections 3-4 and ensure that sections 5 and 6 are more closely aligned with the analytical results presented earlier. A discussion on the impact of Assumption 5 should be included to justify its strength. Additionally, we suggest focusing on the existing setting with better presentation and more comprehensive experiments. The authors should also address the current literature on single-agent reinforcement learning with function approximation to provide context for their contributions. Lastly, justifying the use of Assumption 1 and clarifying the novelty of the spectral dynamic embedding method would enhance the paper's rigor.