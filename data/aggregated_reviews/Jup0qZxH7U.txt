ID: Jup0qZxH7U
Title: Adaptive Layer Sparsity for Large Language Models via Activation Correlation Assessment
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 8, 5, 6, 6, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Adaptive Layer Sparsity (ALS), a novel method for optimizing large language models (LLMs) through selective pruning. The authors propose a technique that estimates the correlation between intermediate layers using information orthogonality, allowing for precise measurement of each layer's importance. The sparsity allocation problem is formulated as a linear programming optimization, enabling efficient global optimization of layer-wise sparsity ratios. Extensive experiments on various LLM families, including LLaMA and OPT, show consistent performance improvements over existing pruning methods. The paper also analyzes factors such as calibration data and feature selection on the method's performance.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel approach for optimizing LLMs, effectively addressing challenges in LLM compression, particularly manual sparsity setting and suboptimal performance from uniform pruning ratios.
2. The experimental design is robust, with extensive evaluations across different LLM families and parameter sizes.
3. ALS is simple, effective, and computationally efficient.
4. The methodology is well-structured and clearly articulated, detailing the correlation matrix estimation and linear optimization for sparse allocation.

Weaknesses:
1. There is significant overlap between the introduction and related work sections, which could be consolidated to enhance flow and reduce redundancy.
2. Key details about ALS, particularly normalization and reweighting strategies mentioned in the ablation study, are insufficiently described in the method section and should be elaborated with formulas.
3. The practical value of the method is questioned, as performance improvements with low sparsity ratios appear marginal, and comparisons with dense models yield close performance metrics.
4. The presentation could be improved, as long paragraphs with mixed discussion points detract from clarity.

### Suggestions for Improvement
We recommend that the authors improve the consolidation of the introduction and related work sections to enhance the paper's flow. Additionally, we suggest that the authors elaborate on the normalization and reweighting strategies in the method section, including relevant formulas. To address concerns about practical value, we encourage the authors to provide a more detailed analysis of performance comparisons between pruned and dense models, particularly in low sparsity scenarios. Lastly, we advise improving the presentation by breaking down long paragraphs and clarifying mixed discussion points for better readability.