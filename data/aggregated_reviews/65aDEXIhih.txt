ID: 65aDEXIhih
Title: Computational Complexity of Learning Neural Networks: Smoothness and Degeneracy
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents results on the nonexistence of efficient learning algorithms for 3-depth ReLU networks with smoothed parameters and Gaussian input. It proves that there is no efficient learning algorithm for 2-depth ReLU networks with smoothed parameters and smoothed inputs. The authors show that learning 3-layer networks is hard under the smoothed analysis framework, even with lower bounds on the smallest singular values of the weight matrices. Additionally, the study explores the hardness of learning depth-4 networks without output activation, challenging the assumption that Gaussian inputs and parameter smoothing suffice for efficient learning. The paper discusses the relevance of the learning problem and its connection to practical neural network training, although the justification for using ReLU in the output layer is questioned.

### Strengths and Weaknesses
Strengths:  
- The paper rigorously analyzes the theoretical aspects of learning neural networks, providing a logical presentation of its results.  
- It addresses an interesting question regarding the complexity of learning neural networks and contributes to understanding the boundaries of hard and easy cases in learning theory.  
- The technical results are presented clearly, and the authors clarify the novelty of their work compared to prior studies, enhancing the understanding of their contributions.  
- The results demonstrate the hardness of learning depth-4 networks under smoothed analysis, which is a significant finding.

Weaknesses:  
- The relevance of the learning problem studied is unclear, particularly in relation to practical training and generalization performance.  
- The motivation for using ReLU in the output neuron is deemed insufficient, as it lacks practical relevance and clarity.  
- The hypothesis in Definitions 2.1 and 2.2 appears to belong to an arbitrary class of functions, raising concerns about computational intractability.  
- The novelty of the paper is limited compared to prior work, particularly regarding the minor differences in activation functions.  
- The organization of Section 4 is convoluted, making it difficult to follow the proof structure.  
- There is ambiguity regarding the generality of the hardness results, with concerns about the claim that no efficient learning algorithm exists for all depth-3 networks.

### Suggestions for Improvement
We recommend that the authors improve the motivation and relevance of the learning problem by clarifying its connection to practical neural network training and generalization performance. Additionally, the authors should provide clearer insights into the justification for using ReLU in the output neuron and specify whether the hypothesis in Definitions 2.1 and 2.2 belongs to the same class of neural networks as defined by \(N_\theta\) to address concerns about computational intractability. A clearer comparison with related work, particularly regarding the novelty of their results compared to prior studies, would enhance the paper's contribution. 

We suggest reorganizing Section 4 to include a high-level proof sketch at the beginning and explicitly stating key lemmas to aid reader comprehension. The authors should also clarify the role of the examples oracle in Section 4.3 and provide more information about the specific architecture used in the proofs, including its implications for widths and input dimensions. Finally, we encourage the authors to explore the possibility of efficient learning for 3-depth networks without activation functions at the output, as this could strengthen their contribution.