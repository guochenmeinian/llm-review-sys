ID: HN05DQxyLl
Title: Approximating mutual information of high-dimensional variables using learned representations
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 6, 7, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for approximating mutual information (MI) between high-dimensional multivariate random variables by leveraging low-dimensional embeddings learned through neural networks. The authors utilize a non-parametric MI estimator on these embeddings, providing theoretical motivation and extensive experimental evaluations across synthetic and biological datasets. The approach aims to address challenges posed by the curse of dimensionality in MI estimation.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and addresses an important problem with an interesting approach.  
- Extensive experimentation supports the utility of the proposed method, particularly in demonstrating its application to domain-specific open problems.  
- The evaluations are comprehensive, with clear presentations that suggest broader applications of MI in scientific contexts.  

Weaknesses:  
- The evaluation of synthetic multivariate Gaussian data is limited, focusing only on specific forms of linear dependence without comparisons to methods like Sliced MI.  
- The motivation for the proposed approach contains inaccuracies regarding previous findings, particularly misrepresenting the results of McAllester and Stratos (2018).  
- The theoretical justification for the method is simplistic, relying on loose approximations and lacking a joint design, which introduces independent sources of error.  
- The learned representations may not adequately capture the dependence between variables, leading to potential failures in MI estimation.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of synthetic multivariate Gaussian data by including comparisons to Sliced MI and related methods. Additionally, clarify the inaccuracies in the motivation section regarding McAllester and Stratos (2018) and ensure that the theoretical foundations are robust. We suggest addressing the limitations of the learned representations explicitly, particularly in terms of their ability to capture dependence structures. Furthermore, consider including standard deviations in all tables and improving figure readability, especially in cases where labels are unclear. Lastly, we advise discussing failure cases more thoroughly to enhance the understanding of the method's limitations and its interaction with data distributions.