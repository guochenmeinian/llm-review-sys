ID: 5H4l37IsZ8
Title: Task-recency bias strikes back: Adapting covariances in Exemplar-Free Class Incremental Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper addresses the Exemplar-Free Class Incremental Learning (EFCIL) challenge by proposing AdaGauss, which adapts covariance matrices and incorporates an anti-collapse loss function to mitigate task-recency bias. The authors demonstrate that adapting both the mean and covariance during training is crucial for effective learning, achieving strong experimental results across multiple datasets, whether training from scratch or using a pre-trained backbone.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and motivated, clearly demonstrating the importance of covariance matrix adaptation in EFCIL.
2. AdaGauss shows promising experimental results on five datasets.
3. The method is straightforward and easy to understand.

Weaknesses:
1. Observations 2 and 3 appear similar, lacking significant novelty compared to existing explanations of task-recency bias. The analysis does not sufficiently differentiate from established literature.
2. The relationship between adapting mean and covariance during training and existing test-time adaptation methods needs clearer delineation.
3. While encouraging linearly independent features can mitigate dimensionality collapse, it does not ensure meaningful feature production. The effectiveness of the proposed loss term \(L_{AC}\) requires further theoretical analysis.
4. The literature review is incomplete, omitting important EFCIL techniques and lacking experimental comparisons with key works.
5. The concept of "dimensionality collapse" is crucial but inadequately explained.
6. The proposed method introduces additional structures requiring extensive training data, necessitating a discussion on model parameters and training time.

### Suggestions for Improvement
We recommend that the authors improve the clarity of observations 2 and 3 to highlight their distinct contributions. Additionally, we suggest including comparisons with existing test-time adaptation methods to clarify the proposed method's innovations. The authors should provide a theoretical analysis of the effectiveness of \(L_{AC}\) in preventing dimensionality collapse. We also encourage the authors to expand the literature review to include significant EFCIL techniques and conduct experimental comparisons with them. Furthermore, a more thorough explanation of "dimensionality collapse" is necessary. Lastly, we recommend discussing the model parameters and training time implications of the proposed method, as well as clarifying the assumptions made regarding the learned adaptation network's applicability to previous tasks.