ID: UPxmISfNCO
Title: Efficiency for Free: Ideal Data Are Transportable Representations
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 6, 3, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to dataset distillation aimed at enhancing representational efficiency and training effectiveness through the Representation Learning Accelerator (RELA). The authors propose that RELA utilizes a pre-trained teacher model to optimize self-supervised learning via mutual information maximization, while also demonstrating that their method can generate efficient data from weak prior models, including randomly initialized models, without the computational overhead of traditional distillation methods. Empirical results indicate that RELA can outperform traditional training methods, even with reduced datasets, although the significance of the ReLA_D model remains unclear. The authors emphasize that RELA fundamentally differs from knowledge distillation, as it leverages various prior models to enhance training performance.

### Strengths and Weaknesses
Strengths:
- The paper tackles critical issues in efficient training and dataset distillation.
- RELA introduces a novel approach that effectively bridges representation learning with data-efficient methods.
- The authors provide empirical evidence demonstrating that both weak and randomly initialized models can effectively enhance training performance.
- The method is presented as a flexible, plug-and-play approach that does not impose restrictions on the architecture or scale of prior models.
- The paper is clearly written and supported by comprehensive theoretical analysis and extensive experimental studies.
- The dynamic decay of the loss function coefficient during training is a novel aspect that contributes to the method's effectiveness.

Weaknesses:
- The writing style could be improved for better accessibility, with some notations lacking clarity.
- The motivation behind the proposed method is based on oversimplified examples, which may not generalize to complex real-world scenarios.
- The practical applications of dataset distillation are not adequately discussed, and comparisons with state-of-the-art methods are missing.
- Some reviewers express skepticism regarding the novelty of the approach, suggesting that the improvements may stem from the use of a pre-trained teacher model rather than the theoretical contributions of the paper.
- The empirical results do not sufficiently differentiate ReLA from existing methods, leading to doubts about the overall impact of the proposed framework.
- The theoretical analysis does not generalize well to complicated data distributions, and the introduction of dynamic dataset distillation appears misaligned with the core concept.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing style, particularly in the notation and presentation of proofs. It would be beneficial to introduce the technical solution earlier in the paper and provide a more detailed discussion on the practical applications of dataset distillation, including neural architecture search and continual learning. Additionally, we suggest including comparisons with state-of-the-art methods in the evaluation section and addressing the limitations of the theoretical analysis regarding complex data distributions. To strengthen the manuscript's argument, we recommend enhancing the discussion on how ReLA fundamentally differs from knowledge distillation and providing more robust comparisons with existing methods. Finally, incorporating additional experiments to validate the theoretical insights would further substantiate the claims made in the paper.