ID: FbXQrfkvtY
Title: Probing the Decision Boundaries of In-context Learning in Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 3, 10, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the decision boundaries of in-context learning in large language models (LLMs) through binary classification tasks. The authors explore the performance of various mainstream models, revealing that despite high test accuracy, the decision boundaries are often irregularly non-smooth. Factors such as model size and prompt influence on decision boundaries are examined, along with methods like supervised fine-tuning (SFT) and adaptive sampling to enhance boundary smoothness. The study also includes empirical results from synthetic datasets and extends to real-world NLP classification tasks.

### Strengths and Weaknesses
Strengths:
- This is the first study to explore the decision boundaries of in-context learning LLMs.
- The experiments are thorough, providing beneficial insights for future research, particularly regarding uncertainty-aware active learning.
- The writing is clear, and the paper includes robust evaluations across various model families and sizes.

Weaknesses:
- The empirical results are limited to toy datasets, raising concerns about generalizability to other tasks.
- The decision boundaries are heavily influenced by prompt design, and the robustness of results to prompt variations is not adequately explored.
- There is a lack of quantitative evaluation of decision boundary smoothness, making comparisons difficult.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by conducting additional experiments on a wider range of tasks beyond binary classification. It would be beneficial to clarify the rationale for using different model series and to explore the robustness of decision boundaries to variations in prompt formats, including different designs and synonym replacements. Additionally, we suggest incorporating quantitative metrics for smoothness evaluation to enhance the comparability of results.