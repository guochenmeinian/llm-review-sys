ID: 8gYRHspcxK
Title: Aligning Large Language Models through Synthetic Feedback
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for aligning large language models (LLMs) with human values using synthetic feedback. The authors propose a framework that generates a synthetic preference ranking dataset to train a reward model, which is then utilized for supervised fine-tuning and reinforcement learning. The resulting model, ALMoST, demonstrates superior performance against several recent LLMs on alignment benchmarks and human evaluations. A key contribution is the intuitive construction of the synthetic dataset, which does not rely on external supervisions, thereby reducing alignment learning costs. The paper also introduces a method called reward-model guided self-play (RMSP) to enhance the quality of supervised fine-tuning data.

### Strengths and Weaknesses
Strengths:
- The method addresses a critical challenge in LLM research by minimizing reliance on human annotations and external supervisions.
- The framework's full recipe for using synthetic feedback is beneficial to the research community.
- Experimental results indicate that the proposed model outperforms existing aligned LLMs, showcasing its effectiveness.

Weaknesses:
- The core assumption that larger models with better demonstrations yield higher quality responses is not comprehensively evaluated.
- The heuristic filter used in synthetic data generation requires significant expert knowledge, contradicting the goal of reducing human effort.
- The applicability of the approach for aligning larger models remains unclear, particularly regarding the choice of teacher models for different sizes.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of the core assumption regarding the quality of responses from larger models by conducting a systematic analysis of various LLM configurations. Additionally, we suggest clarifying the role of the heuristic filter in the synthetic data generation process to align better with the paper's objectives. It would also be beneficial to address how the proposed method can be adapted for aligning larger models, particularly by providing examples of suitable teacher models for different sizes. Finally, we advise revising the terminology used in the paper for clarity and consistency, particularly regarding "pre-aligned" versus "aligned" LLMs.