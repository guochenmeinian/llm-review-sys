ID: wPqEvmwFEh
Title: Small batch deep reinforcement learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the effects of batch size on performance in value-based reinforcement learning (RL) algorithms. The authors find that smaller batch sizes can enhance performance and speed up training, particularly in QR-DQN, while DQN shows no improvement unless deeper networks or n-step returns are utilized. The study includes a comprehensive set of experiments that explore factors such as update variance, representation norm, and network expressivity to elucidate the benefits of smaller batch sizes.

### Strengths and Weaknesses
Strengths:
- The paper provides a robust series of experiments demonstrating the effectiveness of smaller batch sizes across various value-based RL algorithms.
- The analysis offers clear insights into why smaller batch sizes improve performance and addresses the lack of benefits in DQN.
- The writing is clear and well-organized, with thorough empirical evaluations across a range of Atari tasks.

Weaknesses:
- The study is limited to visual tasks with discrete actions, raising questions about the applicability of findings to non-visual RAM observations.
- The relationship between smaller batch sizes and improved exploration is not clearly articulated, leaving ambiguity regarding the observed benefits.
- The authors overlook existing literature that discusses batch size effects, which could enhance the paper's context and credibility.

### Suggestions for Improvement
We recommend that the authors improve the discussion on how n-step returns interact with batch size, particularly regarding the variance implications. Additionally, the authors should clarify how their findings relate to exploration and address the limitations of focusing solely on discrete action tasks. It is crucial to expand the related work section to include prior studies that have explored batch size effects, explaining discrepancies with their findings. Lastly, the authors should refine their arguments and provide stronger evidence to support their claims, particularly regarding the correlation between batch size and performance metrics.