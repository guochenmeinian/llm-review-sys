ID: WPPC7FHtaM
Title: IPO: Interpretable Prompt Optimization for Vision-Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 6, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called Interpretable Prompt Optimization (IPO) aimed at enhancing the performance and interpretability of vision-language models (VLMs). IPO dynamically generates and optimizes text prompts using large language models (LLMs), addressing the shortcomings of traditional prompt optimization methods that often lead to overfitting and produce prompts lacking human readability. By integrating a large multimodal model (LMM) for generating image descriptions, IPO facilitates the creation of dataset-specific prompts that improve generalization while ensuring human comprehension. The authors validate IPO across 11 datasets, demonstrating improved accuracy and interpretability compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The paper tackles a critical issue in prompt tuning by utilizing LLMs to generate interpretable prompts, contrasting with opaque traditional methods.
- The prompt optimization framework is scalable across multiple tasks, as evidenced by thorough ablation studies.
- Extensive testing on 11 datasets showcases the method's effectiveness in enhancing both accuracy and interpretability.

Weaknesses:
- The method shows improved performance primarily for novel classes, with significantly lower performance on base classes across many datasets, necessitating a balance in performance.
- There is a lack of comparison with recent prompt-tuning methods, which could provide a clearer context for IPO's contributions.
- The paper does not adequately discuss the limitations of IPO, particularly regarding its scalability in scenarios with large domain samples.

### Suggestions for Improvement
We recommend that the authors improve the justification for the observed performance disparity between novel and base classes, ensuring a balanced evaluation. Additionally, we suggest adding a dedicated section comparing IPO with recent prompt-tuning methods such as LFA, PLOT, and others to contextualize its contributions. Furthermore, we encourage the authors to extend the discussion on the method's limitations, particularly its applicability in large-scale scenarios, to provide a more comprehensive understanding of IPO's capabilities.