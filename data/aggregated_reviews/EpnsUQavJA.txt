ID: EpnsUQavJA
Title: CoIN: A Benchmark of Continual Instruction Tuning for Multimodel Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 8, 5, 8, 5, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark called CoIN to evaluate Multimodal Large Language Models (MLLMs) in continual learning scenarios using vision-language datasets. The authors empirically demonstrate that sequential fine-tuning can outperform traditional multitask training in certain cases. They also introduce two innovative evaluation metrics: Truth Alignment and Reasoning Capability, which assess the models' outputs and reasoning knowledge, respectively. The findings indicate that MLLMs primarily suffer from catastrophic forgetting in instruction following rather than in retained knowledge.

### Strengths and Weaknesses
Strengths:
1. The construction of a diverse and comprehensive benchmark facilitates the investigation of MLLMs under continual fine-tuning, serving as a cornerstone for future research.
2. The proposed evaluation metrics effectively assess MLLMs' performance, providing valuable insights into their capabilities.
3. Extensive experiments reveal critical insights into the effects of instruction diversity and task order on model performance.
4. The release of the benchmark and corresponding code enhances reproducibility and community engagement.
5. The paper is well-written and clear.

Weaknesses:
1. The evaluation of reasoning capability relies on Qwen-1.5-32B, which may not be the most reliable judge; using GPT-4 could provide a more conventional and trusted assessment.
2. The comparison of methods lacks consideration of other continual learning baselines, such as regularization methods, which could strengthen the analysis.
3. The proposed MoELoRA framework shows minimal improvements in mitigating forgetting, suggesting the need for more robust comparisons with multi-task settings.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of reasoning capability by using GPT-4 as a judge, as it is conventionally accepted in the literature. Additionally, incorporating comparisons with other continual learning methods, such as rehearsal or regularization techniques, would enhance the robustness of the findings. Finally, we suggest that the authors explore the impact of updating overall parameters using multimodal data to further investigate knowledge retention in MLLMs.