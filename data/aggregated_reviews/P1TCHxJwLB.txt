ID: P1TCHxJwLB
Title: Hierarchically Gated Recurrent Neural Network for Sequence Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 25
Original Ratings: 6, 8, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel recurrent neural network architecture, the Hierarchical Gated Recurrent Units (HGRU), aimed at enhancing long-sequence modeling through improved gating schemes and complex forget gate values. The authors introduce a learnable lower bound on the forget gate for each layer to capture both short-term and long-range dependencies. The HGRU is evaluated on benchmarks such as Wikitext, LRA, and Imagenet-1K, with ablation studies conducted to analyze design choices. Additionally, the paper presents the HGRN model, which aims to demonstrate competitive performance in language modeling and LRA benchmarks, asserting that HGRN outperforms prior methods. New experimental results on Wikitext-103 and the Pile dataset show that HGRN achieves parity with Transformers for 125M models and provides insights into the impact of forget gates.

### Strengths and Weaknesses
Strengths:
- The paper addresses a relevant area in linear SSMs/RNNs, presenting an interesting perspective on the linear RNN dynamics matrix akin to the LSTM forget gate.
- The formulation of the forget gate serves as a practical example of input-dependent dynamics, appealing to the deep SSM community.
- The proposed method demonstrates strong performance across evaluated tasks, with new experimental results enhancing the paper's credibility.
- Ablation studies provide valuable insights into the design choices and the positive role of the forget gate in the architecture.
- The release of source code facilitates reproducibility of results.

Weaknesses:
- Key results from relevant works are omitted from results tables, leading to misleading claims about the HGRU's performance compared to methods like SgConv and MEGA.
- The overall presentation is poor, with inconsistencies in tables and missing promised appendix content.
- Lack of clarity regarding which methods were implemented by the authors and insufficient details on training configurations hinder reproducibility.
- The manuscript is difficult to follow due to undefined variables and unclear claims.
- The necessity of forget gates is not convincingly established, as comparisons with other architectures lacking forget gates are insufficient.
- The claims regarding the simplicity of the method are subjective and may require more nuanced discussion.

### Suggestions for Improvement
We recommend that the authors improve the completeness of the results by including omitted methods such as SgConv, MEGA, and S5 in the results tables to avoid misleading statements about HGRU's performance. Additionally, we suggest including a baseline comparison with H3 for the Wikitext-103 experiments to better contextualize the effectiveness of HGRU innovations. 

To enhance clarity, we recommend that the authors explicitly state which results are from their own experiments and which are sourced from other works. It would also be beneficial to provide more comprehensive comparisons with models designed without forget gates, such as S4/S5/LRU, to substantiate the value of the forget gate. 

To improve presentation, we advise clarifying the labeling in Figure 1 and ensuring consistency in the formatting of tables. We also recommend providing detailed hyperparameters and implementation specifics in the appendix to facilitate reproducibility. 

Furthermore, we suggest revising claims regarding the effectiveness of the method to be more defensible, such as changing "proves the effectiveness" to "provides evidence." Lastly, we encourage the authors to address the clarity of variable definitions and the overall organization of the manuscript to improve readability and to include performance metrics on downstream tasks to better contextualize the perplexity results.