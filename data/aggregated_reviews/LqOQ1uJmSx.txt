ID: LqOQ1uJmSx
Title: Compositional Generalization from First Principles
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 5, 6, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates mathematical conditions for compositional generalization, defining compositional representation and compositional support. It derives sufficient conditions on training distribution support and model architecture, conducting empirical validation through a synthetic dataset. The authors propose that the compositional data-generating process is crucial for enabling compositional generalization, supported by a theorem that outlines necessary conditions. The empirical results demonstrate the application of the theory, although the scope of the theorem is limited. Reviewers note the novelty of the theoretical results and the general form and wide application of the theory. The experimental section is described as solid and provides compelling evidence, although the authors acknowledge that additional experiments on datasets like MPI-3D and Shapes3D may not substantially enhance their contribution due to limitations in data access and sampling density.

### Strengths and Weaknesses
Strengths:
- The paper addresses a longstanding issue in compositional generalization, providing a theoretical framework that is clearly articulated.
- The definitions and theorem are well-structured, indicating potential for broad application.
- The empirical validation supports the theoretical claims, and the ablation study effectively illustrates the impact of violating conditions.
- The theoretical results are recognized for their novelty and potential wide application.
- The experimental section is deemed solid and compelling by reviewers.

Weaknesses:
- The theorem's applicability is limited, primarily focusing on disentangled inputs, while many real-world problems involve entangled inputs.
- The empirical experiments are confined to a toy dataset, lacking validation on more complex datasets that could better demonstrate the theory's applicability.
- The paper does not clarify whether the theorem applies to parametric composition functions, which could affect generalization to unseen data.
- Proposed experiments on MPI-3D and Shapes3D are limited by the discrete nature of their latent variables and insufficient sample sizes, which may hinder model fitting and generalization conclusions.
- Implementing a more complex function for latent factor composition in 3D space is seen as a significant overhead with minimal added value.

### Suggestions for Improvement
We recommend that the authors improve the scope of the theorem by addressing cases with entangled inputs, as many practical applications involve such complexities. Additionally, conducting experiments on more diverse and complex datasets, such as MPI-3D or Shapes3D, would strengthen the empirical validation of the proposed conditions. We also suggest including results from a CLEVR-like dataset in the final version, even if only to cover simple cases such as occlusion. Clarifying the applicability of the theorem to parametric composition functions in the discussion section would enhance the paper's rigor. Finally, if the reviewers maintain that further experiments would enhance the paper's value, we encourage the authors to explore these possibilities for the camera-ready version.