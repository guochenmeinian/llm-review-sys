ID: YMAU2kJgzY
Title: Evaluating Large Vision-and-Language Models on Children's Mathematical Olympiads
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 5, 9, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SMART-840, a benchmark dataset for evaluating large vision-and-language models (LVLMs) on mathematical reasoning tasks derived from the Mathematical Kangaroo Olympiad for grades 1-12. The dataset includes 840 problems from 2020-2024, featuring both text-only and image-text questions. The authors systematically evaluate models such as GPT-4o, Gemini, and Claude-3 Opus, revealing a negative correlation between problem complexity and LVLM performance, particularly noting that LVLMs struggle more with problems aimed at younger children. The work aims to provide insights into AI models' mathematical reasoning capabilities relative to human cognitive development.

### Strengths and Weaknesses
Strengths:
- The creation of the SMART-840 benchmark dataset is a significant contribution, providing a challenging and comparable evaluation framework for human performance across age groups.
- The negative correlation between problem complexity and LVLM performance is a noteworthy finding.
- The inclusion of both text-only and image-text problems enhances the robustness of the evaluation.
- Diverse correlation analyses yield insightful comparisons between LVLMs and children's math skills.

Weaknesses:
- The evaluation lacks open-source LVLMs, limiting reproducibility and broader applicability.
- The explanation of the Discriminative Index is unclear, particularly regarding the measurement of learner performance.
- The prompt design for LVLMs could be more comprehensive, exploring various strategies to fully assess mathematical reasoning capabilities.
- The dataset's small scale (840 problems) is insufficient for robust evaluation, especially when compared to larger benchmarks like GSM8K and MathVista.
- Limited categories (geometry, logic, algebra, numbers) hinder comprehensive analysis across mathematical disciplines.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the Discriminative Index explanation and provide a more detailed analysis of learner performance. Additionally, we suggest expanding the prompt design to explore various strategies for assessing LVLMs' mathematical reasoning capabilities. To enhance the dataset's robustness, we encourage the authors to include more problems and categories, as well as to analyze text-only and image-text problems separately. Furthermore, a thorough error analysis of model responses across different grades is necessary to substantiate claims regarding LVLM performance disparities. Finally, we urge the authors to verify and provide evidence that the licensing terms of the dataset allow for the copying and distribution of the math test questions for the intended use.