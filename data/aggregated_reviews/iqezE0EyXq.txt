ID: iqezE0EyXq
Title: Adaptive recurrent vision performs zero-shot computation scaling to unseen difficulty levels
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 4, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel integration of Adaptive Computation Time (ACT) with Convolutional Recurrent Neural Networks (ConvRNNs) through the introduction of the LocRNN architecture. This architecture adapts the number of computation steps based on task difficulty, demonstrating its effectiveness on the Pathfinder and Mazes tasks. The authors claim that LocRNN outperforms ConvGRU and traditional models, showing reliable performance across random initializations and generalizing to unseen task difficulties. However, the novelty of the adaptive computation method is questioned, as it appears limited compared to previous works by Bansal et al. (2022) and Linsley et al. (2020). The clarity of the contributions of the LocRNN architecture is also under scrutiny.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant challenge in deep learning regarding computational efficiency and adaptive processing.
- LocRNN shows consistent performance improvements over ConvGRU, with lower variance across random initializations.
- The incorporation of hidden interneurons and lateral connections is a theoretically motivated design choice that aligns with biological neural architectures.
- The clarity of writing and presentation is commendable, making the research accessible.
- The experimental results convincingly show that LocRNN outperforms competitive baselines, particularly in extrapolation scenarios.

Weaknesses:
- The novelty of the work is questioned, as similar findings regarding adaptive computation and generalization have been reported in prior studies.
- The claim that previous methods are not adaptive is contested, and the authors are encouraged to revise this assertion.
- The specific contributions of the LocRNN architecture compared to similar models remain unclear, necessitating further analysis.
- The paper lacks a thorough discussion of the LocRNN's differences from existing models, particularly in relation to neuroscience-informed architectures.
- There are minor formatting issues and unclear details in the experimental setup, which hinder reproducibility.
- The robustness of the experimental results is questioned, and skepticism remains regarding the clarity of the contributions of the LocRNN architecture.

### Suggestions for Improvement
We recommend that the authors improve the presentation of Section 4.3 by including figures that illustrate the LocRNN architecture and its differences from ConvRNN. Additionally, please ensure that the computational models of cortical processing are adequately described in the supplementary material, as referenced in Line 197. Clarifying the definitions of terms such as $p_t$ and $P_{t'}$, and providing detailed architecture specifications, including hyperparameters, would enhance reproducibility. We also suggest removing the claim that previous methods are not adaptive and explicitly mentioning their downsides. Conducting a systematic analysis of LocRNN's components through lesion studies could clarify its contributions compared to other architectures. Exploring the dynamics of different models at the halting timestep could provide valuable insights into the RNN's behavior and enhance the understanding of its performance. Finally, we encourage the authors to conduct more extensive evaluations across all methods, models, and tasks to substantiate their claims of superiority and to report accuracies as mean and standard deviation over multiple runs to better assess the significance of results. Addressing the biological motivation for variable computation with experimental evidence would also bolster the paper's claims.