ID: oyV9FslE3j
Title: Temperature Balancing, Layer-wise Weight Analysis, and Neural Network Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 7, 6, 9, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 2, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TempBalance, an adaptive learning rate (lr) schedule that assigns learning rates to each layer based on its heavy-tail characterization, specifically estimating PL_Alpha, the exponent of the power law distribution fitting the heavy tail of the empirical spectral density. The authors propose that layers with larger PL_Alpha, indicating they are under-trained, should receive higher learning rates, while those with smaller PL_Alpha, indicating over-training, should receive lower rates. Extensive experiments demonstrate that TempBalance improves generalization performance over various optimizers and learning rate schedulers across different architectures and datasets.

### Strengths and Weaknesses
Strengths:
1. The proposed TempBalance offers a novel perspective by adjusting learning rates based on HT-SR theory, enhancing understanding of training dynamics.
2. The empirical results show significant improvements in generalization performance across a variety of standard benchmarks and architectures.
3. The paper is well-written and clearly organized, with thorough experimental validation and detailed reporting of settings.

Weaknesses:
1. The algorithm's implementation is overly simplistic and lacks theoretical innovation, with unclear motivation for the necessity of a layer-wise learning rate schedule based on HT-SR theory.
2. The paper does not provide a convergence analysis, and the linear relationship between learning rates and PL_Alpha is deemed arbitrary.
3. Experiments are limited to classification tasks on smaller models, lacking validation on larger architectures or diverse datasets.

### Suggestions for Improvement
We recommend that the authors improve the theoretical foundation of TempBalance by providing a convergence analysis and exploring alternative designs for the relationship between learning rates and PL_Alpha. Additionally, conducting experiments on larger models, such as ResNet-101, and a more diverse set of datasets would strengthen the findings. It would also be beneficial to include a comparison with parameter-wise scaling schemes and to address the computational overhead associated with eigenvalue calculations more explicitly. Finally, we suggest that the authors clarify the distinction between their method and existing techniques addressing gradient magnitude excursions in the final version.