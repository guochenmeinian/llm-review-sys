ID: 0GO8Dtl8lJ
Title: Unleashing the Multilingual Encoder Potential: Boosting Zero-Shot Performance via Probability Calibration
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel calibration method, termed "penalty," aimed at enhancing the zero-shot performance of pretrained encoders in classification tasks. The authors propose modifying word probabilities through this calibration technique, which is evaluated across multiple datasets for both monolingual and multilingual scenarios. The experimental results indicate that the method often improves performance compared to the base model.

### Strengths and Weaknesses
Strengths:
- The introduction of the "penalty" calibration method shows potential for performance improvement.
- The experimental setup is robust, utilizing various datasets and comparison methods.
- The paper is well-written and clear, with thorough experiments that exceed expectations for a short paper.

Weaknesses:
- The novelty of the approach is limited, as the use of calibration techniques is straightforward.
- Results are not consistently competitive with existing methods in the literature, raising concerns about the claims of significant enhancements.
- The title and abstract focus predominantly on multilingual encoders, which may mislead readers regarding the study's scope.

### Suggestions for Improvement
We recommend that the authors improve the alignment between the results and the claims made regarding the effectiveness of the calibration method, particularly in addressing the discrepancies noted in the analysis. Additionally, we suggest including comparisons with other zero-shot calibration methods as baselines in the figures, particularly in Figure 2. The authors should also consider reframing the paper to adopt a more exploratory or case-study approach. Furthermore, we advise revising the title to reflect the contributions to both monolingual and multilingual contexts more accurately. Lastly, we encourage the authors to address the missing references related to calibration work in autoregressive language models.