ID: bo5oIoL95U
Title: Active Reasoning in an Open-World Environment
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 3, 5, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 2, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Conan, a benchmark designed to evaluate active reasoning in an interactive open-world environment where an agent acts as a detective, answering questions about the actions and intentions of a vandal agent. The experiments benchmark a reinforcement learning (RL) + vision-and-language baseline against a structured model utilizing Bayesian inference. The paper aims to address the limitations in current benchmarks that do not adequately test reasoning and question-answering capabilities with active information gathering.

### Strengths and Weaknesses
Strengths:
- The benchmark tackles an ambitious and interesting problem, highlighting a gap in the community's focus on reasoning and question answering with active information gathering.
- The problem and setting are well-motivated, providing an engaging detective-vandal scenario.
- The environment is distinct from existing benchmarks, promoting further research in active reasoning.

Weaknesses:
- Many important details are unclear, particularly regarding the benchmark format and the nature of traces available to models. More examples and clarity on reasoning steps are needed.
- The focus on abductive reasoning in the questions is not convincingly established, and heuristic exploration baselines are necessary to contextualize RL agent scores.
- The benchmark conflates memory and exploration challenges with abductive reasoning, potentially complicating the evaluation of reasoning abilities.
- The writing lacks clarity, making it difficult to grasp the task's significance and the design of the reward function.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing more examples of traces and reasoning steps earlier in the paper. Additionally, including heuristic exploration baselines would help contextualize the performance of RL agents. It would be beneficial to clarify the relationship between memory, exploration, and reasoning in the benchmark to ensure it accurately tests the intended reasoning abilities. Furthermore, a qualitative analysis of agent behavior and a human study could enhance understanding of the task's challenges and the models' performance. Lastly, we suggest elaborating on the design of the reward function to clarify its motivations and implications.