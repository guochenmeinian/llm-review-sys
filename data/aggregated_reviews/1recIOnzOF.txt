ID: 1recIOnzOF
Title: Decorate3D: Text-Driven High-Quality Texture Generation for Mesh Decoration in the Wild
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 6, 6, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Decorate3D, a method for text-driven texturing of 3D meshes derived from a NeRF representation. The authors propose a two-stage texturing scheme: first, decomposing the NeRF into a 3D mesh and view-dependent texture map; second, editing the mesh using a modified score-distillation objective that incorporates depth information. To address jittering artifacts in the edited texture map, a few-view resampling technique is introduced. The authors validate their approach through comparisons with existing methods and provide ablation studies to support their design choices. However, the paper also raises concerns about the fairness of comparisons, as prior works have achieved sharp textures with vanilla SDS losses, questioning the effectiveness of the proposed structure-aware SDS and few-view resample training.

### Strengths and Weaknesses
Strengths:
- The visual results achieved by Decorate3D are impressive, surpassing existing texturing methods, and are supported by quantitative evaluations across various objects.
- The task addressed by Decorate3D is novel, combining well-known techniques into a complete pipeline.
- The authors tackle the challenge of operating in a real-world setting without a pre-provided 3D mesh, which is commendable.
- The complexity of the system is well-presented, with clear intuitions aiding the understanding of Decorate3D's design.
- The authors provide effective solutions to common issues in SDS-based methods, particularly through their Neural Renderer and Few-View Resampling Training.
- Numerous ablation studies validate the different components of the method.

Weaknesses:
- Concerns arise regarding the visual results of TEXTure, suggesting potential issues with the authors' implementation, as results appear inferior compared to those in the TEXTure paper and DreamAvatar.
- The method does not allow for geometry editing, raising questions about its robustness in real-world applications, particularly with defective meshes.
- The consistency of the diffuse texture map across views post-decomposition is unclear, and additional visual results are needed to assess the contributions of the structure-aware SDS and few-view resampling training.
- The clarity of the paper could be improved, particularly in distinguishing the core contributions from the end-to-end pipeline details.
- The technical novelty is limited due to reliance on existing methods for reconstruction, and the presence of "starry" patterns and a "toy-ish palette" in results indicates potential issues with texture generation.
- The fairness of comparisons is questionable, as prior works have demonstrated similar results without the proposed enhancements.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by refining the terminology used, particularly replacing "decoration" with "retexturing" to avoid confusion. Additionally, the authors should focus more on the retexturing method in their discussions and treat the end-to-end pipeline as an implementation detail. To enhance the evaluation, we suggest including more visual results that illustrate the efficacy of the refinement method and conducting additional quantitative ablation studies to clarify the contributions of each component. We also recommend improving the clarity of their comparisons by implementing a reasonably naive baseline using depth-conditioned SDS loss to optimize a surface color MLP parameterized in UV space. Furthermore, to address the texture artifacts, the authors should explore ways to mitigate the "starry" patterns and provide more detailed prompts for surface regions. Lastly, it is essential to discuss the concurrent work "Instruct-NeRF2NeRF" to contextualize the contributions of this paper within the broader research landscape.