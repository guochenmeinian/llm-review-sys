ID: 3SK2Nn3SNv
Title: Unlearning in- vs. out-of-distribution data in LLMs under gradient-based methods
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 6, 4
Original Confidences: 3, 3, 3

Aggregated Review:
### Key Points
This paper investigates machine unlearning techniques applied to large language models (LLMs), proposing new evaluation metrics, Generalized Exposure and Relative Exposure, to assess the quality of unlearning. The authors emphasize the differences in unlearning in-distribution (InD) versus out-of-distribution (OOD) data, demonstrating that unlearning OOD data minimally impacts model performance, while unlearning InD data significantly degrades it. The study highlights the necessity of efficient unlearning methods due to privacy concerns associated with LLMs.

### Strengths and Weaknesses
Strengths:
- The introduction of Generalized Exposure and Relative Exposure metrics provides a solid framework for evaluating unlearning quality in LLMs.
- The empirical comparative study fills a literature gap, offering practical insights into the behavior of InD and OOD unlearning.
- The focus on scalable gradient ascent-based methods is relevant for real-world applications of machine unlearning.

Weaknesses:
- The discussion of ethical implications and societal impacts of unlearning in LLMs is insufficient, particularly regarding compliance with data protection laws like GDPR.
- Experiments are limited to a single dataset (WMT14), which may restrict the generalizability of findings; broader testing across various tasks is needed.
- The inability to guarantee complete unlearning with gradient ascent-based methods indicates a significant limitation in the proposed approaches.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing a more comprehensive background introduction to enhance reader understanding of the context and significance of the work. Additionally, the authors should discuss the security implications of machine unlearning in LLMs, including concrete examples of how unlearning contributes to safer outputs. The experimental section requires significant enhancement; specifically, the authors should evaluate the proposed method against sentence-level membership inference and reconstruction attacks, including comparative analyses with existing unlearning methods. Finally, to improve accessibility, we suggest offering intuitive explanations and real-world examples of the model design and the proposed metrics.