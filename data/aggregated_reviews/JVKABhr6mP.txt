ID: JVKABhr6mP
Title: Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 7, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Meteor, a Mamba-based traversal of rationales that enhances the understanding and answering capabilities of large language and vision models (LLVMs) by leveraging detailed image captions. The Mamba architecture allows for linear time complexity in data processing, improving the efficiency of generating multifaceted information from images. Meteor divides the model into two stages: rationale generation and question answering, demonstrating effectiveness against existing MLLMs across various benchmarks.

### Strengths and Weaknesses
Strengths:
1. The proposed approach effectively improves LLVM performance without introducing additional vision encoders.
2. The technical implementation is novel, and the design of the <tor> token effectively bridges two LLMs.
3. Comprehensive performance comparisons and ablation studies provide strong evidence of Meteor's effectiveness.

Weaknesses:
1. The introduction of Meteor-Mamba increases model size, contradicting the abstract, and its inference cost is higher than models like LLAVA and InternLM-XComposer2.
2. Clarity is needed regarding the initialization of the Meteor-Multimodal Language Model and the benefits of the Mamba architecture.
3. The writing lacks organization, making it difficult to understand technical details, and figures are inconsistent in presentation.
4. The benchmarks may not adequately test complex rationale reasoning capabilities.

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of the writing to facilitate understanding of the technical details. Additionally, provide a detailed description of the training process, including the data used and model weights. To address the performance concerns, we suggest conducting further studies on reasoning-oriented benchmarks such as Visual Commonsense Reasoning (VCR) to substantiate claims of enhanced reasoning capabilities. Finally, revise the figures and plots to ensure uniformity and accurately reflect training and evaluation conditions.