ID: MuPlJ9fT4b
Title: Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 6, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a self-supervised pretraining method for training neural operators to solve forward partial differential equation (PDE) problems in data-limited settings. The authors propose a method that relies solely on initial conditions, PDE coefficients, and physical parameters, avoiding the need for PDE solutions. The paper demonstrates that pretrained models outperform randomly-initialized models and those pretrained on general computer vision tasks across various PDE problems and real-world datasets. Additionally, the authors introduce an "In-Context Learning" (ICL) strategy for adapting models to out-of-distribution (OOD) tasks. They also address challenges in OOD performance, particularly noting that the Helmholtz equation exhibits complex unseen patterns that hinder learning compared to the Poisson and Navier-Stokes equations. The authors emphasize the importance of collecting continuous data in various applications, such as weather forecasting and smoke dispersion studies, where temporal dynamics complicate snapshot collection. Furthermore, they present a two-phase training approach for handling time-dependent PDEs, focusing on spatial reconstruction during pretraining and learning temporal dynamics during fine-tuning.

### Strengths and Weaknesses
Strengths:
- The paper addresses significant challenges in neural operators, particularly in reducing reliance on extensive training data and adapting models to OOD settings.
- Extensive experimentation across diverse PDE problems and real-world datasets, including ERA5 temperature, ScalarFlow, and Airfoil, is well-documented.
- The pretraining methods are straightforward and applicable to a wide range of problems, suggesting potential for community adoption.
- The authors provide a clear rationale for the necessity of continuous data collection in complex systems.
- The visualization of OOD performance and the discussion of the ICL method's scaling with demo samples are well-articulated.
- The authors provide clear responses to reviewer questions, enhancing the understanding of their methodology and commit to improving clarity in figure captions and addressing simulation costs.

Weaknesses:
- Concerns arise regarding the experimental design, with claims about reduced data simulation costs appearing overstated. The computational cost of pretraining is not adequately addressed, undermining the paper's claims.
- The paper lacks meaningful baselines for contextualizing results on PDE benchmarks and OOD experiments, limiting the evaluation of the proposed methods.
- The use of unusual unlabeled pretraining data in the ERA5 and ScalarFlow experiments raises questions about the significance of these findings.
- The difficulty of the OOD task is not adequately discussed, leaving the significance of the ICL method unclear.
- The results of the ICL experiments show significant gaps in performance, particularly for the Poisson and Helmholtz datasets.
- There is insufficient detail regarding the distribution of forcing functions and the computational costs associated with pretraining and fine-tuning.
- The lack of comparative analysis with models pretrained on vision data is necessary to substantiate claims of outperforming conventional models.
- The term "in-context learning" is deemed confusing, and its effectiveness is questioned, indicating a need for clearer terminology and focus on the pretraining strategy.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding data simulation cost by addressing the computational burden of pretraining. It would be beneficial to narrow the focus to real-world problem settings where data is scarce. Additionally, we suggest including more meaningful baselines for comparison in both PDE benchmarks and OOD experiments to provide a clearer context for the results. The authors should also clarify the nature of the unlabeled pretraining data used in their experiments and discuss the challenges associated with the OOD task to enhance the significance of their findings. Furthermore, we recommend that the authors improve the clarity of the experimental results by providing more detailed information on the distribution of forcing functions and the computational costs of pretraining and fine-tuning. We also suggest including comprehensive baseline comparisons for the FNO experiments to strengthen the evaluation of their method. Finally, we encourage the authors to reconsider the terminology used for the ICL method, potentially renaming it to "similarity-based inference with pretrained models" to enhance clarity and focus on the pretraining strategy.