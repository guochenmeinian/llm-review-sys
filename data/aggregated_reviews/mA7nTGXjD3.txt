ID: mA7nTGXjD3
Title: Provably Fast Convergence of Independent Natural Policy Gradient for Markov Potential Games
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the independent natural policy gradient algorithm for Markov Potential games, achieving a novel $O(1/\epsilon)$ iteration complexity that guarantees the average Nash gap is smaller than the accuracy $\epsilon$. This improvement over the previously known $O(1/\epsilon^2)$ complexity is derived under specific technical assumptions regarding a sub-optimality gap and access to exact advantage functions. The authors also analyze the convergence rates of the natural policy gradient algorithm in a multi-agent setting, addressing complexities introduced by parameters such as $c$ and $\delta$. They propose that $\delta^* = \lim_{k \to \infty} \delta^k$ is a critical assumption, although it cannot be directly proved. The results can be extended to a more general form of potential functions, allowing for a new lemma that maintains the $O(1/\epsilon)$ order while introducing additional terms related to distribution mismatch and action space size. Simulations in both synthetic potential and congestion games support their findings.

### Strengths and Weaknesses
Strengths:
- The convergence analysis significantly improves upon the previous $O(1/\epsilon^2)$ iteration complexity under certain assumptions.
- The analysis appears to be new, with correct proofs, particularly highlighting the importance of Lemma 3.2 and Lemma B.3 for the results in Theorem 3.3.
- The authors effectively address reviewer concerns and clarify the role of parameters in their analysis.
- The proposed extensions and new lemma enhance the theoretical framework of the paper.
- The paper is well-organized and presents a clear structure.

Weaknesses:
- The claim of an $O(1/K)$ convergence rate is asymptotic and should be explicitly stated, as it relies on the K-dependent quantity $\delta_K$.
- Assumption 3.2, which guarantees that $\delta_K$ is uniformly bounded away from zero, lacks clarity and may not be as mild as suggested.
- The assumption regarding $\delta^*$ lacks direct proof, which may raise questions about its validity.
- The introduction of $c$ complicates the stochastic analysis, which remains unresolved in the current work.
- The discussion of related works is insufficient, missing key references that could provide context and comparison for the results.
- The definition of Markov potential games differs from those in prior literature, which could affect the comparison of results.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the asymptotic nature of the $O(1/K)$ convergence rate and explicitly state this in the abstract and contributions section. Additionally, we suggest providing a more thorough discussion of Assumption 3.2 to clarify its implications and verifiability. The authors should enhance the discussion of related works by including relevant citations and comparisons, particularly regarding the results of Song et al. 2021 and Fox et al. 2022. Furthermore, we encourage the authors to refine the definition of Markov potential games to align with previous definitions in the literature for better comparability. Lastly, improving the overall writing and addressing minor clarity issues throughout the paper would enhance its presentation.