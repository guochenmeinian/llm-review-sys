ID: nFHpVRvOgH
Title: Inverse-Free Sparse Variational Gaussian Processes
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 7, 7
Original Confidences: 4, 3

Aggregated Review:
### Key Points
This paper presents a novel approach to sparse variational Gaussian processes (SVGP) by introducing an inverse-free optimization method that utilizes matrix-free natural gradient updates. This method is particularly relevant for large-scale Gaussian processes. The authors propose a new loss function that enhances the optimization process and demonstrate effective performance through thorough empirical validation across various datasets.

### Strengths and Weaknesses
Strengths:
- The newly proposed loss function significantly improves the optimization process for Gaussian processes.
- The combination of natural gradient updates and preconditioning offers a clear advantage over traditional methods.
- The paper includes solid empirical validation, showing that the proposed approach performs comparably or better than existing methods.

Weaknesses:
- A computational complexity analysis for the natural gradient update approach is lacking, particularly regarding scalability with the number of inducing points and data size.
- There are concerns about potential instability or sensitivity in the choice of step size for natural gradients, particularly in poorly conditioned scenarios.
- The choice of stopping criterion for natural gradient updates needs clarification regarding the threshold selection and its sensitivity.
- Comparisons to other inversion-based methods in terms of accuracy and computational performance are insufficient.
- The practical limitations of the proposed method in high-dimensional or non-stationary data contexts are not discussed.

### Suggestions for Improvement
We recommend that the authors improve the paper by adding a computational complexity analysis for the natural gradient update approach, detailing scalability with the number of inducing points and data size. Additionally, the authors should address potential instability or sensitivity in the choice of step size for natural gradients and clarify how the stopping criterion threshold was chosen and its sensitivity. We suggest making more explicit comparisons to other inversion-based methods regarding accuracy (e.g., RMSE) and computational performance (e.g., iteration count or time to convergence). Finally, please discuss the practical limitations of the proposed method in certain use cases, particularly in high-dimensional or non-stationary data scenarios.