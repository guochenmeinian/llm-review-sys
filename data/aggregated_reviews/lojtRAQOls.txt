ID: lojtRAQOls
Title: Using LLM for Improving Key Event Discovery: Temporal-Guided News Stream Clustering with Event Summaries
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for topical event extraction from news articles, proposing a neural pipeline that integrates large language models (LLMs) with outlier detection. The authors evaluate their approach against outdated topic models, demonstrating its effectiveness in generating coherent event clusters. The framework utilizes the NELA dataset and includes event summaries generated by LLMs to facilitate unsupervised document clustering.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and clearly presents the proposed approach, with detailed implementation information in the appendix.  
- The method is computationally efficient and produces interpretable, coherent clusters of key events.  
- The inclusion of a dataset ('KEYEVENTS') contributes to the research community in social media news analysis.  

Weaknesses:  
- The novelty of the proposed approach is limited, primarily combining existing tools and concepts rather than introducing new methods.  
- The evaluation relies on outdated baselines, lacking comparisons with more recent neural topic models.  
- The motivation and implementation details of event clustering are insufficiently elaborated, and the evaluation indicators require clearer definitions.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their approach by exploring recent state-of-the-art topic modeling models, such as TopicBert. Additionally, clarifying the motivation behind the clustering method and providing detailed explanations of the evaluation indicators would enhance the paper's clarity. We also suggest reorganizing the discussion on event clustering implementation and considering the use of other LLMs, such as Llama2, alongside GPT-3.5 to broaden the applicability of their findings.