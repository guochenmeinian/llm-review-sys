ID: V2e0A2XIPF
Title: QT-ViT: Improving Linear Attention in ViT with Quadratic Taylor Expansion
Conference: NeurIPS
Year: 2024
Number of Reviews: 24
Original Ratings: 7, 7, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents QT-ViT models, which enhance traditional linear self-attention methods by employing a second-order (quadratic) Taylor expansion to approximate softmax attention. The authors utilize a fast approximation algorithm that reduces computational complexity from \(O(n^2d)\) to \(O(nd^3)\) and further to \(O(nd^2)\). Extensive experiments demonstrate that QT-ViT models achieve state-of-the-art accuracy-speed trade-offs across various tasks, including image classification, object detection, and semantic segmentation.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel method combining quadratic expansion with a fast approximation algorithm, providing a fresh perspective on attention mechanisms.
- A clear theoretical foundation is established, detailing the use of quadratic Taylor expansion and Kronecker product to reduce computational complexity.
- The experiments validate the effectiveness of the proposed method across multiple tasks.
- The writing is clear and well-structured.

Weaknesses:
- Concerns arise regarding the diminishing top-1 accuracy of QT-ViT models compared to EfficientViT as model size increases, necessitating a detailed analysis of this trend.
- Latency metrics are not included in Table 1, which limits comprehensive model comparisons; these should be added.
- The time complexities of different kernels in Table 2 are unclear and should be explicitly stated.
- More details are needed regarding the self-multiplication terms in Eq. 11 and their representation of quadratic terms.
- The improvement in segmentation performance compared to image classification and object detection requires further analysis.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the diminishing accuracy trend in QT-ViT models as they scale up. Additionally, including latency metrics in Table 1 would enhance the comparison of models. The authors should clarify the time complexities of the kernels presented in Table 2 and provide more details on the findings related to self-multiplication terms in Eq. 11. Finally, an analysis explaining the larger improvement in segmentation performance compared to other tasks would strengthen the paper.