ID: bOQNd7tWAp
Title: Online Control for Meta-optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 7, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for meta-optimization, viewing it as a sequence of episodic optimization problems. The authors propose a control-motivated algorithm that achieves nearly-optimal performance, demonstrating sub-linear regret for both quadratic and convex smooth costs. The approach utilizes a non-stochastic control framework and introduces a new metric, meta-regret, to evaluate performance against the best static optimizer.

### Strengths and Weaknesses
Strengths:  
- The paper offers a clever transformation of the complex meta-optimization problem into a more manageable online learning framework.  
- The introduction of meta-regret as a performance metric is a valuable contribution, balancing convergence speed and solution quality.  
- The theoretical analysis is robust, providing sublinear regret bounds and a solid foundation for the proposed algorithms.  
- The paper is well-organized, with a concise presentation of technical discoveries and comprehensive background information in the appendix.  

Weaknesses:  
- The presentation lacks clarity in several areas, particularly regarding the generalization of meta-optimization to hyper-parameter tuning.  
- The practical applicability of results is insufficiently demonstrated, with simplistic examples and a lack of source code hindering understanding.  
- Some notations are poorly defined, and key equations are missing from the main text, which detracts from the overall clarity.  
- The experimental section is limited, with simple tasks that do not reflect realistic scenarios for meta-optimization.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their presentation by providing more concrete examples of practical applications for their results, beyond the simplistic example in Appendix I. Additionally, we suggest including source code to facilitate understanding of the algorithm's implementation. The authors should also ensure that all relevant equations, such as Eq. (7) in Algorithm 2, are included in the main text for better accessibility. Furthermore, we encourage the authors to explore more complex experimental scenarios, such as neural network training, to better illustrate the advantages of their approach. Lastly, we advise clarifying the definitions of notations and ensuring that all key concepts are well-explained in the main body of the paper.