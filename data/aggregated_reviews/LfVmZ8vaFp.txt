ID: LfVmZ8vaFp
Title: Off-policy Evaluation for Multiple Actions in the Presence of Unobserved Confounders
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for off-policy evaluation (OPE) in reinforcement learning, specifically addressing multi-action settings with unobserved confounders. The authors propose an auxiliary variable-aided approach that utilizes a single auxiliary variable to achieve unbiased policy value estimation, diverging from existing methods that require multiple auxiliary variables. The theoretical analysis supports the method's unbiased estimation capability, while empirical evaluations in simulated and real-world treatment recommendation scenarios demonstrate its superior performance compared to baseline methods.

### Strengths and Weaknesses
Strengths:
1. The paper pioneers the exploration of OPE in multi-action contexts with unobserved confounders.
2. The manuscript is well-organized, with a detailed introduction and thorough theoretical analysis.
3. Empirical evaluations validate the effectiveness of the proposed method.

Weaknesses:
1. The paper lacks a visual overview diagram of the proposed method.
2. The rationale behind the simulation experiments is insufficiently explained, and details regarding hyper-parameter settings and evaluation metrics need clarification.
3. Specific cases from the autism treatment recommendation experiment could enhance the demonstration of the model's effectiveness.
4. The assumption of static adjustment for confounders may overlook dynamic changes, potentially leading to biased estimates.
5. The experimental results do not include confidence intervals or error bars, which are necessary for clarity.

### Suggestions for Improvement
We recommend that the authors improve the paper by adding an overview diagram to visually present the proposed method. Additionally, the authors should provide a more detailed explanation of the rationale for their simulation experiments, including hyper-parameter settings and evaluation metrics. Including specific cases from the autism treatment recommendation experiment would further demonstrate the model's effectiveness. Furthermore, addressing the assumption of static adjustment for confounders and justifying the choice of linear approximations in the temporal difference error update would strengthen the analysis. Finally, including confidence intervals or error bars in the experimental results would enhance clarity.