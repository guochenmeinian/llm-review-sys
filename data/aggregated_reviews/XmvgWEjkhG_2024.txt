ID: XmvgWEjkhG
Title: Model Manipulation Attacks Enable More Rigorous Evaluations of LLM Capabilities
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 8, 8, 6
Original Confidences: 3, 3, 4

Aggregated Review:
### Key Points
This paper presents a comprehensive benchmark of 5 input-space and 5 model-manipulation attacks against 8 unlearning methods. The authors draw several interesting conclusions, notably that model manipulation attacks can be uniquely helpful for inferring risks from unforeseen types of input space attacks. They also find that model resilience to capability elicitation attacks lies on a low-dimensional robustness subspace. The study explores evaluating LLMs in the context of unlearning via latent space and weight space attacks, complementing traditional prompt-based approaches.

### Strengths and Weaknesses
Strengths:  
- Provides a highly useful and practical recommendation to enhance safety evaluations.  
- Grounded in existing industry approaches, offering significant experimental results.  
- The evaluations are well-executed, presenting new insights and unifying various approaches to unlearning.  
- The direction of using latent/weight space attacks is recognized as a powerful method for evaluating unlearnt information.

Weaknesses:  
- Limited novelty, as similar works have discussed unlearning via latent space attacks and undoing alignment with fine-tuning.  
- The $S_{unlearn}$ score has oddly defined bounds, making it less intuitive for users.  
- Machine unlearning techniques are currently not very challenging to undo, which diminishes the impact of the findings.

### Suggestions for Improvement
We recommend that the authors propose some novel model manipulation methods based on their findings for future exploration. Additionally, improving the intuitiveness of the $S_{unlearn}$ score would enhance user understanding of the tables and figures. It would also be beneficial to apply the benchmarks to other unlearning tasks to broaden the study's applicability.