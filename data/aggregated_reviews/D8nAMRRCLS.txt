ID: D8nAMRRCLS
Title: On Transfer of Adversarial Robustness from Pretraining to Downstream Tasks
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 3, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the transfer of robustness from pretrained representations to downstream tasks, specifically under the linear probing setting. The authors prove that the gap between standard and robust risk can be bounded by the "perturbation on representations" plus a constant. They introduce the *robustness score*, which estimates the transferability of robustness to downstream classifiers, and provide an upper bound for this score. Theoretical results are validated through experiments on various datasets.

### Strengths and Weaknesses
Strengths:
1. The paper presents theoretical results that upper bound the gap between robust and standard loss, and the experimental results align well with the theory.
2. The introduction of the robustness score is a practical contribution that aids in estimating downstream classifier robustness.

Weaknesses:
1. The main theoretical result (Theorem 1) is a relatively simple robust risk bound for linear classifiers, with similar results existing in prior works for other architectures. The theoretical contribution is deemed insufficient.
2. The paper appears to be a work in progress, lacking completeness in addressing future work mentioned in Section 7.
3. The writing requires further refinement; for instance, Figure 1 does not effectively illustrate the theory's intuition, and the related work section is difficult to comprehend.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the related work section by elaborating on the connections between their work and existing literature. Specifically, they should clarify the claims made regarding references [54, 43, 15, 36, 35, 45] and [25], and explain the relationship between adversarial training, self-supervised contrastive learning, and their work. Additionally, we suggest that the authors consider discussing the impact of regularizing the weight matrix of the linear probe on downstream robustness and renaming the robustness score to reflect its implications more accurately. Finally, we encourage the authors to enhance their empirical analysis by exploring different resolutions and hyper-parameter roles, and to address inconsistencies in their experimental results, particularly between Tables 2 and 8.