ID: u1b1dJtyxc
Title: What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 7, 5, 7, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the alignment between large language models (LLMs) and human brain activity, specifically focusing on the "brain score" approach. The authors analyze three neural datasets—Pereira, Blank, and Fedorenko—demonstrating that untrained LLMs' predictivity can largely be explained by simple features such as sentence length and position. They argue that shuffled train-test splits can lead to misleading conclusions due to potential information leakage, advocating for contiguous splits instead. The study emphasizes the need for a nuanced understanding of what LLMs map to in neural signals and introduces the concept of noise ceiling estimates, asserting that prior studies did not incorporate this concept. The authors also clarify that their findings are novel in the context of the specific datasets used, contrasting with previous studies that may have employed different methodologies.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant topic, providing valuable insights into the relationship between LLM representations and brain activity.
- It offers comprehensive analyses across multiple datasets, including control analyses and comparisons between untrained and trained models.
- The authors provide a thorough response to reviewer comments, clarifying key points about their methodology and findings.
- They acknowledge the limitations of their statistical approach and express willingness to incorporate alternative methods, such as bootstrapping, to enhance robustness.

Weaknesses:
- The paper lacks acknowledgment of prior work that has explored similar mappings, missing a relevant literature context.
- Some conclusions remain unconvincing or uninteresting, particularly regarding the novelty of findings related to train-test splits.
- The methodology diverges from previous studies, particularly in evaluation metrics and feature pooling, complicating comparisons.
- The clarity of presentation could be improved, especially regarding the construction of shuffled train-test splits and the specific voxels used in analyses.
- There is a lack of detailed analysis on the contribution of different model layers and trivial features to brain predictivity, which could enhance the paper's depth.

### Suggestions for Improvement
We recommend that the authors improve the literature review to include relevant studies that have previously investigated LLM-brain mappings. Additionally, clarifying the differences in methodology and evaluation metrics compared to prior work would enhance the paper's context. It would be beneficial to provide a more detailed explanation of how shuffled train-test splits are constructed and their implications for model validation. Furthermore, we suggest that the authors clarify which voxels are analyzed in each experiment to avoid confusion. We also recommend improving the clarity of their statistical analyses, particularly regarding the handling of multiple comparisons and the justification for their chosen methods. Moreover, we encourage the authors to provide a more detailed exploration of the contributions of different layers of the model to neural predictivity and to clarify the novelty of their findings in relation to existing literature more explicitly. Lastly, considering the exploration of narrative ecological datasets could enhance the scope of their analysis.