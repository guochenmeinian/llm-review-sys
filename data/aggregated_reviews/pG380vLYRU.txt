ID: pG380vLYRU
Title: Faster Accelerated First-order Methods for Convex Optimization with Strongly Convex Function Constraints
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 5, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents accelerated primal-dual algorithms for minimizing a convex function under strongly convex constraints, achieving an improved complexity bound of $\mathcal{O}(1/\sqrt{\epsilon})$. The authors introduce a new accelerated primal-dual algorithm with progressive strong convexity estimation (APDPro) and a restart algorithm (rAPDPro) that iteratively refines the strong convexity parameter of the associated Lagrangian. Additionally, the paper includes a comparative analysis of first-order and second-order optimization methods, focusing on the performance of the proposed first-order algorithm against MOSEK on various datasets. The authors report that MOSEK excels in medium-scale datasets, while their first-order algorithm shows advantages in large-scale instances, particularly in terms of memory efficiency and convergence rates. The convergence characteristics of their algorithm are discussed, noting both rapid and sublinear convergence behaviors depending on the problem setup.

### Strengths and Weaknesses
Strengths:
1. The authors address theoretical questions in strongly convex-constrained optimization and present novel techniques for improving computational complexity.
2. The writing conveys high-level ideas and contributions clearly.
3. The code is available in supplementary materials, enhancing reproducibility.
4. Comprehensive experimental results comparing their algorithm with MOSEK demonstrate advantages in large-scale scenarios.
5. The analysis of convergence rates and the distinction between first-order and second-order methods is well-articulated.

Weaknesses:
1. The conditions of Theorem 1 are overly strict, limiting the applicability of APDPro.
2. The convergence rate of the proposed method is sublinear, contrary to established results in optimization.
3. The paper lacks sufficient related work citations, particularly regarding existing studies on convex optimization with strongly convex constraints.
4. Some terminological ambiguities exist, particularly regarding the definition of $D_y$ and the implications of Assumption 2, which may confuse readers.
5. The writing quality requires significant improvement for clarity and precision, particularly in algorithmic descriptions, leading to potential inconsistencies.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing, particularly by addressing the numerous typographical and technical issues noted throughout the reviews. Specifically, please clarify the meaning of the 2-norm and the notation $\bot$ (line 116). Additionally, we suggest revising the conditions of Theorem 1 to broaden its applicability and addressing the convergence rate discrepancies. It would also be beneficial to include a more comprehensive literature review, citing relevant studies such as those by Nocedal and Nesterov. Furthermore, we encourage the authors to provide optimality gap vs. wall-clock running time data in their experimental section, as well as results from the MOSEK solver for a fair comparison. We also recommend improving the clarity of the definition of $D_y$ in line 87 to specify that it is a known upper bound and revising the presentation of Assumption 2 to emphasize its importance for the analysis. Lastly, we advise incorporating real-world datasets in future experiments to enhance the practical relevance of their findings.