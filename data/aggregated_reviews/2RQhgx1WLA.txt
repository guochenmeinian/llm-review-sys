ID: 2RQhgx1WLA
Title: DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 6, 6, 7, 5, 6, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 1, 4, 3, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a parameter-free adaptive first-order optimization method, DoWG (Distance over Weighted Gradients), which achieves optimal convergence rates without requiring prior knowledge of function smoothness or minimum values. The authors demonstrate that DoWG improves upon the previous DoG algorithm by incorporating weights that prioritize recent gradients, thereby enhancing practical efficiency. Additionally, a variant of the DoWG algorithm is introduced that incorporates dependence on the initial distance \(D_0\), showing that iterates remain bounded by a constant multiplied by \(D_0\). The authors provide a proof utilizing a running logarithmic factor to establish this boundedness. However, they acknowledge that knowledge of the Lipschitz parameter is necessary for stochastic cases, complicating the parameter-free nature of the algorithm. The paper includes numerical experiments, although comparisons to Adam, a momentum-based method, indicate that DoWG does not perform as well. The authors also plan to clarify that "universality" includes the strongly-convex case but does not apply to normalized GD without further extension.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and accessible, making complex concepts understandable for readers with limited expertise in optimization.
- All proofs are correct, and the theoretical contributions are solid, particularly the analysis of normalized gradient descent (NGD) and its properties.
- The proposed algorithm exhibits desirable characteristics, such as being universal and parameter-free, while not requiring a search subroutine.
- The inclusion of results on rates dependent on \(D_0\) is noted as interesting and valuable, and the proof of boundedness for iterates is well-structured and builds on existing literature.

Weaknesses:
- The contributions beyond the DoWG algorithm are largely known in the literature, and some key references, such as Levy (2017), are missing.
- The theoretical advancements appear incremental, and the performance of DoWG is not competitive with established methods like Adam, which diminishes its practical impact.
- Some concepts, such as "weak" and "strong" adaptivity, lack formal definitions, leading to ambiguity in their application.
- The necessity of the restart scheme extension is questioned, suggesting it may not be essential for the paper.
- The effective stepsize oscillation in NGD is highlighted as a limitation that prevents linear convergence.

### Suggestions for Improvement
We recommend that the authors improve the formalism of key concepts, particularly "weak" and "strong" adaptivity, to provide clearer definitions and discussions. Additionally, addressing the missing citations, especially Levy (2017), would strengthen the literature review. The authors should clarify the motivation behind the improvements from DoG to DoWG, particularly the significance of weighting recent gradients. We also suggest that the authors improve clarity regarding the necessity of the restart scheme and consider omitting it if it does not add significant value. Furthermore, we encourage the authors to explicitly address the oscillation of the effective stepsize in NGD and its implications for convergence. Finally, we suggest including a discussion on the challenges of extending the results to stochastic settings and exploring the performance of DoWG in such contexts, as well as conducting experiments with polynomial decay averaging to provide a clearer comparison against Adam, which would strengthen the paper's contributions.