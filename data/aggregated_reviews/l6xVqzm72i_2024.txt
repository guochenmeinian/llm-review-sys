ID: l6xVqzm72i
Title: MambaLLIE: Implicit Retinex-Aware Low Light Enhancement with Global-then-Local State Space
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 8, 4, 6, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 5, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MambaLLIE, a Mamba-based framework for low-light image enhancement that introduces two key technical contributions: (i) a global-then-local state space block (GLSSB) that combines a local-enhanced state space module (LESSM) and an implicit Retinex-aware selective kernel (IRSK) module to capture intricate global and local dependencies, and (ii) an implicit Retinex-aware selective kernel mechanism that guides deeper neural representations by segregating them into independent positive and negative illumination components. The authors claim that MambaLLIE significantly outperforms state-of-the-art methods across multiple benchmarks.

### Strengths and Weaknesses
Strengths:
1. The idea is novel and timely, addressing a less-explored area of low-light image enhancement using Mamba, which has shown promise in high-level vision tasks.
2. The presentation is clear, with well-designed figures that effectively illustrate the advantages of MambaLLIE over previous methods.
3. The performance metrics indicate that MambaLLIE achieves state-of-the-art results on several datasets.

Weaknesses:
1. The motivation for using Mamba over Transformers for low-light enhancement is unclear, and the insights into the proposed Mamba need more detailed analysis.
2. The scanning operation in the state space module lacks a mathematical explanation or description.
3. While MambaLLIE outperforms some methods, it has higher computational complexity and memory costs compared to Retinexformer, raising questions about efficiency.
4. Visual results suggest that Retinexformer may produce more aesthetically pleasing outputs in some cases.
5. The absence of source code and pre-trained models limits reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation for using Mamba, particularly in comparison to Transformer architectures. Additionally, the authors should provide a detailed explanation of the scanning operation in the state space module. It would be beneficial to address the computational complexity and memory costs more thoroughly, perhaps by including a comparison of training times with other methods. We also suggest including more perceptual evaluation metrics beyond PSNR and SSIM, as well as conducting comparisons with additional state-of-the-art methods like DiffLL. Lastly, the authors should submit the source code and pre-trained models to enhance reproducibility.