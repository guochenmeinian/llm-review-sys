ID: Nijnhwu1Uz
Title: PromptST: Abstract Prompt Learning for End-to-End Speech Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 6
Original Ratings: -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, 4

Aggregated Review:
### Key Points
This paper presents an extended approach to probing machine learning model information for speech-related tasks, introducing a linguistic probing benchmark named Speech-Senteval. The authors analyze how speech and text modalities are fused in end-to-end speech translation (E2E-ST) models and propose a prompt-enhanced model, PromptST, which demonstrates significant improvements in capturing linguistic knowledge and overall performance on multiple datasets, particularly CoVoST-v2.

### Strengths and Weaknesses
Strengths:
- The paper provides a thorough analysis of information processing within models, enhancing understanding of the balance between linguistic and acoustic information.
- The introduction of the Speech-Senteval benchmark facilitates future research in speech encoding methods.
- PromptST shows consistent performance improvements over strong baselines, demonstrating the effectiveness of prompt learning in addressing modality gaps.

Weaknesses:
- The literature review on methods for mitigating the speech-text modality gap is considered too brief, potentially undermining the paper's novelty.
- The benefits of focusing on the encoder while fully fine-tuning the decoder are unclear, lacking sufficient comparison with alternative strategies.
- The analysis primarily emphasizes linguistic performance, which may limit its broader applicability across different fields.
- The absence of evaluation on the MUST-C dataset reduces the generalizability of the results.
- The mechanism by which prompt representations improve linguistic learning in upper encoder layers requires further clarification.

### Suggestions for Improvement
We recommend that the authors improve the literature review to include more comprehensive discussions of existing methods for speech-text fusion and modality gap mitigation. Additionally, the authors should clarify the advantages of their approach regarding the encoder and decoder fine-tuning strategy, possibly by including comparisons with alternative methods. A more balanced analysis that considers implications beyond linguistic performance would strengthen the paper's impact. We also suggest evaluating the proposed model on the MUST-C dataset to enhance the persuasiveness of the results. Lastly, a detailed analysis of how prompt representations enhance linguistic information learning in upper encoder layers would provide valuable insights.