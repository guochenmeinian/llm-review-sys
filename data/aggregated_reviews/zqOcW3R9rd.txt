ID: zqOcW3R9rd
Title: Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Shared Adversarial Unlearning (SAU) method aimed at mitigating backdoor attacks in deep neural networks. The authors recognize that not all adversarial samples are effective for backdoor defense and propose an upper bound on backdoor risk based on benign and backdoor model predictions. They optimize the generation of backdoor samples accordingly. The method identifies shared adversarial examples and unlearns them to sever the link between poisoned samples and target labels. Experimental results indicate the method's effectiveness across various datasets and comparisons with mainstream backdoor defense techniques.

### Strengths and Weaknesses
Strengths:
- Clear research motivation and well-structured writing, supported by theoretical foundations.
- Extensive experimental evaluations confirm the method's effectiveness against multiple backdoor attack types.

Weaknesses:
- The complexity of adversarial training poses practical challenges, and existing methods like ABL offer simpler alternatives for backdoor removal. A combination of the proposed strategy with ABL's unlearning paradigm could simplify the optimization process.
- The SAU method shows significant defense failures against the WaNet attack on the TinyImageNet dataset, necessitating explanations and countermeasures for improved generalization.
- Concerns arise regarding the effectiveness of the SAU defense if attackers are aware of the adversarial training strategy.
- The method's reliance on a clean dataset raises questions about performance degradation with reduced clean sample sizes.
- Important related works on provable robustness against backdoor attacks are not addressed, limiting the paper's comprehensiveness.

### Suggestions for Improvement
We recommend that the authors improve the practical applicability of their method by exploring the integration of their adversarial sample generation strategy with ABL's unlearning paradigm. Additionally, the authors should provide a thorough explanation and countermeasures for the SAU method's performance against the WaNet attack. It is crucial to assess the defense's effectiveness under the assumption that attackers are aware of the training strategy. The authors should also clarify how the performance of the SAU method is affected by varying the scale of clean samples and include comparisons with advanced defense benchmarks like RNP. Finally, addressing the missing references on provable robustness would enhance the paper's depth.