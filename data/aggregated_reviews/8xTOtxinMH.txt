ID: 8xTOtxinMH
Title: TMT-VIS: Taxonomy-aware Multi-dataset Joint Training for Video Instance Segmentation
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 5, 4, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TMT-VIS, a multi-dataset joint training method for video instance segmentation (VIS) that integrates taxonomy embedding to enhance model performance across heterogeneous datasets. The authors propose a two-stage classification aggregation module, comprising a Taxonomy Compilation Module and a Taxonomy Injection Module, to effectively manage the varying category spaces of different datasets. Additionally, the authors introduce a spatio-temporal adapter to generate video-specific modulated taxonomic embeddings, employing a plug-and-use strategy compatible with popular VIS methods across various datasets. They follow the original M2F-VIS settings, using 2 frames per video, which they argue is optimal for building temporal correlations, despite the potential for improved performance with more frames, as evidenced by their experiments with VITA. Experiments conducted on YVIS-2019, YVIS-2021, OVIS, and UVO validate the effectiveness of this unified approach.

### Strengths and Weaknesses
Strengths:
- The proposed method is straightforward and easy to follow, demonstrating good performance under joint training.
- Unifying multiple datasets into a single model is of significant value, with strong results across three datasets and two backbones.
- The method demonstrates compatibility with existing VIS methods and achieves state-of-the-art performance when using 6 frames in VITA.
- The authors provide empirical evidence supporting their choice of using 2 frames in M2F-VIS, indicating it as an effective parameter for performance.

Weaknesses:
- The claim that "TMT-VIS is the first DETR-style framework" is misleading, as prior work like UNINEXT has also trained multiple VIS datasets with a DETR-style framework.
- Table 3 is confusing; all three datasets should be evaluated to clarify performance gains under joint training. The authors should address the performance drop observed when TMT-VIS is jointly trained on YTVIS and UVO.
- The ablation study on taxonomic embedding size is insufficient; the authors should report performance using all embeddings and clarify potential negative effects when top-k embeddings miss ground truth classes.
- More details are needed regarding the taxonomy-aware matching loss.
- The current design of M2F-VIS may not effectively capture temporal relationships, raising concerns about its end-to-end nature and resource demands.
- The related work section contains misleading content, which could undermine the paper's credibility.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Table 3 by splitting columns by dataset and reporting numbers for all datasets. Additionally, the authors should provide specific formulas for the taxonomy-aware matching loss, mask loss, and classification loss. It would be beneficial to clarify whether the model requires a known vocabulary during inference and if it can generalize to open-world scenarios. Furthermore, the authors should enhance the discussion on the unique applicability of their methods to video instance segmentation, particularly in light of the performance observed in image settings. We also recommend that the authors improve the related work section to ensure accuracy and credibility, as it is crucial for expert readers. Additionally, we suggest that the authors conduct ablation studies using VIS methods other than M2F-VIS to provide a broader context for their findings. Finally, we encourage the authors to delve deeper into the implications of using different input frame counts for capturing video-oriented knowledge in their revised paper.