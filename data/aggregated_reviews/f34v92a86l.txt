ID: f34v92a86l
Title: Efficient Grammatical Error Correction Via Multi-Task Training and Optimized Training Schedule
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper tackles grammatical error correction (GEC) by proposing two main techniques: the use of multiple auxiliary tasks (Correct, Explain, Apply, Edit) modeled as sequence-to-sequence tasks, and the optimization of dataset ordering during fine-tuning. The authors report a 3% improvement in F0.5 over similar-sized models and outperform larger models (up to 11B parameters). The paper also explores the effects of training order, providing an empirically recommended sequence (Lang8 → NUCLE → FCE → W&I+L) and presents thorough experiments, including ablation studies.

### Strengths and Weaknesses
Strengths:
- The introduction of a novel multi-task strategy with auxiliary tasks that can enhance GEC performance.
- Systematic optimization of dataset ordering, contributing valuable insights to the field.
- Strong results achieved with a significantly smaller model and a simple, reproducible method.

Weaknesses:
- Lack of in-depth error analysis, making it difficult to appreciate the gains from auxiliary tasks.
- Absence of a definitive methodology for determining training order, limiting practical applicability.
- Concerns regarding the novelty of dataset ordering and the effectiveness of multi-task training, as indicated by marginal performance improvements.

### Suggestions for Improvement
We recommend that the authors improve the related work section by contextualizing their contributions against existing literature. Additionally, we suggest conducting a thorough error analysis to elucidate the performance of different auxiliary task combinations. It would also be beneficial to provide a concrete methodology for classifying datasets as noisy or clean to enhance the practical implications of their training order recommendations. Furthermore, clarifying the dataset orderings used in their experiments and ensuring accurate comparisons in Table 2 would strengthen the paper's credibility.