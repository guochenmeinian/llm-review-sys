ID: 3Wrolscjbx
Title: Learning via Wasserstein-Based High Probability Generalisation Bounds
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 8, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of PAC-Bayes generalization bounds based on the Wasserstein distance, introducing novel high-probability bounds applicable to both batch and online learning. The authors propose a new SRM training strategy leveraging their theoretical findings and provide empirical evidence of its effectiveness. The bounds accommodate weaker assumptions than previous PAC-Bayes bounds, allowing for discrete Dirac distributions for both prior and posterior distributions.

### Strengths and Weaknesses
Strengths:
- The Wasserstein-distance PAC-Bayes bounds require fewer or weaker assumptions compared to previous results.
- The proofs are presented clearly, and the results are validated empirically.
- The bounds are applicable to heavy-tailed loss functions and allow for data-dependent priors.

Weaknesses:
- The paper lacks empirical comparisons of the Wasserstein regularization with similar regularization styles, such as weight decay and 'distance to initialization'.
- There are concerns regarding the justification of using the $\ell_2$ norm of weights in the training objective, given that the loss is only Lipschitz with respect to outputs.
- The experimental results do not demonstrate significant improvements over previous methods, and some writing lacks clarity.

### Suggestions for Improvement
We recommend that the authors improve the empirical comparison of the Wasserstein regularization with other regularization techniques like weight decay and 'distance to initialization'. Additionally, we suggest clarifying the justification for using the $\ell_2$ norm in the training objective, as it may not be appropriate given the Lipschitz condition. Furthermore, enhancing the clarity of certain sentences and providing a more robust discussion on the limitations of the proposed approach would strengthen the paper. Lastly, we encourage the authors to explore the conceptual novelty of their online learning algorithm compared to existing methods.