ID: sn3UrYRItk
Title: The Impact of Initialization on LoRA Finetuning Dynamics
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 5, 8, 6, 5, -1, -1
Original Confidences: 3, 3, 2, 3, -1, -1

Aggregated Review:
### Key Points
This paper investigates the impact of initialization schemes on the finetuning dynamics of Low Rank Adaptation (LoRA) in large language models. The authors propose two initialization approaches: Init[A], where A is initialized randomly and B to zero, and Init[B], where B is initialized randomly and A to zero. The findings indicate that Init[A] allows for larger learning rates and more efficient feature learning compared to Init[B], albeit with some internal instability. The theoretical framework is based on large width limits, supported by empirical experiments on both toy models and real-world language tasks.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important and often overlooked aspect of LoRA, specifically the impact of initialization schemes.
- The theoretical analysis is rigorous, providing principled insights into LoRA finetuning dynamics.
- Empirical results substantiate the theoretical findings, demonstrating practical relevance.
- The paper presents a clear trade-off between feature learning efficiency and internal stability.

Weaknesses:
- The novelty is limited, as much of the theoretical analysis has been previously presented in [1], and the authors should clarify how their work differs.
- The experiments primarily focus on language models, and including other domains could enhance the paper's impact.
- There is insufficient discussion on the choice of hyperparameters, particularly regarding epochs and weight decay.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how their analysis differs from the results in [1], especially since many formulas and notations are identical. Additionally, exploring other initialization schemes beyond Init[A] and Init[B], such as Gaussian initialization, Kaiming initialization, and principal components, would strengthen the experimental section. Furthermore, providing a more intuitive explanation of why Init[A] allows for larger learning rates would make the insights more accessible. Finally, a clearer explanation of the toy model setup and justifications for fixed hyperparameters would enhance the overall clarity and robustness of the paper.