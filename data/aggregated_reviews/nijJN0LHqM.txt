ID: nijJN0LHqM
Title: Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 7, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the convergence properties of Sharpness-Aware Minimization (SAM) using a constant perturbation size $\rho$ and gradient normalization, which has not been previously explored. The authors demonstrate that for strongly convex and smooth functions in the deterministic setting, SAM converges to the global minimum, while for general convex and smooth functions, it only converges to a stationary point. Importantly, the authors find that in various other settings, including smooth non-convex and stochastic cases, SAM fails to converge to global or local minima. This work contrasts with prior analyses that considered decaying perturbation sizes, highlighting that a constant $\rho$ can inhibit convergence.

### Strengths and Weaknesses
Strengths:
1. The paper addresses the practical use of constant perturbation size and gradient normalization in SAM, which aligns with real-world applications.
2. It provides a comprehensive theoretical analysis across common function classes in both deterministic and stochastic settings.
3. The authors present matching lower bounds to support their upper bounds, with some novel techniques introduced.
4. The writing is generally clear and accessible.

Weaknesses:
1. The claim of non-convergence for SAM with constant $\rho$ is not surprising and requires further justification, particularly regarding the asymptotic convergence results.
2. There are issues in Theorem 3.2 related to the condition $\frac{\beta}{\mu} \geq 2$, which may lead to vacuous lower bounds due to the use of one-dimensional quadratics.
3. The convergence bound in Theorem 3.4 appears loose, and the necessity of the Lipschitz assumption is questionable.
4. The paper lacks empirical results demonstrating SAM's impact on generalization.
5. The presentation could be improved for readability, and the absence of experiments limits the practical implications of the theoretical findings.

### Suggestions for Improvement
We recommend that the authors improve the justification for the non-convergence of SAM with constant $\rho$, particularly in relation to the asymptotic convergence results in Theorems 3.1 and 3.3. Additionally, we suggest addressing the issues in Theorem 3.2 by reconsidering the condition $\frac{\beta}{\mu} \geq 2$ and clarifying the implications of using one-dimensional quadratics. Furthermore, we encourage the authors to tighten the convergence bound in Theorem 3.4 and reconsider the necessity of the Lipschitz assumption. Lastly, we advise including empirical results to illustrate how SAM improves generalization and discussing the practical implications of the findings, particularly regarding the bias term in the convergence results.