ID: lA48H7pW3q
Title: QUEST: Quadruple Multimodal Contrastive Learning with Constraints and Self-Penalization
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 8, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Quadruple Multimodal Contrastive Learning Framework (QUEST) aimed at capturing shared and unique task-relevant information through contrastive learning. The authors propose a quadruple embedding space that optimizes shared and unique information simultaneously, employing a self-penalization mechanism to guide this optimization. Evaluations on datasets like Flickr30k and COCO demonstrate significant performance improvements over state-of-the-art methods.

### Strengths and Weaknesses
Strengths:
1. The method achieves a significant performance improvement (average of 97.95% on the CLIP model) across multiple datasets and models.
2. The decomposition of features into task-relevant unique and shared features using quaternion vector spaces is novel and effective.
3. The motivation is clear and logical, making the paper easy to understand.
4. High performance is demonstrated on both popular public datasets and the shortcuts dataset.

Weaknesses:
1. Inconsistent parameters in the shared Decoder of Equation 1 raise questions about clarity.
2. Lack of explanation for terms like QUEST-SIC and QUEST-UIC in the ablation study section.
3. Ambiguity surrounding the meaning of $Z_j^n+$ in Equation 5.
4. Absence of theoretical analysis regarding the disentanglement of shared and unique representations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the shared Decoder in Equation 1 by ensuring consistent parameters. Additionally, please provide explanations for QUEST-SIC and QUEST-UIC in the ablation study section. Clarifying the meaning of $Z_j^n+$ in Equation 5 is also necessary. Furthermore, we suggest including a theoretical justification for the disentanglement of representations and discussing recent multimodal shortcut learning literature to enhance the paper's depth.