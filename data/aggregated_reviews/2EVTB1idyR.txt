ID: 2EVTB1idyR
Title: Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 7, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 2, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a refined analysis of Thompson Sampling (TS) in Bayesian regret reinforcement learning, deriving regret bounds for tabular, linear, and finite mixture MDPs. The authors utilize an information-theoretical approach, analyzing the information ratio to represent the exploration-exploitation trade-off, and bounding it by episode length. The paper concludes with a discussion of related work and the optimality of the obtained regret bounds.

### Strengths and Weaknesses
Strengths:  
- The paper provides state-of-the-art Bayesian regret bounds for TS through a refined analysis of surrogate environments and the information ratio across various settings, including tabular, linear, and finite mixtures. 
- The writing is clear, and the analysis leverages the novel notion of Kolmogorov dimension, contributing significantly to the theoretical understanding of TS in RL.

Weaknesses:  
- The proposed analysis and bounds may have limited applicability to real-world RL problems beyond the considered settings. 
- The paper lacks experimental results or empirical validation of the derived bounds, and some assumptions and prior works are discussed without detailed explanations or comparisons. 
- The bounds presented do not improve upon existing state-of-the-art results in certain scenarios, and the significance of the new notion $\lambda$ remains unclear.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation of the derived bounds by including experimental results or comparisons with existing algorithms. Additionally, providing more detailed explanations of assumptions and prior works would enhance clarity. We suggest clarifying the relationship between the Kolmogorov dimension and the proposed analysis, and discussing the implications of using TS-based policies over OFU ones. Furthermore, addressing the potential overclaim regarding the definition of Bayesian RL in time-inhomogeneous settings would strengthen the paper. Lastly, we encourage the authors to include a comprehensive table summarizing the results for better comparison.