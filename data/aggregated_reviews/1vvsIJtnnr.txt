ID: 1vvsIJtnnr
Title: Boosting with Tempered Exponential Measures
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 6, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 2, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a generalization of the popular ADABOOST algorithm, termed t-ADABOOST, which incorporates a tempering parameter \( t \) to modify the weight optimization process. The authors derive their method by substituting the standard relative entropy with tempered relative entropy and solving a constrained optimization problem. The approach retains the original ADABOOST updates when \( t=1 \) and maintains exponential convergence for values of \( t \) between 0 and 1. Additionally, the paper provides a thorough analysis of the generalized ADABOOST procedure and its implications for overfitting risks. The authors propose clarifications in the algorithm's presentation, particularly regarding the notation of parameters such as \( \mu_j \) and \( Z_t \). Experimental results indicate that tuning \( t \) can yield significant performance improvements over standard ADABOOST and demonstrate the impact of parameter choices on overfitting risks.

### Strengths and Weaknesses
Strengths:
- The paper demonstrates strong theoretical and experimental results, showing compelling improvements over baseline methods.
- The generalization of ADABOOST has potential applications for machine learning practitioners and can be implemented in existing libraries.
- The writing is clear and well-organized, with a comprehensive theoretical analysis accompanying the proposed algorithm.
- The authors satisfactorily addressed all reviewer comments, enhancing the clarity of the paper.
- The analysis of overfitting risks is well-supported by experimental results.

Weaknesses:
- The performance of t-ADABOOST is highly sensitive to the choice of \( t \), and the current method for selecting \( t \) requires expensive hyperparameter sweeps, which may hinder practical adoption.
- There are no ablation experiments to evaluate the impact of the new losses independently from the exponential update, leaving questions about their effectiveness and selection criteria for practitioners.
- Some notational choices, such as the use of \( Z_t \) as a scalar, may lead to confusion.
- The initial definition of \( \mu_j \) is delayed, which could confuse readers when first encountering the term.
- Notational choices and technical presentation could be clearer, particularly regarding key terms and equations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the notation used throughout the paper, particularly in equations and definitions, to enhance reader comprehension. Specifically, providing more intuitive explanations for terms like \( \log_t \) and \( \exp_t \) would be beneficial. Additionally, we suggest conducting ablation experiments to isolate the effects of the new losses from the exponential update, which would help clarify their utility. Furthermore, we encourage the authors to explore mechanisms for selecting \( t \) that do not rely solely on hyperparameter sweeps, as this would enhance the practical applicability of their approach. We also recommend improving the presentation of the algorithm by mentioning in the caption that the values involved are chosen from theorem 2 to aid readability. Consider using a different letter than \( L \) in line 219 to avoid confusion with the italicized notation for a function. It would also be beneficial to clarify the notation for \( q \) by adopting the \( q^{(1)} \) format for iterations. Lastly, we suggest explicitly stating the dependence of \( Z_t \) on the leveraging coefficients to enhance precision.