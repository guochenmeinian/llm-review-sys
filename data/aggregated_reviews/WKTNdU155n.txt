ID: WKTNdU155n
Title: LLaMo: Large Language Model-based Molecular Graph Assistant
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LLaMo, a Large Language Model-based Molecular graph assistant that integrates a GNN encoder, a multi-level graph projector, and a language model to enhance molecular graphs' understanding and generation capabilities. The authors propose a two-stage training process that aligns text and molecular modalities, achieving promising performance across various downstream tasks. Extensive empirical studies validate the effectiveness of the proposed method.

### Strengths and Weaknesses
Strengths:
- The framework effectively captures multi-scale information through a multi-level graph projector, mitigating the over-smoothing problem.
- The paper is well-structured, with clear writing, comprehensive implementation details, and extensive experiments that provide insights into the framework's components.
- The design of learnable query tokens is innovative, and the paper includes practical examples demonstrating the framework's functionality.

Weaknesses:
- The evaluation of the quality and validity of GPT-4 generated instruction data is insufficient, raising concerns about the reliability of performance improvements.
- The implementation details of functional groups are lacking, and the treatment of the graph modality during instruction tuning is unclear.
- The paper does not sufficiently compare different GNN and LLM backbones, limiting the understanding of the model's versatility and potential shortcomings.

### Suggestions for Improvement
We recommend that the authors improve the evaluation methodology for the GPT-4 generated instruction data to rigorously assess its accuracy and relevance. Additionally, please provide more details on the implementation of functional groups and clarify how the graph modality is treated during instruction tuning. We also suggest including comparisons with various GNN and LLM backbones to better understand their impact on model performance.