ID: FVgCwcwpJw
Title: Policy Improvement using Language Feedback Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 4, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for policy improvement using Language Feedback Models (LFM) in decision-making tasks. The authors propose a two-stage approach: first, training an LFM based on feedback generated from an initial policy's rollouts, and second, using this LFM to filter desirable actions for policy training. The method is evaluated across three benchmarks: Touchdown, ScienceWorld, and ALFWorld, demonstrating improvements over existing methods like Behavioral Cloning (BC) and ActPred.

### Strengths and Weaknesses
Strengths:
- The idea of using an LFM to provide text feedback on actions sampled from the initial policy is innovative.
- The experiments cover a diverse range of tasks, showcasing the method's general applicability.
- The LFM effectively distills feedback from a larger model (GPT-4) into a smaller model (Flan-T5 770M), enhancing computational efficiency.

Weaknesses:
- The lack of comprehensive baselines limits the assessment of the method's efficiency and generalizability.
- The paper does not clarify the importance of the initial policy training phase using BC, raising questions about its necessity.
- There is insufficient exploration of how the LFM's outputs can be utilized for human interpretability, and comparisons to relevant literature are missing.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how desirable behaviors are collected and the implications for generalizability. Additionally, providing a more comprehensive set of baselines, including comparisons with reinforcement learning methods, would strengthen the evaluation. An ablation study contrasting the LFM with expert LLMs should also be included to better demonstrate the method's effectiveness. Finally, addressing the questions regarding the dataset's role in training and the efficiency of the LFM's feedback generation would enhance the paper's depth.