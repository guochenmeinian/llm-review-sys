ID: FOfU3qhcIG
Title: TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TuneTables, a method designed to enhance the performance of prior-data fitted networks (PFNs) on large datasets with varying features and classes. TuneTables employs soft prompt tuning to learn a compact set of parameters, utilizing feature subselection methods and a new decoder to address the limitations of existing PFNs like TabPFN. The authors demonstrate that TuneTables outperforms TabPFN on larger datasets and can optimize fairness objectives, while also providing a comprehensive analysis of its capabilities.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and presents a clear motivation for addressing PFN limitations.
- Extensive experiments show that TuneTables outperforms strong baselines, including CatBoost and XGBoost.
- The methodology is sound, and the paper includes a rich set of results and analyses.

Weaknesses:
- Some proposed methods for handling increased features, training examples, and classes are disconnected and rely on classical techniques, diminishing their novelty.
- The presentation, particularly in Section 4, is dense and difficult to navigate, with many references to appendix tables that disrupt flow.
- Important details, such as hyperparameter spaces and comparisons with other methods like Excelformer, are missing, which raises concerns about the robustness of the results.

### Suggestions for Improvement
We recommend that the authors improve the presentation of Section 4 by reducing its density and integrating more tables and figures directly into the main text for clarity. Additionally, we suggest providing a clearer distinction between the dataset names and method names to avoid confusion. The authors should also include comprehensive hyperparameter details for both TuneTables and the baselines to enhance the evaluation's transparency. Finally, we encourage the authors to conduct comparisons with Excelformer and other relevant methods on large-scale datasets to strengthen their claims.