ID: Wq6aY6fC2H
Title: The Prevalence of Neural Collapse in Neural Multivariate  Regression
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel extension of neural collapse to neural regression collapse (NRC), demonstrating that similar phenomena exist in multivariate regression tasks. NRC is characterized by three phenomena: last-layer feature vectors collapsing to the subspace spanned by principal components, feature vectors collapsing to the subspace spanned by weight vectors, and the Gram matrix for weight vectors converging based on the targets' covariance matrix. The authors validate NRC through experiments on multiple datasets and discuss the role of regularizers, providing comprehensive proofs that demonstrate NRC's performance under the Unconstrained Feature Model (UFM) setting.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written, structured, and presents a significant extension of neural collapse to regression tasks, contributing to the understanding of deep learning behaviors.  
- The theoretical results and experimental validations are solid and well-organized, with detailed explanations of NRC following the framework of neural collapse.

Weaknesses:  
- The distinction between neural multivariate regression and traditional neural collapse is not clearly articulated, raising questions about the significance of the results.  
- The experiments lack comprehensive representation across datasets, and there are concerns regarding the absence of error bars in figures, which diminishes the robustness of the findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how NRC differs from traditional neural collapse, particularly in the context of multivariate regression. Additionally, it would be beneficial to include error bars in the experimental plots to reflect variability across multiple seed trials. We suggest that the authors provide further discussion on the implications of neural collapse for generalization and robustness in regression tasks. Furthermore, including additional datasets in the experiments and addressing the questions raised regarding the significance of the regularization parameters would enhance the paper's impact.