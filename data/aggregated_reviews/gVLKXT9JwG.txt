ID: gVLKXT9JwG
Title: Global Convergence Analysis of Local SGD for Two-layer Neural Network without Overparameterization
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 7, 3, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the global convergence of local SGD for one-hidden-layer Convolutional Neural Networks (CNNs) without overparameterization or noise injection. The authors provide a solid theoretical analysis, demonstrating that local SGD can achieve global convergence through novel proof techniques. They categorize the training dynamics into two phases: self-correction and convergence, contributing to a better understanding of optimization dynamics in non-convex settings.

### Strengths and Weaknesses
Strengths:  
- The paper is clear and well-written, presenting contributions in a structured manner.  
- It offers a significant theoretical analysis of optimization dynamics without relying on overparameterization or noise.  
- The division of the objective function landscape into regions is a novel approach not commonly found in existing literature.  
- Simulation results align well with theoretical predictions.

Weaknesses:  
- The significance of the findings is somewhat limited due to the focus on Gaussian input data.  
- Assumption 1 regarding the target function raises concerns about its realism and applicability.  
- The writing is overly technical at times, which may hinder understanding.  
- The online learning assumption may not reflect practical settings, as it assumes i.i.d. sampling from the same distribution.  
- The choice of CNN architecture over simpler models adds unnecessary complexity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by providing more intuitive interpretations of key terms, lemmas, and equations. Specifically, clarifying the significance of the intermediate step $\check{v}_{t+1}$ and the implications of Theorem 1 on the self-correction of the second layer would enhance understanding. Additionally, we suggest that the authors address the concerns regarding Assumption 1 by providing justification for its applicability and exploring its implications with different values of $k$. It would also be beneficial to simplify the writing and consider including visual aids to illustrate the dynamics of each layer during global convergence. Lastly, we encourage the authors to reconsider the choice of CNN in favor of a more straightforward model to avoid confusion.