ID: Lh9CKbFV2I
Title: Batched Low-Rank Adaptation of Foundation Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 2, 4

Aggregated Review:
### Key Points
This paper presents fLoRA, a modification of LoRA that utilizes element-wise multiplication to adjust the hidden states of fine-tuned models, aiming to enhance throughput in batched inference setups. The authors demonstrate that fLoRA is theoretically and experimentally more computationally efficient than LoRA, particularly for ranks of 8 or 16, and performs comparably to LoRA while significantly outperforming (IA)^3 on the MultiPL-E benchmark. 

### Strengths and Weaknesses
Strengths:
1. The paper effectively addresses a practical problem with significant real-world implications.
2. It provides solid experimental results, showcasing fLoRA's performance against relevant baselines.
3. The methodology is conceptually simple and visually appealing, with clear mathematical notation.

Weaknesses:
1. Evaluation is limited to code generation tasks, which may restrict the generalizability of the findings.
2. The paper lacks extensive performance evaluations across a broader range of benchmarks and tasks.
3. There is insufficient detail regarding the gradient computation of fLoRA point-wise adapters and its impact on memory and throughput during training.

### Suggestions for Improvement
We recommend that the authors improve the manuscript by conducting additional experiments on fLoRA's performance with varying ranks and including a wider array of benchmarks. An ablation study assessing the impact of language modeling rank on downstream task performance would enhance the analysis. Furthermore, we suggest adding a row for "Average" in Table 1 and specifying the ranks of LoRA and fLoRA in the table caption for clarity. The method description could be strengthened by specifying tensor shapes in Equations 5 and 6 and including a minimal pseudo-PyTorch implementation of fLoRA to improve readability. Lastly, providing details on the gradient computation of fLoRA adapters would clarify its effects on training throughput and memory footprint.