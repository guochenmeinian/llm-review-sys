ID: VUuOsBrqaw
Title: FUG: Feature-Universal Graph Contrastive Pre-training for Graphs with Diverse Node Features
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 7, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a feature-universal graph contrastive pre-training strategy (FUG) aimed at enhancing model rebuilding and data reshaping by addressing the inherent complexities in graphs. The authors introduce a theoretical analysis that demonstrates a reduction in time complexity from O(n^2) to O(n) through a global uniformity constraint. Additionally, the authors explore the relationship between contrastive learning and Principal Component Analysis (PCA), proposing a model that combines the strengths of both frameworks to improve generalization across diverse graph datasets.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a theoretical analysis of the proposed method.
2. The authors propose a novel feature-universal graph contrastive pre-training strategy.
3. The method is clearly described and well-motivated, with extensive experimental validation.

Weaknesses:
1. The discussion on the shortcomings of language models is outdated, as recent LLM4graph papers address feature diversity effectively.
2. The performance improvements shown in Table 3 are marginal, and the compared methods are not the latest, raising concerns about the effectiveness of the proposed method.
3. The results in Table 4 indicate that while DE significantly impacts model performance, the contributions of the other two modules remain unproven.
4. The theoretical analysis lacks rigor, particularly in Theorem 3.1, where the definitions of Dim. and Fea. are not well-defined.
5. The proposed model lacks a projector between representations and the loss, a common component in GCLs.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis to clarify the definitions of Dim. and Fea. and provide a more formal decomposition of PCA. Additionally, the authors should address the outdated references to language models and include a discussion of recent advancements in LLM4graph. To enhance the credibility of the performance claims, we suggest that the authors compare their method against more recent approaches in graph pre-training. Furthermore, we encourage the authors to provide a sensitivity analysis for the hyperparameters in Equation 11 and clarify the implications of reducing z in Equation 9 on feature diversity. Lastly, we recommend that the authors consider incorporating a projector between representations and the loss to align with standard practices in GCLs.