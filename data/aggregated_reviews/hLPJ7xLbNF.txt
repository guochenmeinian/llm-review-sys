ID: hLPJ7xLbNF
Title: Self-Supervised Motion Magnification by Backpropagating Through Optical Flow
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 6, 5, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a self-supervised approach to Lagrangian motion magnification utilizing a pre-trained optical flow network. The method employs magnification loss to align the optical flow of the magnified frame with that of a reference frame, while color loss ensures consistency between frames. Test-time adaptation is incorporated to enhance magnification quality on out-of-domain data. The authors raise questions about the appropriate terminology used, particularly regarding "self-supervised" versus "weakly-supervised" learning, proposing to label results from ARFlow as fully self-supervised and RAFT results as weakly supervised. The experiments demonstrate competitive performance against prior methods in terms of SSIM and other evaluation metrics, although the evaluation metrics are discussed with suggestions for improvement.

### Strengths and Weaknesses
Strengths:
1. The algorithm is simple and effectively leverages an off-the-shelf optical flow network for training on large-scale unlabeled videos.
2. The method allows for targeted magnification and test-time adaptation, potentially improving user experience.
3. The paper includes a comprehensive literature review and sufficient implementation details, facilitating reproducibility.
4. The evaluation is thorough, providing both quantitative and qualitative results.
5. The proposed method is recognized as innovative and potentially superior to existing techniques.
6. The authors are open to feedback and willing to clarify terminology and evaluation metrics, showing a commitment to enhancing the manuscript.

Weaknesses:
1. The term "self-supervised" may be misapplied, as the optical flow network used is trained via supervised learning; a self-supervised optical flow network should be employed for main experiments.
2. The evaluation metrics are limited, particularly regarding the underperformance in SSIM, which raises concerns about the quantitative results.
3. The reliance on the optical flow network, which struggles with subtle motion estimation, should be addressed through an ablation study.
4. The targeting application of the method is unclear, and the authors should clarify its importance and potential use cases.
5. The paper does not adequately address how occlusions and disocclusions are handled, which could impact performance.
6. The use of terminology regarding supervision types is unclear and requires careful discussion.
7. The absence of specific results from ARFlow in the tables raises concerns about transparency.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the term "self-supervised" by using a self-supervised optical flow network in their main experiments. Additionally, the authors should expand the evaluation metrics to provide a more comprehensive assessment of performance, particularly in relation to SSIM. It would be beneficial to include an ablation study investigating the limitations of the optical flow network in estimating subtle motions. We suggest that the authors clarify the practical applications of their method and provide examples beyond the current illustrative cases. Furthermore, we recommend that the authors include a careful discussion of self-supervision and weak supervision in the context of their method. We also suggest removing the phrase comparing motion error and SSIM to avoid confusion and encourage the inclusion of fully trained ARFlow results under the label of fully self-supervised. Lastly, the authors should explicitly address the handling of occlusions and disocclusions in their method to enhance understanding of its limitations and conduct experiments with different qualities of optical flow to strengthen their findings.