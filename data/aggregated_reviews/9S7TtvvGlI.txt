ID: 9S7TtvvGlI
Title: Beyond tip of the Iceberg: Debiased Self-training for Long-tailed Semi-supervised Node Classification
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents IceBerg, a framework designed to tackle class-imbalanced and few-shot learning challenges in Graph Neural Networks (GNNs). By employing a debiased self-training approach, IceBerg enhances GNN performance in semi-supervised settings. The framework incorporates techniques such as Double Balancing and a propagate-and-transform message passing mechanism, demonstrating improved classification performance across various Class Imbalanced Graph Learning (CIGL) baselines.

### Strengths and Weaknesses
Strengths:
- The writing is clear, and the motivation is well-articulated, particularly illustrated through figures.
- The proposed method shows strong performance and is supported by extensive empirical evidence.
- The organization of the paper is commendable, with comprehensive experiments validating the framework's effectiveness.

Weaknesses:
- The claim of a highly imbalanced labeled set is overstated, as k-fold cross-validation and random sampling could mitigate this issue.
- The authors need to cite existing literature on class-imbalanced semi-supervised learning and clarify how their work differs.
- Experiments primarily focus on homophilic datasets, necessitating validation in heterophilic settings.
- The methodology lacks clarity in certain areas, such as the computation of Eq.(8) and the notation used in Eq.(9).
- The datasets employed are small, raising concerns about the generalizability of the results.
- Presentation issues exist, including typographical errors and the need for clearer pseudocode.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology, particularly regarding the computation of Eq.(8) and the notation in Eq.(9). Additionally, the authors should include hyperparameter experiments to strengthen their findings. It would be beneficial to expand the dataset size or demonstrate performance on larger graphs to address generalizability concerns. We also suggest enhancing the presentation quality by correcting typographical errors and providing pseudocode for the entire framework to facilitate understanding. Finally, the authors should clarify the distinctions between their work and existing literature on class-imbalanced semi-supervised learning.