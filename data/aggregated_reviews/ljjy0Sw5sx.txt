ID: ljjy0Sw5sx
Title: Descriptive Prompt Paraphrasing for Target-Oriented Multimodal Sentiment Classification
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents UnifiedTMSC, a model designed for Target-Oriented Multimodal Sentiment Classification (TMSC) that integrates descriptive prompt paraphrasing and image prefix tuning. The authors propose that sentiment polarity is context-dependent rather than type-dependent, and experimental results demonstrate that UnifiedTMSC outperforms existing methods across four datasets.

### Strengths and Weaknesses
Strengths:
- The proposed UnifiedTMSC model effectively combines prompt-based language modeling and descriptive prompt paraphrasing, achieving high performance on both entity-level and aspect-level tasks.
- The rigorous experiments conducted on four diverse datasets lend credibility to the approach and suggest potential for future research in TMSC.
- The paper is well-written and easy to follow, with a smooth presentation.

Weaknesses:
- The novelty of the proposed model is limited, primarily relying on rule-based prompt data enhancement, which resembles existing methodologies.
- The experiments do not include several recent large vision-language models (VLMs) that could provide valuable comparisons.
- Important experimental details are lacking, such as the pre-training parameters of the Transformer used for cross-attention and the number of paraphrased prompts utilized during training.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by incorporating comparisons with recent methodologies, such as Flamingo/OpenFlamingo and InstructBLIP. Additionally, the authors should clarify the utility of the task description and seed prompt in the design of paraphrased prompts, as these steps currently appear redundant. It would be beneficial to provide more experimental details, including whether the Transformer loads pre-training parameters and the number of paraphrased prompts used. Furthermore, we suggest considering the utilization of LLMs for generating paraphrased prompts and conducting comprehensive tests to assess the robustness of UnifiedTMSC. Lastly, enhancing clarity in the writing, particularly in the description of model implementation and variable definitions, would improve the paper's overall readability.