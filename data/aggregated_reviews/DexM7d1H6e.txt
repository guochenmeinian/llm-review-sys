ID: DexM7d1H6e
Title: Animal-Bench: Benchmarking Multimodal Video Models for Animal-centric Video Understanding
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Animal-Bench, a video question answering benchmark centered on animals, addressing a gap in existing video benchmarks that primarily focus on human activities. Animal-Bench is derived from six datasets and encompasses 13 tasks across 8 animal categories, evaluating eight video-language models. The authors assess model robustness by simulating real-world conditions, such as weather and shooting parameter changes, through video editing techniques.

### Strengths and Weaknesses
Strengths:
1. Animal-Bench is the first animal-centric benchmark, potentially enhancing AI applications in animal studies and advancing video-language model development.
2. The benchmark aligns with real-world applications by incorporating domain-specific tasks and simulating realistic scenarios.
3. The paper provides a thorough evaluation of recent video-language models.

Weaknesses:
1. Missing dataset statistics, such as the number of questions per video and the distribution of video durations.
2. The robustness evaluation may be compromised by outpainting, which could introduce new animals and alter answers.
3. The benchmark lacks training and validation sets, limiting the evaluation of fine-tuned model performance.
4. The writing could benefit from clearer explanations of task abbreviations and the inclusion of absolute model accuracy in results.

### Suggestions for Improvement
We recommend that the authors improve the dataset statistics by including the number of questions per video and the distribution of video durations in Section 3.1 and Table 3. Additionally, we suggest manually checking a subset of outpainted videos to assess the accuracy of answers post-outpainting. To enhance the benchmark's impact, we recommend including training and validation sets for fine-tuning evaluations. Furthermore, please clarify task abbreviations in Table 1 and provide absolute model accuracy alongside accuracy drops in Table 2.