ID: ZqSx5vXOgC
Title: Bypass Exponential Time Preprocessing: Fast Neural Network Training via Weight-Data Correlation Preprocessing
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 3, 1, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new preprocessing method for training shallow overparametrized sparse neural networks, significantly improving preprocessing time while maintaining performance during query time. The authors analyze two-layer neural networks with ReLU activation, showing that the proposed algorithm reduces the computational complexity from \(O(mnd)\) to \(O(m^{4/5}n^2d)\) in the overparameterized regime. The core technical contribution is the Correlation Tree data structure, which efficiently updates based on the sparsity of activated neurons. The authors provide both upper and lower bounds for their preprocessing complexity, indicating that their method is nearly optimal.

### Strengths and Weaknesses
Strengths:
1. The paper is clearly written, well-structured, and presents a thorough theoretical analysis.
2. The connection between applied algorithms and deep learning is insightful, and the inclusion of a lower bound demonstrates the results' proximity to optimality.
3. The proposed algorithm offers a novel approach to achieving sublinear cost per iteration.

Weaknesses:
1. Not all assumptions are clearly stated in Theorem 1.1, particularly regarding the sparsity assumption.
2. The empirical practicality of the method is questionable, with no experimental results provided to demonstrate its effectiveness.
3. The space complexity of the proposed method is high, requiring \(O(nm)\) storage, which may be prohibitive for large \(n\) and \(m\).

### Suggestions for Improvement
We recommend that the authors improve the clarity of the assumptions in Theorem 1.1, particularly regarding the sparsity assumption. Additionally, we suggest including empirical studies, even on toy architectures, to validate the efficiency of the proposed method compared to traditional training protocols. Addressing the practical implications of the high space complexity and providing remarks on the theoretical challenges compared to previous studies would also strengthen the paper.