ID: 4xckZu4MPG
Title: Attention as Implicit Structural Inference
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 8, 5, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 1, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical framework that interprets attention mechanisms in transformers as inference over possible adjacency structures in graphical models. The authors demonstrate this connection for cross- and self-attention heads and interpret iterative attention mechanisms as performing gradient descent on an approximate variational free energy. They propose two new attention mechanisms that accommodate more complex connectivity structures, linking their work to variational inference and existing models like slot attention and modern Hopfield networks.

### Strengths and Weaknesses
Strengths:
- The framework provides a principled interpretation of attention mechanisms in transformers.
- It allows for the design of new attention types based on the proposed framework.
- The paper contributes original insights by linking iterative attention to variational inference.

Weaknesses:
- The new multi-hop and expanding attention designs are evaluated only on toy tasks that match their structure.
- Significant discussions and results are relegated to supplementary material, making the main text dense and potentially less coherent for readers.
- The experimental evaluation lacks rigor, relying on toy problems that do not adequately demonstrate the approach's effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by defining terms such as $d_k$ and referencing the attention equation in the introduction. Merging sections 3.2 and 3.3 could free space for larger, more readable figures and additional explanations for non-expert readers. We suggest incorporating temperature scaling in the softmax to explore potential generalizations. The authors should also provide a more rigorous evaluation of their approach against existing stochastic attention formulations and clarify how their framework scales with popular architectures. Additionally, addressing the questions raised regarding the implications of their framework on decoding algorithms and including relevant references would strengthen the paper.