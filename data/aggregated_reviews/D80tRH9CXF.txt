ID: D80tRH9CXF
Title: Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 5, 7, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 2, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the prediction and estimation risk of the ridgeless least squares estimator under a general error structure, moving beyond the typical i.i.d. assumption. The authors propose a theoretical framework that quantifies the variance components of both risks, particularly in contexts such as time series and clustered data. The findings indicate that the advantages of overparameterization are applicable even in dependent error structures, and the paper includes numerical experiments to substantiate the theoretical claims.

### Strengths and Weaknesses
Strengths:
- The investigation of prediction and estimation risk under non-i.i.d. regressor errors, focusing on time series and clustered data.
- Explicit quantification of variance components for both risks, dependent on the trace of the error covariance matrix and a function of the design matrix.
- Detailed analysis of variance and bias terms in high-dimensional asymptotics.
- Well-constructed numerical experiments that support the theoretical findings.

Weaknesses:
- The theoretical results, particularly in the bias component analysis, lack rigor and clarity, with notational discrepancies and inconsistencies.
- Additional remarks following Theorems 3.4 and 3.5 regarding known distributions of the design matrix would enhance understanding.
- Some notations, such as $a(X)$ and $b$, are clarified only in the appendix; introducing them earlier in the proof sketch would improve comprehension.
- The technical contributions appear limited, with some proofs being relatively straightforward, raising concerns about the depth of the analysis.

### Suggestions for Improvement
We recommend that the authors improve the rigor and clarity of the theoretical results, particularly in the bias component analysis, to address notational discrepancies and inconsistencies. It would be beneficial to include examples following Theorems 3.4 and 3.5 that utilize known distributions of the design matrix for better insight. Additionally, we suggest introducing notations like $a(X)$ and $b$ earlier in the proof sketch to aid reader understanding. Finally, consider exploring the implications of removing the assumption that $\varepsilon$ is independent of $X$ and addressing the complexity of real-world regression challenges more comprehensively.