ID: 64V40K2fDv
Title: Exploring Molecular Pretraining Model at Scale
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 8, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Uni-Mol2, a molecular pretraining model that utilizes a two-track transformer to integrate features at the atomic, graph, and geometric structure levels. The authors systematically investigate the scaling law within molecular pretraining models, demonstrating the first power-law correlations between validation loss and model size, dataset size, and computational resources. The study introduces the largest dataset for molecular pretraining, comprising approximately 884 million 3D conformations, and shows that Uni-Mol2, with 1.1 billion parameters, achieves state-of-the-art performance on downstream tasks.

### Strengths and Weaknesses
Strengths:
1. The creation of the largest molecular pretraining dataset provides a robust foundation for training large-scale models.
2. The exploration of scaling law behavior in molecular pretraining models is a novel contribution.
3. Consistent performance improvements are observed with increasing model size, indicating the model's potential.

Weaknesses:
1. The paper lacks detailed information on pretraining hyper-parameters and the computational cost associated with training Uni-Mol2.
2. The evaluation of downstream tasks is limited, with some results showing minimal improvement as model size increases, raising questions about the significance of these gains.
3. The presentation of results could be enhanced by including parameter sizes of baseline models and correcting grammatical errors.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions and the rationale for choosing an LLM model over GNN models, potentially considering a combination of both. Additionally, we suggest including more diverse and complex downstream tasks to better demonstrate the advantages of larger models. It would be beneficial to report the model training time and provide an ablation study on pretraining hyper-parameters to facilitate reproducibility. Lastly, addressing the statistical significance of performance gains and including the parameter sizes of baseline models in the results tables would enhance the paper's rigor.