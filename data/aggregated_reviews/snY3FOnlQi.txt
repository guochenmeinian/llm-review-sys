ID: snY3FOnlQi
Title: AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 8, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel task called "real-world audio-visual scene synthesis," which aims to synthesize new videos with spatial audio along arbitrary camera trajectories. The authors propose a NeRF-based approach that integrates audio generation with 3D visual environments, utilizing a coordinate transformation module to express view direction relative to sound sources. A new Real-World Audio-Visual Scene (RWAVS) dataset is introduced to support this task and benchmark the proposed method, which demonstrates effectiveness through extensive experiments.

### Strengths and Weaknesses
Strengths:
1. The task of audio-visual scene synthesis is essential and represents a significant contribution to the field, being the first to generate both video and spatial audio in real environments.
2. The paper includes a well-motivated model pipeline, and the extensive experiments validate the proposed method's effectiveness against strong baselines.
3. The dataset collected is valuable for future research and the paper is generally well-written and easy to follow.

Weaknesses:
1. The novelty of the proposed network appears limited, primarily combining existing audio-NeRF and visual-NeRF architectures, with the main innovation being the AV-mapper.
2. The focus on static scenes with a single fixed sound source limits the applicability of the term "real-world," as real environments typically involve multiple sound sources.
3. The AV-mapper's design raises concerns regarding its ability to extract material information from RGB images, and the depth images used do not capture all relevant geometry.

### Suggestions for Improvement
We recommend that the authors improve the AV-mapper's design to better extract material and geometry information, possibly by testing with color jittering or cross-scene evaluations. Additionally, to enhance the realism of the synthesis, the authors should adapt the method to handle multiple sound sources. We suggest including human subjective evaluations in future experiments to assess perceptual differences in generated audio. Furthermore, addressing the limitations of the current evaluation protocols and providing a new useful metric for spatial audio modeling would strengthen the paper. Lastly, we encourage the authors to compare their method with the recently published ViGAS model to provide a more comprehensive evaluation.