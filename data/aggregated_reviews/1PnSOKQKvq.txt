ID: 1PnSOKQKvq
Title: Gaussian Partial Information Decomposition: Bias Correction and Application to High-dimensional Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 8, 6, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new method for computing Partial Information Decompositions (PIDs) on multivariate Gaussian distributions, addressing the computational challenges associated with high-dimensional neural recordings. The authors propose a bias-correction method and demonstrate its effectiveness through simulations and real data applications. The method is shown to be efficient, scalable, and capable of handling non-Gaussian data while correcting for bias in PID estimates. The computational complexity of the method is confirmed to be cubic in the dimensionality of M, X, and Y, specifically \(\mathcal O(d^3)\) for each gradient descent iteration. The authors address concerns regarding the stability of gradient computation, particularly in relation to the projection step and the handling of eigenvalues, employing regularization techniques to stabilize matrix inverses and utilizing RProp to manage learning rates amid gradient fluctuations. The authors assert that the ultimate test of their method's effectiveness is its ability to recover ground truth in various scenarios.

### Strengths and Weaknesses
Strengths:
1. The introduction clearly articulates the problem and the significance of PIDs in neuroscience.
2. The paper is well-written, with clear definitions and structured exposition.
3. The method extends prior work on PID, demonstrating new properties and rigorous testing on simulated data.
4. The computational scalability of the proposed method is a notable advantage, allowing for higher dimensionality analysis.
5. The authors effectively address bias in PID estimates, which is a critical issue in the field.
6. There is a clear explanation of computational complexity and its implications.
7. Effective use of regularization enhances stability in gradient computation.
8. The method demonstrates the capability to recover ground truth in high-dimensional settings.

Weaknesses:
1. The paper lacks sufficient empirical validation on real-world data, particularly regarding non-Gaussian distributions beyond Poisson data.
2. There is insufficient analysis of the stability of the proposed method over increasing dimensionality, particularly concerning the differences between $\delta$-PID and $\sim$-PID.
3. The application of the method to PCA-reduced data raises questions about its effectiveness in truly high-dimensional settings.
4. Potential confusion exists regarding the distinction between RProp and RMSProp, which could mislead readers.
5. There is a lack of clarity on the handling of small positive eigenvalues in the projection step.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation of their method by testing it on a wider range of non-Gaussian distributions, particularly those relevant to neural data, such as zero-inflated models. Additionally, we suggest conducting a thorough analysis of the stability of $\delta$-PID over increasing dimensionality, including how the differences between $\delta$-PID and $\sim$-PID impact performance. Clarifying the computational complexity and addressing the stability of the gradient computation in the context of eigenvalue fluctuations would enhance the robustness of the paper. Furthermore, we recommend that the authors improve clarity by explicitly stating the distinction between RProp and RMSProp, ensuring the correct reference to Riedmiller and Braun (1993) is used. Lastly, providing a more detailed discussion on the implications of small positive eigenvalues in the projection step would enhance understanding of their method's stability.