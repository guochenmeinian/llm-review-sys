ID: UlgNWOzMz2
Title: Isotropic Representation Can Improve Zero-Shot Cross-Lingual Transfer on Multilingual Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multilingual language model fine-tuning methodology aimed at enhancing the isotropy of token representations. The authors propose a loss function that increases isotropy and a code-switching methodology to improve representation consistency. They fine-tune two popular multilingual LMs (mBERT, XLM-R) and evaluate their performance across seven training methodologies, concluding that their approach yields the best cross-lingual performance while preserving syntactic properties. The paper includes extensive analyses, such as dependency parsing probing and various visualization techniques, to support its claims.

### Strengths and Weaknesses
Strengths:
- The proposed method demonstrates beneficial effects on both semantic and syntactic properties of the model.
- It outperforms state-of-the-art methods in Section 4.4.
- The analysis section is robust and provides substantial evidence for the proposed methodology.

Weaknesses:
- Different sets of methods are used across experiments, making it difficult to draw comprehensive conclusions about their performance.
- There is a discrepancy in hyperparameter tuning budgets between baseline and proposed methods, with the latter having at least 4x more training runs.
- Confidence intervals for results are not reported, which is critical given the small sample size of N=5 runs.
- Some results, such as those for MARC, are based on only 25% of the training data, raising concerns about generalizability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their experimental design by providing a comprehensive overview of the methods used across different sections. Additionally, it is crucial to report confidence intervals for the results to enhance the reliability of the findings. The authors should also address the hyperparameter tuning discrepancies and clarify the rationale for using two auxiliary losses, including empirical evidence to support their necessity. Finally, we suggest revising the terminology used in the paper, ensuring consistent use of "token representations" instead of "word embeddings," and providing specific details about the version of XLM-R utilized.