ID: Fe6fDq65aZ
Title: On the Trade-off of Intra-/Inter-class Diversity for Supervised Pre-training
Conference: NeurIPS
Year: 2023
Number of Reviews: 5
Original Ratings: 4, 6, 6, -1, -1
Original Confidences: 5, 4, 2, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical study and theoretical analysis of the class-to-sample ratio in supervised pre-training datasets, focusing on the trade-off between intra-class and inter-class diversity. The authors conclude that the optimal class-to-sample ratio is invariant to the size of the pre-training dataset, providing guidance for scaling laws in pre-training. The analysis reveals that the best downstream performance is achieved with a balance of intra- and inter-class diversity, supported by both theoretical and empirical results.

### Strengths and Weaknesses
Strengths:
- The paper reveals an important conclusion regarding the invariance of the optimal class-to-sample ratio to dataset size, which can guide data scaling.
- It provides a rigorous study on pretraining using ImageNet, clarifying the empirical motivation for theoretical results.
- The theoretical analysis is well-explained and offers a method to balance intra-class and inter-class diversity, enhancing downstream task performance.
- The claims regarding the effects of diversity on performance are well-supported by empirical results.

Weaknesses:
- The pre-training dataset is restricted to ImageNet, raising concerns about the generalizability of the results to other datasets.
- The model ResNet-18 is relatively small and may not fully reveal the conclusions; other larger models or vision transformers could yield different insights.
- Evaluation is limited to only 7 small downstream datasets, and the study does not cover other task types, such as detection and segmentation.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by including additional pre-training datasets beyond ImageNet. Exploring larger models and vision transformers would also be beneficial to validate the conclusions drawn. Furthermore, expanding the evaluation to include various downstream tasks and larger datasets would strengthen the paper's contributions. Additionally, we suggest clarifying the clustering method used for obtaining labels, as mentioned in line 195, to enhance the transparency of the methodology.