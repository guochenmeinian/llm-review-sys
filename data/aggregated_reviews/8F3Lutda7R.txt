ID: 8F3Lutda7R
Title: Exploiting Connections between Lipschitz Structures for Certifiably Robust Deep Equilibrium Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 7, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to the robustness certification of deep equilibrium models (DEQs) by generalizing classical Lipschitz-constrained networks as special cases of Lipschitz-bounded equilibrium networks (LBEN). The authors provide conditions for a DEQ to be L-Lipschitz and extend these conditions to the reparameterization of DEQs. They also establish connections between various Lipschitz layers, including SDP-based Lipschitz layers (SLL), almost orthogonal layers (AOL), and sandwich layers, encapsulating these methods within the LBEN framework. The experimental results are demonstrated using the MNIST dataset.

### Strengths and Weaknesses
Strengths:  
- The paper is well-structured, clearly written, and reader-friendly.  
- It offers a significant contribution by linking DEQs with 1-Lipschitz neural networks, providing a novel perspective on their analysis.  
- The authors effectively discuss the reparameterization technique, which maintains the Lipschitz prescription across different structures.  

Weaknesses:  
- The experimental section is limited, lacking comprehensive coverage of the propositions made.  
- There are no baselines with unconstrained DEQs, and the justification for not including results for AOL and sandwich layers is insufficient.  
- The results on CIFAR10 are not convincing, as LBEN initialized from SLL does not clearly outperform the SLL network.  

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including additional datasets beyond MNIST and CIFAR, such as CIFAR-100 and TinyImageNet, to validate their findings more robustly. Additionally, a clearer discussion of the application scenarios for the proposed method is necessary, particularly regarding its limitations in providing certified robustness guarantees. We also suggest that the authors clarify the advantages of fine-tuning models from MonDEQ, SLL, and AOL by reparameterizing them as LBEN compared to training an LBEN model from scratch.