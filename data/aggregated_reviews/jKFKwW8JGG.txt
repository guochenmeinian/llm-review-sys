ID: jKFKwW8JGG
Title: Cola: A Benchmark for Compositional Text-to-image Retrieval
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 6, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Cola, a text-to-image retrieval benchmark aimed at evaluating the compositional grounding abilities of large vision-language models. The benchmark includes both single-object and multi-object queries, utilizing distractor images to enhance difficulty. The authors propose novel fine-tuning strategies, including cross-modal approaches, to improve performance on this challenging task. Additionally, the authors curate evaluation sets from existing datasets to assess multiple object-attribute compositionality, analyzing architectural choices and training paradigms that can enhance vision-language (VL) models' compositional performance. Key findings indicate that multimodal attention is superior to split-modality attention for distinguishing fine-grained attribute-object differences, and that training on attribute-object fine-grained data is essential for effective performance. The evaluation reveals that existing models like CLIP and FLAVA struggle with compositionality, highlighting the benchmark's significance.

### Strengths and Weaknesses
**Strengths:**
1. The problem addressed is both interesting and practical, contributing to grounded language learning and multimodal learning.
2. The benchmark effectively highlights the performance gap between human baselines and state-of-the-art models, indicating areas for future research.
3. The methodology for selecting distractor images is efficient and enhances the challenge of the task.
4. The paper demonstrates the importance of multimodal attention and the need for training on fine-grained data.

**Weaknesses:**
1. The dataset size is limited, with only 210 images, which may hinder the generalizability of results.
2. The experimentation lacks a diverse range of baseline models, which could provide deeper insights into model performance.
3. Clarity issues exist in the presentation of the proposed methods and figures, which could benefit from additional annotations and explanations.
4. Some aspects of the evaluation methodology, such as the use of precision and accuracy over F1 scores, were questioned, although the authors have acknowledged this and plan to include F1 scores in the appendix.

### Suggestions for Improvement
We recommend that the authors improve the dataset size to ensure its relevance for future research, as 210 images may not be sufficient for robust model training. Additionally, increasing the number of baseline models in the experimentation would provide a more comprehensive evaluation of the proposed methods. The authors should enhance clarity in their figures and explanations, particularly regarding the motivation for using their dataset over existing ones, and provide a more detailed analysis of model failures. We suggest improving Figure 2 by adding a legend for clarity and ensuring consistent naming conventions. It would also be beneficial to clarify the differences in the number of captions and images for single-object and multi-object splits. Including a systematic analysis of failure cases in the main paper, as well as a distribution of object classes and attributes, would strengthen the paper. We encourage the authors to visualize performance on challenging objects and attributes to enhance understanding and explicitly articulate potential future research directions. Lastly, discussing the computational complexity of the proposed fine-tuning approaches would add valuable context to the contributions made.