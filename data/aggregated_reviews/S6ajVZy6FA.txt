ID: S6ajVZy6FA
Title: A3FL: Adversarially Adaptive Backdoor Attacks to Federated Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 4, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents A3FL, a backdoor attack designed for Federated Learning (FL) that adapts backdoor triggers in an adversarial manner to enhance robustness against global training dynamics. By addressing the discrepancies between global and local models, A3FL ensures the trigger's effectiveness even when the global model attempts to negate it. The authors also discuss backdoor attacks in FL, specifically focusing on trigger unlearning and stealthiness. They propose a model that simulates the defender's goal of mitigating backdoor triggers, acknowledging that their current formulation of "worst-case" scenarios is an approximation rather than a rigorous definition. The authors clarify the distinction between fixed-trigger attacks and their proposed trigger-optimization approach, A3FL, emphasizing the need for a nuanced understanding of stealthiness in this context. Through extensive experiments, the authors demonstrate that A3FL significantly outperforms existing backdoor attacks, achieving superior efficiency against established defenses.

### Strengths and Weaknesses
Strengths:
- The paper is easy to follow and addresses a critical issue in federated learning.
- A3FL employs a bi-level objective to optimize trigger patterns, considering worst-case global dynamics.
- The authors provide code for replication, enhancing the study's accessibility.
- Experimental results show A3FL's effectiveness against twelve existing defense mechanisms.
- The authors demonstrate a willingness to engage with reviewer feedback and clarify complex concepts.
- The proposed model effectively simulates the defender's goal, contributing to the understanding of backdoor mitigation strategies.

Weaknesses:
- The paper requires editorial revision to eliminate redundancy and improve clarity.
- There is a lack of theoretical analysis regarding attack performance and convergence.
- Claims about the limitations of existing backdoor attacks need further substantiation with references.
- The evaluation of A3FL does not include comparisons with recent state-of-the-art attacks like 3DFed and FLAME.
- The discussion differentiating Neurotoxin from A3FL lacks clarity, particularly regarding the worst-case assumption.
- The explanation of trigger unlearning remains unconvincing within the context of the threat model.

### Suggestions for Improvement
We recommend that the authors improve the paper's clarity by revising it to reduce redundancy and allow for the inclusion of additional results. Additionally, please provide references to support claims regarding the sub-optimality of triggers in existing backdoor attacks and clarify the implications of using local training data from benign clients. We also suggest including a theoretical analysis of the attack's performance and efficiency, as well as evaluating A3FL against more recent state-of-the-art attacks and practical benchmark datasets. Furthermore, we recommend that the authors improve the clarity of the distinction between Neurotoxin and A3FL by explicitly outlining the differences in their methodologies and objectives. Lastly, we suggest providing a more robust discussion on trigger unlearning, ensuring that it aligns with the threat model and addresses potential reviewer concerns regarding its feasibility and implications.