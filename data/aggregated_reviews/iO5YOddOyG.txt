ID: iO5YOddOyG
Title: Is ChatGPT a Good Multi-Party Conversation Solver?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the zero-shot performance of ChatGPT and GPT-4 on multi-party conversation (MPC) tasks, including emotion detection, addressee recognition, speaker identification, response selection, and response generation. The authors evaluate the impact of incorporating speaker and addressee information in prompts on performance. The study utilizes existing datasets (EmoryNLP, MELD, and Ubuntu IRC) to assess the dialogue capabilities of large language models in more complex conversational scenarios.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents an interesting problem relevant to the understanding of multi-party conversations.
- The experimental design is generally well described, and the evaluation of LLMs in MPC is timely and significant.

Weaknesses:
- The contribution is limited, primarily providing zero-shot results without substantial technical novelty, as it mainly involves prompt design.
- The training data for the evaluated models is unknown, raising concerns about potential test leakage and the generalizability of results.
- There is a lack of in-depth error analysis of failure cases, which would enhance understanding of the models' limitations.

### Suggestions for Improvement
We recommend that the authors improve the depth of their analysis by including an in-depth error analysis of failure cases to identify systematic errors and differences from supervised models. Additionally, we suggest clarifying the training data used for the models to address concerns about test leakage. It would also be beneficial to explore the inclusion of few-shot learning results, as this could provide a more comprehensive evaluation of the models' capabilities. Furthermore, we advise revising the title to more accurately reflect the paper's focus on the models' ability to follow conversations rather than suggesting they solve multi-party conversations. Lastly, we encourage the authors to consider testing additional large language models beyond GPT-3.5 and GPT-4 to strengthen their findings.