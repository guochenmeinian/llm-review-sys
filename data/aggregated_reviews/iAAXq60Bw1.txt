ID: iAAXq60Bw1
Title: Geodesic Multi-Modal Mixup for Robust Fine-Tuning
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 3, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Geodesic Multi-Modal Mixup, a method that mixes heterogeneous modality embeddings on the hypersphere to create harder negative samples for contrastive loss in multi-modal models. The authors argue that this training scheme enhances modality alignment and uniformity, and they evaluate the method across various tasks, including retrieval, calibration, few-shot classification, and embedding arithmetic. Additionally, the authors propose a few-shot classification setup using the CLIP-ViT-B/16 model, diverging from standard full dataset approaches in existing CLIP fine-tuning literature. They detail their experimental configurations, including the use of various optimizers and learning rate settings, while addressing reviewer concerns regarding the necessity of additional experiments. The paper also introduces three mix-up inspired regularizers for fine-tuning CLIP to address the visual-text feature gap, supported by theoretical analysis and empirical results.

### Strengths and Weaknesses
Strengths:
- The multi-modal mixup approach is timely and presents a novel idea of mixing different modalities.
- The method is comprehensive, straightforward, and backed by extensive experiments demonstrating significant performance gains.
- The authors provide a clear explanation of their experimental setup and rationale for using a few-shot classification approach.
- Detailed descriptions of the training parameters and methodologies enhance the reproducibility of their results.
- The paper provides a new perspective on understanding multi-modal embeddings in terms of uniformity and alignment.

Weaknesses:
1. The key hyperparameter lambda, which controls the "hardness" of mixed samples, requires sensitivity analysis.
2. The authors misrepresent prior work on the modality gap phenomenon, failing to adequately cite relevant studies.
3. The theoretical analysis is weak, with limitations in the applicability of theorems and unclear justification for the necessity of additional regularization.
4. Some reviewers express that the authors have not fully addressed concerns regarding the first contribution of the paper.
5. The necessity for additional experiments has been highlighted, indicating that current results may not be sufficient to support the claims made.
6. Experimental results are limited to specific datasets and architectures, raising concerns about generalizability and comparison to state-of-the-art methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by explicitly discussing the differences from prior work on the modality gap and providing a more nuanced analysis of their findings. Additionally, we suggest conducting a sensitivity test on the hyperparameter lambda and including full tables with both m^2-Mix and m^3-Mix methods to facilitate comparison. The authors should also clarify the necessity of additional regularization beyond m^2-Mix and provide more extensive experiments across various datasets and architectures to strengthen their claims. Furthermore, we recommend conducting further experiments to strengthen the findings and provide a more comprehensive comparison with existing studies. Lastly, addressing minor presentation issues, such as font size in figures and clarity in equations, would enhance the paper's readability.