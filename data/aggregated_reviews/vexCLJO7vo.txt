ID: vexCLJO7vo
Title: MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new dataset named MenatQA, aimed at evaluating the temporal comprehension and reasoning abilities of large language models (LLMs). It incorporates three temporal factors: scope factor, order factor, and counterfactual factor, with a total of 2,853 samples. The authors conduct experiments on several mainstream models using specific prompts and tool learning, revealing that most LLMs perform poorly on the MenatQA dataset.

### Strengths and Weaknesses
Strengths:  
- The proposed dataset is a valuable resource for assessing LLMs' temporal reasoning abilities and may inspire future research.  
- Comprehensive experiments and analyses are conducted, and the proposed prompt designs are effective.  

Weaknesses:  
- The paper lacks significant technical innovation and does not advance understanding beyond existing knowledge.  
- Insufficient analysis of experimental results, particularly regarding the reasons behind LLMs' poor performance on certain tasks.  
- The rationale for selecting the three temporal factors is unclear, and prompt design is not adequately emphasized.

### Suggestions for Improvement
We recommend that the authors improve the analysis of when and why models make mistakes to enhance understanding of their performance. Additionally, the authors should clarify the rationale behind the selection of the three temporal factors and provide more detailed discussions on effective prompt design. It would also be beneficial to include examples of unanswerable questions and elaborate on the two-round validation process for MenatQA.