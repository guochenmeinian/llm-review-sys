ID: HWNl9PAYIP
Title: Video Prediction Models as Rewards for Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 6, 7, 5, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Video Prediction Rewards (VIPER), an architecture designed to extract reward functions from action-free expert demonstrations. VIPER optimizes the agent's trajectory distribution by maximizing a log-likelihood estimated through an auto-regressive video model, supplemented by an entropy term for exploration. The authors propose a reward formulation that captures distributions over trajectories, leading to more informative rewards. They demonstrate the effectiveness of VIPER through experiments on DMC, Atari, and RLBench, showing that the extracted reward functions can generalize across various tasks. The manuscript discusses the non-Markovian nature of rewards and acknowledges the need for precise terminology regarding out-of-distribution (OOD) generalization, with plans to clarify experiments on cross-embodiment generalization and address aliasing in future work.

### Strengths and Weaknesses
Strengths:
- VIPER effectively addresses the challenge of reward function extraction from action-free expert demonstrations, which is particularly beneficial in applications like self-driving.
- The method shows strong performance across multiple visual control tasks and generalizes well to out-of-distribution (OOD) tasks, demonstrating superior performance over baselines (AMP, GAIfO, BCO) in both single and multi-task settings, indicating better training stability and avoidance of mode collapse.
- The paper is well-written, with clear presentation and comprehensive experimental validation, and VIPER can effectively learn a meaningful reward model from minimal expert video data.

Weaknesses:
- VIPER's data efficiency is low, requiring nearly 10 million data points to converge in DMC, and it does not leverage sub-optimal demonstrations, which could enhance data efficiency.
- The acquisition of a generative video model may be challenging and costly in real-world scenarios, especially with visual distractions.
- The generalization ability of VIPER is not convincingly demonstrated, as experiments only evaluate a limited number of OOD combinations, and the discussion on OOD generalization is not fully within the paper's scope and requires clearer terminology.
- Some reviewers found the explanation of the most probable sequence of coin flips to be confusing and in need of clarification.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of VIPER's generalization capabilities by testing on a broader range of OOD task combinations to substantiate claims of generalization. Additionally, we suggest that the authors enhance the clarity of the discussion surrounding the most probable sequence of coin flips by explicitly stating that they are considering the ordering of heads and tails. The authors should also consider addressing the potential overestimation issues associated with the use of pre-trained video prediction models, particularly in the context of limited expert data. Clarifying the implications of non-Markovian rewards in reinforcement learning and refining the terminology around "OOD generalization" to "cross-embodiment generalization" could enhance the paper's clarity and rigor. Finally, including more baselines and ablation studies would strengthen the empirical support for VIPER's effectiveness.