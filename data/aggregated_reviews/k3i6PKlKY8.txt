ID: k3i6PKlKY8
Title: mRedditSum: A Multimodal Abstractive Summarization Dataset of Reddit Threads with Images
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new multimodal abstractive summarization dataset, mRedditSum, curated from 3,033 Reddit threads that include text and images. The authors propose a cluster-based multi-stage summarization method that outperforms existing baselines by effectively incorporating visual information. The dataset and the CMS model are well-documented, showcasing their potential contributions to the field of multimodal summarization.

### Strengths and Weaknesses
Strengths:  
- The proposed dataset and CMS model are novel and well-described, with clear details on their contributions.  
- The method demonstrates improved performance by integrating visual information, achieving state-of-the-art results.  

Weaknesses:  
- The comparability of the CMS model is limited, as it is evaluated solely on the proposed dataset without performance metrics on other datasets.  
- There is insufficient detail regarding the annotation process, including the total number of annotators and inter-annotator agreement, which raises concerns about the quality and consistency of the summaries.  
- The paper lacks a comparison with text-only baseline models that can handle longer input sequences, which could strengthen the claims regarding the proposed model's capabilities.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing additional details on the annotation process, including the total number of annotators and whether the same annotator generated both the comment and full summary. Additionally, including inter-annotator agreement metrics for a subset of samples would enhance the understanding of summary quality variance. Furthermore, we suggest that the authors compare their model with text-only baselines that support longer sequence lengths, such as LongFormer or LongT5, to bolster their claims about handling longer inputs effectively.