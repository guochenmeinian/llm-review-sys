ID: r9fzp8eyhZ
Title: Learning Invariant Molecular Representation in Latent Discrete Space
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 7, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel graph neural network architecture and objective function aimed at enhancing out-of-distribution generalization in molecular property prediction tasks. The authors propose a "first-encoding-then-separation" paradigm, utilizing a GNN for encoding and a residual vector quantization module to balance expressiveness and generalization. The method focuses on invariant feature extraction in the latent space, supported by a self-supervised learning objective. Experimental results indicate improved performance over state-of-the-art methods across various datasets.

### Strengths and Weaknesses
Strengths:
- The proposed method is innovative and demonstrates improved performance across multiple datasets.
- Extensive experimentation is conducted, with useful ablation and sensitivity analyses.
- The model is clearly described, and the code is made available for future research.

Weaknesses:
- There are incorrect definitions in Section 3.1 regarding probability distributions that require re-normalization.
- The term "discrete latent space" is unclear, as the model's embeddings appear continuous due to residual connections.
- The learning objective's notation, particularly in Equation (11), is confusing, and the role of certain parameters is inadequately described.
- Baseline presentations are conflated, leading to confusion about model architectures and objectives.
- The complexity of the method is high, with several tunable parameters that affect performance significantly.

### Suggestions for Improvement
We recommend that the authors improve the definitions in Section 3.1 to ensure valid probability distributions. Clarifying the use of "discrete latent space" and addressing the confusion in Equation (11) regarding dimensionality and parameter roles would enhance understanding. Additionally, we suggest that the authors clearly delineate baselines in terms of architecture and objectives to avoid conflation. Providing more detailed explanations of the experimental setup and hyperparameter choices would also strengthen the paper's contributions. Lastly, addressing the complexity of the methodology and discussing potential limitations in relation to previous work would provide a more balanced perspective.