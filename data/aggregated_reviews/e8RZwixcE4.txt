ID: e8RZwixcE4
Title: Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 6, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the use of surrogate labels in computational social science, focusing on bias, coverage, and performance properties. The authors propose a design-based semi-supervised learning (DSL) estimator that combines surrogate labels from Large Language Models (LLMs) with gold-standard labels, ensuring asymptotic unbiasedness and proper uncertainty quantification. The method is validated through extensive experiments across 18 datasets, demonstrating its effectiveness in achieving valid statistical inference while maintaining comparable error rates to existing methods. The authors clarify that their work does not involve "Self-Supervised Learning" and assert that their SSL method matches and outperforms state-of-the-art baselines, specifically addressing concerns about RMSE gains. They also discuss the relevance of F1 scores, stating that their primary output is a vector of continuous variables rather than multi-class classifications, and emphasize that their method is agnostic to surrogate labels.

### Strengths and Weaknesses
Strengths:
1. The paper proposes a unified framework for utilizing imperfect surrogate labels while maintaining unbiasedness and proper coverage.
2. The theoretical guarantees for the proposed method are robust and well-articulated.
3. Extensive experiments across multiple datasets provide a solid foundation for the claims made.
4. The authors effectively clarify the distinction between "Semi-Supervised Learning" and "Self-Supervised Learning."
5. They provide a strong rebuttal regarding RMSE performance, demonstrating superiority over existing methods.
6. The incorporation of F1 scores for underlying LLM annotations shows responsiveness to reviewer feedback.

Weaknesses:
1. The title may mislead, as the paper primarily focuses on mathematical proofs and the utility of semi-supervised learning rather than the application of LLM annotations.
2. The assumptions of the proposed approach limit its broader applicability, particularly in contexts where test sets are not known.
3. The asymptotic behavior concerning gold-standard annotations is inadequately discussed, lacking empirical data on downstream coefficients and variance.
4. The paper contains numerous notations without an accompanying explanation table, and the experimental sections lack sufficient detail regarding evaluation tasks and dataset distributions.
5. The insistence on LLMs may limit the perceived applicability of the method, as noted by the reviewers.
6. The authors' explanations regarding the goals of their method may not fully address reviewer concerns about performance improvement.

### Suggestions for Improvement
We recommend that the authors improve the title to better reflect the core contributions of the paper, emphasizing the mathematical and methodological aspects rather than focusing on LLM annotations. Additionally, the authors should address the limitations of their assumptions to enhance the general applicability of their approach. A more thorough discussion on the asymptotic behavior of the method, including empirical data on downstream coefficients, would strengthen the paper. Furthermore, including an explanation table for notations and providing detailed descriptions of datasets and evaluation tasks in the main text would enhance clarity and comprehensibility. We also suggest that the authors improve clarity regarding the applicability of their method beyond LLMs, potentially by reducing specific mentions of LLMs while ensuring that the relevance of their approach remains evident. Additionally, we encourage the authors to provide more experimental results that vary the accuracy of LLM annotations to strengthen their claims about surrogate-only estimation. Finally, we recommend clarifying the goal of their study to prevent misunderstandings about the focus on downstream inference rather than individual document classification.