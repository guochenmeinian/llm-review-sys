ID: HGLvAAKNKx
Title: An Empirical Study of Translation Hypothesis Ensembling with Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies methods to enhance LLM-based machine translation (MT) by reranking multiple hypotheses. The authors propose that MBR decoding with COMET improves translation performance across various language pairs. The study demonstrates that simple techniques can effectively boost LLM-based MT performance, although the novelty of the approach is questioned, and its applicability to other settings remains unclear.

### Strengths and Weaknesses
Strengths:  
- The paper shows improved performance of LLM-based MT using straightforward methods.  
- Experiments cover eight language pairs and utilize multiple LLMs, providing comprehensive empirical evidence.  
- The structure and clarity of the paper are commendable, with detailed comparisons supported by tables and graphs.  

Weaknesses:  
- The approach lacks novelty, as MBR decoding is a well-known technique.  
- The performance of LLM-based approaches (ChooseBest and GenerateBest) was unexpectedly low, raising questions about their effectiveness.  
- The study is limited to European language pairs, leaving the generalizability to other language pairs uncertain.  
- Some methods did not yield consistent results across models and metrics, indicating a primarily empirical nature of the work.  

### Suggestions for Improvement
We recommend that the authors improve the novelty of their approach by exploring more innovative techniques beyond known methods. Additionally, it would be beneficial to discuss the reasons behind the underperformance of the LLM-based approaches in the context of recent findings on LLM-based quality estimation. Expanding the language coverage to include non-European language pairs would enhance the generalizability of the results. Lastly, we suggest clarifying the terminology used, particularly the term "ensembling," to avoid confusion with its conventional meaning in machine learning.