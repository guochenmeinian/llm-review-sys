ID: I5IIhtSbMe
Title: RLNet: Adaptive Fusion of 4D Radar and Lidar for 3D Object Detection
Conference: thecvf
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 6, 6, 7, 6, 7, 6
Original Confidences: 4, 4, 4, 3, 4, 4, 4

Aggregated Review:
### Key Points
We note that the submission presents a novel fusion method for 3D object detection using 4D radar and lidar sensor data. The proposed adaptive fusion method demonstrates superior results compared to baseline solutions, as evidenced by clear ablation studies. The framework includes three modules: a speed compensation module, an adaptive feature fusion module, and a random modality dropout module. The experiments are conducted on the VoD and ZJUODset datasets, showing improvements over baseline methods.

### Strengths and Weaknesses
Strengths include the clarity and organization of the paper, as well as the effective integration of radar and lidar data, which enhances robustness in various conditions. The adaptive feature fusion module is a significant contribution, and the ablation studies provide valuable insights. However, weaknesses are evident in the limited scope of experiments, which are confined to small, non-mainstream datasets, and the outdated comparison methods used. The technical novelty is perceived as limited, and the performance improvements may not fully reflect the algorithm's potential.

### Suggestions for Improvement
We suggest discussing the rationale for selecting the VoD and ZJUOD datasets over other publicly available lidar and radar datasets. A more detailed explanation of the speed compensation module's role in detection and its relationship to the synchronization method is necessary. Additionally, we recommend conducting comparative experiments on more commonly used datasets, such as nuScenes, and analyzing the reasons for the subpar vehicle detection results on the ZJUODset dataset. Finally, clarifying how radar timestamps are represented and utilized in the network would enhance understanding.