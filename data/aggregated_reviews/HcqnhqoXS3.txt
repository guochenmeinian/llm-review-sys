ID: HcqnhqoXS3
Title: Decomposed Prompt Decision Transformer for Efficient Unseen Task Generalization
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 7, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two significant contributions to the field of reinforcement learning (RL). First, it introduces the Multi-Task Prompt Decision Transformer (MPDT) algorithm for zero-shot multi-task offline RL, utilizing a pre-trained language model (PLM) with prompt tuning. The authors decompose multi-task prompts into task-specific and cross-task components, achieving zero-shot generalization through test time prompt alignment and demonstrating superior performance across various benchmarks compared to prior multi-task offline RL methods. Second, the authors explore the impact of retraining layers in language models on their understanding of RL data, proposing that preserving the language model's knowledge is crucial. They argue that prompts can effectively bridge the gap between tasks without retraining the embedding layer or GPT blocks, with an ablation study indicating that freezing these layers leads to better performance.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, making it easy to follow.
- The concept of prompt decomposition effectively addresses multi-task learning challenges.
- Comprehensive experiments validate the effectiveness of MPDT against strong baselines.
- The authors provide a thorough analysis of the implications of freezing layers in language models, supported by experimental results.
- The discussion of trade-offs in different training approaches highlights the computational efficiency of their method.
- The authors engage with existing literature, offering a comparative perspective that strengthens their argument.

Weaknesses:
- Concerns regarding the novelty of the MPDT paper arise, particularly around the contributions of prompt decomposition and distillation.
- The design of the model and loss does not guarantee that cross-task prompts encapsulate common knowledge, as element-wise multiplication may lead to $P_c$ acting merely as a scaling factor.
- The fairness of comparisons with baselines like MT-BC and Soft-Prompt is questionable due to the reliance on test-time information, which may not be uniformly applied across all methods.
- The reliance on specific studies ([1] and [2]) may limit the generalizability of the findings regarding freezing layers.
- Performance discrepancies noted when applying methods from [2] raise questions about the robustness of those approaches.
- The paper could benefit from clearer articulation of the implications of the findings for broader RL applications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the novelty by providing a more detailed explanation of how cross-task prompts are designed to encapsulate common knowledge. Additionally, the authors should consider including the distribution of $P_c$ values to verify its effectiveness. It would also be beneficial to train a randomly initialized $P_k$ for the test task using the alignment loss to enhance test time adaptation. Furthermore, addressing the concerns about the fairness of comparisons with other methods and providing insights into hyper-parameter tuning for baselines would strengthen the paper's contributions. We also recommend improving the clarity of the discussion regarding the implications of their findings for broader RL applications and providing more detailed comparisons with other methods beyond [1] and [2]. Further exploration of the performance discrepancies when applying [2] would also strengthen the paper's argument. Finally, engaging in a more detailed discussion about the potential limitations of freezing layers could provide a more balanced perspective.