ID: K35sqjeg5J
Title: Semi-supervised multimodal coreference resolution in image narrations
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a semi-supervised cross-modal coreference resolution model that links mentions in text to corresponding regions in images. The authors propose a framework utilizing various supervised and unsupervised loss functions, achieving state-of-the-art results on the CIN dataset. The methodology includes a multimodal encoder trained through self-learning techniques and extensive experiments comparing against existing baselines.

### Strengths and Weaknesses
Strengths:
- The paper demonstrates solid methodology and theoretical backing, with experiments supporting the claims.
- It is accessible for implementation, providing clear instructions and thorough experimental sections.
- The results indicate significant improvements over previous state-of-the-art methods.

Weaknesses:
- Some architectural choices lack sufficient motivation, particularly regarding the impact of individual loss functions.
- The paper does not validate its text coreference against recent models, potentially overlooking relevant work.
- Clarity issues arise regarding which methods are trained and evaluated in a zero-shot setting, leading to potentially misleading results.

### Suggestions for Improvement
We recommend that the authors improve the motivation for their architectural choices and provide a deeper investigation into the contributions of individual loss functions. Clarifying the use of gold data for text mentions versus automatically extracted image regions should be prioritized. Additionally, we suggest validating the model against recent coreference models and clearly distinguishing between trained and evaluated methods in the zero-shot setting. Consider using LEA as an evaluation metric, and ensure that all tables are larger and more readable. Finally, addressing the clarity of claims regarding the supervised contrastive loss and simplifying equations where possible would enhance the paper's presentation.