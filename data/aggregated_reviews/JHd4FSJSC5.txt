ID: JHd4FSJSC5
Title: Anchoring Fine-tuning of Sentence Transformer with Semantic Label Information for Efficient Truly Few-shot Classification
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AncSetFit, a method aimed at enhancing few-shot text classification by incorporating semantic label information, particularly in low-data scenarios. The authors address two research questions: whether small models in the SetFit method can retain efficiency while being label-aware, and how variations in textual information affect model performance. The proposed method is tested against SetFit and ADAPET, demonstrating some improvements in performance.

### Strengths and Weaknesses
Strengths:  
- AncSetFit improves upon SetFit without increasing computational demands.  
- The method is simple and effective for few-shot text classification, with sufficient support for its claims.  
- The authors have made efforts to address reviewer concerns and enhance the comprehensiveness of their work.  

Weaknesses:  
- The experimental comparisons are limited, primarily focusing on SetFit and neglecting other methods.  
- Performance relative to ADAPET is lacking, as ADAPET outperformed AncSetFit in three out of four instances.  
- The paper does not provide a comprehensive comparison with other methods, which diminishes the overall contribution.  

### Suggestions for Improvement
We recommend that the authors improve the experimental design by including comparisons with a broader range of methods beyond SetFit and ADAPET. Additionally, it would be beneficial to clarify why ADAPET is only reported in tables and not in graphs. We suggest aligning the ablation study data with the experimental results for consistency. Finally, we encourage the authors to extend the abstract with 1-2 sentences that provide a more detailed description of the proposed method.