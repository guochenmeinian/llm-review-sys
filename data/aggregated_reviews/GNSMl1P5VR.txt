ID: GNSMl1P5VR
Title: Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 5, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Visual SKETCHPAD framework, which integrates visual reasoning into multimodal language models (MLMs) through a chain-of-thought approach. The framework enables MLMs to generate plans, execute actions, and iteratively update context with visual outputs, enhancing problem-solving capabilities in math and vision tasks. The authors propose a variety of tools for visual reasoning, including code generation for drawing and specialized vision models, demonstrating significant performance improvements over text-only models.

### Strengths and Weaknesses
Strengths:
- The framework effectively incorporates visual intermediate steps, extending chain-of-thought reasoning for MLMs.
- A comprehensive evaluation across various tasks shows substantial performance gains, and the framework does not require additional training or tuning.
- The manuscript is well-organized, clear, and includes practical prompt examples.

Weaknesses:
- The title and positioning may mislead, as "sketching" primarily refers to drawing auxiliary lines for geometric problems, not a broader sketching capability.
- Concerns about the novelty of the work arise, as the incorporation of visual artifacts in reasoning is not entirely new.
- The evaluation lacks comparisons with existing chain-of-thought frameworks, and there are questions regarding the robustness and potential noise introduced by additional context.

### Suggestions for Improvement
We recommend that the authors improve clarity by refining the title to better reflect the framework's capabilities beyond geometric sketching. Additionally, the authors should elaborate on the novelty of their contributions compared to existing works, particularly addressing how their approach differs from similar methodologies. It would be beneficial to include comparisons with existing chain-of-thought frameworks in the evaluation. Furthermore, we suggest discussing the robustness of the framework and how it handles potential issues with code generation for visual outputs. Lastly, addressing the questions raised in the weaknesses section would enhance the paper's depth and clarity.