ID: F9mNL6vR27
Title: Newton Informed Neural Operator for Solving Nonlinear Partial Differential Equations
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Newton-informed Neural Operator designed to solve elliptic partial differential equations (PDEs) with nonlinear, solution-dependent forcing terms, which can exhibit multiple solutions. The authors propose approximating Newton's method using a Deep-Operator-Network architecture to enhance computational efficiency. They provide experimental results demonstrating the method's capability to recover multiple solutions effectively. Additionally, the paper introduces a model that utilizes both supervised and unsupervised training data to improve performance in solving multiple initial conditions. The authors show that while supervised data alone is insufficient, the incorporation of unsupervised data through Newton's loss function allows the model to handle a broader range of patterns. The parallelization of their approach enables efficient exploration of multiple solutions simultaneously.

### Strengths and Weaknesses
Strengths:
- The topic is timely and addresses a significant challenge in machine learning applications for PDEs, particularly in handling multiple solutions.
- The methodology combines classical Newton methods with neural networks, offering a novel approach to solving nonlinear PDEs.
- The paper includes theoretical results on approximation and generalization errors, indicating a solid foundation for the proposed method.
- The model effectively incorporates unsupervised data to improve performance and handle a wider variety of initial conditions.
- The parallelization of the approach enhances computational efficiency, allowing for the simultaneous prediction of multiple solutions.

Weaknesses:
- The paper inadequately addresses the implications of mapping inputs to a distribution of solutions, a critical aspect in the context of generative modeling.
- There is insufficient justification for why approximating Newton's method yields better results for PDEs with multiple solutions, as the deterministic nature of Newton's method does not guarantee sampling all solutions.
- Previous work on approximating Newton's method is not cited, which undermines the novelty of the approach.
- Clarity issues in exposition, including the lack of a clear definition of the Newton-informed Neural Operator and its architecture, hinder understanding.
- There is a significant discrepancy between training and testing losses, raising concerns about the method's applicability for exploring initial conditions.
- The lack of guarantees for convergence with unsupervised initial conditions remains a critical issue.
- The presentation in Table 1 is confusing and may mislead readers regarding the number of initial conditions being solved.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the implications of multiple solutions being treated as a distribution and relate this to generative modeling techniques. Additionally, the authors should clarify how approximating Newton's method leads to better approximations for PDEs with multiple solutions, possibly by providing a theoretical justification or empirical evidence. It is crucial to cite relevant prior work on approximating Newton's method to contextualize the contribution accurately. Furthermore, enhancing the clarity of the exposition, particularly regarding the architecture and derivation of the loss function, will significantly improve the paper's readability. We also suggest that the authors improve the clarity of Table 1 to accurately reflect the number of initial conditions being solved. Additionally, addressing the concerns regarding the large testing errors and providing further analysis on the convergence of the method for various unsupervised initial conditions would be beneficial. Lastly, we encourage the authors to revise Figure 3 to clarify the relationship between training loss and testing error, ensuring that the metrics are not directly compared in a misleading manner.