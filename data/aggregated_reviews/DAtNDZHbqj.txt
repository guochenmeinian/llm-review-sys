ID: DAtNDZHbqj
Title: Variational Delayed Policy Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 8, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm, Variational Delayed Policy Optimization (VDPO), which reformulates delayed reinforcement learning (RL) as a variational inference problem. The authors theoretically motivate maximizing the return of a reference policy in a delay-free Markov Decision Process (MDP) and derive an objective for behavioral cloning to learn this policy. They provide theoretical analysis, establishing state-of-the-art (SOTA) sample complexity and performance bounds, and demonstrate that VDPO achieves SOTA sample efficiency and performance in empirical evaluations across various domains.

### Strengths and Weaknesses
Strengths:  
- The writing is clear and well-structured, effectively summarizing related work and theoretical results.  
- The combination of theoretical insights and empirical validation contributes significantly to the field, positioning VDPO as a reference benchmark for delayed RL.  
- The proposed algorithm is well-motivated and demonstrates improved sample efficiency compared to existing methods.  

Weaknesses:  
- The method requires training in a delay-free environment, which may limit its applicability.  
- The use of transformers as the policy may introduce computational delays in real-world applications, which the authors do not address experimentally.  
- The assumption in Equation 10 regarding the subsequence state following a Gaussian distribution may not hold in all scenarios, potentially affecting performance.  
- Empirical results suggest that VDPO may not perform well with a large number of constant delays.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their approach by exploring extensions to stochastic delays. Additionally, it would be beneficial to address the computational implications of using transformers in real-world applications. We also suggest clarifying the assumptions behind Equation 10 and considering alternative loss functions that may perform better in non-Gaussian scenarios. Finally, providing a comparison of inference time between VDPO and baseline methods would enhance the evaluation of the algorithm's practical performance.