ID: S7THlpvH8i
Title: Normalization Layer Per-Example Gradients are Sufficient to Predict Gradient Noise Scale in Transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 4, 6, 5, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for efficiently computing the gradient noise scale (GNS) during transformer training, emphasizing the utility of normalization layers for estimating GNS without significant computational overhead. The authors propose a batch size scheduling approach based on GNS, which can reduce training time. They demonstrate that the GNS of normalization layers alone is sufficient for effective training, and they validate their method through a case study on a large language model.

### Strengths and Weaknesses
Strengths:
- The proposed method allows for efficient computation of per-example gradient norms and GNS, particularly leveraging normalization layers.
- The paper provides valuable insights into batch size scheduling, demonstrating time-saving benefits in training.

Weaknesses:
- The method's practical applicability is hindered by numerical issues, and the claimed 18% speed improvement may be overstated, as it appears contingent on specific training setups.
- The writing lacks clarity, making it difficult to follow the core ideas, and the use of Einstein notation detracts from accessibility.
- Figures are poorly labeled and captioned, complicating interpretation, and the related work section does not adequately cover other relevant methods.

### Suggestions for Improvement
- We recommend that the authors drop the Einstein notation to enhance clarity and accessibility.
- Consider rewriting the introduction to better introduce GNS, perhaps with a diagram to summarize its importance before delving into details.
- Improve figure descriptions, ensuring that captions clearly convey the main messages and relationships depicted in the figures.
- For Figures 2 and 3, clarify the significance of Li et al. and normalize the I/O costs for a more meaningful comparison.
- In Figure 5, specify the expected outcomes and key takeaways.
- Clarify the "batch size schedule" in relation to GNS estimations, detailing whether it is based on prior estimates or determined online.
- For Figure 8, utilize learning rate schedules of varying lengths to make statements like "2x" more meaningful.
- Address the need for more comprehensive experiments across different models and datasets to validate the generalizability of the method, including ablation studies.