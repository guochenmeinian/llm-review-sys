ID: E18kRXTGmV
Title: CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 7, 9, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new benchmark for Culturally-diverse Visual Question Answering (CVQA), comprising approximately 9,000 visual questions collected from 28 countries, 26 languages, and 11 scripts. The dataset includes both local language and English translations, validated by annotators with lived experience in the respective cultures. The authors benchmark eight multi-modal language models (MLLMs), revealing that state-of-the-art models perform poorly on culturally nuanced questions. The contributions include a comprehensive dataset and a suite of baselines that provide insights into the capabilities of current models.

### Strengths and Weaknesses
Strengths:
- The dataset encompasses a wide array of regions, languages, and scripts, surpassing previous works.
- It employs a systematic methodology for data collection, with validation from culturally knowledgeable annotators.
- The paper effectively situates itself within the ongoing discourse on measuring culture.
- The benchmark metrics, focusing on a 4-way multiple-choice task, are well-constructed and described.
- The inclusion of recent models like GPT4o in the baselines adds relevance.
- The writing is clear and the structure is logical.

Weaknesses:
- The primary evaluation setting as a 4-way multiple-choice task is overly simplistic and may not foster progress towards culturally-inclusive MLLMs.
- The exploration of open-ended generation tasks is limited and lacks detailed results.
- There is no analysis on the impact of option ordering in multiple-choice settings.
- The training of annotators is vaguely described, raising concerns about quality control in the dataset.
- The justification for including CLIP and mCLIP among the baseline models is insufficient.
- Closed-source models are analyzed only partially, limiting the comprehensiveness of the findings.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by expanding the evaluation beyond the simplistic 4-way multiple-choice task to include more realistic scenarios. Additionally, we suggest that the authors provide a more thorough exploration of the open-ended generation task, including detailed results in tabular form. It would be beneficial to include ablation studies to analyze the effects of option ordering in multiple-choice tasks. We also encourage the authors to clarify the training process for annotators and consider implementing more rigorous quality control measures. Justifying the inclusion of CLIP and mCLIP in the baseline models with a brief explanation would enhance clarity. Finally, extending analyses to closed-source models, if feasible, would provide a more comprehensive understanding of model performance.