ID: Tck41RANGK
Title: MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and Provable Convergence
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 4, 6, 3, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MicroAdam, a memory-efficient optimizer based on Adam, which utilizes gradient compression through Top-K sparsification and quantization, alongside an error feedback vector. The authors provide theoretical convergence guarantees and demonstrate competitive performance on fine-tuning tasks with BERT and LLaMA, achieving lower memory usage compared to AdamW.

### Strengths and Weaknesses
Strengths:
- The paper includes theoretical convergence guarantees under commonly accepted assumptions, moving beyond heuristics used in previous memory-efficient Adam variants.
- Empirical results indicate that MicroAdam performs comparably to uncompressed AdamW while utilizing less memory.

Weaknesses:
- The experiments are limited to fine-tuning tasks, raising questions about effectiveness in LLM pre-training or non-language tasks.
- The theoretical analysis relies on AMSGrad rather than Adam, which may mislead regarding the optimizer's performance.
- The complexity of the compressors used may hinder practical implementation and impact.
- The presentation lacks clarity, particularly in explaining the motivation behind certain figures and the specifics of the algorithm.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, particularly by elaborating on the significance and novelty of the compression design in Algorithm 1. Additionally, we suggest including more extensive experimental results, particularly for LLM pre-training and non-language tasks, to better validate the optimizer's effectiveness. It would also be beneficial to clarify the relationship between the theoretical and practical memory consumption during actual training scenarios. Lastly, addressing the convergence speed in relation to AMSGrad and providing insights into improving computational efficiency would enhance the paper's contributions.