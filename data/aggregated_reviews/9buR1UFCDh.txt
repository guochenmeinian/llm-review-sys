ID: 9buR1UFCDh
Title: Iterated Deep Q-Network: Efficient Learning of Bellman Iterations for Deep Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 7, 6, 4, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Iterated Deep Q-Network (iDQN), a novel DQN-based algorithm that incorporates multiple Bellman iterations into the training loss to enhance learning efficiency. The authors propose using multiple Q networks to build targets for successive online networks, addressing the limitations of traditional RL methods that consider only a single Bellman iteration. The empirical evaluation demonstrates iDQN's superiority over various baselines on 54 Atari 2600 games, highlighting improvements in approximation error and performance. Additionally, the paper analyzes the performance of iIQN compared to IQN in Atari games, clarifying that iDQN can enhance the performance of various DQN variants, particularly in the game Frostbite, despite IQN initially outperforming iIQN in StarGunner.

### Strengths and Weaknesses
Strengths:
- The iDQN algorithm introduces a novel approach to incorporate multiple Bellman iterations, effectively addressing traditional RL limitations.
- The paper provides a well-structured review of relevant literature, enhancing understanding of iDQN's motivation and its relationship with other methods.
- Empirical results show iDQN's superiority over DQN and Random Ensemble Mixture, providing strong evidence of its effectiveness.
- The authors provide a clear rationale for the experimental design and the purpose of the figures, effectively addressing reviewer concerns regarding the performance metrics of iIQN and IQN.

Weaknesses:
- The paper lacks comparisons with widely-known baselines, such as R2D2, which would strengthen the evaluation.
- Some explanations, particularly regarding the loss function and graphical representations, could be more concise and intuitive.
- The choice of $K$ significantly impacts behavior, and the paper would benefit from formal analysis to clarify expectations regarding larger $K$.
- The initial performance of iIQN in StarGunner is less efficient than IQN, raising questions about its overall effectiveness.
- The lack of mention regarding the quantile settings in the initial analysis may lead to misunderstandings about the comparative performance of the algorithms.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the loss function explanation and graphical representations to enhance reader understanding. Additionally, including comparisons with more widely-known baselines, such as R2D2, would provide a more comprehensive evaluation of iDQN's performance. We suggest incorporating formal analysis regarding the impact of $K$ on the algorithm's behavior, including its variance and transition kernel characteristics. Furthermore, we recommend that the authors explicitly state the quantile settings used for iIQN and IQN in the main text to avoid confusion. Lastly, consider providing a more comprehensive analysis across all Atari games to strengthen the claims regarding the performance of iIQN relative to IQN.