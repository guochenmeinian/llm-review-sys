ID: Gij638d76O
Title: Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for automatically identifying the optimal latent geometry for machine learning models through a neural latent geometry search (NLGS). The authors propose a latent geometry composed of a product of constant curvature model spaces, utilizing the Gromov-Hausdorff distance to evaluate different metric spaces. A graph search space is constructed, where nodes represent latent space product manifolds and edges are weighted by the inverse GH distance, with Bayesian optimization employed to find the optimal geometry. The Gromov-Hausdorff Bayesian optimization (GH BO) method demonstrates superior performance compared to naive baselines and fully connected graph Bayesian optimization in both synthetic and real-world experiments. Experimental results validate the effectiveness of the proposed approach.

### Strengths and Weaknesses
Strengths:
- The authors introduce a modeling approach that represents latent geometry using the Cartesian product of Riemannian manifolds with constant curvature.
- NLGS is a well-motivated and potentially impactful framework for model tuning, with GH BO likely to become a valuable tool in hyperparameter search.
- The construction of a graph search space effectively facilitates the search for optimal latent geometries.
- The theoretical framework and methodology for constructing the Gromov-Hausdorff search space are original and creative.
- The paper is well-written, with clear presentation and accessible appendices.
- Empirical results convincingly show that the proposed method outperforms reasonable baselines.

Weaknesses:
- There are concerns regarding potential overfitting in Section 3.1 due to the similarity between the objective function $L_{T,A}(g)$ and the downstream task's loss function.
- The rationale for using the Cartesian product of Riemannian manifolds is insufficiently explained, lacking insights into the advantages of this choice.
- The choice of Gromov-Hausdorff distance as the metric lacks clear justification regarding its relevance to ML model performance. The relationship between the objective function and the search space needs more exploration.
- The experiments lack reproducibility due to insufficient detail on network architectures and the absence of code, which hinders future adoption.
- The quality of the models used in experiments is unclear, raising questions about the scalability of GH BO in realistic settings.
- Several theoretical simplifications reduce the algorithm's complexity and interest, such as restricting the search space to products of manifolds of dimension 2 and using only unitary curvature values.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the rationale behind utilizing the Cartesian product of Riemannian manifolds, including potential advantages such as efficient computation. Additionally, we suggest providing a clearer connection between the Gromov-Hausdorff distance and ML model performance, along with conducting an ablation study comparing Bayesian optimization with uniform edge weights against GH weights to assess the impact of the weights. To enhance reproducibility, please include detailed descriptions of network architectures and release the code. Clarifying the quality of models used in experiments will also help assess the scalability of GH BO. Lastly, consider addressing the simplifications made in the theoretical framework either by explicitly stating them as limitations or by justifying them as necessary for the algorithm's current implementation.