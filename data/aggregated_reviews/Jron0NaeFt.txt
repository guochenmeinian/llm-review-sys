ID: Jron0NaeFt
Title: The Empirical Impact of Data Sanitization on Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 6, 6
Original Confidences: 2, 5, 4

Aggregated Review:
### Key Points
This paper empirically analyzes the effects of data sanitization on language models (LLMs) across various benchmark tasks, including comprehension question answering, entailment, sentiment analysis, and text classification. The authors investigate the performance impact of redacting personally identifiable information (PII) and other sensitive entities from training datasets, revealing that the impact of redaction varies by task. They propose a strategy to repair already redacted datasets using content-based subsampling, contributing to the understanding of the trade-offs between privacy protection and model performance.

### Strengths and Weaknesses
Strengths:
- The paper presents a thorough analysis of data sanitization's impact on LLMs across a wide range of NLP and GenAI benchmark tasks.
- The authors conduct robust experiments using different language models and datasets, revealing that data sanitization minimally impacts model performance for most tasks, but significantly affects extractive question answering.
- The structured experimental methodology and empirical findings provide valuable insights, particularly regarding the resilience of tasks like sentiment analysis compared to question answering.

Weaknesses:
- The findings may not generalize to all types of LLMs and tasks, as experiments were conducted on a specific set of models and datasets.
- The paper lacks detailed descriptions of the redaction techniques and their differences across datasets.
- The originality of the proposed redaction repair strategy could be further developed, as it largely presents empirical observations.

### Suggestions for Improvement
We recommend that the authors improve the paper by expanding the experimental scope to include comparisons with other redaction techniques, such as pseudo-anonymization and entity masking. Additionally, providing a more detailed explanation of the methodology, datasets, and results analysis would enhance clarity. We suggest that the authors simplify the repair strategy and include a more direct discussion of how findings could apply to privacy regulation compliance, as well as expanding the discussion section to cover implications, limitations, and future research directions.