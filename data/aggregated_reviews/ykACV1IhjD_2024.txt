ID: ykACV1IhjD
Title: Controlling Continuous Relaxation for Combinatorial Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 6, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel continuous relaxation annealing (CRA) strategy aimed at addressing the local optima and rounding issues prevalent in existing unsupervised learning (UL)-based solvers for combinatorial optimization (CO). The authors propose an auxiliary function to enhance training, which empirically shows improved solution quality and accelerated learning. The method is designed to be generalizable across various CO problems, although its theoretical foundations and empirical evaluations primarily build upon the PI-GNN framework. Additionally, the paper explores the CRA approach for solving Traveling Salesman Problems (TSP), demonstrating its effectiveness through experiments on the TSPLIB dataset, achieving an average performance ratio (ApR) exceeding 0.9 across various instances, with notable success in finding the global optimal solution for the "burma14" problem. The authors acknowledge that the optimal $p$ value may vary based on GNN architecture and problem structure, suggesting the need for a comprehensive ablation study in future work.

### Strengths and Weaknesses
Strengths:  
1. The CRA method is sound, easy to implement, and effective, demonstrating consistent improvements over PI-GNN.  
2. The paper is well-written and provides extensive qualitative and quantitative analyses.  
3. The approach is dataless, enhancing its generalizability across different graph distributions.  
4. The CRA approach shows strong performance, achieving high ApR values across multiple TSP instances.  
5. The authors effectively demonstrate the generalizability of CRA to UL-based solvers and other relaxation-based methods.  

Weaknesses:  
1. The reliance on PI-GNN limits the generalizability of the findings, as the framework's effectiveness may not extend to other solvers.  
2. The paper lacks a thorough discussion on hyper-parameter tuning, which poses challenges despite the absence of training data.  
3. The exploration of local minima and rounding issues is not sufficiently detailed, and the paper does not adequately address the implications of over-parameterization.  
4. The evaluation is limited to specific graph types (d-regular graphs), neglecting performance on other graph structures and densities.  
5. The paper does not include comparisons with several relevant baselines, such as ILP solvers and other dataless methods.  
6. There is confusion regarding the distinction between the first-order Taylor expansion used in iSCO and the gradient approach in CRA, which may undermine the clarity of the paper.  
7. The paper is perceived as potentially out-of-date, especially in light of recent advances in sampling-based methods, raising questions about its significance in the current landscape of Type II solvers.  

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by conducting a broader evaluation beyond the PI-GNN framework, including comparisons with various solvers and graph types. Additionally, a more detailed discussion on hyper-parameter tuning and its implications should be included. The authors should also address the convergence speed under different parameter settings and provide parameter sensitivity analyses for a wider range of CO problems. Furthermore, we suggest that the authors clarify the feasibility of solutions produced in their computational study and enhance the writing for better clarity, particularly regarding the non-convexity and rounding issues associated with continuous relaxations of CO problems. We recommend improving the clarity of the differences between the first-order Taylor expansion in iSCO and the gradient approach in CRA to address reviewer confusion. It would also be beneficial to provide a more detailed comparison of CRA and iSCO, including hyperparameter settings, to substantiate the claims of CRA's superiority. Lastly, we suggest that the authors consider expanding the theoretical analysis to better contextualize the significance of their work within the broader field of combinatorial optimization.