ID: W0okTgsPvM
Title: Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 1, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Multimodal Task Vectors (MTV) to enhance the in-context learning (ICL) capabilities of large multimodal models (LMMs), which face limitations in context length when processing multimodal data. MTV compresses many-shot examples into compact implicit representations within the model's attention heads by calculating mean activations across multiple inference iterations and selecting optimal locations using an adapted REINFORCE algorithm. This approach enables LMMs to utilize more examples than their context length allows, improving performance on various vision-and-language tasks without finetuning. Experiments demonstrate that MTV scales with more examples, generalizes to similar out-of-domain tasks, and works alongside explicit few-shot examples.

### Strengths and Weaknesses
Strengths:
1. MTV is an efficient and lightweight method for utilizing training data with off-the-shelf LMMs.
2. The writing is generally clear and easy to follow.
3. The results are promising, with evaluations across two tasks, four benchmarks, and three models, including an efficiency assessment.

Weaknesses:
1. Minor: The dataset could include more examples, such as MMMU, OpenFlamingo, and Otter.
2. Minor: Additional models could be considered, such as Mantis.
3. The paper lacks a clear discussion on how the method is uniquely suited for multimodal QnA compared to text-only domains.
4. Claims regarding the expense of image embeddings need better substantiation through analysis.
5. There is no evidence presented showing degradation of model performance with increasing context length.
6. The necessity of REINFORCE-based selection is unclear; a comparison with simply replacing all heads with mean activations would be beneficial.
7. The interpretability benefits of many shots in context versus weight space need further exploration.
8. The empirical results are limited, focusing mainly on comparisons with vanilla ICL without demonstrating performance on long-context tasks where vanilla ICL fails.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset and model comparisons, particularly by including references to concurrent works and elaborating on the differences between methods. Additionally, we suggest providing a more detailed analysis of the token space for images and context lengths in benchmarks. The authors should also clarify the motivation for using REINFORCE-based selection and explore the implications of losing interpretability when moving shots to weight space. Finally, expanding the empirical results to include a wider range of longer-context multimodal benchmarks would strengthen the paper.