ID: KexMPvrFgJ
Title: Learning Depth-regularized Radiance Fields from Asynchronous RGB-D Sequences
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 7, 5, 6, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for training Neural Radiance Fields (NeRF) using asynchronous RGB and depth images in UAV city modeling scenarios. The authors propose a novel time-pose function to model the continuous RGB camera trajectory, enabling the optimization of depth image poses based on RGB poses. They introduce a 3-stage optimization pipeline that incorporates depth supervision, demonstrating improved performance over previous RGB-only methods. Additionally, a new synthetic dataset is created for evaluation.

### Strengths and Weaknesses
Strengths:
1. The paper addresses the significant challenge of synchronizing RGB and depth measurements in large-scale UAV scenarios.
2. The novel time-pose function effectively leverages the relationship between RGB and depth captures, enhancing NeRF training.
3. Experimental results validate the proposed method's effectiveness compared to RGB-only approaches, with ablation studies highlighting the drawbacks of using unsynchronized depth images.
4. The paper is well-structured and clearly articulated, making it accessible to readers.

Weaknesses:
1. The comparison with baseline methods like Mega-NeRF and NeRF-W is not entirely fair, as these only utilize RGB images. The authors should demonstrate that the problem cannot be easily addressed through trivial methods, such as joint pose optimization of depth images.
2. Most experiments rely on synthetic datasets, with limited real-world results presented. More real-world data would strengthen the findings.
3. A runtime comparison and analysis are lacking, which would provide additional insights into the method's efficiency.

### Suggestions for Improvement
We recommend that the authors improve the fairness of their experimental comparisons by including baselines that utilize joint optimization of depth image poses. Additionally, it would be beneficial to present more results from real-world datasets to validate the proposed method's applicability. We also suggest including a runtime analysis to assess the efficiency of the approach. Finally, clarifying the assumptions regarding pose estimation and discussing related work in the context of pose optimization would enhance the paper's depth and rigor.