ID: tBwRbgsol1
Title: Replicable Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 6, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on replicable reinforcement learning (RL), addressing the replicability crisis in the field by focusing on various RL components such as the Markov Decision Process (MDP), value function, and policy. The authors propose definitions of rho-replicable and introduce two algorithms: Rep-PVI and Rep-RMAX, demonstrating their reproducibility properties through proofs. They suggest that replicability may not be computationally impossible even in finite-sized MDPs and emphasize that their approach involves clever randomized techniques to ensure formal replicability. The effectiveness of the proposed algorithms is validated through simple experiments, although these experiments are noted to be limited.

### Strengths and Weaknesses
Strengths:
1. The paper introduces the novel concept of replicable reinforcement learning, which is significant given the challenges of reproducibility in RL.
2. The mathematical and theoretical analysis of the proposed methods is strong and convincing.
3. The authors provide a clear framework for discussing replicability, which opens new research opportunities and facilitates understanding of complex concepts.
4. The algorithms proposed show promise in stabilizing learned policies in stochastic settings.

Weaknesses:
1. The experimental evaluation is insufficient, lacking comprehensive tests in standard RL environments, which diminishes the demonstration of the proposed methods' effectiveness.
2. Some definitions, such as the replicability of the MDP and the value function, are considered vague or artificial.
3. The treatment of certain concepts, like rSTAT, lacks clarity and may confuse readers without a strong background in the topic.
4. The assumptions made, such as finite state space and known deterministic reward functions, may be overly restrictive, potentially limiting the generality of the findings.
5. The organization of the paper could be improved for better cohesion and clarity regarding the relationships among definitions and lemmas.
6. The paper does not sufficiently address the implications of internal randomness in the algorithms discussed.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including more extensive experiments in standard RL environments, such as classical navigation tasks or the MiniGrid suite, to better illustrate the trade-offs between replicability and performance. Additionally, addressing the limitations posed by the strong assumptions in their definitions would enhance the generality of their contributions. We suggest that the authors improve clarity by providing explicit definitions of key terms, such as "sampling failure probability" and "replicability in policy learning," earlier in the paper, preferably in the abstract or introduction. Furthermore, elaborating on the concept of rSTAT, including its polynomial time complexity and its role in the proof of Theorem 2.1, would enhance the paper's depth and accessibility. Lastly, we encourage the authors to provide a brief discussion on the applicability of their methods to deep RL settings, exploring how insights from their work could reduce performance variance in existing algorithms.