ID: FXdMgfCDer
Title: Replay-and-Forget-Free Graph Class-Incremental Learning: A Task Profiling and Prompting Approach
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 5, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to graph class-incremental learning (GCIL) called Task Profiling and Prompting (TPP), which addresses the challenge of unknown task identities. The authors propose a method that utilizes Laplacian smoothing-based task profiling for accurate task ID prediction and a graph prompting technique to mitigate catastrophic forgetting. Extensive experiments on four datasets demonstrate that TPP achieves 100% task ID prediction accuracy and significantly outperforms existing methods in average CIL accuracy.

### Strengths and Weaknesses
Strengths:
- The use of Laplacian smoothing for task ID prediction and graph prompting to capture task-specific knowledge is innovative and contributes to the field of graph class-incremental learning.
- The method is both replay-free and forget-free, requiring only a single training of a GNN.
- The paper is well-motivated, clearly written, and provides theoretical support for its claims.

Weaknesses:
- The reliance on the graph transduction setting limits the generalizability of the proposed method to other settings.
- There is a lack of clear definitions for different categories of methods in graph continual learning, and empirical comparisons with training separate GNNs are missing.
- Some ablation studies are absent, and the experimental setup lacks sufficient detail.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions for the categories of methods in graph continual learning and include descriptions of the baselines. Additionally, the authors should provide empirical comparisons between graph prompting and training separate GNNs for each task, as well as clarify the differences in task prediction between image and graph data. It would be beneficial to include experiments under different task formulations to validate the robustness of the task prediction performance. Furthermore, the authors should address the observed low performance of TPP without task prediction and provide a detailed analysis of the phenomenon where TPP's performance stabilizes with larger graph prompts. Lastly, we encourage the authors to release the source code to enhance transparency and allow for verification of results.