ID: CiRHWaRbp0
Title: Benchmarking Robustness to Adversarial Image Obfuscations
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 7, 7, 5, 8, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark dataset consisting of 22 transformations of ImageNet to evaluate the robustness of classifiers against adversarial obfuscations and human attackers, addressing the challenges of image corruption. The authors evaluate 33 pretrained ImageNet models, demonstrating that scaling data and parameters enhances generalization. They propose using a 2-step PGD for adversarial training, referencing parameters from existing literature for strong ImageNet performance. The study reveals that distribution shift algorithms and adversarial training do not yield performance improvements, nor does training on a different set of augmentations. The authors acknowledge the limitations of their dataset, particularly regarding images that may be too corrupted to classify, and emphasize the importance of label integrity in their benchmarking goals. They also provide comprehensive benchmarks of pretrained models and analyses of augmentation techniques, finding that MixUp is typically the most effective. The inclusion of a datasheet and a public repository for the dataset enhances transparency.

### Strengths and Weaknesses
Strengths:
- The dataset addresses a significant real-world issue of robustness against image obfuscations, relevant to the NeurIPS community.
- The design is informed by actual attacks experienced by Google systems.
- The authors have clarified their adversarial training methodology and improved the paper's formulation.
- Comprehensive benchmarks of pretrained models and analyses of augmentation techniques are provided.
- The authors acknowledge the unsolved nature of the explored problem, indicating ongoing research potential.
- The inclusion of a datasheet and a public repository for the dataset enhances transparency.
- The authors are open to creating a public leaderboard for further research engagement.

Weaknesses:
- Concerns arise regarding the adversarial training results, particularly the unclear distinction between "2-step FGSM-based PGD" training and the lack of mitigation strategies for robust overfitting.
- The manual inspection of obfuscation examples is limited in sample size and lacks independent verification.
- The dataset may overestimate risks by including images that are too corrupted to be useful for evaluating safety systems against human attackers.
- The authors need to implement the evaluation in PyTorch, as suggested by reviewers.
- The current download process for the dataset is cumbersome, requiring users to download categories individually.
- The paper does not sufficiently justify the choice of obfuscation methods and hyperparameters, raising questions about their effectiveness against future threats.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the adversarial training methodology by explicitly defining whether they used FGSM or PGD and addressing robust overfitting mitigation strategies. Additionally, increasing the sample size for the manual inspection of obfuscations and involving an independent third party for validation would enhance credibility. We suggest openly discussing the limitation of including overly corrupted images to improve dataset transparency. Implementing the evaluation in PyTorch, as requested, would also be beneficial. Finally, we encourage the authors to provide a script or a single link for downloading the entire dataset at once to enhance user experience and to compare model performance against important corruptions from ImageNet-C, considering the implications of different class renditions from ImageNet-R. Providing a clearer definition of the 'worst-case' accuracy metric in the main paper would aid reader comprehension.