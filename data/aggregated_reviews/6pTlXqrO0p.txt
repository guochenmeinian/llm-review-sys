ID: 6pTlXqrO0p
Title: xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 4, 3, 4, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents xRAG, a context compression method for retrieval-augmented generation (RAG) that maps document embeddings into the language model's representation space, achieving extreme compression to a single token. The authors demonstrate that xRAG significantly outperforms non-retrieval methods and previous context compression techniques while maintaining performance comparable to traditional RAG, thus saving over three times the RAG FLOPs. 

### Strengths and Weaknesses
Strengths:
- The method achieves a high compression rate from long contexts to a single token.
- The only trainable component comprises less than 0.1% of the LLMâ€™s parameters, making it an efficient plug-in.
- xRAG matches or exceeds the performance of the original RAG with just one token per document, with reasonable analyses of the Resilience Rate and Boost Rate.
- The paper is well-written and presents clear findings.

Weaknesses:
- The main experiments are limited to one retrieved document with an average length of 175 tokens, which is considered too short for modern LLMs; demonstrating effectiveness on longer contexts with multiple documents is crucial.
- The total training FLOPs of the projector need consideration, as the forward process during training could incur significant costs, especially with large sample sizes.
- The projector's tight coupling with the retriever and LM raises concerns about generalization to other LLMs where direct training from the LM's feedback is not possible.
- The inclusion of NQ and TriviaQA in the Context-aware Instruction Tuning data contradicts the claim of no need for dataset-specific tuning.
- Missing comparisons with other context compression methods, such as Gist-COCO, and the potential benefits of adding more compression tokens are not addressed.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by testing xRAG on longer contexts and multiple documents to validate its real-world applicability. Additionally, it is essential to provide a detailed analysis of the total training FLOPs associated with the projector and clarify how the projector's architecture impacts performance. The authors should also address the potential for bias in projector tuning and its implications for hallucinations. Lastly, we suggest including comparisons with a broader range of context compression methods and exploring the effects of varying the number of compression tokens on performance.