ID: jfwRLudQyj
Title: A Toolkit for Reliable Benchmarking and Research in Multi-Objective Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 7, 7, 7, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MO-Gymnasium, an environment suite for multi-objective reinforcement learning (MORL), along with a collection of state-of-the-art MORL algorithms and benchmarking results. The authors aim to enhance the reliability and credibility of MORL research by providing standardized APIs and a well-documented open-source codebase. Additionally, the paper emphasizes the relevance of the toolkit to the research community and includes an extended discussion of MORL algorithms, clarifying the distinction between minimizing and maximizing objectives, and detailing the hypervolume reference point. The authors also provide proof-of-concept empirical analyses to demonstrate the toolkit's capabilities while acknowledging the need for further hyperparameter optimization (HPO) and ablation studies.

### Strengths and Weaknesses
**Strengths:**
- The introduction of MO-Gymnasium offers a standardized set of environments that facilitate the rapid construction and comparison of MORL algorithms.
- The paper includes a robust set of benchmark results and comparisons using the openrlbenchmark framework, enhancing scientific rigor.
- The emphasis on reproducibility is notable, with a clean API and well-maintained documentation.
- Comprehensive updates enhance clarity, particularly in detailing algorithms, environments, and presentation style, which supports the toolkit's potential for future experimental analyses.

**Weaknesses:**
- The paper lacks a discussion on the specific structures that make environments suitable for MORL benchmarks, making it difficult to discern differences among proposed environments.
- There is insufficient detail on the baseline algorithms, particularly regarding their suitability for various tasks and their performance scaling with the number of objectives.
- The absence of thorough evaluations of existing algorithms limits the paper's immediate applicability for comparative studies.
- The lack of ablation studies and hyperparameter influence analysis restricts the understanding of algorithm performance.

### Suggestions for Improvement
We recommend that the authors improve the rationale behind the choice of tasks for the MORL benchmarks and provide clearer distinctions among the environments. Additionally, a brief anatomy of the algorithms should be included to elucidate their relationships to the tasks and their performance characteristics. Conducting a thorough comparison of existing algorithms through HPO and ablation studies would significantly strengthen the paper. Providing approximate Pareto fronts identified by the baselines would also enhance user assessment of their methods. Lastly, we suggest including real-world problem scenarios to illustrate the applicability of the proposed toolkit and expanding Section 5.1 to better explain the implemented algorithms. Furthermore, the authors should explicitly discuss the implications of maximizing versus minimizing objectives to improve clarity.