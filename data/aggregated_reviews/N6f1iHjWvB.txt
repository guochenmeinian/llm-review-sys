ID: N6f1iHjWvB
Title: Automatic Analysis of Substantiation in Scientific Peer Reviews
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 6
Original Ratings: -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on automatic claim substantiation evaluation in scientific reviews, specifically focusing on the NLP domain. The authors propose a new dataset, SubstanReview, consisting of 550 annotated reviews, which serves as a foundation for claim-evidence pair extraction. The paper aims to address the critical issue of review quality by measuring the substantiation of claims made in peer reviews, thereby enabling automatic quality control measures.

### Strengths and Weaknesses
Strengths:
- The introduction of the SubstanReview dataset, the first annotated dataset for this task, is a valuable resource for the NLP community.
- The paper provides a clear description of data collection guidelines and inter-annotator agreements, demonstrating the dataset's rigor.
- The task of evaluating substantiation in peer reviews is highly relevant and applicable beyond the NLP domain.

Weaknesses:
- The dataset's limited size (550 reviews) raises concerns about its generalizability and the representativeness of the findings across different review styles and regions.
- Some claims in the paper lack sufficient support, particularly regarding the substantiation score and the relationship between claims and evidence.
- Potential biases exist due to the authors' involvement in the annotation process and the use of a sentiment classifier based on tweets.

### Suggestions for Improvement
We recommend that the authors improve the dataset's robustness by expanding its size and diversity to enhance generalizability. Additionally, we suggest providing a more thorough discussion of the substantiation score and its implications. Addressing the potential biases in the annotation process and justifying the use of a sentiment classifier based on tweets would strengthen the paper. Finally, clarifying the contribution claims to avoid misinterpretation of the dataset's applicability would be beneficial.