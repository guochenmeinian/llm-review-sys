ID: SRWs2wxNs7
Title: U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 7, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to enhancing diffusion models for image generation using Transformers, specifically through the introduction of U-shaped Diffusion Transformers (U-DiTs). The authors propose employing token downsampling within the self-attention mechanism of U-Net architectures to reduce computational costs while maintaining or improving image generation quality. The paper demonstrates that U-DiTs outperform traditional isotropic diffusion transformer models with significantly lower computational requirements, supported by extensive experimental results.

### Strengths and Weaknesses
Strengths:
- The introduction of token downsampling in the self-attention process of U-Net style transformers is a novel approach that addresses key efficiency issues in image generation tasks.
- Extensive experimental results show U-DiTs achieving better performance metrics than larger, more computationally intensive models.
- The reduced computational demand of U-DiTs could make high-quality image generation more accessible and cost-effective.

Weaknesses:
- The paper lacks important baseline comparisons, failing to include other efficient DiT models such as Pixart-sigma, limiting the fairness of comparisons.
- The training data is restricted to ImageNet, with no large-scale experiments on datasets like Laion or JourneyDB, which limits the work's potential impact.
- The operation of token downsampling is unclear, particularly the process of merging downsampled tokens, which requires more detailed explanation.

### Suggestions for Improvement
We recommend that the authors improve the comparison by including additional efficient DiT models as baselines to enhance the fairness of their evaluations. Expanding the training data to include large-scale datasets like Laion and JourneyDB would also strengthen the work's impact. Furthermore, we urge the authors to clarify the token downsampling process, particularly the merging of tokens, to enhance the understanding of this crucial aspect of their methodology.