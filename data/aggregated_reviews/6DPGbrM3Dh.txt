ID: 6DPGbrM3Dh
Title: Question Difficulty Consistent Knowledge Tracing
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for knowledge tracing, termed Question Difficulty Consistent Knowledge Tracing, which shifts the focus from predicting whether a student can answer a specific question to predicting their ability to answer any question of a given skill at a specific difficulty level. The authors utilize a LSTM-based network to achieve this, aiming to address the cold-start problem in knowledge tracing. The method is evaluated through extensive experiments, including ablation studies and comparisons against baseline models.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and structured, clearly conveying its ideas and contributions.
- It provides extensive experimental evaluations across multiple datasets, demonstrating the model's performance.
- The approach effectively addresses the cold-start problem, which is a significant issue in knowledge tracing.

Weaknesses:
- The novelty of the technique is limited, as replacing question IDs with difficulty levels is intuitive and results in only incremental improvements.
- The rationale for excluding question IDs from the model is not adequately justified, and the experiments conducted appear incomplete without this consideration.
- The paper lacks a thorough discussion on potential data leakage and the implications of the model's performance metrics, which may not translate effectively to real-world applications.

### Suggestions for Improvement
We recommend that the authors improve the justification for excluding question IDs from the model and consider including both difficulty levels and question IDs as input features in their experiments. Additionally, we suggest providing more detailed statistics about the dataset in Table 1, enhancing explanations for the plots in Section 4.4, and addressing the claims made in the introduction with experimental evidence. To strengthen the paper's impact, consider conducting an online evaluation with A/B testing to demonstrate practical implications and sharing source code for reproducibility. Finally, employing radar plots for result visualization may help clarify the trade-offs among different methods.