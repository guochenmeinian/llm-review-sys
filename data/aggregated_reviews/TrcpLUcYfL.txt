ID: TrcpLUcYfL
Title: Block-local learning with probabilistic latent representations
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 2, 6, 7, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 5, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a block-wise learning strategy for training neural networks, addressing the weight transport and weight locking issues inherent in standard backpropagation. The authors propose a parametrized twin network to compute error signals through local loss functions, utilizing a probabilistic framework and variational inference to facilitate forward and backward message passing between blocks. The proposed method is evaluated on various datasets, including MNIST, F-MNIST, and CIFAR-10, demonstrating comparable performance to backpropagation in some cases, but with notable degradation in others. The framework is also shown to scale to complex architectures, including convolutional and transformer-based models.

### Strengths and Weaknesses
Strengths:
- The paper tackles an interesting problem by framing block-wise local training within a probabilistic context.
- The probabilistic interpretation and message passing view of neural network training are well articulated.
- Empirical results show the efficacy of the proposed method across different architectures and tasks, suggesting potential for biologically plausible credit assignment.
- The paper is well-written and presents an interesting combination of belief propagation and backpropagation.
- Experiments indicate that the proposed method can scale to complex architectures.

Weaknesses:
- The writing and structure are often unclear, hindering a deep understanding of the proposed approach.
- Key details regarding the learning rule are relegated to the appendix, which detracts from clarity.
- The algorithm exhibits significant overfitting on CIFAR-10 without adequate explanation or discussion of contributing factors.
- The novelty is limited due to similarities with existing frameworks like Local Representation Alignment and others.
- Key citations in related work are missing, and the experimental setup lacks comparisons with several state-of-the-art bio-plausible approaches.
- Clarity issues exist in sections 3.1 and 3.4, making it difficult to reproduce results.
- The algorithm's convergence analysis is insufficient, and the reported performance on CIFAR-10 suggests optimization issues.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by:
- Presenting the derivation of the learning rule more clearly in the main text, as suggested in the review.
- Clearly stating the intractability of certain components of the ELBO gradient and better explaining the heuristic "data mixing."
- Distinguishing between the parameters of the feedback and feedforward networks to avoid ambiguity.
- Removing claims about parallelization of forward and backward passes across layers, as the theoretical foundation does not support this assertion.
- Including a detailed pseudo-algorithm for the proposed training procedure to enhance understanding.
- Investigating the learning dynamics of earlier blocks to determine if they are effectively learning, as current results suggest they may not be.
- If uncertainty estimation is a focus, we suggest exploring the MVTech dataset for anomaly detection capabilities.
- Discussing the limits of biological plausibility regarding the twin architecture, particularly concerning parameter requirements.
- Improving clarity in sections 3.1 and 3.4, possibly by providing a clear description of the algorithm in pseudo-code in the supplementary material.
- Comparing their approach against existing frameworks such as Local Representation Alignment, Difference Target Propagation, and predictive coding methods to enhance the paper's contribution.
- Reporting model performance across various hyperparameter settings and providing analysis on convergence, including the update angle compared to backpropagation.
- Addressing the optimization issues indicated by the low accuracy on CIFAR-10 to strengthen the paper's claims.