ID: JNl6h3U3oW
Title: ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization," which introduces a novel post-training reparameterization method that enables multiplication-free operations in large language models (LLMs). The authors achieve higher accuracy than existing quantization methods through a modified Adaptive Power of Two (APoT) quantization and a quick quantization process that avoids retraining. The method enhances efficiency and reduces computational demands while maintaining accuracy, making it suitable for resource-constrained edge devices. Additionally, the ShiftAddLLM quantization scheme differentiates itself from MSFP by eliminating shared exponents and mantissa bits, using distinct power-of-two integers for scaling factors. The authors argue that their method achieves lower quantization errors and KL divergence compared to MSFP, demonstrating superior performance at 2, 3, and 4-bit quantization levels. They emphasize their focus on GPU acceleration through dedicated CUDA kernel support while acknowledging the potential energy savings of ASIC designs without making them the primary target of their work.

### Strengths and Weaknesses
Strengths:
1. The innovative approach reduces both activation and weight errors through a Multi-Objective Optimization method, which is a first in this context.
2. The automated bit allocation strategy simplifies the application of this new method to LLMs.
3. The proposed method shows significant performance improvements over existing quantization strategies, particularly in terms of perplexity scores and quantization errors.
4. The authors provide clear experimental results demonstrating that ShiftAddLLM outperforms MSFP in terms of quantization errors and KL divergence.
5. The focus on GPU acceleration and the development of dedicated CUDA kernel support is well-articulated.

Weaknesses:
1. The reliance on specialized hardware support and the need for further testing in diverse scenarios limit the method's applicability.
2. The impact of batch size on performance is not discussed, which is crucial for throughput considerations.
3. The paper lacks a comprehensive discussion on state-of-the-art quantization methods and does not adequately clarify the ASIC design implications.
4. There is insufficient discussion on the limitations of the proposed method for different target systems.
5. The manuscript appears to mix multiple messages, leading to potential confusion regarding its core contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, particularly by distinguishing between the generality of the proposed method and its specific application to LLMs. Additionally, the authors should include a discussion on the impact of batch size on performance and throughput, as well as provide comparisons with recently published state-of-the-art quantization methods such as FlexRound and OmniQuant. It would also be beneficial to clarify the ASIC design implications and provide detailed explanations about the Eyeriss architecture. Furthermore, we suggest that the authors improve the clarity of the relationship between ShiftAddLLM and MSFP by explicitly discussing the significance of the Shift and Add operations in the context of their optimization method. Finally, we encourage the authors to streamline the manuscript to focus on the unique features of ShiftAddLLM, ensuring that relevant works and limitations are adequately recognized.