ID: gMqaKJCOCB
Title: Understanding the Gains from Repeated Self-Distillation
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical investigation into the effects of multiple rounds of self-distillation (SD) in linear regression. It demonstrates that under specific conditions on the ground truth model $\theta^{\ast}$ and the data matrix $X$, multi-step SD can enhance the risk bound by a factor of $r = \text{rank}(X)$. The authors establish that this improvement occurs when the non-zero singular values of $X$ are distinct and when $\theta^{\ast}$ is perfectly parallel to the leading singular vector $u_1$. The necessity of these conditions is discussed, and empirical results on simple regression tasks support the theoretical claims.

### Strengths and Weaknesses
Strengths:  
1. The paper is among the first to theoretically characterize multi-step SD.  
2. The analysis of excess risk for models trained with multi-step SD is solid, supported by experiments on synthetic and real datasets.  
3. The paper is well-written and easy to follow.  

Weaknesses:  
1. The condition of $\theta^{\ast}$ being perfectly parallel to $u_1$ is overly strong and unrealistic, diminishing the results' interest. A more realistic approach would involve quantifying gains with a bounded angle $\beta$ between $\theta^{\ast}$ and $u_1$.  
2. There is a lack of practical insights for multi-step SD in classification problems, which are currently more relevant.  
3. The experiments are insufficient to support the proposed theory, particularly lacking results for more than 2-step self-distillation and widely used datasets.  

### Suggestions for Improvement
We recommend that the authors improve the relevance of their results by relaxing the condition on $\theta^{\ast}$ to allow for a bounded angle $\beta$ with $u_1$. Additionally, we suggest including practical insights for multi-step SD in classification tasks. The authors should also expand their experimental analysis to include more datasets and investigate the performance of multi-step SD beyond 2 steps, particularly on widely used datasets like CIFAR-10. Furthermore, clarifying the implications of Assumption 2.2 and providing more detailed discussions on the limitations of their theoretical analysis would enhance the paper's depth.