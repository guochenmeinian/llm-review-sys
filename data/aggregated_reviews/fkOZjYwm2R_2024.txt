ID: fkOZjYwm2R
Title: Imitation guided Automated Red Teaming
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 6
Original Confidences: 2, 3

Aggregated Review:
### Key Points
This paper presents a novel approach that integrates supervised learning with a separate diversity module to enhance the reinforcement learning (RL) procedure for large language model (LLM) red teaming. The method demonstrates effectiveness through experiments, although there are concerns regarding implementation details and training stability, particularly in balancing the conflicting objectives similar to Generative Adversarial Networks (GANs).

### Strengths and Weaknesses
Strengths:  
- The paper is well-written, with clear motivation and easy-to-follow methodology.  
- It includes thorough experiments and appropriate baselines, demonstrating that iART effectively balances quality and diversity in red teaming while maintaining computational efficiency, outperforming baselines.

Weaknesses:  
- In some experiments, iART does not significantly outperform baselines in terms of diversity (Figure 2(b)).  
- The ablation study shows that iART without the imitation module outperforms iART in diversity, contradicting the claim in Section 4.1 that imitation guidance enhances output diversity.  
- There is a missing citation in the checklist regarding limitations/justification.  
- Inconsistent usage of 'red-teaming' and 'red teaming' is noted.

### Suggestions for Improvement
We recommend that the authors improve the clarity of implementation details and address the challenges of training stability when balancing the two objectives. Additionally, we suggest that the authors clarify the findings of the ablation study to resolve contradictions regarding the imitation module's impact on diversity. It would also be beneficial to ensure consistent terminology throughout the paper and to rectify the missing citation in the checklist.