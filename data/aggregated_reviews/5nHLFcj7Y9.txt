ID: 5nHLFcj7Y9
Title: Text Representation Distillation via Information Bottleneck Principle
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a knowledge distillation method called IBKD, which utilizes the Information Bottleneck (IB) principle to enhance text representation by addressing the performance gap between large-scale pre-trained models and smaller models. The authors propose maximizing mutual information between the teacher and student models while minimizing it between the student model and input. Additionally, they introduce a contrastive learning loss and HSIC method to reduce computational costs. The paper claims substantial performance improvements on downstream tasks (STS, DR) and validates the method through extensive experiments.

### Strengths and Weaknesses
Strengths:
- The model design effectively incorporates the Information Bottleneck and a two-stage training method.
- The experimental results provide robust evidence for the method's effectiveness, demonstrating state-of-the-art results.
- The writing is clear and easy to follow, with detailed parameter settings.

Weaknesses:
- The generalizability of the method is questioned due to limited evaluation on only two tasks (STS and DR) and a narrow range of teacher models (Roberta and CoCondenser).
- Insufficient details on estimating mutual information with HSIC, particularly regarding the application of formula (7) for RBF kernel calculation.
- The claim of improved generalization ability lacks supporting results.

### Suggestions for Improvement
We recommend that the authors improve the breadth of their evaluation by including additional benchmark datasets like GLUE to assess distillation performance comprehensively. Additionally, clarifying the selection rationale for STS and DR as downstream tasks and providing details on the data used for the unsupervised learning stage would strengthen the paper. Furthermore, addressing the calculation of the RBF kernel using formula (7) and considering the application of their method to a wider variety of teacher models would enhance the robustness of their findings. Lastly, we suggest modifying Figure 1 to better represent the relationship between teacher embeddings and task-relevant information, indicating potential information loss.