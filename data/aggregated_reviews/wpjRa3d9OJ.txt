ID: wpjRa3d9OJ
Title: Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for Temporal Knowledge Graph (TKG) forecasting using in-context learning with large language models (LLMs). It is the first study to introduce LLMs for TKG forecasting, proposing a pipeline that includes history modeling, prompt construction, and output decoding to enable zero-shot predictions. The authors compare LLMs against strong supervised baselines on benchmark datasets, demonstrating competitive performance without training. Additionally, the paper introduces a new evaluation methodology and dataset for TKG forecasting, providing insights into various factors affecting LLM effectiveness.

### Strengths and Weaknesses
Strengths:  
- The paper provides a thorough evaluation of diverse baselines and state-of-the-art models for TKG forecasting.  
- It introduces a new evaluation methodology and dataset, valuable for future research.  
- Extensive experiments demonstrate the promise of LLMs for knowledge graph reasoning tasks.  

Weaknesses:  
- Experiments are limited to small-scale open-source models, potentially affecting generalizability.  
- The method only considers 1-hop neighbors, neglecting long-range neighbor structures and rich semantic information in TKGs.  
- The "Multi Step" experimental setting may not align with real-world scenarios, as intermediate events are typically not predicted.  
- There are concerns regarding the fairness of LLMs compared to supervised models, as test triples may exist in textual form during LLM pretraining.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by including larger models in their experiments. Additionally, the authors should address the limitations of their methodology regarding link prediction settings and consider incorporating long-range neighbor structures to leverage the full semantic richness of TKGs. Clarifying the rationale behind the "Multi Step" experimental setting and ensuring that it reflects realistic prediction scenarios would enhance the paper's applicability. Finally, the authors should provide more details on the #Params of gpt-3.5-turbo and address concerns about the potential redundancy in in-context learning samples.