ID: JMBWTlazjW
Title: Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 4, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the core components of training from preference feedback (RLHF), identifying four main elements: labeled preference data, learning algorithm, reward model, and policy training prompts. The authors conduct experiments to assess the impact of each component on performance, concluding that preference data quality is paramount, followed by algorithm choice (notably PPO over DPO), with smaller gains from reward model capabilities and training prompts. The authors suggest a recipe combining the best elements from their findings.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with clear claims and a coherent narrative.
- The experimental setup is comprehensive, providing valuable insights into various preference datasets and benchmarks.
- The authors offer significant observations regarding the effects of preference datasets on dimensions like truthfulness and instruction following.

Weaknesses:
- Claims often lack adequate experimental support, being overstated or misrepresentative of the data. For example, the assertion that "PPO outperforms DPO" is misleading without careful hyperparameter consideration.
- The analysis of preference datasets is limited, relying primarily on a single model and lacking comparative insights across multiple algorithms.
- The discussion on the performance of synthetic data is insufficient, lacking an analysis of why it performs best.
- The experiments on the reward model are biased, as they depend on previously selected datasets, limiting the ability to independently assess each component's contribution.

### Suggestions for Improvement
We recommend that the authors improve the rigor of their claims by providing a more detailed analysis of hyperparameter configurations for both PPO and DPO, ensuring that comparisons are valid across all datasets. Additionally, we suggest expanding the analysis of preference datasets to include multiple algorithms and hyperparameters to substantiate claims regarding their effectiveness. The authors should also include a discussion on the reasons behind the superior performance of synthetic data. Finally, we encourage the authors to conduct more independent experiments on the reward model to avoid bias and to clarify the selection criteria for datasets used in their comparisons.