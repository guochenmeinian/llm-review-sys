ID: z3HACY5CMa
Title: Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 3, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel adversarial approach called LECI for addressing the graph out-of-distribution (OOD) generalization problem by leveraging label and environment causal independence. The authors propose a method that identifies causal and invariant subgraphs, enhancing prediction accuracy. Additionally, the paper introduces a subgraph selector, \( f_\theta \), which is claimed to maintain consistency in OOD test graphs. The authors address concerns regarding the novelty of their approach and the performance discrepancies between OOD and in-distribution validations, clarifying that the results in Table 2 reflect OOD test performances and emphasizing that using OOD validation generally yields better results than in-distribution validation. The approach is supported by extensive experimental evaluations on various datasets, demonstrating its effectiveness compared to existing methods.

### Strengths and Weaknesses
Strengths:
1. The paper is well-organized and clearly articulates its ideas, particularly the integration of environment information, which distinguishes it from prior work.
2. The proposed method is theoretically grounded, providing guarantees for causal subgraph discovery and supporting the consistency of \( f_\theta \) in OOD settings.
3. Empirical evaluations are robust, featuring diverse datasets, relevant baselines, and comprehensive analyses, including sensitivity and ablation studies.
4. Clarifications regarding the performance metrics in Table 2 enhance understanding of the OOD generalization capabilities of the proposed method.

Weaknesses:
1. The paper lacks a formal definition of the OOD task, failing to clarify the training and test data distributions and their differences.
2. While theoretical analyses are present, they do not directly connect to OOD generalization error, leaving the performance implications in various OOD settings unaddressed.
3. The emphasis on the Covariate SCM assumption in experiments requires clarification regarding the specific SCM satisfied by the data.
4. There are unresolved doubts about the theoretical and experimental components, particularly regarding the potential for misleading results due to shared K-hop subgraphs.
5. The justification for better OOD validation performance compared to in-distribution performance remains unclear, raising questions about the selected tasks' ability to validate OOD generalization.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the OOD task definition, specifying the training and test data distributions and their differences. Additionally, it would be beneficial to explicitly connect the theoretical analysis to OOD generalization error and clarify the specific SCM satisfied by the datasets used in experiments. Furthermore, addressing the computational complexity of LECI and exploring the use of environment labels as auxiliary information would enhance the paper's comprehensiveness. We also recommend improving the justification for the performance differences observed between OOD and in-distribution validations, providing clearer insights into the implications of their experimental results. Lastly, further clarification on the conditions under which \( f_\theta \) may be misled by shared K-hop subgraphs would strengthen the theoretical foundation of the paper, and correcting instances of "casual" to "causal" will improve accuracy and clarity.