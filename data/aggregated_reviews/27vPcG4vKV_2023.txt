ID: 27vPcG4vKV
Title: ProteinShake: Building datasets and benchmarks for deep learning on protein structures
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 8, 6, 7, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a software package, ProteinShake, designed to facilitate the creation and utilization of protein structure datasets for training deep learning models. It offers 10 datasets for various prediction tasks, primarily sourced from the RCSB PDB database and specialized databases like PDBbind, enabling users to generate protein representations in three formats: graph, voxel, and point cloud. The authors propose a systematic approach to dataset curation and model evaluation, emphasizing the importance of pretraining on AlphaFold predicted structures to enhance performance. The documentation has been updated to clarify dataset creation, quality control, and the integration of standard test datasets, while acknowledging the need for improved evaluation metrics and user guidance.

### Strengths and Weaknesses
Strengths:
- The manuscript addresses a significant need in the field of machine learning for biology by providing standardized datasets and benchmarks.
- The systematic design of the software library, including criteria for dataset utility, enhances its relevance and accessibility.
- The authors have effectively addressed several reviewer comments, enhancing the clarity and accessibility of the documentation.
- The inclusion of tutorials for custom dataset creation and integration of standard benchmarks is a valuable addition.
- The paper provides a broad range of biological applications and machine learning problems, showcasing the library's versatility.
- The clarity of writing and quality of figures contribute to the paper's overall effectiveness in communicating its contributions.

Weaknesses:
- There is insufficient detail on how datasets are curated for quality and completeness, and how users can create their own datasets.
- Some prediction tasks are poorly defined or overlap with existing tasks, lacking clear differentiation.
- The evaluation metrics used may not be appropriate for all tasks, particularly in cases of class imbalance.
- The documentation lacks comprehensive information on model evaluation metrics and user integration of pretrained models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of dataset curation by providing detailed descriptions of how quality and completeness are ensured, including a comparison with standard datasets. Additionally, the authors should include guidance on how users can create their own datasets using the software package. It would be beneficial to elaborate on the model evaluation module, including metrics and justifications for their use, and to clarify its functionality. Furthermore, we suggest allowing users to integrate standard test datasets into the data split process and providing options to control redundancy among similar proteins in the datasets. We also encourage the authors to consider integrating more appropriate evaluation metrics for specific tasks, particularly addressing the challenges of class imbalance, and to define homology reduced splits at lower levels than 50% to enhance dataset diversity. Lastly, the authors should discuss the rationale behind the choice of tools for calculating solvent accessibility scores and explore the inclusion of additional baseline models for benchmarking.