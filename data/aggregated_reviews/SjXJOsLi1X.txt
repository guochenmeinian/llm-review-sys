ID: SjXJOsLi1X
Title: Joint Learning for Visual Reconstruction from the Brain Activity: Hierarchical Representation of Image Perception with EEG-Vision Transformer
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 5, 6
Original Confidences: 4, 4, 3

Aggregated Review:
### Key Points
This paper presents a novel framework for reconstructing visual images from EEG recordings by integrating hierarchical visual features with a vision transformer-based approach called "Hierarchical-ViT." The authors utilize CLIP-based joint learning and StyleGAN for high-resolution image generation, which is notable as most studies typically rely on fMRI signals for such tasks. The research highlights the feasibility and cost-effectiveness of using EEG for brain-computer applications.

### Strengths and Weaknesses
Strengths:
- The motivation is clear, addressing the challenge of using EEG data instead of fMRI.
- The results demonstrate that the proposed method performs well for both classification and reconstruction tasks.
- Ablation studies indicate that hierarchical features encode distinct information, supported by statistical tests.

Weaknesses:
- The method for extracting image features relies on terms like V1, V2, V3, and V4, which are not accurately representative of biological processes. The authors should avoid excessive references to biological vision.
- There is a lack of clarity regarding the use of labels during inference, which could impact the understanding of the model's efficacy.
- Missing references to relevant literature, including the debate around Palazzo's dataset and the analysis by Lin et al., weaken the paper's context.

### Suggestions for Improvement
We recommend that the authors improve the accuracy of their feature extraction terminology by focusing on computer vision concepts rather than biological references. Additionally, please provide original images, reconstructed images from EEG, and comparisons with other methods to better assess reconstruction quality. More detailed information on the training and fine-tuning of StyleGAN, as well as the parameters related to contrastive learning, should be included. It is also essential to describe the EEG preprocessing methods used, as they can significantly affect reconstruction quality. Please ensure that the reference style is corrected, particularly for missing journal names, and address the missing table number in the appendix. Lastly, we suggest removing the term "motion" from section V3, as it is not relevant to the content.