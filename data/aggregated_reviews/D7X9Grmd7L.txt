ID: D7X9Grmd7L
Title: Segment Any Change
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 4, 6, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AnyChange, a novel framework for zero-shot change detection in remote sensing imagery, utilizing a method called bitemporal latent matching through the Segment Anything Model (SAM). The authors propose a mechanism that identifies changes by leveraging semantic similarities in SAM's latent space, enabling interactive, object-centric change detection. Comprehensive experimental results demonstrate AnyChange's effectiveness across various datasets, achieving notable F$_1$ score increases, although concerns about its performance in detecting minor changes and robustness to environmental variations are raised. The authors acknowledge a trade-off between precision and recall and address previous reviewer concerns regarding the model's generalization ability.

### Strengths and Weaknesses
Strengths:
- The approach of utilizing a foundation model for object change detection is innovative and relevant to the remote sensing field.
- The authors provide extensive experimental results across four datasets, with reasonable baselines demonstrating the method's capabilities.
- The proposed method shows improved F$_1$ scores across multiple datasets, indicating its effectiveness in zero-shot change detection.
- The framework supports a point query mechanism, enhancing user interaction and potentially reducing false positives.

Weaknesses:
- There are significant concerns regarding SAM's capacity to detect minor changes due to its optimization on dense masks and high-level semantics.
- Lingering doubts exist about the generalization ability of the model to unseen images and the adequacy of the theoretical explanations for the choice of SAM's feature space for latent matching.
- The robustness of AnyChange to illumination/color changes and viewpoint variations has not been adequately evaluated.
- The experimental results do not convincingly demonstrate an advantage over existing algorithms, particularly under supervised settings, raising questions about the method's overall effectiveness.
- Some reviewers question whether the novelty of the approach is sufficient to meet the standards of the NeurIPS community, given its reliance on existing models.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of AnyChange's robustness to illumination and viewpoint changes, providing additional demonstrations of its capacity to detect minor changes. Furthermore, it would be beneficial to report experimental results using both 10% and 100% ground truth annotations to clarify the method's performance relative to existing algorithms. Additionally, we suggest creating a new table that combines results from the S2Looking dataset to enhance comparisons and clarify the claims regarding AnyChange's capabilities. We also recommend improving the theoretical grounding of their choice to utilize SAM's feature space for latent matching, providing a more intuitive explanation of how semantic dissimilarity relates to change detection. Addressing the generalization ability of the model to unseen images with more robust empirical evidence would strengthen the paper's claims. Finally, clarifying the unique contributions of their method in relation to existing literature could help alleviate concerns regarding its novelty and significance.