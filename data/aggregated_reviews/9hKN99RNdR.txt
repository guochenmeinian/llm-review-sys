ID: 9hKN99RNdR
Title: Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 7, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called "Cluster Edge Exploration" (CE2) for goal selection in goal-conditioned reinforcement learning, enhancing exploration efficiency. The authors propose clustering reachable states in a latent space and sampling goals near the frontier of explored states. The method builds on the Go-Explore principle, utilizing Gaussian Mixture Models (GMMs) for goal selection and demonstrating improved performance across various simulated control benchmarks.

### Strengths and Weaknesses
Strengths:
- The method's core idea of sampling feasible yet underexplored goals is sound and well-motivated.
- The paper is clearly written, with a thorough explanation of the algorithm and training objectives.
- Extensive empirical studies across diverse domains showcase the method's effectiveness, with insightful visualizations of sampled goals.

Weaknesses:
- The core algorithmic contribution beyond Go-Explore is limited, with the proposed exploration strategy being a small modification.
- Evaluations are primarily restricted to standard artificial benchmark tasks, lacking comparisons in more challenging environments.
- Some tasks appear not to have been trained to convergence, complicating assessments of sample efficiency and final performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Equation 2, particularly regarding potential unit problems. Additionally, elaborating on Equation 4 and the representation of densities p and q would enhance understanding. A more rigorous comparison with Go-Explore is necessary to substantiate the claimed advantages of the goal selection method. Furthermore, addressing the computational costs associated with optimizing goal states compared to other methods would provide valuable insights. Lastly, exploring the method's performance in vision-based control environments could reveal additional challenges and applicability.