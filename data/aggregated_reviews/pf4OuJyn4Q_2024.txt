ID: pf4OuJyn4Q
Title: Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper analyzes over-optimization in direct alignment algorithms (DAA), demonstrating that even without an explicit reward model, a phenomenon similar to that observed by Gao et al. occurs, where the "gold" reward increases and then decreases as the KL budget increases. This behavior is consistent across various model sizes and DAAs (DPO, IPO, SLiC). The authors fit scaling curves to this phenomenon, revealing that at low KL budgets, optimal performance is achieved early in training, and that model training statistics are not predictive of downstream performance. The paper also includes theoretical analysis and a toy MDP to illustrate why reward exploitation can occur in DAAs.

### Strengths and Weaknesses
Strengths:
- The analysis is novel, providing insights into over-optimization in DAAs, a topic not previously explored.
- The quality of the analysis is high, with clear and neutral discussions of results, addressing many pertinent questions.
- The paper is well-presented and accessible, contributing significantly to understanding the limitations of DAAs.

Weaknesses:
- All analyses are conducted solely on the TL;DR summarization dataset, which differs from typical applications of DAAs, limiting generalizability. Testing core hypotheses on other datasets would enhance robustness.
- Using GPT-4 winrate as the "gold" reward diminishes generalizability, as the preference distribution for training differs from that used for evaluation. Empirical validation in a consistent setting is recommended.
- Section 4 lacks a clear explanation of the phenomena observed in DAAs, which could benefit from further elaboration.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by testing core hypotheses on additional datasets, such as Alpaca Farm or Anthropic HH. Additionally, we suggest that the authors provide empirical validation of results using a consistent preference distribution for both training and evaluation, potentially utilizing data from Coste et al. or Alpaca Farm GPT-4 preference data. Furthermore, we encourage the authors to enhance the clarity of Section 4 by providing a more intuitive explanation of the observed phenomena in DAAs. Lastly, unifying the axes in figures and improving text size in figures would enhance readability.