ID: c0utj9Q4YY
Title: Toward Joint Language Modeling for Speech Units and Text
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a joint modeling approach for speech and text, proposing a multimodal decoder trained with four objectives: unitLM (uLM), textLM (tLM), concatenated speech-text (CST), and alternating speech-text (AST). The authors introduce a new evaluation metric, Context Retrieval Accuracy (CRA), to assess cross-modal transfer capabilities without fine-tuning. The paper conducts experiments on sentiment analysis and named entity recognition, providing a detailed analysis of the contributions of each pre-training objective.

### Strengths and Weaknesses
Strengths:
- The writing is clear and well-structured, effectively situating the work within the existing literature.
- The paper explores a timely topic, contributing to the understanding of joint language modeling for speech and text.
- Detailed analysis of the contribution of each pre-training objective enhances the paper's rigor.

Weaknesses:
- The CRA metric lacks discernment, as it may yield high probabilities for well-trained language models, raising concerns about its reliability.
- Evaluation is limited to cross-modal transferability, neglecting diverse downstream tasks that could provide a more comprehensive assessment of the model's capabilities.
- The experimental setup lacks clarity regarding baseline models and does not include evaluations on generation tasks or other relevant tasks.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including diverse downstream tasks, such as speech-only and text-only tasks, to better demonstrate the model's capabilities. Additionally, clarifying the baseline models used in the experiments and providing results for CST-only and AST-only methods would enhance the paper's rigor. It would also be beneficial to explore other evaluation metrics beyond CRA to substantiate the effectiveness of the joint language model. Lastly, addressing the questions raised regarding the performance of multi-task learning and the implications of multilingual data would strengthen the manuscript.