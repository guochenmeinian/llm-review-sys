ID: 5WoYFypPv0
Title: Deep Support Vectors
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 6, 8, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the concept of Deep Support Vectors (DSVs) as an adaptation of support vectors from Support Vector Machines (SVMs) to deep learning models. The authors propose the DeepKKT condition, which generalizes traditional Karush-Kuhn-Tucker (KKT) conditions to address the complexities of high-dimensional and multi-class deep learning problems. DSVs are shown to encode decision boundaries and facilitate model reconstruction from a limited sample set. The paper demonstrates DSVs' applicability in few-shot dataset distillation and enhancing model interpretability by providing visual explanations of decision criteria. Additionally, the DeepKKT condition is shown to enable the transformation of conventional classification models into generative models with high fidelity.

### Strengths and Weaknesses
Strengths:
- The introduction of Deep Support Vectors offers a novel perspective on bridging SVM concepts with deep learning.
- The DeepKKT condition effectively generalizes traditional KKT conditions for deep learning applications.
- Practical experiments validate the utility of DSVs in various contexts, including few-shot dataset distillation and generative modeling.
- The paper provides a thorough comparison of DSVs with existing methods, demonstrating their advantages under practical constraints.

Weaknesses:
- The paper lacks a rigorous mathematical derivation of the DeepKKT condition and its relationship to traditional KKT conditions, which would enhance its theoretical foundation.
- There is insufficient analysis regarding the method's sensitivity to hyperparameters and its robustness across different architectures and datasets.
- The computational complexity of generating DSVs is not adequately discussed, particularly in relation to dataset size and model complexity.
- The requirement for models to be fully pre-trained before DSV generation deviates from traditional SVM practices, raising questions about pre-training datasets and potential domain gaps.

### Suggestions for Improvement
We recommend that the authors improve the mathematical rigor by providing a detailed derivation of the DeepKKT condition and its connection to traditional KKT conditions. A more comprehensive analysis of the method's sensitivity to hyperparameters and robustness across various architectures and datasets would strengthen the paper. Additionally, we suggest including a detailed discussion on the computational complexity of generating DSVs, particularly how it scales with dataset size and model complexity. Finally, addressing the limitations of requiring pre-trained models for DSV generation and clarifying the implications of this requirement would enhance the paper's clarity and applicability.