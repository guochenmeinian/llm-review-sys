ID: yT0f93CeTw
Title: Fast Conditional Mixing of MCMC Algorithms for Non-log-concave Distributions
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 1, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of MCMC algorithms, particularly Langevin dynamics and Gibbs sampling, for sampling from non-log-concave distributions. The authors introduce the concept of conditional convergence, which refers to the convergence of the distribution when restricted to specific regions of the state space. They demonstrate that while MCMC algorithms may converge slowly to the global distribution, they can exhibit rapid local convergence under certain conditions, particularly when isoperimetric inequalities hold. Additionally, the authors clarify the distribution $\bar{\pi}_T$, defined as $\bar{\pi}_T = \frac{1}{T} \int_0^T \pi_t dt$, where $\pi_t$ is the distribution of the Langevin iterate $X_t$. They emphasize that $\bar{\pi_T}$ should not be confused with the chain distribution derived from a single initialization point, as it represents the underlying distribution of the Langevin Monte Carlo (LMC) process. The authors assert that $\bar{\pi_T}$ can approximate all modes well, even if a specific LMC trajectory visits only one mode, and they address concerns about the relationship between the chain's time spent in a particular region and the approximation quality of $\bar{\pi_T}$.

### Strengths and Weaknesses
Strengths:  
The paper tackles an important problem in sampling from multimodal distributions and introduces the novel concept of conditional convergence. The combination of techniques used to show local mixing of MCMC algorithms is clever, and the results appear to be novel and correct. The authors provide a clear and detailed clarification of the definition and significance of $\bar{\pi}_T$, addressing potential misunderstandings effectively. The paper is well-illustrated with examples and provides experimental evidence supporting the findings.

Weaknesses:  
The results, particularly Corollary 1, are weaker than expected, as they suggest a time-dependent step size that complicates practical implementation. The assumptions regarding Gaussian mixtures, specifically that components share the same covariance, lack justification. Additionally, there is some ambiguity in the reviewer's comments regarding the implications of the results, which the authors find challenging to address without clearer statements. The presentation contains grammatical errors and could benefit from improved clarity, especially regarding the relationship between different MCMC approaches and the impact of discretization error.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, particularly in defining the relationship between the unadjusted Langevin algorithm and Langevin diffusion, and explicitly discuss the implications of discretization error on their results. Additionally, we suggest that the authors provide a more convincing explanation of the practical importance of Corollary 1, ensuring that it accurately reflects the conditions under which the MCMC chain approximates the conditional distribution well. Furthermore, addressing the assumptions regarding Gaussian mixtures and exploring the necessity of shared covariance would strengthen the theoretical foundation of the paper. Finally, providing more explicit examples or counter-examples could help clarify the implications of their results and address potential reviewer concerns more effectively.