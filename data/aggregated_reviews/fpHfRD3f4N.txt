ID: fpHfRD3f4N
Title: Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 5, 4, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on reinforcement learning in episodic settings for linear bandits and linear MDPs under heavy-tailed rewards with potentially infinite variance. The authors propose algorithms based on adaptive Huber regression and optimism in the face of uncertainty, utilizing conditional reward variance. They derive variance-dependent T-round regret for heavy-tailed linear bandits and instance-dependent K-episode regret for linear MDPs, with results hinging on Huber loss regression techniques.

### Strengths and Weaknesses
Strengths:  
- The authors propose a method for tuning the robustification parameter in heavy-tailed linear bandits, balancing bias and robustness dynamically.  
- The use of adaptive Huber regression to estimate heavy-tailed rewards and transition kernels is a notable technical contribution.  
- The paper is well-contextualized, with sound claims and a clear presentation of assumptions and theorem statements.  

Weaknesses:  
- The derived regrets depend on multiple factors, including feature dimension and variance, which may limit applicability to small-dimensional problems.  
- Assumption 2.6 regarding realizability of centralized moments appears unrealistic.  
- The absence of empirical evaluations undermines the practical applicability of the proposed algorithms, despite theoretical soundness.  
- The writing is dense and could benefit from improved clarity and flow, particularly in Sections 2.2 and 5.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manuscript by moving dense content to the appendix and integrating remarks into the main text for better flow. Additionally, we suggest providing empirical evaluations to demonstrate the practical effectiveness of the proposed algorithms, addressing concerns about computational costs. Furthermore, the authors should consider discussing the implications of relaxing the strong assumption regarding the known central moments of rewards and clarify the distinction between heavy-tailed noise and heavy-tailed rewards in the context of their work.