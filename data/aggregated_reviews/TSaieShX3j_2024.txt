ID: TSaieShX3j
Title: SGD vs GD: Rank Deficiency in Linear Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 6, 8, 7, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the implicit bias of stochastic gradient descent (SGD) in two-layer linear networks, demonstrating that while gradient descent (GD) preserves the rank of parameter matrices, SGD reduces it, leading to simpler solutions. The authors derive a stochastic differential equation (SDE) to characterize the eigenvalue evolution of the parameter matrix, supporting the claim that SGD converges to low-rank solutions. The findings are validated through small-scale experiments.

### Strengths and Weaknesses
Strengths:
- The paper provides a novel insight into the implicit bias of SGD, contributing to the theoretical understanding of optimization in neural networks.
- It establishes a clean invariant for gradient flow (GF) and shows that the determinant of the Gram matrix decays under stochastic gradient flow (SGF), a significant contribution to SGD theory.
- The methodology is solid, and the conclusions are well-presented within the context of the study.

Weaknesses:
- The presentation could be improved; for instance, the discussion on initialization is unclear, and there are minor typographical errors.
- The experiments are limited in scale, raising questions about the generalizability of the results to more complex networks.
- The paper does not sufficiently address the implications of SGD's rank-reducing property on prediction error, nor does it explore the extension of results to non-linear networks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the initialization discussion and correct typographical errors, such as the missing ">" in line 228 and the notation change to $\Delta(t)$ between lines 149-150. Additionally, we suggest expanding the experiments to larger and more complex models to better validate the theory. It is crucial to discuss the potential implications of SGD's rank-reducing property on generalization and to provide insights into how these results might extend to non-linear networks. Finally, we advise citing relevant literature, particularly the work by Feng Chen et al., to delineate similarities and differences in findings.