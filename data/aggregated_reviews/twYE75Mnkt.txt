ID: twYE75Mnkt
Title: Derandomizing Multi-Distribution Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 7, 6, 6, -1, -1, -1
Original Confidences: 3, 3, 3, 2, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of multi-distribution learning in the binary label setting, focusing on the challenge of derandomizing classifiers. The authors show that learning a deterministic classifier in this context is computationally hard, as it relates to the NP-hard problem of discrepancy minimization. They also demonstrate that if the distributions share a common conditional distribution, a deterministic classifier can be derived. The work contributes both a negative result regarding the hardness of derandomization and a positive result under specific conditions, particularly label-consistency.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written, with proofs and statements that are easy to follow and appear correct.
- The motivation for exploring the hardness of derandomization is well-founded, and the results are derived through an elegant reduction.

Weaknesses:
- The formal definition of multi-distribution learning in the uniform computational model is lacking, which is essential for establishing the NP-hardness argument.
- There is insufficient motivation provided for preferring deterministic classifiers over randomized ones.
- The restriction to finite domains is presented as a mere technicality, which may overlook significant implications for the theory.
- The storage reduction for derandomized classifiers seems unlikely and requires further clarification.

### Suggestions for Improvement
We recommend that the authors improve the formal definition of multi-distribution learning within the uniform computational model, specifying the inputs and their representations. Additionally, providing a stronger motivation for the preference of deterministic classifiers over randomized ones would enhance the paper's depth. The authors should elaborate on the significance of the restriction to finite domains and outline steps toward establishing results for continuous domains. Lastly, clarifying how the storage requirement for derandomized classifiers can be reduced would address potential concerns regarding feasibility.