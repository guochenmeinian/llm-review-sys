ID: 6U8iV9HVpS
Title: Robust Neural Contextual Bandit against Adversarial Corruptions
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 6, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents R-NeuralUCB, a neural contextual bandit algorithm designed to enhance robustness against adversarial reward corruptions. The authors provide a theoretical analysis of the algorithm's regret under over-parameterized neural network settings, without relying on the arm separateness assumption. Empirical comparisons with baseline algorithms on real datasets demonstrate the advantages of R-NeuralUCB in various corruption scenarios.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel algorithm for neural contextual bandits, utilizing a technique that customizes network parameters for each candidate arm, thereby improving robustness under adversarial conditions.
- Theoretical analysis yields a robust regret bound that depends on the effective dimensions of the neural network.
- The experiments conducted on publicly available datasets convincingly illustrate the algorithm's superior performance compared to baseline methods.

Weaknesses:
- The computational cost of R-NeuralUCB is substantial, as it requires maintaining separate neural networks for each arm, leading to increased gradient descent computations for every arm in each round.
- There are contradictions in the theoretical results, particularly between Theorem 4.12 and Theorem 5.6 regarding regret under different corruption levels.
- The tuning of parameter $\alpha$ in Theorem 5.6 is unclear, raising questions about its practical implementation.

### Suggestions for Improvement
We recommend that the authors improve the discussion on computational efficiency, particularly addressing the high costs associated with maintaining separate neural networks for each arm. Additionally, clarifying the contradictions between Theorems 4.12 and 5.6 would enhance the theoretical robustness of the paper. We also suggest providing more details on how to achieve the tuning of parameter $\alpha$ effectively. Lastly, including comparisons with other baselines and extending the experimental settings could further validate the algorithm's performance.