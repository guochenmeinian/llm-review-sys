ID: 47CdPNiWUB
Title: Mitigating the Impact of Labeling Errors on Training via Rockafellian Relaxation
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 5, 5, 5, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a methodology called Rockafellian Relaxation (RR) to mitigate the impact of labeling errors in neural network training. The authors propose an automated loss reweighting scheme, the Rockafellian Relaxation Method (RRM), which integrates adversarial training concepts to enhance robustness against label noise and adversarial attacks. The method is architecture-independent and demonstrates significant improvements in performance across various datasets, including MNIST and Toxic Comments, while addressing label corruption and class imbalance without requiring clean validation sets.

### Strengths and Weaknesses
Strengths:  
- The originality of using Rockafellian Relaxation for labeling errors is notable, particularly its innovative combination with adversarial training concepts.  
- The method is grounded in solid theoretical justification, validated by empirical results that show marked improvements over existing techniques.  
- The clarity of the methodology and algorithms enhances understanding of the proposed solution.  
- The significance of this work lies in its potential to improve training robustness across diverse and error-prone real-world environments.  

Weaknesses:  
- The computational complexity of the RR algorithm may limit its practical application in resource-constrained scenarios.  
- The scalability of RRM concerning computational cost and effectiveness with larger datasets remains unclear.  
- The method's effectiveness against different types of noise beyond uniform label noise is not thoroughly investigated.  
- Dependence on hyperparameter tuning could affect reproducibility, as the paper lacks extensive guidance on hyperparameter selection.  
- The experimental section lacks relevant baselines for comparison, making it difficult to assess the purported benefits empirically.  

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including a more comprehensive comparison with state-of-the-art methods for handling noisy labels, such as Generalized Cross Entropy (GCE) and Early-Learning Regularization (ELR). Additionally, we suggest that the authors provide insights into the computational complexity of the RR algorithm, particularly for large-scale datasets. It would also be beneficial to investigate the performance of RR under different models of label noise and to clarify the relationship between label and adversarial feature corruptions earlier in the paper. Finally, we encourage the authors to include additional experiments beyond MNIST, such as CIFAR, CIFAR-N, and Clothing-1M, to further support the efficacy of the proposed method.