ID: A954O4tDmU
Title: AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 3, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AGD, a novel optimizer that integrates Hessian information into a preconditioning matrix and features an auto-switching function between SGD and adaptive optimizers. The authors provide theoretical convergence guarantees in both convex and non-convex settings and validate AGD across six public datasets from various domains, demonstrating its potential to outperform or match state-of-the-art optimizers.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and provides appropriate context for commonly used optimization algorithms.
- The experimental framework is well-designed, covering a good range of optimization tasks.
- The novel approach of using successive optimization steps to approximate Hessian information is promising.

Weaknesses:
- The idea is not fully novel, as similar concepts have been previously explored.
- The explanation of related works is minimal, and the specific problems addressed by existing second-order methods are not clearly articulated.
- The experiments lack detailed reporting on computational costs and hyperparameter settings, which are crucial for understanding the generalization ability of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the problem addressed by AGD in relation to existing methods, particularly in the context of second-order optimization techniques. Additionally, we suggest including more comprehensive experimental results on larger and state-of-the-art networks to validate the performance of AGD in high-dimensional optimization problems. It would also be beneficial to provide detailed discussions on hyperparameter settings, regularization methods, and computational costs associated with AGD to enhance the robustness of the findings.