ID: DILUIcDmU9
Title: HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: -1, 7, 8, 6, 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: -1, 3, 4, 4, 3, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HA-ViD, a multi-modality human assembly video dataset designed to enhance comprehensive assembly knowledge understanding for industrial applications. The dataset includes RGB, depth, and skeleton data, with extensive annotations for action recognition, action segmentation, object detection, and multi-object tracking. Unique features such as human pauses and errors are included, and the dataset is benchmarked against several state-of-the-art methods, showcasing its potential for advancing research in assembly knowledge. The authors clarify that each video captures one assembly task and address confusion regarding the dataset's multi-modality claim. They acknowledge the importance of depth maps and skeleton sequences for understanding assembly knowledge and discuss the limitations of skeleton data precision, while also highlighting the potential for future work to include more detailed 3D annotations for tasks such as hand-object interaction and assembly quality evaluation.

### Strengths and Weaknesses
Strengths:  
1. The dataset is well-motivated and addresses a significant gap in assembly knowledge understanding.  
2. It is comprehensive, featuring a large volume of multi-view, multi-modal videos with detailed annotations.  
3. The authors have made significant revisions to clarify contributions and enhance discussions on reasoning tasks and assembly quality evaluation.  
4. The inclusion of quantitative comparisons with existing datasets strengthens the dataset's relevance.  

Weaknesses:  
1. The dataset lacks a detailed comparison with existing datasets regarding quantity and tasks, although some quantitative comparisons have been added.  
2. The reliance on Azure Kinect for skeleton data extraction may not yield the highest precision, as alternative pose estimation methods could be more effective.  
3. The focus on low and mid-level tasks overlooks higher-level reasoning and planning tasks, which may limit the dataset's applicability for complex action understanding and human-robot cooperation.  
4. There are concerns regarding the clarity of certain insights and the potential overuse of superlatives in the writing.

### Suggestions for Improvement
We recommend that the authors improve the dataset by including a broader range of tasks and scenes, particularly those requiring 3D reasoning and two-hand cooperation. A detailed comparison with existing datasets should be added to highlight the dataset's advantages. Additionally, we suggest improving the precision of skeleton data by considering alternative pose estimation algorithms, such as DWPose for 2D keypoints and SMPL-X for 3D interactions. We encourage the authors to redesign the benchmark to include more high-level reasoning and planning tasks, such as next-step prediction and instruction VQA, to enhance the dataset's applicability. Finally, we suggest addressing the clarity of specific insights and moderating the use of superlatives to maintain an academic tone, while also incorporating hand-object interaction detection and discussing the implications of incorrect action annotations to further illustrate the dataset's utility.