ID: 4dmwvbs4Ea
Title: Offline RL via Feature-Occupancy Gradient Ascent
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 6, 7, 3, -1
Original Confidences: 4, 4, 3, -1

Aggregated Review:
### Key Points
This paper studies the offline policy optimization problem, aiming to identify a policy whose value function closely approximates the optimal value function using offline samples. The authors propose a gradient ascent algorithm under the linear MDP assumption, which achieves strong sample complexity reliant solely on the feature coverage of the best policy, without requiring coverage over other policies. Additionally, the paper presents a new algorithm for offline reinforcement learning in linear infinite-horizon discounted MDPs, transforming the problem into an unconstrained saddle-point optimization problem.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written, with clear algorithms, theorems, and lemmas.  
- The proposed algorithm is computationally efficient, easy to implement, and operates under the weakest data coverage assumption.  
- The techniques and observations regarding the optimization problem transformation are insightful.  

Weaknesses:  
- The algorithm's idea is somewhat similar to that of Zanette (2021), particularly in using the actor-critic update and optimization methods.  
- The linear MDP assumption is stronger than that in Zanette (2021), limiting the generalizability of results.  
- The paper is notation-heavy, making it difficult to follow; a table of notations with descriptions is recommended. The mixed use of $D_{\pi}$ and $D_{\theta}$ is particularly confusing.  
- There are no experimental results provided, raising concerns about the approach's applicability to non-linear MDPs.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the notation by including a table of notations with descriptions and optimizing the choices of notation to avoid confusion. Additionally, we suggest that the authors address the limitations of their approach regarding non-linear MDPs and provide experimental results to demonstrate the effectiveness of their method. Furthermore, we encourage the authors to clarify the significance of Theorem 3.1 and the feature coverage ratio in their results.