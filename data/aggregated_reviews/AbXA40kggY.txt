ID: AbXA40kggY
Title: BLESS: Benchmarking Large Language Models on Sentence Simplification
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents BLESS, a comprehensive performance benchmark evaluating 44 state-of-the-art Large Language Models (LLMs) on the task of text simplification (TS). The evaluation utilizes three datasets from diverse domains (Wikipedia, medical texts, and news articles) and employs both automatic metrics (SARI, BERTScore, FKGL, LENS) and manual qualitative analysis of model outputs. The study reveals that closed-weight commercial models generally outperform open-source models, with instruction tuning enhancing meaning preservation. A trade-off between simplicity and meaning preservation is noted, with larger models better balancing this trade-off.

### Strengths and Weaknesses
Strengths:
- The paper evaluates a large and diverse set of 44 LLMs, providing a comprehensive view of current capabilities in text simplification.
- It combines automatic metrics with qualitative analysis, offering nuanced insights into model performance.
- The standardized benchmark serves as a strong baseline for future research in LLM-based simplification.
- Key findings highlight both the potential and limitations of LLMs, particularly regarding domain generalization.

Weaknesses:
- The paper lacks innovation, primarily extending previous work without introducing significant new insights.
- Evaluation is limited to three domains, suggesting a need for broader dataset inclusion.
- The small test set size (915 sentences) and limited manual evaluation (300 samples) restrict statistical power and depth of analysis.
- There is insufficient analysis of common failures, which could provide deeper insights into model behavior.

### Suggestions for Improvement
We recommend that the authors improve the innovation aspect by incorporating novel methodologies or insights beyond existing metrics. Expanding the evaluation to include more domains and languages would enhance the robustness of the findings. Increasing the size of the test sets and conducting a more extensive qualitative analysis would provide a deeper understanding of model performance. Additionally, analyzing and categorizing common failures could illuminate areas needing improvement, and discussing broader implications of the findings would enrich the paper's contribution.