ID: Swh8LxuycA
Title: Learning Goal-Conditioned Representations for Language Reward Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 8, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method that integrates contrastive representation learning and goal-conditioned reinforcement learning (RL) to enhance reward models for language model alignment. The authors propose an additional contrastive loss term for training the reward model, which aims to learn "goal-conditioned representations" that encode expected rewards for partially complete sequences. The results indicate improvements in reward model accuracy and downstream applications in reinforcement learning from human feedback (RLHF) and guided generation.

### Strengths and Weaknesses
Strengths:  
- The paper demonstrates originality by creatively combining goal-conditioned RL techniques to improve reward models for language model alignment, resulting in a novel training method that enhances expressiveness.  
- A comprehensive set of experiments validates the proposed method's superior performance across various metrics and benchmarks.  
- The clarity of methods and results is commendable, effectively communicating the analysis and findings.

Weaknesses:  
- The definition of "goal state" in the language space is ambiguous, particularly regarding the averaging of representations from different preferred responses.  
- The mechanism for selecting prototypes for Q-value estimation appears arbitrary, warranting further exploration.  
- The authors acknowledge that performance gains from the reward model are less than expected, raising questions about the absence of experiments updating the reward model during training.  
- Statistical significance claims lack rigorous grounding, as confidence intervals are not reported.

### Suggestions for Improvement
We recommend that the authors improve the clarity surrounding the definition of "goal state" and the method for averaging representations, as this is crucial for understanding the implications of their approach. Additionally, we suggest conducting experiments that update the reward model during training to better assess performance gains. It would also be beneficial to provide confidence intervals for the reported metrics to substantiate claims of statistical significance. Lastly, consider comparing the proposed method with established techniques like DPO and other methods utilizing Q-values during decoding to provide a more comprehensive evaluation.