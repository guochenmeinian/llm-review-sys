ID: 9o6KQrklrE
Title: Geometric Transformer with Interatomic Positional Encoding
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 5, 6, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Geoformer, a Transformer-based architecture that incorporates Interatomic Positional Encoding (IPE) derived from Atomic Cluster Expansion (ACE) for molecular modeling. The authors demonstrate that Geoformer achieves state-of-the-art performance on various prediction tasks across two prominent molecular datasets, QM9 and Molecule3D. They propose various scaling methods, including a "multiplication before activation" approach, which shows superior performance. The integration of ACE-based information into the model and its residual refinement are highlighted as significant contributions. The authors have conducted extensive ablation studies and comparisons with existing Transformer models, addressing concerns regarding the theoretical underpinnings of their architecture and its practical implications in bridging the performance gap between Transformer-based methods and EGNNs.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and presents a novel approach by integrating physical knowledge (ACE) into the Transformer architecture, enhancing the modeling of interatomic interactions.  
- Geoformer shows superior performance compared to existing methods, demonstrating the effectiveness of the proposed IPE.  
- The extensive experiments and ablation studies validate the expressiveness of the architecture and demonstrate the robustness of the proposed method.  
- The authors have effectively incorporated relevant literature and addressed reviewer concerns, enhancing the manuscript's depth.  
- The integration of physics concepts into AI is a significant contribution to the field.

Weaknesses:  
- The paper lacks a comprehensive literature review, which diminishes the perceived novelty of the proposed method. It cites only two related works and compares its performance to a low-accuracy Transformer model.  
- The architecture suffers from numerical and training instabilities, and there is insufficient detail regarding training parameters and model size comparisons.  
- Some notations and figures are confusing, and the ablation studies are not adequately highlighted in the main text, which could affect the clarity of the contributions.  
- Some reviewers noted that the clarity and presentation of the manuscript could be improved for a wider audience.  
- There were initial challenges in training and reporting results on point cloud datasets, which may limit the scope of the findings.

### Suggestions for Improvement
We recommend that the authors improve the literature review to include more relevant works in the field of Transformer-based molecular property prediction, as this will enhance the novelty of their contributions. Additionally, providing a clearer explanation of the relationship between ACE and IPE, as well as the rationale behind the proposed model's architecture, would strengthen the manuscript. 

We suggest including a detailed comparison of model sizes and training times with existing methods, as well as addressing the numerical instabilities mentioned. Furthermore, we encourage the authors to present the ablation studies more prominently in the main text to emphasize their importance. Improving the clarity and presentation of the manuscript will ensure accessibility for a broader audience. Addressing the concerns regarding point cloud datasets more thoroughly could strengthen the paper's impact. Lastly, clarifying confusing notations and figures will improve accessibility for readers unfamiliar with the underlying concepts.