ID: GI5j6OMTju
Title: Accelerating Sparse Autoencoder Training via Layer-Wise Transfer Learning in Large Language Models
Conference: EMNLP/2024/Workshop/BlackBoxNLP
Year: 2024
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 3, 4, 2

Aggregated Review:
### Key Points
This paper presents an extensive analysis of Sparse Autoencoders (SAE) training methodologies, focusing on the computational costs associated with training SAEs from scratch. The authors propose reusing weights from SAEs trained on adjacent layers, demonstrating that this approach can enhance reconstruction loss, mean max cosine similarity, and human evaluations. They validate their hypothesis convincingly and provide a new method to expedite SAE training.

### Strengths and Weaknesses
Strengths:
1. The paper includes exhaustive experiments with various metrics, providing solid empirical backing for the transferability of SAEs across layers.
2. Insightful observations on the transferability of SAE (forward-backward) contribute significantly to the technical literature.
3. The clarity of writing and presentation of results is commendable, making the findings accessible to the community.

Weaknesses:
1. The importance of the metrics could be better explained and summarized, particularly regarding their relevance to the findings.
2. Some experimental results are presented too briefly, leading to confusion, particularly with Figure 5 and the DLA score.
3. The scope of experiments is limited concerning model size, families, and datasets, which may affect the generalizability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the metrics used, providing a paragraph on their significance. Additionally, conducting ablation studies on the dimensions of the SAE across different LLMs could enhance the analysis. Exploring the transferability of SAEs trained on non-adjacent layers and providing a clearer presentation of experimental results, especially regarding compute metrics (FLOPs, steps), would also be beneficial. Finally, including performance metrics on downstream tasks alongside reconstruction scores could provide a more comprehensive evaluation of the proposed methods.