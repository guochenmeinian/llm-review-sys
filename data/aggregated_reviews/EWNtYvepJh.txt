ID: EWNtYvepJh
Title: Generative Neural Fields by Mixtures of Neural Implicit Functions
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 5, 5, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel representation of neural fields through a mixture of implicit neural representations (INRs), enhancing the training of generative models across various data modalities. The authors propose a mixture model that utilizes a shared set of INRs, with weights optimized per data point. The effectiveness of this model is demonstrated on datasets such as Celeba-HQ, ShapeNet, and SRN Cars, showing competitive performance against existing methods. Additionally, the authors reconcile efficient inference with expansive capacity by constructing mixtures within the INR's weight space, achieving significant efficiency improvements over INR baselines, particularly in NeRF scenes, with metrics indicating 199 times fewer FLOPS, 49 times faster fps, and 22 times reduced memory compared to Functa during single view inference. A two-step training process for context adaptation and latent diffusion model training is employed, influencing both reconstruction and generation performance.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a novel INR representation that leverages recent advancements in generative model training across different data modalities.  
- The proposed method shows remarkable efficiency in handling large coordinate inputs while maintaining visual quality.  
- Detailed performance metrics are provided, showcasing the advantages over existing methods.  
- The methodology is well-articulated, and the paper is generally well-written, making it accessible.  
- The authors effectively address reviewer concerns regarding model internals and provide visualizations that support their claims.

Weaknesses:  
- The model description lacks clarity, particularly in comparing it to previous works, and essential aspects of the mixture representation are not sufficiently explored.  
- The paper does not adequately specify the latent sampling strategy, which is crucial for generating new data.  
- Important related works are missing from the discussion, which could affect the perceived novelty of the proposed method.  
- There is a noted lack of discussion on existing works, which has been pointed out by multiple reviewers.  
- Some reviewers expressed concerns about the choice of baselines and the need for deeper analysis.  
- Limitations regarding scalability and representation size are not thoroughly addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the model description and provide a more detailed comparison with previous works. It is essential to elucidate what the mixture representation captures, possibly by showing reconstructed images of individual components. Additionally, the authors should clarify the latent sampling strategy used in their experiments. We suggest including a direct comparison of run-time memory and inference time with baseline methods to substantiate claims of efficiency. Furthermore, we recommend that the authors enhance the discussion on existing works to provide a more comprehensive context for their contributions. Finally, addressing the limitations of the proposed method, particularly concerning scalability and representation size, in the main text would strengthen the paper.