ID: QMCjppVJbB
Title: SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SEAHORSE, a multilingual, multifaceted dataset for summarization evaluation, comprising 96,000 summaries with human ratings across six quality dimensions: comprehensibility, repetition, grammar, attribution, main ideas, and conciseness. The dataset encompasses six languages and nine systems, including human-created summaries, and is derived from four existing datasets (mlsum, xsum, xlsum, wikilingua). SEAHORSE serves as both a benchmark for existing systems and a training resource, demonstrating strong performance in evaluations on TRUE and mFACE.

### Strengths and Weaknesses
Strengths:
- The dataset fills a significant gap in summarization research, being the largest multilingual and multi-dimensional resource available.
- It features a substantial size and comprehensive human annotations across multiple languages and dimensions.
- The paper is well-structured and clearly written, providing thorough descriptions of the dataset and its annotation process.

Weaknesses:
- The models generating the summaries lack structural diversity, primarily comprising variations of the T5 and palm models, which may limit the learning of diverse error patterns.
- The binary scale used for quality evaluation questions may not capture the nuances of summary quality, particularly for dimensions like conciseness and attribution.
- The first three evaluation questions are often trivially answered positively, potentially diminishing the dataset's contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of terminology by reconsidering the frequent use of "systems" in the abstract and main body, possibly specifying it includes human reference summaries. In Table 1, please use full language names instead of acronyms, and in Table 3, consider adding the specific quality dimensions to the column heads. Additionally, we suggest exploring a multi-level scoring system for the evaluation questions to enhance the granularity of the assessments. Providing more details on the training and fine-tuning of the models used for summary generation would also be beneficial. Lastly, we encourage the authors to analyze the types of errors made by the systems to gain insights into the dataset's utility for future research.