ID: kvjbFVHpny
Title: EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 6, 6, 7, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents EvoCodeBench, a new benchmark for evaluating Large Language Models (LLMs) in code generation, addressing critical limitations such as data leakage and lack of domain-specific evaluations. The benchmark is dynamically updated every six months to mitigate data leakage and features a taxonomy of ten popular programming domains. The first version, EvoCodeBench-2403, includes 275 samples from 25 repositories and provides comprehensive annotations, including natural language requirements and test cases.

### Strengths and Weaknesses
Strengths:
1. EvoCodeBench effectively addresses data leakage issues through its evolving nature.
2. The domain taxonomy allows for nuanced performance analysis across various programming areas.
3. The benchmark aligns well with real-world repository characteristics, enhancing its practical relevance.
4. Comprehensive annotations and evaluation metrics (Pass@k and Recall@k) are provided.
5. The benchmark reveals significant insights into LLM performance across different domains.

Weaknesses:
1. The current version has a relatively small sample size of 275, which may limit comprehensive evaluations.
2. The focus is exclusively on Python, restricting applicability to other programming languages.
3. The explanation of the benchmark construction process lacks sufficient detail.
4. The proposed Domain-Specific Improvement (DSI) metric is not formally defined.
5. There are potential biases in auto-generated annotations, despite human evaluation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the technical aspects of the benchmark construction process by including a figure that illustrates the overall workflow. Additionally, expanding the dataset to include more samples across various domains would enhance its robustness. The authors should also consider incorporating cross-domain evaluations to assess LLM performance across multiple programming contexts. Furthermore, we suggest that the authors explore the inclusion of additional programming languages beyond Python to broaden the benchmark's applicability. Lastly, it is essential to provide a formal definition and calculation for the DSI metric to clarify its significance in the evaluation framework.