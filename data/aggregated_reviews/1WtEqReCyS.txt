ID: 1WtEqReCyS
Title: Multilingual Diversity Improves Vision-Language Representations
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 8, 8, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a systematic study exploring the performance benefits of using translated non-English data for English vision tasks. The authors demonstrate that continual pre-training on translated multilingual image-text pairs enhances the performance of English models across various tasks. They provide empirical evidence showing that diverse training data improves benchmark results requiring geographical or cultural diversity.

### Strengths and Weaknesses
Strengths:
- The authors empirically show that incorporating translated multilingual datasets can improve English model performance on English-only tasks.
- The paper includes interesting analyses indicating that diverse training data enhances benchmark results that necessitate geographical or cultural diversity.
- The writing is clear and the claims are substantiated with substantial empirical evidence.

Weaknesses:
- The novelty of insights is limited, as improvements from translated data diminish with training convergence, and using more training data to enhance evaluation results is expected.
- The evaluation is restricted to one LLM, necessitating further evaluations to validate the generality of the claims.
- There is a lack of quantitative assessment for translation quality and the analysis could benefit from deeper exploration of the diversity afforded by multilingual versus English-only data.

### Suggestions for Improvement
We recommend that the authors improve the paper by adding more multicultural evaluation datasets and conducting additional evaluations to demonstrate the benefits of diverse training data across multiple model sizes and types. It would be beneficial to perform similar training and evaluation in other languages to show that all languages could benefit from diverse training images and captions. Additionally, we suggest including a deeper analysis of the diversity of visual concepts in both English and multilingual datasets, potentially through clustering experiments. Finally, discussing the implications of increased train-test similarity due to filtering on translated image-text pairs would enhance the paper's insights.