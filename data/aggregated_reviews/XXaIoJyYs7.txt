ID: XXaIoJyYs7
Title: MedJourney: Benchmark and Evaluation of Large Language Models over Patient Clinical Journey
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 5, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MedJourney, a comprehensive benchmark dataset designed to evaluate large language models (LLMs) based on the clinical journey of patients, segmented into four stages: planning, access, delivery, and ongoing care. The benchmark includes 12 datasets, totaling 4,960 medical cases, dialogues, and question-answer pairs, with five datasets newly constructed and annotated by experienced physicians. The authors propose a structured evaluation framework for LLMs, assessing their performance through accuracy, natural language generation (NLG) metrics, and entity-level recall, while also addressing ethical considerations in dataset construction. However, the relevance of the clinical process to LLM performance evaluation is inadequately justified, and concerns about data quality and ethical considerations are raised.

### Strengths and Weaknesses
Strengths:
- The introduction of the MedJourney benchmark represents a significant advancement in medical AI, focusing on real-world clinical questions.
- The dataset features a comprehensive range of tasks across the clinical journey stages, including new task curation.
- The evaluation includes zero- and one-shot scenarios, enhancing the understanding of LLM performance.
- The datasets are well-structured and adhere to ethical guidelines for data privacy, demonstrating a thorough understanding of the regulatory landscape regarding medical data in China.
- Collaboration between annotators and meta-annotators minimizes disagreement and enhances data quality.

Weaknesses:
- The description of the clinical process lacks depth and reliable references, undermining its significance.
- There is insufficient discussion on data quality control and potential biases in the dataset.
- The evaluation relies heavily on automated metrics, which may not fully capture LLM performance nuances.
- Lack of transparent criteria for scenario selection and dataset composition.
- Insufficient representation of longitudinal care and interconnectedness of stages.
- Claims of real-world applicability are questionable due to methodological limitations.
- The transformation process of DR and PCD data may lead to significant deviations from original patient entries, impacting the authenticity of the datasets.

### Suggestions for Improvement
We recommend that the authors improve the introduction by elaborating on "the clinical process" and its importance for readers unfamiliar with the medical domain. Additionally, enhance the description of the clinical process and substantiate its relevance with peer-reviewed sources. Clarifying the quality control measures for the dataset is essential. We suggest incorporating qualitative assessments by clinical experts to complement automated metrics and discussing potential biases related to gender, age, and socioeconomic status. Furthermore, the authors should explore how LLMs can be integrated into clinical workflows and provide practical guidelines or case studies. 

We also recommend improving the transparency of the dataset by clearly defining and publishing the criteria used for disease selection and scenario construction. It is essential to disclose what specific aspects of medical knowledge and decision-making the benchmark evaluates, such as diagnostic reasoning or treatment planning. A comprehensive guide detailing what researchers can learn about LLM performance using this benchmark, including its strengths and limitations, should also be published.

Moreover, we suggest conducting an analysis to quantify how well the benchmark reflects the complexity and uncertainty of real clinical environments. Developing a scoring system that evaluates LLM performance across the entire patient journey, assessing consistency and appropriateness of decisions, would enhance the benchmark's utility. Lastly, addressing the concerns regarding the evaluation metrics and providing more details on the methodology behind the GPT-4 ratings would strengthen the study's claims. Additionally, we recommend improving the clarity of the transformation process for DR and PCD data to ensure that the extent of deviation from original entries is transparent, and providing more detailed examples of the ethical considerations addressed in their work to alleviate any lingering concerns.