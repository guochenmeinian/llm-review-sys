ID: fvKaLF1ns8
Title: InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 8, 7, 7, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents InterCode, an interactive coding benchmark that formalizes coding tasks as a reinforcement learning (RL) interaction problem, where actions are code and observations are execution feedback. The authors implemented two test sets in Bash and SQL, alongside new tasks and experiments, including Python and CTF tasks, with the platform being language-agnostic and extensible. The framework enables execution-based methods and evaluations, ensuring safety through Docker execution. Evaluation of instruction-tuned LLMs on InterCode demonstrates improved performance in error correction and context discovery with multi-turn prompting, showcasing the framework's effectiveness and allowing exploration of diverse languages and tasks.

### Strengths and Weaknesses
**Strengths:**
- The interactive evaluation benchmark is novel and well-implemented, making it user-friendly and safe.
- It effectively quantifies LLMs' capabilities in error correction and context discovery, an area previously underexplored.
- The empirical contributions are robust, showcasing a multi-task coding benchmark that facilitates the exploration of new tasks and evaluation methods.
- The clean abstraction and accessible implementation via GitHub are likened to the value brought by OpenAI Gym to the RL field.

**Weaknesses:**
- The current framework primarily supports Bash and SQL, with limited applicability; inclusion of general-purpose languages like Python or Java is necessary.
- The benchmark lacks comprehensiveness, with a small number of problems and predefined system directories in the Bash task, which could reduce diversity.
- Support for compiled languages is less preferable, relying on third-party interpreters or Bash environments, which may not reflect real-world software development practices.
- Some experimental settings and comparisons to related work, such as WOB and MiniWoB++, are insufficiently addressed, and the paper's conceptual contributions may require further clarification to emphasize their importance.

### Suggestions for Improvement
We recommend that the authors improve the framework by expanding language support to include at least one general-purpose programming language, such as Python or Java. Additionally, the authors should enhance the benchmark's comprehensiveness by increasing the number of problems and diversifying the Bash environment. Clarifying experimental settings, particularly regarding the absence of certain models on specific tasks, would also strengthen the paper. Furthermore, we suggest that the authors improve support for compiled languages by providing a more integrated approach rather than relying on less preferable third-party interpreters. Finally, a more thorough discussion of related work and a clearer emphasis on the significance of the conceptual contributions would be beneficial.