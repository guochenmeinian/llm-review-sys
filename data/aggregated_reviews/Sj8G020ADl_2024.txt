ID: Sj8G020ADl
Title: Inexact Augmented Lagrangian Methods for Conic Optimization: Quadratic Growth and Linear Convergence
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 8, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents new theoretical results on the convergence of primal iterates in inexact augmented Lagrangian methods (ALMs) for conic optimization. The authors establish quadratic growth and error bound conditions for primal and dual programs under the strict complementarity assumption, leading to linear convergence for both iterates. The paper also discusses the application of these methods to various conic problems, including linear programming (LP), second-order cone programming (SOCP), and semidefinite programming (SDP), supported by experimental results demonstrating linear convergence.

### Strengths and Weaknesses
Strengths:
- The work provides a thorough introduction to the problem background.
- The theory is presented rigorously, with high-quality mathematical proofs.
- The paper is well-structured and clearly written, making complex content accessible.

Weaknesses:
- The presentation of results is lacking; it takes too long to reach the main contributions, particularly Theorem 3, which is not adequately proven in the main text.
- The notation and terminology can be complex, making it difficult to follow; a more focused discussion on a specific problem class (e.g., LPs) could enhance clarity.
- There is insufficient experimental evaluation of LPs and SOCPs, limiting the practical validation of the theoretical claims.
- The relationship of the work to machine learning is underexplored, and the numerical validation section is brief and lacks depth.

### Suggestions for Improvement
We recommend that the authors improve the presentation by integrating Theorem 3 more prominently within the main discussion and providing a complete proof in the main text or a more comprehensive appendix. Additionally, we suggest focusing on a specific problem class, such as LPs, to simplify notation and enhance understanding. Expanding the experimental evaluation to include LPs and SOCPs, as well as providing a more extensive numerical validation section, would strengthen the paper. Furthermore, we encourage the authors to elaborate on the implications of their work for machine learning applications, possibly by demonstrating linear convergence on a conic program relevant to this field. Lastly, addressing the error tolerance schedule's impact on convergence guarantees would be beneficial.