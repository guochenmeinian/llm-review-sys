ID: IQU5NsX7Mj
Title: Memorize and Rank: Enabling Large Language Models for Medical Event Prediction
Conference: AAAI
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 6, 6, 6, 4
Original Confidences: 4, 4, 3, 4, 3

Aggregated Review:
### Key Points
This paper presents a novel strategy for medical event prediction using a large language model (LLM). The authors propose a two-step method involving (i) medical concept memorization to adapt the LLM's vocabulary, and (ii) contrastive learning to capture inter and intra-visit relations in medical events. The method achieves state-of-the-art performance on the MIMIC-III and MIMIC-IV datasets. Additionally, the authors introduce a foundation model based on LLaMA-2-7B, fine-tuned for medical code prediction tasks.

### Strengths and Weaknesses
Strengths:
- Clear motivation for the problem and effective LLM pre-training tasks that address main challenges.
- Inclusion of a task formulation section enhances understanding of the task and data format.
- The method outperforms state-of-the-art models in diagnosis prediction and heart failure prediction.

Weaknesses:
- Lack of an ablation study to identify which pre-training tasks contribute to performance gains.
- No consideration of Clinical LMs as base models, which may already understand medical vocabulary, potentially negating the need for the memorization phase.
- Poor readability in certain sections, with grammatical errors and unclear explanations, particularly regarding the proposed method.
- Missing clarity on the impact of input sequence perturbation and the evaluation metrics used for accuracy/precision/recall.

### Suggestions for Improvement
We recommend that the authors improve the clarity of writing throughout the manuscript, addressing grammatical errors and enhancing readability. Additionally, we suggest conducting an ablation study to clarify the contributions of different pre-training tasks. It would be beneficial to consider incorporating Clinical LMs in experimentation and to provide justification for the exclusion of transformer-based models as baselines. Furthermore, we encourage the authors to clarify whether "diagnosis prediction" is a multi-label classification and to elaborate on the role of clinical text in the input data. Finally, we recommend releasing the train/test split and dataset characteristics to standardize research in medical event prediction.