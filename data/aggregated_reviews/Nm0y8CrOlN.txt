ID: Nm0y8CrOlN
Title: A Quantum Annealing Instance Selection Approach for Efficient and Effective Transformer Fine-Tuning
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: 1, 2, -1, 1, -1
Original Confidences: 3, 4, 3, 4, 5

Aggregated Review:
### Key Points
This paper presents a novel Quantum Annealing Instance Selection approach aimed at enhancing the efficiency of Transformer fine-tuning for automatic text classification. The authors propose a QUBO formulation that allows for the reduction of training dataset size by up to 28% without compromising model effectiveness. Extensive experiments demonstrate the feasibility and benefits of the proposed method, including comparisons with simulated annealing and state-of-the-art approaches, highlighting its efficiency, effectiveness, and scalability.

### Strengths and Weaknesses
Strengths:
- The integration of quantum computing paradigms with instance selection is well-motivated and novel.
- Extensive and well-structured experimental results support the claims made in the paper.
- The approach addresses a significant issue in NLP regarding the environmental impact of large training datasets.

Weaknesses:
- The accessibility of quantum annealing hardware for researchers in the IR community is unclear, necessitating further discussion on hardware requirements.
- The paper primarily focuses on BERT, lacking evaluations with more recent models like RoBERTa, BART, and XLNet.
- Some sections, particularly those detailing the QUBO formulation and minor embedding process, may be challenging for readers unfamiliar with quantum computing.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the accessibility of quantum annealing hardware to better inform researchers unfamiliar with quantum computing. Additionally, we suggest including comparisons with more traditional non-quantum QA baselines and evaluating the proposed method with other Transformer models such as RoBERTa, BART, and XLNet within the paper rather than as future work. Simplifying complex sections or providing supplementary explanatory materials could enhance readability for a broader audience.