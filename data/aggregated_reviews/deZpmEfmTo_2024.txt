ID: deZpmEfmTo
Title: Domain Adaptation for Large-Vocabulary Object Detectors
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 5, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a solution to the domain generalization problem in large-vocabulary object detectors by introducing the Knowledge Graph Distillation (KGD) method. KGD leverages the implicit knowledge graphs within the CLIP model to enhance the adaptability of detectors across various downstream datasets without requiring additional annotations. The effectiveness of the proposed method is validated through experiments on multiple benchmarks.

### Strengths and Weaknesses
Strengths:
1. The proposed method demonstrates strong performance across 11 widely used datasets.
2. Detailed implementation details are provided in the appendix, including thorough analysis from motivation to results.
3. The method is intuitive and easy to understand, exploiting prior knowledge from visual-language models (VLM) effectively.

Weaknesses:
1. The training speed is a concern, as cropping proposals and utilizing CLIP can be time-consuming, especially with larger datasets like Object365.
2. The paper lacks a comprehensive comparison of memory usage and computational overhead.
3. The experimental comparisons are insufficient and unfair, as traditional methods are not directly comparable to KGD, which utilizes VLM capabilities.
4. The motivation for using knowledge graphs is not clearly articulated, and the terminology may confuse readers regarding the nature of the graphs used.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation for using knowledge graphs and consider alternative approaches, such as using CLIP for pseudo-labeling followed by semi-supervised learning. Additionally, we suggest conducting a more comprehensive comparison with contemporary methods based on visual-language models, such as RegionCLIP, and including memory usage and computational overhead in the analysis. The authors should also clarify the terminology used in the paper to align with common definitions and enhance readability. Finally, we encourage the authors to expand the ablation studies to include analyses across all datasets to demonstrate consistency in performance improvements.