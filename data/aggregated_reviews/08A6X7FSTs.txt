ID: 08A6X7FSTs
Title: Director3D: Real-world Camera Trajectory and 3D Scene Generation from Text
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Director3D, a text-to-3D generation framework that creates realistic 3D scenes with adaptive camera trajectories. The framework includes a Cinematographer (Traj-DiT) for generating camera trajectories, a Decorator (GM-LDM) for initial scene generation, and a Detailer (SDS++ loss) for refinement. The use of 3D Gaussians for scene representation is highlighted, and extensive experiments demonstrate its effectiveness.

### Strengths and Weaknesses
Strengths:
- The results exhibit high spatial consistency and impressive performance in both quantitative and qualitative evaluations.
- The writing is clear, and the novel approach of treating camera parameters as temporal tokens is effective.
- The design of using a trajectory generator and 3D Gaussian splats is innovative.

Weaknesses:
- The quantitative comparison with object-level methods is perceived as unfair, as the prompts include scene information affecting clip scores. Comparisons with scene-level methods like LucidDreamer are necessary.
- The evaluation is limited, lacking comparisons with camera control video generation methods.
- The formulation of SDS++ loss is unclear due to undefined variables, and its differences from other losses need clarification.
- Concerns about the diversity of generated trajectories and potential overfitting to specific orientations in the dataset are raised.

### Suggestions for Improvement
We recommend that the authors improve the quantitative comparisons by including scene-level generation methods such as LucidDreamer and ZeroNVS. Clarifying the formulation of SDS++ loss and its differences from existing losses would enhance understanding. Additionally, addressing the diversity of generated camera trajectories and discussing the limitations of the method, particularly regarding camera movement range, would strengthen the paper. Finally, providing visualizations of the geometry and depth rendering of the generated 3D Gaussians would solidify the results presented.