ID: s6qtLyR6uJ
Title: NeuroEvoBench:  Benchmarking Evolutionary Optimizers for Deep Learning Applications
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 1, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for evaluating the performance of evolutionary optimization (EO) methods, specifically focusing on deep learning applications. The authors assess various gradient-free optimization techniques, including Evolution Strategies and Genetic Algorithms, across multiple non-synthetic datasets. The benchmark is designed to facilitate comprehensive evaluations of EO methods in real-world scenarios, with all task evaluations implemented in JAX for efficiency. Additionally, the authors propose to enhance their work by incorporating a Batch Bayesian Optimization baseline and addressing specific reviewer feedback. They have added visualizations to demonstrate hyperparameter robustness and included results from non-neural network tasks to emphasize the need for a tailored benchmark for the deep learning community.

### Strengths and Weaknesses
**Strengths:**
- The work effectively addresses the challenge of benchmarking EO methods with real-world data and state-of-the-art optimization techniques.
- The paper is well-structured and clearly written, with a thorough introduction and thoughtful dataset curation.
- A broad range of experiments is conducted, covering multiple EO methods and diverse tasks, which enhances the study's relevance.
- The authors have proactively addressed reviewer comments, enhancing the manuscript with additional experiments and visualizations.
- The inclusion of a Batch Bayesian Optimization baseline is a valuable addition.

**Weaknesses:**
- The benchmark does not include comparisons with Bayesian optimization, which limits its comprehensiveness.
- A dependency error in the Colab notebook caused inconvenience for users.
- Figures 2, 3, and 4 are difficult to read and not color-blind friendly, making result interpretation challenging.
- There is insufficient discussion on prior work related to benchmarking EO methods for neural networks, and the paper lacks clarity on the characteristics and relevance of the presented problems.
- The color palette used in visualizations may not be suitable for color-blind individuals.

### Suggestions for Improvement
We recommend that the authors improve the presentation of results by enhancing the readability of figures and ensuring they are color-blind friendly. Summarizing results with final model performance comparisons and calculating ranking statistics would also clarify findings. Additionally, we suggest that the authors improve the discussion on Bayesian optimization by elaborating on its applicability to neural network tasks and including the benchmark results in the manuscript. We advise using line styles or marker styles to differentiate plots for better accessibility. Furthermore, the authors should discuss prior work more thoroughly, particularly differentiating their contributions from existing benchmarks like nevergrad. Clarifying the target ML applications and justifying the inclusion of non-EO algorithms would strengthen the paper. Finally, providing insights on the search space difficulty and landscape complexity would add depth to the analysis.