ID: Hd2EOwKItm
Title: Classification Done Right for Vision-Language Pre-Training
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 7, 6, 6, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel classification-based vision-language pretraining method called SuperClass, which utilizes a subword-level tokenizer to derive classification labels from raw text without preprocessing. The authors aim to enhance efficiency by eliminating the text encoder, training the vision encoder with a multi-label softmax loss weighted by Inverse Document Frequency (IDF). The authors claim competitive performance in zero-shot retrieval, segmentation, and various vision-language tasks, achieving higher accuracy in linear probing on ImageNet-1K compared to CLIP. They emphasize that their method does not rely on manually curated labels, using tokenized raw text instead, which enhances scalability and applicability. The experiments demonstrate promising results across classification and vision-language tasks, supported by extensive ablation studies.

### Strengths and Weaknesses
Strengths:
1. The method effectively highlights the potential of classification in vision-language pretraining, providing empirical evidence for further research.
2. SuperClass's straightforward framework allows for large-scale training on paired image-text data without preprocessing, showcasing practical applicability.
3. The approach is training-efficient, yielding favorable results compared to traditional methods, as evidenced by thorough experimental validation.
4. The proposed method demonstrates competitive performance across multiple tasks, including zero-shot retrieval and segmentation.
5. The approach utilizes tokenized raw text for supervision, avoiding the need for curated labels, which is a significant advantage in scalability.

Weaknesses:
1. The robustness of SuperClass across different model types is unclear, as all experiments utilize ViT, lacking evidence for other architectures like ResNet.
2. The paper does not adequately demonstrate the zero-shot capacity of the proposed model, particularly in comparison to CLIP and other competitors.
3. There is insufficient comparison with related works, particularly those utilizing weakly-supervised or unsupervised methods, limiting the contextual understanding of SuperClass's effectiveness.
4. The analysis of the use of subwords as labels lacks depth, raising questions about how suitable representations are learned.
5. The superiority of the proposed method over self-supervised learning approaches, particularly DINOv2, is not convincingly established, as it relies on a larger dataset.
6. Comparisons with existing models like CatCLIP are insufficient, and the necessity of the proposed method in the context of other strong models is questioned.

### Suggestions for Improvement
We recommend that the authors improve the robustness analysis by including experiments with various encoder architectures beyond ViT. Additionally, we suggest that the authors demonstrate the zero-shot capabilities of SuperClass on downstream tasks, such as zero-shot text-image retrieval. It is also essential to include comparisons with other weakly-supervised and self-supervised methods to contextualize the performance of SuperClass. Furthermore, we recommend that the authors improve the clarity of their comparisons with existing models, particularly addressing the performance of CatCLIP and other recent methods. We also suggest providing a more detailed explanation of the zero-shot COCO Text-to-Image results to clarify discrepancies with CLIP's reported outcomes. Finally, we encourage the authors to provide a more detailed analysis of how subwords as labels contribute to learning suitable representations and to emphasize the unique contributions of their method more clearly to justify its necessity in the current landscape of pre-trained models.