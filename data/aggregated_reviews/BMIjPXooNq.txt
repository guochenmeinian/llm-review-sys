ID: BMIjPXooNq
Title: Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method that utilizes dataset cartography to enhance compositional generalization in Transformer models. By selecting relevant data subsets (easy-to-learn, hard-to-learn), the approach achieves up to a 10% accuracy improvement on CFQ and COGS datasets. The authors implement dataset cartography as a criterion for curriculum learning, which mitigates hyperparameter tuning needs while improving performance. The findings challenge common assumptions about curriculum learning, particularly the effectiveness of starting with hard-to-learn samples.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with clear visualizations and comprehensive analyses.
- It offers interesting insights into leveraging training dynamics for compositional generalization, which has not been extensively explored.
- The results demonstrate significant improvements in compositional generalization.

Weaknesses:
- The scope is narrow, focusing solely on synthetic datasets and Transformer architectures, limiting broader applicability.
- The relationship between dataset cartography and compositional generalization is not sufficiently justified; hard examples may not necessarily be compositional.
- The experiments lack a detailed analysis to differentiate the benefits of hard-to-learn samples from those of curriculum learning.

### Suggestions for Improvement
We recommend that the authors expand the analysis to include a wider range of model architectures and real-world datasets to enhance the paper's impact. Additionally, we suggest providing more theoretical justification for how the proposed method enables better compositional generalization. Clarifying the effects of curriculum learning versus the selection of hard examples would strengthen the findings. Lastly, addressing the inconsistencies in curriculum learning performance across different datasets would improve the robustness of the conclusions.