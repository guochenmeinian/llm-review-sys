ID: 3JLtuCozOU
Title: A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 8, 6, 5
Original Confidences: 4, 3, 3

Aggregated Review:
### Key Points
This paper presents a comprehensive evaluation of privacy purification methods, focusing on the effectiveness of data sanitization techniques. The authors propose a framework that utilizes a linking attack and a semantic similarity metric to assess privacy protection capabilities. The study includes experiments on two datasets and analyzes the performance of four state-of-the-art anonymizers, revealing significant findings that challenge conventional privacy protection methods.

### Strengths and Weaknesses
Strengths:
- The work demonstrates high quality, clear expression, and strong practical significance.
- It includes adequate experiments and presents interesting key findings.
- The introduction of human assessments enhances the credibility of the results.
- The framework incorporates semantic similarity, providing stricter privacy guarantees beyond lexical matching.

Weaknesses:
- The categories of privacy-preserving technologies are inadequate, and the dataset's breadth limits the generalizability of the results.
- The rationale for using specific assessments is unclear, raising questions about their accuracy.
- The paper lacks suggestions for "appropriate privacy metrics" necessary for real-world applications, as smaller privacy indicators may not be better.
- The attack method assumes auxiliary information about the subject, which may not reflect real-world scenarios.

### Suggestions for Improvement
We recommend that the authors improve the breadth of privacy-preserving technology categories to enhance generalizability. Clarifying the rationale behind the chosen assessments and ensuring their accuracy is essential. Additionally, we suggest that the authors propose specific "appropriate privacy metrics" for practical applications, adhering to the principle of "Protection as needed." Incorporating discussions on generative data privacy efforts, such as "Security and privacy on generative data in AIGC: A survey" and "On protecting the data privacy of large language models (LLMs): A survey," would enrich the paper. Lastly, exploring the impact of perturbing auxiliary information in experiments could provide insights into real-world privacy leakage scenarios.