ID: wDfXP6uAkR
Title: Towards LLM-driven Dialogue State Tracking
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on prompting ChatGPT for task-oriented dialogue (TOD) benchmarks SGD and MultiWOZ, demonstrating strong performance. However, the authors note that ChatGPT is not a reliable backbone for building a TOD system due to its closed-source nature, request restrictions, data privacy concerns, and lack of local deployment capabilities. They propose LDST, a framework utilizing LoRA finetuning of a smaller open-source pretrained LLM (LLaMa) for dialogue state tracking (DST). The authors evaluate LDST across zero-shot, few-shot, and full-data regimes, reporting state-of-the-art results.

### Strengths and Weaknesses
Strengths:
- LDST is a novel approach to TOD, being the first to apply LoRa tuning of LLaMa in this context.
- The methodology is simple and effective, with thorough evaluations across multiple settings on key benchmarks.
- The Assembled Domain-Slot Instruction Generation method for creating instruction-tuning datasets is innovative.

Weaknesses:
- The paper does not cite the correct previous state-of-the-art results, omitting stronger results on SGD and MultiWOZ.
- Insufficient detail on how ChatGPT is prompted, impacting reproducibility.
- The specific approach of LoRa tuning on LLaMa lacks originality and relevance to the TOD setting, as similar methods are being explored in other NLP tasks.
- The paper does not clarify the contribution of using LLaMa over other models, necessitating additional ablation experiments.

### Suggestions for Improvement
We recommend that the authors improve the citation of previous state-of-the-art results, specifically including Ma et al. 2020 and Zhao et al. 2022 for a more accurate comparison. Additionally, please provide more details on the prompting methodology for ChatGPT to enhance reproducibility, including clarifications on terms like "multiple return" and "no/one demo." It would be beneficial to include comparisons with prior work on prompting ChatGPT for MultiWOZ and to conduct an ablation study replacing LLaMa with other language models to assess its impact. Furthermore, we suggest adding results on MultiWOZ 2.1 for completeness and addressing the discrepancies in JGA results across different settings. Lastly, please ensure that all implementation details and data preprocessing methods are thoroughly documented in the appendix.