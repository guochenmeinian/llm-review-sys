ID: iNB4uoFQJb
Title: Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 8, 7, 7, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Easy2Hard, a benchmark aimed at enhancing research in LLMs generalization and understanding through six datasets across various domains, including mathematics and chess puzzles. A significant contribution is the inclusion of a difficulty score that quantifies the transition from easy to hard problems. The authors conduct experiments with open-source LLMs, yielding insights into the generalization performance of current models.

### Strengths and Weaknesses
Strengths:
1. The introduction of a difficulty score for LLM generalization is novel and crucial for practical applications, with diverse datasets providing a broad perspective for exploration.
2. The methodology for difficulty estimation, utilizing IRT and Glicko-2 models, shows moderate performance compared to human evaluation.
3. Extensive experiments validate the benchmark and offer valuable insights into LLM performance and generalization capabilities.
4. The paper is well-motivated and clearly written.

Weaknesses:
1. The difficulty estimation scheme requires improvement, as alignment with human evaluation on the ARC and Winogrande datasets is only around 73%, indicating a need for better reflection of human assessments.
2. The presentation of figures could be enhanced for clarity.

### Suggestions for Improvement
We recommend that the authors improve the alignment of the difficulty estimation scheme with human evaluations to enhance its reliability. Additionally, we suggest enhancing the clarity of figure presentations. Furthermore, we encourage the authors to provide more motivation and explanation for their choice of "subjective" difficulty measurements over "objective" computational complexity metrics, as well as to discuss the representativeness of the selected tasks more thoroughly. Lastly, we advise clarifying the use of Glicko-2 metrics in the context of the Lichess dataset to avoid misrepresentation of contributions.