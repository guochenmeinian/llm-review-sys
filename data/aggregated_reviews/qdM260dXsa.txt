ID: qdM260dXsa
Title: Cross-Domain Policy Adaptation via Value-Guided Data Filtering
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 6, 7, 5, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for online dynamics adaptation in reinforcement learning (RL), specifically addressing the challenge of generalizing from a source domain with abundant data to a target domain with limited online interactions. The authors propose Value-Guided Data Filtering (VGDF), which selects source domain transitions based on value consistency, allowing for effective data sharing. The method is theoretically justified and empirically evaluated in various environments, including modified versions of the DeepMind control suite. Additionally, the authors discuss the use of TD-targets versus value functions in the context of Model-based Reinforcement Learning (MBRL), clarifying that learning the target domain reward function $r_{tar}(s, a)$ is necessary for compatibility with existing MBRL implementations. They argue that the differences between TD-targets and values are not significant in their problem setting, as empirical results support this claim. The authors also acknowledge the omission of a reference for the optimistic exploration technique and commit to including it in the revised manuscript.

### Strengths and Weaknesses
Strengths:  
- The paper establishes a strong connection between theoretical motivation and practical implementation, with an original "value discrepancy perspective."  
- The writing is clear and the paper is easy to follow, making it accessible to readers.  
- The empirical results demonstrate VGDF's effectiveness, showing significant contributions to the field of sim-to-real transfer in robotics and control.  
- The authors provide clear explanations addressing reviewer concerns, enhancing the clarity of their work.  
- Additional experiments in new environments demonstrate the generalizability of their method, showing VGDF's performance compared to baselines.

Weaknesses:  
- Some aspects of the paper, particularly the comparison to DARC, lack clarity and depth, raising questions about the conservativeness of DARC's estimations.  
- The experimental analysis is limited, primarily focusing on short-term control scenarios, which may not generalize to more complex tasks.  
- The exploration policy $\pi_E$ appears to be a minor component, and its significance is not well established.  
- Some reviewers noted that the paper lacks sufficient empirical breadth, particularly in comparing across diverse environments.  
- The discussion on the differences between TD-targets and values could be more thoroughly elaborated in the main text rather than relegated to the appendix.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their comparisons to DARC, particularly addressing the implications of its conservative estimations. Additionally, we suggest expanding the experimental analysis to include environments that require long-horizon planning, such as physical manipulation tasks. It would also be beneficial to provide more details on the implementation of the Gaussian dynamics models and clarify the discrepancies between the paper's methodology and its code implementation. Furthermore, we recommend that the authors improve the discussion on the differences between TD-targets and values by incorporating more detailed explanations in the main body of the paper. We also suggest enhancing the empirical investigations by including comparisons across a wider range of environments to address concerns about the limited breadth of their analyses. Lastly, we encourage the authors to ensure that all relevant references are included in the context of their discussions, particularly regarding the optimistic exploration technique.