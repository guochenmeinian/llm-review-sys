ID: CFQBcz7k8n
Title: Adversarially Robust Learning with Uncertain Perturbation Sets
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 7, 7, 6, 6, -1, -1, -1
Original Confidences: 3, 3, 3, 3, 5, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on adversarially robust learning with uncertain perturbation sets, where the perturbation set is random and drawn from a known class. The authors demonstrate learnability for concept classes with finite VC dimension under certain conditions, including access to a perfect attack oracle (PAO). They introduce a new notion of finite disagreement cover related to the perturbation class and explore robust learning with abstentions. The results indicate that while robust learning is achievable under specific assumptions, challenges remain in the agnostic setting.

### Strengths and Weaknesses
Strengths:
- The work removes the unrealistic assumption of fixed perturbation sets in prior theoretical studies.
- The authors provide examples illustrating the utility of abstention and the complexities of agnostic learning.
- The paper includes proofs or sketches for major results and compares them to prior works, clarifying its contributions.

Weaknesses:
- The assumption of realizability for the perturbation set is strong and warrants further discussion. 
- The paper lacks positive results in the agnostic case, which could limit its applicability.
- Some results, such as Theorem 5, are not adequately discussed.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the realizability assumption, specifying that it pertains to U rather than H. Additionally, please address typos found in lines 113, 358, and 364. Consider including positive results in the agnostic case within the main body (lines 408-413). Lastly, we suggest adding a conclusion section to summarize limitations and further questions, such as the lack of positive results in the agnostic case and the tightness of sample complexity bounds.