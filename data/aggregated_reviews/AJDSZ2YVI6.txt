ID: AJDSZ2YVI6
Title: PALS: Personalized Active Learning for Subjective Tasks in NLP
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for selecting data for annotation in subjective tasks using active learning, aiming to determine individual thresholds for acceptability with fewer annotations. The authors validate their approach on three datasets, demonstrating that their selected annotations yield improved accuracy. They identify five metrics, including controversy and ratio distance, to enhance the annotation process. The results indicate that their method outperforms a random baseline while requiring fewer training samples.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written, with clear research questions and effective presentation of intuitions through figures.  
- The approach addresses an important issue in subjective task annotation, potentially reducing the psychological burden associated with excessive labeling.  
- The experimental design follows established literature in active learning, and the results support the authors' claims.

Weaknesses:  
- The analysis lacks comparisons with generalized active learning strategies, such as an ablation study without user dimensions.  
- There are concerns regarding data splitting and potential data leakage due to overlapping user data across different timeframes.  
- The performance plateau for each dataset is not discussed, leaving questions about the model's capabilities when trained on the entire dataset.  
- The presentation of figures could be improved for better readability, and there are several grammatical errors throughout the text.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their data splitting methodology to avoid confusion regarding potential data leakage. Additionally, we suggest including comparisons with generalized active learning strategies to strengthen their analysis. It would be beneficial to discuss the performance plateau for each dataset to provide a clearer understanding of the model's capabilities. We also encourage the authors to enhance the presentation of figures for better readability and to address the grammatical errors noted in the review. Finally, justifying the choice of the XLM-RoBERTa model over other pre-trained models and explaining the varying performance of different strategies would add depth to the paper.