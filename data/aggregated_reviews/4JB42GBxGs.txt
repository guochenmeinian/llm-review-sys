ID: 4JB42GBxGs
Title: Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 6, 5, 6, 5, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for learning Wasserstein distances and other symmetric and factor-wise group invariant (SFGI) functions. The authors propose a neural network architecture that approximates SFGI functions, integrating a sketching mechanism that allows model complexity to remain independent of input point set sizes. Empirical results demonstrate the effectiveness of this approach, particularly in approximating the p-th Wasserstein distance between point sets.

### Strengths and Weaknesses
Strengths:  
1. The introduction of SFGI functions and a general neural network architecture for their approximation is a novel contribution that addresses the need for invariant distance functions on complex objects.
2. The bounded model complexity of the proposed architecture is significant, as it allows for approximation of the p-th Wasserstein distance without dependence on input sizes.
3. The integration of sketching ideas enhances the efficiency of the neural network, making it applicable to various geometric optimization problems.
4. Empirical evaluations show promising results, with the proposed architecture outperforming existing models in generalization and training speed.

Weaknesses:  
1. The comparison of the proposed architecture with existing models, such as the Siamese Autoencoder, is limited and primarily focuses on the 1-Wasserstein distance, lacking a broader evaluation including p=2.
2. The generalizability of the framework to other complex objects or distance functions is not thoroughly explored, which could strengthen the contribution.
3. The proposed sketching algorithm's complexity is exponentially dependent on the dimension of the points, which may limit its applicability in high-dimensional settings.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the necessity of learning Wasserstein distances and include results in the appendix that demonstrate better inference times than Sinkhorn. In Theorem 2.1, consider updating the output dimension of phi to 2kn+1, as indicated in Corollary 2.2 from [1] and Theorem 3.3 from [2], which may lead to improved complexity in subsequent theorems. Clarify the definition of the infinity norm on line 127, ensuring that F_1 and F_2 are subsets of the same C(X,R). In Definition of W_p, replace D with D^p and adjust the index of w_n' to w_m'. Specify that the group G acts on X in Definition 3.1. In Lemma 3.2, consider whether f needs to be uniformly continuous if working on compact metric spaces. Revise line 185 to "then we can choose" and clarify the strength of the topological embedding assumption on line 191. Ensure consistency in notation, such as changing M back to N on line 238. Provide a clearer explanation of the first inequality under line 254, and reorder quantifiers in Corollary 3.5 to specify that delta depends on epsilon. In Corollary 3.6, mention the dependence on dimension in addition to epsilon. Lastly, clarify the reference to the permutation+orthogonal problem on line 354.