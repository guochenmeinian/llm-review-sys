ID: NXLjaYdgaL
Title: Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 5, 7, 6, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the modality gap in the CLIP model, arguing that the optimal proxy for visual tasks resides solely in the visual space. The authors propose an intra-modal proxy learning method to enhance the performance of the CLIP model using only unlabeled test data. Their experiments across various datasets validate the effectiveness of their approach.

### Strengths and Weaknesses
Strengths:
- The motivation is clear, and the theoretical analysis is robust.
- The method derived from this analysis is convincing.
- The paper is well-organized and easy to understand.
- Extensive experiments demonstrate the effectiveness of the proposed method.

Weaknesses:
- Clarification is needed regarding whether Tau_T in the submission corresponds to the temperature parameter in CLIP, which is initialized at 0.07 and becomes 100 after training.
- The contribution of alpha appears minimal, as indicated by the drop in accuracy from 63.74 to 63.49 when ignoring this step.
- Results on common datasets like UCF101 and Eurosat are missing.
- The authors should provide results using the default prompt instead of the modified one.
- The TPT results seem unfairly compared to the CLIP baseline, which is significantly higher; if the baseline uses an ensemble prompt, this should be addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the relationship between Tau_T and the temperature parameter in CLIP. Additionally, the authors should include results on UCF101 and Eurosat, and provide results using the default prompt. It is also essential to clarify the comparison of TPT results with the CLIP baseline to ensure fairness in evaluation. Finally, we encourage the authors to discuss the differences between standard zero-shot learning methods and CLIP-based approaches to provide a more comprehensive context for their work.