ID: bqaW5sGZOq
Title: Revisiting Machine Translation for Cross-lingual Classification
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an assessment of machine translation (MT) in cross-lingual classification, comparing two primary approaches: translating training data into the target language ("translate-train") and translating the test set into English ("translate-test"). The authors demonstrate that translate-test can outperform translate-train in various tasks, contingent on the adaptation of MT output to align with the English data used for finetuning. They also categorize the sources of performance loss in classification when transitioning to non-English languages and analyze their impact through subtraction operations between model accuracies.

### Strengths and Weaknesses
Strengths:
- The paper offers valuable insights for developing text classification systems for low-resource languages.
- It presents a clear methodology and well-justified experimental design.
- The analysis framework for understanding cross-lingual transfer gaps is commendable.

Weaknesses:
- The variance analysis in Section 4 lacks theoretical motivation and oversimplifies the interactions between degradation factors.
- Some results, particularly in Section 4, are challenging to interpret, and the significance of improvements in quantitative experiments is questionable.
- The relevance of the translate-test approach to advancing cross-lingual studies is unclear, and the paper does not adequately address potential biases in train-test distributions.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the variance analysis in Section 4 and clarify the implications of negative values in Figure 2. Additionally, it would be beneficial to provide a clearer explanation of the relevance of the translate-test approach within the context of cross-lingual studies. We also suggest including a figure that illustrates the data generation process for MT to enhance clarity. Finally, addressing the potential biases in the assumption of comparable distributions between training and test data would strengthen the methodology.