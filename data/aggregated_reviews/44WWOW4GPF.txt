ID: 44WWOW4GPF
Title: Learning symmetries via weight-sharing with doubly stochastic tensors
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 3, 4, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for learning approximate equivariances to unknown groups using soft weight sharing schemes, specifically through the application of doubly stochastic tensors on canonical weights. The approach generalizes group convolutional neural networks (GCNNs) and demonstrates the ability to learn natural symmetries in empirical experiments. The authors claim that their method can represent arbitrary group equivariances and improve generalization performance. Additionally, the paper offers a technically solid approach to automatic (partial) symmetry discovery, with the potential for high impact in the community. However, concerns are raised regarding the empirical validation of the method, particularly in practical applications that could benefit from equivariance.

### Strengths and Weaknesses
Strengths:
1. The background section is well-written and provides clarity beyond other related works.
2. The method offers a natural generalization of group convolutions, allowing for the capture of non-group symmetries.
3. The paper is well-structured, with thorough empirical validation supporting the proposed approach.
4. The authors provide a clear explanation of their approach and its theoretical foundations.

Weaknesses:
1. The justification for the claim regarding non-elementwise activations is insufficient and requires more specific evidence.
2. The direct parameterization of kernels for each group element is computationally expensive, necessitating an empirical analysis of runtime and memory usage.
3. The empirical results are limited, with few baselines considered, and the proposed method does not show significant improvement over existing baselines.
4. The empirical experiments primarily focus on basic datasets (MNIST and CIFAR10), which may not reflect real-world applicability.
5. There is a lack of experiments demonstrating the method's scalability and effectiveness in more complex tasks that could benefit from equivariance.
6. Current experiments suggest that the invariance learning properties work in the underfitting regime, lacking a comparison with non-invariant models.
7. Missing comparisons with prior works, particularly regarding the advantages of double stochasticity, and a lack of discussion on the generalizability of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the justification for the claim about non-elementwise activations by providing specific evidence. Additionally, conducting an empirical runtime and memory analysis would clarify the computational costs associated with the direct parameterization of kernels. Expanding the empirical results to include more baselines and ablation studies, such as removing the Sinkhorn projection, would strengthen the evaluation of the method. We suggest conducting experiments in application areas that significantly benefit from equivariance to validate the method's effectiveness. It would also be beneficial to include a baseline comparison with non-invariant models to demonstrate the advantages of learning invariances. Finally, exploring the scalability of the method in more complex environments is crucial for future research, along with a clearer discussion on the quantitative benefits of double stochasticity and its implications for metric spaces.