ID: nbG6zfJtIe
Title: Learning Curves for Deep Structured Gaussian Feature Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 5, 6, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 1, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the generalization performance of models using multi-layered Gaussian random features, specifically focusing on the impact of feature anisotropy. The authors analyze the influence of weight structure on generalization, demonstrating that correlations within the first layer can enhance performance, while additional structure generally detracts from it. The study employs the replica method to derive learning curves for deep linear random feature models, providing a general expression for limiting generalization error that aligns with previous findings.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with a solid mathematical foundation and interesting theoretical results regarding the correlation between the first layer's features and generalization.
- The inclusion of numerical experiments and visualizations, particularly in Figure 2, effectively supports the theoretical claims.
- The exploration of deep RFMs with weight anisotropy is a relevant and timely contribution to the field.

Weaknesses:
- The presentation lacks clarity, particularly for non-expert readers, and could benefit from a more detailed introduction and connections between main results.
- The discussion of related works is limited, missing references to recent studies on random feature models that could provide context.
- Some theorems and results are difficult to interpret due to complex notation and numerous constants, which could confuse readers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by expanding the abstract and introduction to better guide the reader, including explicit connections to the literature on random feature models. Additionally, we suggest that the authors summarize the implications of their theorems in simpler terms to enhance understanding. Including proofs in supplementary material could allow for more interpretive text in the main body. Furthermore, addressing the questions raised regarding the impact of data point numbers on Figure 2, the gap between theoretical and experimental results, and the intuition behind certain constraints would strengthen the manuscript. Lastly, we encourage the authors to clarify the notation used throughout the paper to improve readability.