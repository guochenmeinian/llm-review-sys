ID: HvcLKgtbco
Title: Towards a Comprehensive Benchmark for High-Level Synthesis Targeted to FPGAs
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 7, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark named HLSYN for machine learning applications in high-level synthesis (HLS), consisting of 42 unique programs/kernels and over 42,000 labeled designs. The dataset is designed for training and evaluating machine learning-based quality prediction models, encompassing performance metrics and resource utilization. The authors propose a thorough comparison of state-of-the-art neural network models for predicting HLS circuit latency and resource utilization on FPGAs.

### Strengths and Weaknesses
Strengths:
- HLSYN includes a diverse range of optimization pragmas and kernels, providing a comprehensive resource for advancing research in HLS.
- The dataset is significantly larger than previous databases, facilitating valuable insights into design quality prediction.
- The paper is well-structured, clear, and easy to understand, even for researchers without an HLS background.

Weaknesses:
- The claim of being the 'first work' in this area is misleading, as prior works have also released HLS datasets.
- The dataset's diversity is limited, with many kernels targeting similar operations, and the sampling strategy may not adequately represent the entire design space.
- Certain technical aspects, such as the classification/regression result metrics and the rationale for fixed pragmas, require further clarification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the abstract by avoiding the claim of being the 'first work' in this area. Additionally, please provide more explanation for Figure 3 and clarify the rationale behind fixing the number of pragmas in each kernel. It would be beneficial to include the classification/regression result metrics in Tables 7, 8, 9, and 10. 

We suggest exploring additional split methods for training, validation, and testing to ensure that the ML model is tested on truly 'unseen' new kernels. Furthermore, consider including WNS or max frequency as quality metrics for designs. 

To enhance the dataset's representativeness, we recommend providing a proof-of-concept demonstrating that the dataset adequately covers the search space and does not lead to out-of-distribution issues. Lastly, we encourage the authors to improve the README.md documentation to facilitate reproducibility and troubleshooting.