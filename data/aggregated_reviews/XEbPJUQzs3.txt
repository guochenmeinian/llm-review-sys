ID: XEbPJUQzs3
Title: Prospective Learning: Learning for a Dynamic Future
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a formalization of “prospective learning,” which models data as sampled from a stochastic process, contrasting with the static distribution assumption of the PAC framework. The authors develop a theoretical framework to characterize conditions under which a stochastic process can be learned by a time-aware model, providing guarantees for function approximation through time-varying hypotheses. They illustrate their theory with simple examples and conduct experiments to validate their theoretical claims in more complex scenarios. Additionally, the paper examines prospective learning in the context of Markov chains and task switching, proposing a framework that highlights the implications of steady-state distributions and class clashes on prospective Bayes risk. The authors clarify that while the time-agnostic MLP model could utilize context information, such information is not available in prospective learning. They also discuss how a time-aware model can infer hidden states of the data generation process, converging to the Bayes risk under certain conditions.

### Strengths and Weaknesses
Strengths:
- The manuscript provides a solid theoretical foundation for understanding time-aware learning, offering explicit solutions for binary classification with Gaussian inputs and analyzing the complexity of learning periodic processes.
- The authors effectively illustrate their theoretical work with simple examples and conduct well-executed experiments that validate their theoretical assumptions.
- The paper is well-structured and clearly written, making complex concepts accessible, with a helpful appendix that relates their framework to existing work in meta-learning and continual learning.
- The authors provide clear and thorough responses to reviewer concerns, enhancing the understanding of their methodology.
- The proposed framework effectively contrasts different scenarios, aiding in the comprehension of prospective Bayes risk.
- The inclusion of technical details, such as the use of Baum-Welch for estimating transition matrices, adds depth to the discussion.

Weaknesses:
- The complexity of the theoretical framework may challenge some readers, although the authors attempt to bridge this gap with explanations and intuitive insights.
- The clarity of the experimental results, particularly in section 5, is lacking, especially for scenario 3, which requires further investigation due to unexpected outcomes.
- The appendix's "plain English" explanations could be enhanced by establishing more formal connections between their framework and established concepts in reinforcement learning and meta-learning.
- The original Scenario 3 example may require clearer differentiation from the new example to avoid confusion among readers.
- An oversight regarding label swapping in Figure 5 needs correction to maintain accuracy.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental results in section 5, particularly for scenario 3, to align better with theoretical predictions. Additionally, consider providing a more formal comparison of prospective learning with existing concepts in reinforcement learning and meta-learning to enhance the appendix's explanatory power. Clarifying the distinction between MLE and time-aware MLE models in section 2.1 could also improve understanding. Furthermore, addressing how prospective learning handles unlabeled data and non-IID scenarios would strengthen the paper's applicability to real-world situations. Lastly, we recommend correcting the labels in Figure 5 to eliminate any inaccuracies and including a remark at the end of Section 4 to discuss the implications of time-aware ERM converging to the Bayes risk to enhance the paper's depth.