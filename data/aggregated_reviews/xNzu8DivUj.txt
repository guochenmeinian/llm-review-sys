ID: xNzu8DivUj
Title: Continually Improving Extractive QA via Human Feedback
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to continually improve extractive QA systems through human feedback. The authors propose a novel approach that demonstrates high practical value and applicability in other NLP applications. Extensive experiments show that interaction with human users can significantly enhance the performance of extractive QA systems, contributing to the understanding of continual learning with user feedback.

### Strengths and Weaknesses
Strengths:  
- The methodology is clear and well-structured, making it easy to follow.  
- The approach is innovative and relevant, providing significant improvements in extractive QA.  
- Comprehensive experiments and discussions on various settings support the claims made.  
- The authors' responses during the rebuttal period, including the release of annotated datasets and guidelines, enhance reproducibility.

Weaknesses:  
- The motivation for using human feedback instead of human labels is not clearly articulated.  
- There are uncertainties introduced by human interactions that need careful control and evaluation of crowdworker quality.  
- The paper lacks a clear assessment of the optimal number of interaction rounds and the performance upper bound of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind using human feedback compared to human labels for continuous training. Additionally, we suggest that the authors provide a well-defined guideline to control uncertainties introduced by human interactions and clarify how they ensure the quality of feedback from crowdworkers. Finally, addressing the optimal number of interaction rounds and the potential performance upper bound would strengthen the paper's contributions.