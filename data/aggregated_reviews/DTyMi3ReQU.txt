ID: DTyMi3ReQU
Title: You Are What You Annotate: Towards Better Models through Annotator Representations
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for modeling subjective tasks by embedding annotators and their annotations, addressing the issue of annotator disagreement. The authors evaluate their approach on various datasets, demonstrating that these embeddings can enhance model performance. The paper emphasizes the importance of understanding annotator characteristics and their impact on learning from disagreements.

### Strengths and Weaknesses
Strengths:
- The paper introduces a significant direction in modeling annotators and provides a novel integration of annotator idiosyncrasies.
- The analysis of the approach is detailed and insightful, contributing to a deeper understanding of the learning process.
- Extensive evaluation across multiple datasets and methods supports the findings.

Weaknesses:
- The paper's contribution is perceived to stem more from the analysis than from performance improvements, as it assumes a closed set of annotators.
- The experimental design raises concerns regarding the generalization of results, particularly in practical scenarios where training and test annotators differ.
- Some sections lack clarity and organization, which may hinder comprehension.

### Suggestions for Improvement
We recommend that the authors improve the organization of the paper, possibly by including a diagram of the experimental setup to clarify the methodology. Additionally, we suggest addressing the concerns regarding the creation of annotator embeddings, particularly how they may be influenced by the specific instances assigned to annotators. It would be beneficial to provide a more thorough discussion on the implications of using unknown annotators and to elevate the relevance of this experiment in the main body of the paper. Furthermore, we encourage the authors to ensure consistent naming conventions for datasets and to consider including a summary of main contributions in the introduction for better presentation.