ID: 7RMGI4slcb
Title: Order Matters in the Presence of Dataset Imbalance for Multilingual Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 5, 7, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multi-task learning strategy involving pretraining on high-resource tasks followed by fine-tuning on a combination of high and low-resource tasks. This approach significantly enhances performance on low-resource tasks while maintaining or improving performance on high-resource tasks. The authors validate their method through experiments on machine translation and language modeling across multiple languages.

### Strengths and Weaknesses
Strengths:
- The proposed method is simple, effective, and easy to implement, achieving lower validation loss compared to existing methods.
- The empirical analysis is extensive, with reasonable baselines and clear experimental design.
- The narrative is well-structured and easy to follow.

Weaknesses:
- There are unresolved questions regarding the importance of the pretraining objective and the influence of the chosen high-resource language on low-resource performance.
- The algorithm's sensitivity to sampling proportions necessitates extensive hyperparameter tuning, which could limit practical application.
- The evaluation metrics primarily focus on perplexity, lacking downstream task evaluations that would demonstrate broader applicability.

### Suggestions for Improvement
We recommend that the authors improve the clarity around the pretraining objective and its impact on performance, particularly regarding the choice of high-resource language. Additionally, addressing the sensitivity of the algorithm to sampling proportions through a more robust hyperparameter search would enhance the method's applicability. We suggest including BLEU score results in the main draft to demonstrate the correlation between validation loss and translation quality. Furthermore, it would be beneficial to clarify that both pretraining and fine-tuning objectives are cross-entropy losses for the NMT task. Lastly, we encourage the authors to explore the implications of their findings in more realistic settings and consider the inclusion of intermediate pretraining stages as suggested in prior work.