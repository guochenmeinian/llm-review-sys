ID: u69aCtohTC
Title: Unveiling the Implicit Toxicity in Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a reinforcement learning-based method to induce implicit toxicity in large language models (LLMs), demonstrating that such outputs are challenging for existing toxicity classifiers to detect. The authors propose optimizing LLMs to prefer implicit toxic outputs over explicit toxic and non-toxic outputs using a reward model. The effectiveness of this approach is validated through extensive experiments, showing improved performance of fine-tuned classifiers on generated datasets.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant safety risk associated with LLMs by exploring implicit toxicity.
- It employs a reinforcement learning method that effectively generates implicit toxic text, supported by comprehensive experimental results.
- The writing is clear and the analysis is accessible, with thorough experimental design and provided annotation guidelines.

Weaknesses:
- The framing of the work as "attacks" lacks explicit constraints and assumptions, which are crucial for understanding the attack's context and limitations.
- The methodology, while effective, appears to be a minor variation of standard reinforcement learning from human feedback (RLHF), which may limit its perceived novelty.
- The need for two datasets raises questions about the efficiency of the proposed method, and further ablation studies, particularly regarding hyperparameters, are warranted.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the constraints and assumptions surrounding their attack methodology to enhance understanding. Additionally, providing details on the training time for the LLMs would be beneficial. We suggest conducting more ablation studies to explore the effects of hyperparameters, such as Î±, and including comparison results that do not rely on reinforcement learning. Finally, addressing the questions regarding the Fleiss kappa range and the potential release of the dataset for public use would strengthen the paper.