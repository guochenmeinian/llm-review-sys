ID: LSYQB4CwD3
Title: Three Towers: Flexible Contrastive Learning with Pretrained Image Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Three Towers (3T) method, which enhances contrastive learning in vision-language models by integrating pretrained image classifiers. The authors argue that completely freezing the image encoder, as done in previous works like LiT, may hinder performance when downstream data diverges from pretraining data. They propose training both the image and text encoders from scratch, supplemented by an additional contrastive loss using pretrained representations. The results indicate that 3T outperforms both standard CLIP training and LiT, particularly in retrieval tasks. Furthermore, the paper provides a detailed evaluation of how predictions differ between 3T, LiT, and the baseline on a per datapoint and per task level, revealing that 3T learns a novel predictive mechanism distinct from both LiT and the baseline, leading to improved performance in various tasks. However, concerns remain regarding the scalability of gains with increasing pretraining data sizes and ongoing memory issues related to the g-scale JFT results.

### Strengths and Weaknesses
Strengths:
- The empirical gains in retrieval tasks are significant and well-documented.
- The methodology is straightforward and clearly articulated.
- Comprehensive ablation studies support the findings.
- The paper provides a comprehensive analysis of predictive diversity among the models, highlighting the unique strengths and weaknesses of each.
- The results demonstrate that 3T outperforms both LiT and the baseline in several tasks, indicating effective knowledge integration.

Weaknesses:
- The novelty is limited, as the benefits of distilling knowledge from pretrained models are already established.
- The proposed approach may not be as cost-effective as LiT, which could be downplayed in the paper.
- The exploration of scaling pretraining data is insufficient; further experiments with varying dataset sizes are recommended to assess diminishing returns.
- The paper lacks a thorough comparison with SLIP, which could enhance understanding of the differences between the two approaches.
- There is limited exploration of different pretrained models for the third tower, which could strengthen the findings.
- There are ongoing memory issues related to the g-scale JFT results that have not yet been resolved.

### Suggestions for Improvement
We recommend that the authors improve the exploration of pretrained classifier embeddings by conducting experiments with various pretrained models for the third tower, similar to those done in SLIP. Additionally, we suggest including a detailed discussion comparing 3T with SLIP to clarify their distinctions. It would also be beneficial to conduct mini-experiments scaling the pretraining dataset from 2M to 64M samples to evaluate performance changes. Furthermore, we encourage the authors to modify the paper's title to reflect the limitations in zero-shot and few-shot evaluations, emphasizing the strengths of 3T in retrieval tasks. Lastly, we recommend improving the clarity of the results presentation, particularly regarding the ongoing memory issues for the g-scale JFT results, and including the results discussed in the rebuttal in the final manuscript to enhance the overall quality of the submission.