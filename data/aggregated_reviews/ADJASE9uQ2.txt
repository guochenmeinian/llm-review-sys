ID: ADJASE9uQ2
Title: 2DQuant: Low-bit Post-Training Quantization for Image Super-Resolution
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 3, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents 2DQuant, a low-bit post-training quantization technique for transformer-based image super-resolution. The authors propose a two-stage optimization process that includes Distribution-Oriented Bound Initialization (DOBI) and Distillation Quantization Calibration (DQC), achieving significant performance improvements with minimal accuracy loss for SwinIR-light. The method demonstrates effective compression and acceleration of super-resolution models for edge deployment. Additionally, the authors introduce a layer-wise quantizer with trainable upper and lower bounds and a unique loss function granularity, asserting that their Dynamic Quantization Controller (DQC) applied to the four Transformer layers of the SwinIR architecture is more efficient than the per-residual block quantization used in previous studies. They emphasize the importance of the DOBI method for initial value assignment, which mitigates issues of model collapse seen in other implementations.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to understand.
2. The proposed method is practical for obtaining accurate quantized transformer-based super-resolution models.
3. Experimental results and visualizations effectively demonstrate the method's efficacy.
4. The authors provide a clear differentiation from existing methods, particularly in quantization strategies and loss function granularity.
5. The use of static quantizers eliminates the need for additional floating-point modules, enhancing inference speed.
6. The implementation of the DOBI method for initial value assignment demonstrates significant performance improvements.

Weaknesses:
1. The proposed DOBI and DQC methods are similar to existing techniques like DBDC and PaC, lacking clear differentiation.
2. The introduction's motivation is confusing, as it misrepresents the common issue of self-attention degradation in quantized transformers.
3. The selected baselines are inadequate, with insufficient comparisons to other transformer-based post-training quantization methods, such as PTQ4ViT, FQ-ViT, NoiseQuant, and RepQ-ViT.
4. The paper lacks detailed experimental results for finer granularity quantization due to page limitations.
5. There are concerns regarding the implementation errors in previous works that could mislead the quantization approach.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the differences between DOBI, DQC, and existing methods to strengthen their contribution. Additionally, we suggest revising the introduction to accurately reflect the challenges of quantized transformers. The authors should also include comparisons with relevant transformer-based post-training quantization methods to enhance the robustness of their experimental results. Furthermore, we recommend including detailed experimental results for the finer granularity quantization to strengthen their claims and addressing the implementation errors in prior works more explicitly to enhance the clarity and impact of their contributions.