ID: FiClXlUqA7
Title: A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 7, 5, 6, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for domain incremental learning that integrates concepts from existing methods. It begins with a theoretical analysis to formally define the problem and identifies three methods to bound the risk of error: 1) a naive ERM-based bound, 2) an intra-domain model-based bound, and 3) a cross-domain model-based bound. The authors propose a unified generalization bound that combines these three approaches using dynamic coefficients. The paper categorizes existing methods according to this framework and introduces a method with learnable coefficients, supported by experimental results on toy and benchmark datasets.

### Strengths and Weaknesses
Strengths:
- The paper demonstrates originality at both theoretical and experimental levels, providing a comprehensive analysis of generalization bounds in domain incremental learning and categorizing state-of-the-art methods.
- It offers significant insights through rigorous theoretical analysis, which is highly relevant for researchers in the field.
- The writing quality is excellent, with clear presentation and extensive details in both the main text and supplementary material.

Weaknesses:
- The experimental results indicate that the proposed method is competitive but not significantly superior to existing methods, particularly DER++.
- There is a lack of detail regarding the computational complexity and cost of the method, especially considering the additional difficulty of learning coefficients.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the transition from Equation 19 to 20 by explicitly defining the ghost example in the supplementary material. Additionally, the authors should address whether storing models at every training stage in the sample replay setting is appropriate and discuss the potential memory burden. It would be beneficial to provide explicit parameter regularization to mitigate issues related to domain discrepancies. Furthermore, we suggest that the authors clarify the computational burden associated with updates as the number of domains increases and discuss the feasibility of implementing their method on large-scale datasets like DomainNet. Lastly, we encourage the authors to elaborate on the novelty of Theorem 3.4, as it appears to be a combination loss over all domains.