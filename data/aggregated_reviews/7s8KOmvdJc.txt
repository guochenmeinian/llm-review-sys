ID: 7s8KOmvdJc
Title: InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents INHERITSUMM, a distilled model from GPT-3.5, aimed at addressing the high costs of fine-tuning large models while maintaining strong zeroshot and fewshot summarization capabilities. The authors propose a new dataset, GPTSUMM, which is derived from the Pile corpus and includes supervised summaries. The model's performance is evaluated against existing models, demonstrating comparable results to GPT-3.5.

### Strengths and Weaknesses
Strengths:
- The introduction of INHERITSUMM as a compact summarization model shows strong zeroshot and fewshot performance.
- The creation of the GPTSUMM dataset is a valuable resource for future summarization research.
- The experimental results, supported by ROUGE scores, indicate the model's effectiveness.

Weaknesses:
- The reliance solely on ROUGE scores raises doubts about the model's superiority over baselines, and the lack of case studies limits performance insights.
- Insufficient analysis of model outputs and the absence of human evaluation or factuality discussion weaken the claims made.
- The paper does not clarify whether the dataset or model will be shared, raising reproducibility concerns.

### Suggestions for Improvement
We recommend that the authors improve the analysis of model outputs to provide better context for performance, including discussions on factual consistency and qualitative differences across tasks. Additionally, we suggest including case studies to support the claims regarding model performance. Clarifying the sharing status of the dataset and model will enhance reproducibility. Finally, addressing the peculiarities in the results, particularly the performance discrepancies between InheritSumm-base and InheritSumm-large, will strengthen the paper's contributions.