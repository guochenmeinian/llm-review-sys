ID: 6BZS2EAkns
Title: In-Context Learning Unlocked for Diffusion Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 7, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Prompt Diffusion, an image generation framework that enables in-context learning in diffusion-based generative models. The authors propose a vision-language prompt that replaces text examples with paired image examples and text queries with image queries, allowing the model to generalize across various vision-language tasks. Trained jointly on six distinct tasks, Prompt Diffusion demonstrates high-quality in-context generation and emerging image editing capabilities. However, the claims regarding in-context learning and generalization to new tasks are questioned by reviewers. The authors also examine the generalization ability of Prompt Diffusion, particularly concerning unseen tasks, and commit to clarifying their discussion on generalization in the next revision. They acknowledge the limitations of the Stable Diffusion backbone, particularly its inability to generate images with physical modifications such as rotation.

### Strengths and Weaknesses
Strengths:
- The problem setting of visual prompting conditioned on both visual examples and text is novel and interesting.
- The paper is well-written, with clear presentation and motivation.
- The integration of pretrained diffusion models enhances visual quality and text guidance in the framework.
- The authors effectively address reviewer concerns and demonstrate a commitment to improving the clarity of their discussion on generalization.
- Empirical evidence through additional experiments illustrates the limitations of Prompt Diffusion while maintaining the integrity of their claims regarding in-context learning.

Weaknesses:
- The claims of "in-context learning" and generalization to new tasks are not convincingly supported by empirical results, as the model is trained on a limited number of similar tasks.
- The necessity of visual examples versus text-only guidance remains unclear, raising questions about the model's flexibility.
- The current generation results of high-fidelity images are underwhelming, and the model's robustness with misaligned query images is inadequate.
- The authors acknowledge a significant limitation in their model's inability to generate rotated images, which may impact the perceived effectiveness of Prompt Diffusion in handling physical transformations.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation of their claims regarding in-context learning by testing the model on a broader range of diverse tasks beyond those it was trained on. Additionally, we suggest exploring the performance of the model with standard image input-output pairs using text guidance, similar to InstructPix2Pix. It would also be beneficial to provide more results to clarify the importance of different modalities in the prompts and to discuss the computational trade-offs involved. Furthermore, we recommend that the authors improve the discussion on the limitations of Prompt Diffusion by elaborating on how the constraints of the Stable Diffusion backbone affect its performance. Lastly, we suggest including pertinent references related to models trained for image generation tasks and their generalization capabilities towards image physical transformation tasks, such as rotations.