ID: gUEekxYr6D
Title: BiSLS/SPS: Auto-tune Step Sizes for Stable Bi-level Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 5, 7, 8, 6, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the bilevel optimization problem, focusing on developing effective learning rate schemes. The authors propose two adaptive step-size methods: stochastic line search (SLS) and stochastic Polyak step size (SPS), which replace the constant $\gamma_{b,0}$ with a non-increasing $\gamma_{b,k}$. These methods are applied to both upper and lower levels of bilevel optimization, with convergence analysis provided for single-level and bilevel problems. The paper also presents a bi-level line-search algorithm, BiSLS-Adam/SGD, with convergence guarantees.

### Strengths and Weaknesses
Strengths:
1. The investigation of adaptive learning rates in bilevel optimization is interesting and under-explored in the literature.
2. The proposed methods are more practical than existing approaches, requiring milder conditions.
3. The algorithm is compatible with accelerated solvers like Adam, enhancing computational efficiency.
4. The illustrations in the figures aid in understanding the contributions.

Weaknesses:
1. The paper is well-written but difficult to follow due to numerous assumptions, notations, and inequalities, particularly in Section 2.
2. The introduction of additional hyperparameters raises concerns about practical applicability and the lack of sensitivity analysis in experiments.
3. The complexity analysis does not demonstrate improved sample or computational complexity, which requires further elaboration.
4. The experiments are not convincing, relying on toy examples rather than comprehensive tests on practical neural network architectures and larger datasets.
5. The paper lacks a thorough comparison with existing algorithms for bilevel optimization, limiting the understanding of the proposed methods' relative performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by simplifying the presentation of assumptions and notations. Additionally, we suggest providing sensitivity analysis for the introduced hyperparameters and conducting more comprehensive experiments on practical neural network architectures and larger datasets. It would also be beneficial to elaborate on the complexity analysis to clarify the computational implications of the proposed methods. Finally, we encourage the authors to include a more thorough comparison with existing algorithms to contextualize their contributions within the broader field of bilevel optimization.