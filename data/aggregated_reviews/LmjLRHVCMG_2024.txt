ID: LmjLRHVCMG
Title: An Improved Empirical Fisher Approximation for Natural Gradient Descent
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a modification of the empirical Fisher matrix, termed improved empirical Fisher (iEF), which re-scales a datum's parameter gradient by its logit gradient to address the 'inversely-scaled projection issue.' The authors demonstrate that this modification does not incur significant computational costs while aligning the approximate natural gradient more closely with the true natural gradient. The paper includes a theoretical convergence analysis and empirical results showing that iEF outperforms existing methods like SGD and AdamW in certain optimization tasks. Additionally, the authors provide further experimental results for the iEF method applied to a CNN architecture, specifically ResNet32 on the CIFAR10 dataset, achieving a test accuracy of 85.6%, which significantly surpasses the 58.6% from their previous MLP setup. These results validate the claims in the main paper, demonstrating that iEF outperforms other optimizers in terms of generalization and convergence.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written, with well-justified theoretical foundations and thorough experimental evaluations that support its claims.
- The proposed iEF method shows significant improvements in approximating the natural gradient without substantial computational overhead, making it a promising direction for future research.
- The new experiments provide stronger evidence for the effectiveness of the iEF method in practical scenarios, achieving a competitive test accuracy with ResNet32.
- The evaluation framework for approximation quality supports the robustness of iEF across varying damping factors.

Weaknesses:
- The explanation of how the iEF addresses the inversely-scaled projection issue lacks clarity; a theoretical argument could strengthen this aspect.
- The experimental scope is limited, focusing primarily on fine-tuning settings without sufficient exploration of other scenarios, such as traditional image classification tasks.
- The absence of batch normalization layers in the ResNet32 setup may limit the generalizability of the results.
- The initial MLP experiment is considered synthetic and provides weak evidence due to its mediocre performance.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for how addressing the empirical Fisher's bias leads to better approximations of the natural gradient. This could involve analyzing the data distribution changes resulting from the proposed modification. Furthermore, we suggest expanding the experimental scope to include traditional image classification tasks, such as CIFAR10 or CIFAR100, and exploring the scalability of the implementation by considering structural approximations like block diagonals or Kronecker factorizations. Additionally, it would be beneficial to include experiments comparing iEF with other state-of-the-art optimizers, such as CAME and Sophia, and to clarify the relationship between the proposed method and the Gauss-Newton algorithm. We also recommend that the authors improve their experimental setup by including batch normalization layers in future iterations of the ResNet32 model to enhance the robustness of their findings. Exploring a wider range of CNN architectures that are representative of practical applications could further strengthen their claims.