ID: IoRT7EhFap
Title: Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 4, 6, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to address the spectral bias in deep neural networks (DNNs), where DNNs tend to prioritize learning low-frequency components over high-frequency features. The authors propose Multi-Grade Deep Learning (MGDL), which decomposes high-frequency functions into compositions of low-frequency functions. MGDL incrementally trains DNNs, focusing on low-frequency information at each grade. The effectiveness of MGDL is demonstrated through experiments on synthetic, manifold, and MNIST datasets, showing improvements in capturing high-frequency components compared to traditional methods.

### Strengths and Weaknesses
Strengths:
- The paper provides a thorough theoretical foundation and supports its claims with experimental results, although these are not entirely convincing.
- It is well-organized, with clear explanations and effective use of figures and mathematical expressions.
- The logical progression from the motivation to the proposed method is sound, and the idea is clear and intuitive.

Weaknesses:
- The originality is limited, lacking new theoretical contributions for the MGDL method.
- Concerns about the performance and stability of MGDL are evident, particularly regarding loss spikes and insufficient loss reduction in real-world tasks.
- The experimental scope is narrow, primarily involving synthetic scenarios and only MNIST for real-world tasks, which raises questions about scalability and applicability.
- Minor punctuation errors and small legends in figures hinder readability.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the significance of addressing spectral bias and provide more compelling practical scenarios where neglecting high-frequency components leads to failures. Additionally, we suggest including comparisons with other architectures, such as CNNs and Transformers, to validate the applicability of MGDL. The authors should also enhance the experimental section by expanding the range of datasets and architectures tested, ensuring that the experiments substantiate their claims more convincingly. Furthermore, we advise correcting the minor punctuation errors and increasing the font size of figure legends for better readability.