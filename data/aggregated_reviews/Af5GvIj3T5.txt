ID: Af5GvIj3T5
Title: To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 3, 6, 8, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical study on the scaling of transformer models under limited training data, termed a *token crisis*. The authors demonstrate that training T5 models for multiple epochs leads to performance degradation in both pre-training and downstream tasks, regardless of dataset quality. They find that dropout can mitigate this multi-epoch degradation and that the behavior of Mixture-of-Expert (MoE) models can predict the training behavior of dense models, allowing for optimal dropout rate selection.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with clear and useful empirical takeaways.
- The *token crisis* is an important issue that warrants further study, and this paper contributes valuable insights.

Weaknesses:
- The study is limited to a single downstream task, raising questions about the generalizability of results across different datasets.
- It focuses solely on encoder-decoder models like T5, leaving the applicability of findings to decoder-only models (e.g., GPT-3, Chinchilla, LLaMA) uncertain.
- Some conclusions are overly assertive and lack full experimental support, such as the attribution of performance differences to dataset size without considering batch size variations.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by including additional downstream tasks and exploring decoder-only models. We suggest conducting experiments with a smaller number of data repetitions to determine optimal repetition rates. Additionally, we encourage the authors to provide statistical significance comparisons for performance degradation and clarify the balance between regularization and model performance. Lastly, addressing the unclear conclusions regarding dataset quality and its impact on performance would strengthen the paper.