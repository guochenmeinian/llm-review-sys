ID: Mktgayam7U
Title: Scalable Kernel Inverse Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an innovative approach to inverse optimization (IO) using kernel methods, extending the hypothesis class of IO objective functions to a reproducing kernel Hilbert space (RKHS). The authors propose a Sequential Selection Optimization (SSO) algorithm to address scalability issues, demonstrating its effectiveness through learning-from-demonstration tasks in the MuJoCo benchmark. The paper also discusses the challenges associated with kernel methods, including the dependence on kernel functions and hyperparameters, which can significantly affect performance.

### Strengths and Weaknesses
Strengths:  
- The extension of inverse optimization to RKHS enhances the complexity and expressiveness of the objective function, allowing for more complex decision problems.  
- The SSO algorithm effectively addresses scalability, improving training efficiency for large datasets.  
- The paper is well-written, providing clear insights into the kernel method's integration into the IO framework and validating the KIO model's practical application through thorough experimental evaluation.

Weaknesses:  
- The nonlinear and high-dimensional nature of kernel methods reduces model interpretability, necessitating clearer explanations of the optimization model.  
- The performance of the KIO model is sensitive to the choice of kernel functions and hyperparameters, which should be discussed in more detail.  
- There is a lack of convergence guarantees for the SSO algorithm, and the evaluation of baseline methods lacks sufficient detail regarding hyperparameters and data used.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the selection of kernel functions and hyperparameters, including reporting standard deviations in experiments. Additionally, it would be beneficial to provide more details on the evaluation of baseline methods, including hyperparameters and convergence metrics. We suggest adding a table comparing different initialization strategies and discussing the impact of noise in demonstration data on the model's effectiveness. Finally, we encourage the authors to explore the performance of KIO on simpler tasks where IO performs reasonably well to strengthen the paper's findings.