ID: E8EAeyTxOy
Title: InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 8, 6, 8, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents InfiBench, a new dataset for evaluating the question-answering (QA) capabilities of large language models (LLMs) in the code domain. The dataset consists of 234 high-quality questions sourced from Stack Overflow, annotated by domain experts across 15 programming languages. It employs four automatic evaluation metrics: keyword matching, blank filling, unit testing, and dialogue similarity. The evaluation of over 100 recent code LLMs reveals significant insights into model performance and scaling behavior, particularly highlighting differences between pretrained and instruction-tuned models.

### Strengths and Weaknesses
Strengths:
- InfiBench addresses a critical gap in evaluating LLMs for free-form coding QA, distinguishing itself from existing code generation benchmarks.
- The dataset is diverse, covering various programming languages and topics, and the human annotation process enhances its quality and relevance.
- The open-source nature of the benchmark promotes transparency and facilitates ongoing improvements in LLM-based code assistants.
- Extensive evaluation provides valuable insights into the scaling laws and performance of different model families.

Weaknesses:
- The dataset's size (234 questions) may limit statistical significance and robustness in evaluating model performance.
- The complexity of the evaluation metrics could complicate future maintenance and integration of the benchmark.
- The narrative lacks a strong motivation for the necessity of this benchmark compared to existing datasets, such as ArenaHard and CodeXGLUE.
- There is a lack of correlation analysis between automatic metrics and human judgment, which is essential for assessing the reliability of the proposed evaluation framework.

### Suggestions for Improvement
We recommend that the authors expand the dataset size to around 500 questions to enhance statistical significance and coverage of diverse programming scenarios. Additionally, we suggest providing a clearer comparison with existing benchmarks to strengthen the motivation for InfiBench's development. It would be beneficial to include a correlation analysis between the ranks of automatic metrics and human evaluations to validate the proposed metrics. Furthermore, the authors should consider simplifying the evaluation criteria to facilitate easier integration into different frameworks and provide a more in-depth discussion of the insights gained from the evaluation results.