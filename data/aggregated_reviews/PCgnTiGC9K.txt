ID: PCgnTiGC9K
Title: Credal Deep Ensembles for Uncertainty Quantification
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 5, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Credal Deep Ensembles (CreDEs), an ensemble framework of Credal-Set Neural Networks (CreNets) aimed at producing high-quality epistemic uncertainty through a credal set combined with Distributionally Robust Optimization (DRO). The authors clarify that epistemic uncertainty arises from the difference between the training and test data distributions, and their method aims to improve uncertainty estimation beyond just out-of-distribution (OoD) scenarios. Key contributions include a rigorous CreNet final layer that doubles the number of nodes to provide confidence intervals for each class, a training procedure utilizing CreNet loss with upper and lower class probability bounds, and empirical results demonstrating that CreDEs outperform traditional Deep Ensembles (DEs) in various tasks, including active learning, classification performance, higher test accuracy, and lower Expected Calibration Error (ECE).

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and organized, making the proposed method and its components clear.  
- The motivation for improving uncertainty and robustness in Neural Networks is significant and relevant to AI safety.  
- The proposed method is novel and compelling, with detailed explanations of its improvements over existing methods.  
- Empirical evaluations cover a range of tasks and settings, showing consistent improvements over traditional deep ensembles, indicating strong practical applicability.  
- The authors acknowledge the potential for extending their work to other domains, enhancing its relevance.  

Weaknesses:  
1. The theoretical contributions are weak, lacking robust theories or guarantees to support claims of improved uncertainty or robustness quality.  
2. The lack of theoretical guarantees or robustness certificates for CreDEs raises concerns about their generalization.  
3. The computational efficiency of CreDEs is a concern, as they are less efficient than DEs and other uncertainty-aware models, with increased complexity during training and inference, particularly for deployment in resource-constrained environments.  
4. The method is limited to classification tasks and relies heavily on heuristic hyperparameter tuning, which can significantly affect performance.  
5. There is a lack of empirical comparisons with important uncertainty baselines, such as MC Dropout, BatchEnsemble, and simpler post-processing methods that could achieve comparable results.  
6. The evaluation of uncertainty quality is insufficient, with a need for more comprehensive assessments beyond OOD detection performance.  
7. The training complexity of CreNets is notably higher than standard neural networks, raising concerns about scalability.  

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis to provide a clearer justification for the effectiveness of the proposed methods, particularly regarding the Interval SoftMax function and the application of DRO. We also suggest incorporating generalization bounds or robustness certificates for CreDEs to strengthen their claims. Additionally, we advise including comparisons with a broader range of uncertainty estimation methods, such as Mahalanobis distance and recent state-of-the-art techniques, as well as conducting further comparisons with more computationally efficient models to provide clearer insights into the trade-offs involved. To address computational efficiency, we encourage the authors to evaluate the model's performance across various hardware settings and explore the potential of lightweight neural networks and quantization techniques for enhancing computational efficiency in real-world applications. Furthermore, we recommend providing clearer guidelines on hyperparameter selection and its impact on performance, particularly regarding the adversarial weights for DRO, the number of classes in the ensemble, and the implications of hyperparameter choices like $\delta$. Lastly, we suggest enhancing the empirical evaluation of uncertainty quality through reliability diagrams and additional quantitative assessments.