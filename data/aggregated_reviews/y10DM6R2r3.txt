ID: y10DM6R2r3
Title: MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 7, 6, 7, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents MMLU-Pro, an enhanced dataset designed to evaluate advanced Large Language Models (LLMs) by addressing the limitations of the original MMLU benchmark. MMLU-Pro incorporates more challenging, reasoning-focused questions and expands answer choices from four to ten, thereby reducing trivial questions and enhancing complexity. The dataset undergoes rigorous expert review to ensure quality, and experimental results indicate that MMLU-Pro increases difficulty, decreases accuracy by 16-33%, and reduces sensitivity to prompt variations, making it a more robust benchmark for LLM evaluation.

### Strengths and Weaknesses
Strengths:
1. The proposed dataset introduces more challenging questions and expands answer choices, reducing the likelihood of guessing.
2. Rigorous quality control through two rounds of expert reviews eliminates trivial, noisy, or incorrect questions.
3. The authors demonstrate reduced sensitivity to prompt variations, ensuring stable performance measurements.
4. Emphasis on reasoning over knowledge recall requires models to perform deliberate reasoning to answer correctly.
5. Comprehensive evaluation of over 50 LLMs on the dataset solidifies its credibility.

Weaknesses:
1. More examples of the newly added challenging questions could enhance understanding and significance.
2. Enhancements may be perceived as incremental rather than novel, primarily focusing on question complexity and choice expansion without introducing new evaluation methods.
3. Increased difficulty might limit the benchmark's applicability for lower-performing or smaller-scale models.
4. There is a risk of models overfitting to the specific types of reasoning required by MMLU-Pro, potentially neglecting other important language understanding capabilities.
5. The dataset may reflect implicit biases from the sources used, and the degree of subjectivity from expert reviews could influence the final dataset.

### Suggestions for Improvement
We recommend that the authors improve the transparency of the expert review process by detailing the number of experts involved, their qualifications, and how disagreements were resolved. Additionally, we suggest providing more examples of the newly added questions to better illustrate their complexity. It would be beneficial to extend the dataset to include more diverse data sources beyond STEM websites and to apply the same filtering strategy used for original MMLU questions to those from other sources. We also encourage the authors to explore the impact of the number of shots on model performance and to discuss potential privacy issues and social impacts associated with the dataset. Finally, including practical applications of MMLU-Pro could enhance its relevance and utility.