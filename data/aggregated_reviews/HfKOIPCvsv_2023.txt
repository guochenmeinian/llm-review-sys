ID: HfKOIPCvsv
Title: RealTime QA: What's the Answer Right Now?
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 9, 7, 5, 7, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents REALTIME QA, a dynamic platform for evaluating question answering (QA) systems using time-sensitive questions sourced from news websites. The platform releases approximately 30 new questions weekly, allowing models to utilize both Google search results and their retrieval systems for answering. The authors propose a set of baselines, including GPT-3 and RAG, and assess model performance over time, highlighting the challenges of real-time question answering. Additionally, the authors introduce an automatic quiz extraction script to enhance sustainability and facilitate ongoing evaluations. They acknowledge the English-centric focus of their topics and plan to seek diverse source data. The authors evaluate models immediately after quiz announcements to address potential discrepancies in accuracy due to information changes throughout the week and emphasize the importance of attribution to source information for building trust in AI systems.

### Strengths and Weaknesses
Strengths:  
- Evaluating NLP systems against ever-changing information is crucial for tasks like fact-checking and question answering.  
- The weekly updates provide a balance between long-term effects and short-term information changes.  
- The study contributes to ongoing research on model performance degradation and dynamic benchmarking.  
- The evaluation benchmark is recognized as significant and valuable for time-sensitive systems.  
- The automatic quiz extraction script enhances sustainability and supports ongoing evaluations.  
- The authors provide access to retrieved documents, promoting advanced research.  

Weaknesses:  
- The longevity and sustainability of the benchmark are uncertain, raising questions about how model performance will be compared over time.  
- The dataset's reliance on a limited selection of Western news outlets may overlook marginalized perspectives and popular topics, leading to potential biases.  
- The manual annotation of weekly questions raises concerns about the platform's sustainability and accuracy over time.  
- The benchmark's focus on English-centric topics may limit its applicability.  
- Potential discrepancies in accuracy due to temporal gaps in information are acknowledged but remain a concern.  

### Suggestions for Improvement
We recommend that the authors improve the longevity of the benchmark by clarifying how long updates will be provided and how researchers will compare model performance over time. Additionally, consider expanding the dataset to include more diverse sources beyond the current Western news outlets to enhance representation. We suggest exploring the potential discrepancies in accuracy between articles published at different times within the week. Furthermore, we encourage the authors to analyze the experimental results more thoroughly, particularly focusing on error breakdowns related to retrieval and comprehension errors. We also recommend that the authors improve their documentation by providing more detailed instructions on running evaluations with the latest data. Lastly, we encourage the authors to further explore the implications of the observed performance patterns over time, particularly regarding the stability of third-party hosted searches.