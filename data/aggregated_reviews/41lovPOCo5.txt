ID: 41lovPOCo5
Title: TableRAG: Million-Token Table Understanding with Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TableRAG, a framework designed to enhance large language model (LLM) performance in table understanding by addressing scalability challenges through advanced query expansion and retrieval mechanisms. The authors propose a schema-cell retrieval method that improves the efficiency of encoding large tables and achieves state-of-the-art results on newly established benchmarks for million-token-sized tables. The framework demonstrates improved cost efficiency and effectiveness across various datasets.

### Strengths and Weaknesses
Strengths:
- The schema-cell retrieval approach shows better performance compared to baselines on certain question types.
- The method is intuitive and effectively addresses large-scale table comprehension, as evidenced by strong experimental results.
- The paper contributes to new benchmarks for large tables, enhancing the understanding of LLM performance in this domain.

Weaknesses:
- The generalizability of the schema-cell retrieval method is unclear, particularly for complex reasoning tasks that cannot be directly addressed by table manipulation.
- The experiments are limited to GPT-3.5-turbo and Gemini-Pro, necessitating further testing on a broader range of models to validate the method's consistency.
- The contribution is perceived as relatively small, primarily building on existing Row Column Retrieval techniques without substantial novelty.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of the schema-cell retrieval method by addressing its performance on complex reasoning tasks, such as those found in the Tabfact dataset. Additionally, we suggest expanding the experimental evaluation to include more open-sourced and closed-sourced models, such as Mistral and Llama, to strengthen the findings. Furthermore, enhancing the baseline comparisons by incorporating methods like BM25 for cell value retrieval and exploring schema linking approaches could provide a more robust evaluation framework. Lastly, clarifying the handling of long descriptions in cell retrieval and addressing potential failures in query expansion would enhance the paper's comprehensiveness.