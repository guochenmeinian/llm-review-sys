ID: PuXYI4HOQU
Title: Fundamental Convergence Analysis of Sharpness-Aware Minimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 4, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a fundamental convergence analysis of Sharpness-Aware Minimization (SAM) algorithms, including both normalized and unnormalized variants. The authors utilize inexact gradient descent methods to establish convergence properties for SAM in convex and non-convex settings, focusing on last-iterate convergence. The analysis is applicable to various SAM versions, contributing to a deeper understanding of their convergence behavior.

### Strengths and Weaknesses
Strengths:
- The study of SAM's convergence is significant in optimization, and the paper establishes theoretical convergence for both normalized and unnormalized SAM algorithms, a key contribution given that most existing results focus solely on unnormalized variants.
- The paper is well-written and easy to follow, with a comprehensive understanding of SAM and its variants.

Weaknesses:
- The analysis of the convex case is deemed less meaningful, as convex programming methods typically yield global optimal solutions.
- The results do not provide substantial insights into the quality of solutions in non-convex cases, limiting their practical relevance.
- The use of inexact gradient descent (IGD) is seen as overly general for studying SAM, lacking sufficient motivation.
- Some results appear to be straightforward applications of prior work, raising questions about the novelty of the contributions.

### Suggestions for Improvement
We recommend that the authors improve the motivation for using IGD in the context of SAM, elaborating on how this framework is appropriate given SAM's structured nature. Additionally, we suggest that the authors clarify the theoretical contributions compared to existing methods and address the analysis of the convex case to enhance its relevance. Furthermore, we advise including citations for Theorem 3.3 and Theorem 3.4 if they are derived from existing works. Lastly, we encourage the authors to revise the paper for clarity, ensuring that complex notions and formulae are well-explained for a broader audience.