ID: AAuVIl8Aeo
Title: Characterizing and Verifying Scientific Claims: Qualitative Causal Structure is All You Need
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a graph-based method for verifying scientific claims, introducing a complex pipeline consisting of five modules. The authors propose an attention-based graph neural network model that facilitates reasoning about causally crucial factors in scientific claims. The model constructs trees representing causal structures for both claims and evidence, comparing them to predict labels, and demonstrates superior performance over the state-of-the-art model (T5) on the HealthVER claim dataset.

### Strengths and Weaknesses
Strengths:
- The experiments appear well-conducted, and the model effectively decomposes complex scientific claims into atomic subclaims for detailed scrutiny.
- The proposed model is well-motivated, showing good performance and encouraging further research in reasoning over causal structures.
- The paper is well-written, with thorough analysis through architectural variants and ablation tests.

Weaknesses:
- The comparison with state-of-the-art models is limited as the systems evaluated (BERT, SciBERT, T5) are outdated, and Instruct LLMs are not included.
- The number of evaluated datasets is limited, and clarity in writing could be improved.
- Baselines could be enhanced through fine-tuning or prompt tuning on HealthVER tasks for fairer comparisons.

### Suggestions for Improvement
We recommend that the authors improve the clarity of writing in the final version of the paper. Additionally, the authors should consider including comparisons with more recent models, such as Instruct LLMs, and expand the number of evaluated datasets. It would also be beneficial to explore the effectiveness of the proposed model in low-label scenarios or domain generalization scenarios, and to clarify the importance of the SciBERT embedding layer in performance. Finally, addressing the baseline improvements through fine-tuning or prompt tuning on HealthVER tasks would strengthen the comparisons.