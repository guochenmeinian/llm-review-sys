ID: cNb5hkTfGC
Title: A Scalable Neural Network for DSIC Affine Maximizer Auction Design
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 8, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AMenuNet, a scalable neural network for automated mechanism design that ensures Dominant Strategy Incentive Compatibility (DSIC) and Individual Rationality (IR). The authors propose a deep variant of Automated Mechanisms Auctions (AMAs) using a permutation-equivariant attention-based network to learn auction parameters. The theoretical framework demonstrates that the learned auctions maintain DSIC and IR properties. Additionally, the paper discusses the importance of permutation-equivariance in auction mechanisms, emphasizing the need to permute both bidder and item IDs alongside bids. The authors argue that their approach extends beyond symmetric auctions to include asymmetric scenarios, supported by references to previous works by Duan et al. [2022] and Qin et al. [2022]. Extensive experiments validate the method's effectiveness across various auction settings.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured, clearly written, and effectively situates itself within the existing literature.
- The pursuit of a deep variant of AMAs is a significant contribution, relevant to the conference.
- Extensive experiments and ablations support the proposed method, particularly the diverse allocations observed in the case study.
- The theoretical proof of DSIC and IR properties is a crucial aspect of the work.
- The authors provide a clear and comprehensive definition of permutation-equivariance, addressing its application in both symmetric and asymmetric auctions.
- The incorporation of neural networks into auction mechanisms is presented as a novel contribution, enhancing the scalability and generalization of their method.

Weaknesses:
- The experimental comparison lacks fairness, as it omits relevant baselines like CITransNet and RegretFormer, which could provide a more accurate assessment of AMenuNet's performance.
- Hyperparameter selection for RegretNet in novel settings is unclear, potentially leading to unfair comparisons.
- AMenuNet's reliance on a hand-selected menu size raises concerns about scalability; an ablation study on varying menu sizes is needed.
- The architecture does not present significant novelty beyond existing methods, aside from the softmax trick for deterministic allocations.
- There is a lack of clarity regarding the specific novelty of the neural network-based AMA method, particularly in distinguishing it from previous approaches.
- The authors do not sufficiently elaborate on how permutation symmetry is achieved in practice concerning bids after allocations and boosts.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by including comparisons with CITransNet and RegretFormer to provide a more comprehensive evaluation of AMenuNet's performance. Additionally, clarify how hyperparameters for RegretNet are selected in novel settings to ensure fair comparisons. Conduct an ablation study to assess the impact of varying menu sizes on revenue, and consider including an ablation where attention layers in AMenuNet are replaced with fully-connected layers to demonstrate the advantages of the attention mechanism. Furthermore, we suggest that the authors improve the clarity of the novelty of their neural network-based AMA method, explicitly detailing how it differs from prior literature. Lastly, provide a more thorough explanation of how permutation symmetry is maintained in the context of bids after allocations and boosts, particularly in relation to the Lottery AMA parameters.