ID: 8jx5gvWEhU
Title: Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 8, 4
Original Confidences: 4, 3

Aggregated Review:
### Key Points
This paper presents a novel algorithm, DUETS (Distributed Uniform Exploration of Trimmed Sets), aimed at addressing critical challenges in distributed kernel bandits, including managing potentially infinite-dimensional communication costs and achieving performance comparable to centralized learners. The authors propose that DUETS offers improved regret and communication cost bounds, supported by rigorous theoretical analysis and empirical studies demonstrating its superiority over existing methods.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel algorithm that effectively balances learning speed and communication efficiency.
- The theoretical results are robust, showing that DUETS achieves optimal regret order with sublinear communication costs.
- Empirical evidence strongly supports the algorithm's performance, highlighting its advantages in regret and communication costs compared to existing approaches.

Weaknesses:
- The assumption regarding the smoothness and structure of the reward function (Assumption 2.1) is overly restrictive, raising concerns about the algorithm's performance in less ideal scenarios.
- The focus on fixed-arm settings may limit the algorithm's applicability to more complex contexts, such as contextual bandits or dynamic arm sets.
- The experiments lack a benchmark comparison with a single bandit learner, making it difficult to assess the quantitative achievements of DUETS.
- The literature review is incomplete, omitting recent advancements that enhance regret and communication cost bounds against similar benchmarks.
- The discussion of communication costs is overly optimistic, as it does not account for real-world factors such as communication loss and the implications of recent developments in asynchronous algorithms.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their algorithm by addressing the restrictive assumptions about the reward function. Additionally, incorporating benchmarks with single bandit learners in the experiments would clarify the algorithm's quantitative performance. The authors should also expand the literature review to include recent progress in the field, particularly studies that enhance regret and communication cost bounds. Lastly, we suggest that the authors consider real-world communication challenges, such as communication loss, to provide a more practical evaluation of their algorithm's performance.