ID: S3Y0VvegGm
Title: The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 7, 6, 7, -1, -1, -1
Original Confidences: 4, 3, 3, 2, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical exploration of distributional reinforcement learning (RL), emphasizing the advantages of small-loss bounds. The authors propose distributional algorithms for contextual bandits, online RL, and offline RL, demonstrating that these algorithms achieve small-loss regret bounds. The concept of small-loss bounds, which depend on the minimum achievable cost, is introduced to explain the improved performance of distributional methods in various RL tasks.

### Strengths and Weaknesses
Strengths:
- The investigation into small-loss bounds offers a novel perspective on distributional RL, enhancing theoretical understanding.
- The paper rigorously addresses three significant RL settings, showcasing the provable benefits of distributional MLE methods.
- The combination of MLE and confidence intervals presents an innovative approach distinct from traditional mean return considerations.

Weaknesses:
- The algorithm's reliance on the (distributional) Bellman completeness assumption raises concerns about its general applicability, particularly in non-monotone scenarios.
- The technical novelty is limited due to similarities with existing literature, and the proofs based on the Banach fixed point theorem are not adequately discussed.
- The readability suffers from dense notation and complex mathematical expressions, making it challenging to follow.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing more intuitive explanations for the triangular discrimination technique and its implications. Additionally, addressing the limitations of the Bellman completeness assumption and discussing its applicability in broader contexts would enhance the theoretical contribution. We also suggest including empirical results for online and offline RL settings to strengthen the connection between small-loss bounds and practical performance. Finally, a review of the notation and a reduction of text density would significantly improve readability.