ID: e5srDjF9l7
Title: Accessing Higher Dimensions for Unsupervised Word Translation
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 7, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents coocmap, a method for unsupervised word translation that utilizes high-dimensional co-occurrence statistics instead of low-dimensional vectors. The authors argue that low-dimensional vectors can lead to suboptimal denoising methods and overlook valuable world knowledge in high dimensions. They provide a detailed exploration of word embeddings, particularly focusing on the implications of subword information, asserting that while subword information may not significantly impact benchmarks, it is essential for downstream tasks involving unknown words. Through extensive experiments across various languages and domains, including low-resource languages, the authors demonstrate that coocmap can achieve effective translation with less data and in a broader range of scenarios than previously thought. They also highlight their contributions to theoretical analysis in word embeddings, specifically the differences between continuous and discrete vectors.

### Strengths and Weaknesses
Strengths:
- The paper is well-presented, with clear figures and extensive experiments on different language pairs, including both similar and dissimilar languages.
- The proposed method, coocmap, is straightforward yet effective, outperforming baselines like vecmap, particularly on smaller datasets.
- The authors provide empirical evidence supporting the effectiveness of high-dimensional co-occurrence statistics for unsupervised translation.
- The authors offer a thorough examination of the implications of using higher dimensions in word embeddings and address the limitations of subword information relevant to practical applications.
- The paper includes valuable insights into the performance of low-resource language pairs.

Weaknesses:
- The paper lacks clarity in certain areas, such as the definition of "most common" words and the treatment of identical translation pairs in experiments.
- Coocmap's inability to handle unseen words due to the absence of a rotation matrix limits its versatility compared to vecmap.
- The motivation for using an unsupervised method is not sufficiently articulated, and the paper does not explore the implications for downstream tasks like machine translation.
- The simultaneous variation of dimensions and data size in the experimental setup is not adequately represented in the figures.
- There is a lack of robust evidence for the generalization of their approach to more complex tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup by defining "most common" words and explaining how identical translation pairs were processed. Additionally, the authors should consider including more baseline methods, such as LNMAP and FIPP, for a comprehensive performance comparison. It would be beneficial to discuss the treatment of unseen words and the potential for using subwords in their approach. We also suggest including word translation performance for low-resource language pairs in the final version, potentially utilizing the XLing-Eval dataset. To enhance clarity, we advise revisiting the figures to better illustrate the simultaneous variation of dimensions and data size. Finally, we encourage the authors to explore the implications of their method for downstream tasks, particularly in machine translation, and to validate their findings on standard monolingual tasks to strengthen the paper's contributions.