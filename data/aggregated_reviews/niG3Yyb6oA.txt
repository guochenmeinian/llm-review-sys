ID: niG3Yyb6oA
Title: A Layer-Wise Natural Gradient Optimizer for Training Deep Neural Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 7, 4, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel optimization algorithm called Layer-wise Natural Gradient Descent (LNGD), which approximates the Fisher information matrix (FIM) using a block diagonal form computed through a layer-wise sampling method, eliminating the need for complete backpropagation. The authors introduce an adaptive layer-wise learning rate to enhance training efficiency and provide a global convergence analysis of LNGD, demonstrating its competitive performance on image classification and machine translation tasks. Additionally, the framework integrates layer-wise sampling and adaptive layer-wise learning rates with existing methods like KFAC, EKFAC, and TKFAC. The authors conducted ablation studies to isolate the effects of these components on LNGD performance, acknowledging that incorporating these techniques into KFAC and its variants results in new methodologies, making direct comparisons with LNGD inappropriate.

### Strengths and Weaknesses
Strengths:
- The authors find a clever way to approximate the Fisher information matrix using a block diagonal form, which simplifies computations.
- The adaptive learning rate for each layer is an innovative approach to optimizing weights.
- The experimental section is robust, showcasing superior performance compared to existing methods in terms of epoch and wall-clock time.
- The authors provide a thorough explanation of the integration of new strategies into existing NGD methods.
- The ablation studies conducted offer valuable insights into the contributions of various components within the LNGD framework.

Weaknesses:
- The paper lacks comparisons with recent algorithms, with the most recent being KFAC from 2015, which diminishes its relevance.
- The convergence analysis is unconventional and does not clearly demonstrate the advantages of LNGD over other optimization methods.
- The empirical validation is limited, with only three experiments conducted on outdated architectures, raising concerns about the robustness of the approach.
- The assumption of Gaussian distribution for layer outputs is inadequately justified, and the complexity of the adaptive learning rate calculation may not provide clear advantages over existing optimizers.
- The paper lacks comparative experiments between the new variants of KFAC, EKFAC, and TKFAC against LNGD, which could enhance understanding of their relative performances.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation by conducting experiments on a wider variety of models and benchmarks, including comparisons with recent algorithms such as EKFAC and TKFAC. Additionally, we suggest clarifying the justification for the Gaussian distribution assumption and simplifying the adaptive learning rate calculation to demonstrate its practical advantages over existing methods. It would also be beneficial to include error bars in the experimental results and provide a clear distinction between LNGD and similar methods like THOR. Finally, we encourage the authors to release their code to enhance reproducibility and transparency in their findings, and to incorporate layer-wise sampling and adaptive layer-wise learning rates into EKFAC and TKFAC, subsequently comparing their performance against the baseline for a more comprehensive analysis.