ID: IiRHQ7gvnq
Title: Benchmarking Foundation Models with Language-Model-as-an-Examiner
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new benchmarking framework called Language-Model-as-an-Examiner to evaluate the performance of foundation models in open-ended question answering. The framework utilizes a language model (LM) as an examiner to generate questions and evaluate responses in a reference-free manner. Key contributions include the framework's extensibility, the use of multiple domains and follow-up questions for comprehensive evaluation, the combination of scoring and ranking measurements for reliable results, and a decentralized peer-examination method to mitigate biases. The authors also explore domain-specificity and its implications for idea generation, proposing a framework that includes categories of questions to enhance understanding and effectiveness. Additionally, the discussion on the value of k is highlighted as significant. The authors release a dataset and benchmarking results to promote reproducibility and further research.

### Strengths and Weaknesses
Strengths:
- The use of an LM as an examiner is an innovative approach.
- The framework's extensibility allows for the adoption of various LMs and the continuous updating of questions.
- The combination of scoring and ranking measurements enhances evaluation accuracy and aligns closely with human annotations.
- The decentralized peer-examination method effectively addresses biases in evaluation.
- The exploration of domain-specificity is well-articulated and relevant to the field.
- The detailed discussion on the value of k adds depth to the paper.
- The provision of a dataset and benchmarking results promotes reproducibility and further research.

Weaknesses:
- The claims regarding the reduction of evaluator LM hallucination and testing leakage lack thorough quantitative evaluation.
- The paper does not clearly explain how LM-generated questions are ensured to be novel and not influenced by prior training data.
- The scalability of the proposed framework in real-world scenarios is not adequately discussed.
- The paper could benefit from including categories of questions alongside examples to better illustrate effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of claims regarding the reduction of hallucination by providing quantitative analysis. Additionally, the authors should clarify how they ensure that LM-generated questions are new and not influenced by testing leakage. A detailed analysis of the categories of comprehensiveness affected by the use of LMs as examiners would also be beneficial. Furthermore, discussing the scalability of the benchmarking framework in practical applications would enhance the paper's contributions. Lastly, incorporating categories of questions with examples would improve clarity and demonstrate the effectiveness of the proposed framework.