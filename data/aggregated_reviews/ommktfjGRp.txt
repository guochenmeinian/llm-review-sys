ID: ommktfjGRp
Title: FlexTrain: A Dynamic Training Framework for Heterogeneous Devices Environments
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4

Aggregated Review:
### Key Points
This paper presents a training framework for heterogeneous devices, allowing local model training based on device capabilities while enhancing a global model through active layer sampling and auto-distillation. The authors demonstrate the effectiveness of this approach on the CIFAR-100 dataset and extend it to federated learning settings. The framework significantly reduces FLOPS (62% difference from single training and approximately 40% from independent training) while maintaining comparable accuracy.

### Strengths and Weaknesses
Strengths:  
- Highly reduced computations and training time.  
- Adaptative framework for heterogeneous devices.  
- Experimentally validated with two different models on two datasets in both centralized and decentralized settings.  
- Novel and efficient algorithm for training small and high-performance DNN models.  
- Effective integration with federated learning, enabling knowledge sharing among devices.  

Weaknesses:  
- Code is not open-sourced.  
- Restricted to ResNet and Transformers.  
- No analysis of energy consumption, despite its relevance.  
- Lack of clarity on the extra training overhead introduced by the approach.  
- The necessity of the sampling configuration method is questionable, as sparser configurations might enhance training speed.  
- Limited theoretical analysis, particularly regarding federated learning extensions.  

### Suggestions for Improvement
We recommend that the authors improve the paper by including an analysis of energy consumption, as it is crucial for understanding the framework's efficiency. Additionally, we suggest that the authors clarify the experiment settings prior to presenting the results. It would also be beneficial to explore the algorithm's applicability to other architectures beyond ResNets and Transformers. Finally, we encourage the authors to provide a more detailed theoretical analysis, especially concerning the federated learning extension.