ID: DpP5F3UfKw
Title: Divergences between Language Models and Human Brains
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 7, 6, 7, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the differences in language representations between large language models (LLMs), specifically GPT-2 XL, and human brain activity as measured by MEG. The authors propose that LLM representations inadequately capture social/emotional intelligence and physical commonsense, and demonstrate that fine-tuning GPT-2 XL on these domains enhances alignment with human brain responses. The methodology includes using LLMs to generate hypotheses about text types that lead to poor MEG signal predictions.

### Strengths and Weaknesses
Strengths:
1. The paper addresses significant topics of social/emotional intelligence and physical commonsense, emphasizing the need for improved LLM alignment with human cognition.
2. The methodology utilizing proposer and verifier LLMs is innovative and leverages the growing capabilities of LLMs to advance cognitive neuroscience.
3. The authors conduct further investigations, including human behavioral experiments and dataset annotations, enhancing the study's depth.

Weaknesses:
1. The reliance on layer 7 of GPT-2 XL for claims about brain alignment is questionable, as this layer is relatively low compared to findings in prior research that suggest higher layers capture more complex properties.
2. The results in Table 1 raise concerns about the validity of claims regarding LLMs' deficiencies in social/emotional intelligence and physical commonsense, given that a significant number of participants found the top hypotheses unconvincing.
3. The potential confounding effect of additional training data on the observed improvements in brain alignment needs clearer exploration, particularly regarding whether the enhancements stem from statistical similarities or improved understanding.

### Suggestions for Improvement
We recommend that the authors improve their analysis by exploring higher layers of GPT-2 XL to substantiate claims about brain alignment for complex properties. Additionally, we suggest that the authors clarify the discrepancies in Table 1, addressing the participant ratings that challenge their conclusions. To mitigate confounding variables, we encourage the authors to conduct control experiments that assess the impact of additional training data without supervision. Finally, providing more details about the prompts and API usage for the proposer and verifier LLMs would enhance clarity and reproducibility.