ID: fmYmXNPmhv
Title: Permutation Equivariant Neural Functionals
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 7, 7, 3, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents permutation equivariant neural functional networks (NFNs) designed to process weights from other neural networks, including MLPs and CNNs. The authors propose NF-Layers, which are S-equivariant, ensuring that permutations of input weights lead to corresponding permutations in output weights. Additionally, the authors introduce an equivariant weight-sharing scheme based on the permutational symmetries of neural networks, allowing for permutations of internal neurons (the “HNP” case) and, in some instances, input/output neurons (the “NP” case). The NP case is more beneficial but less frequently applicable due to the need for additional symmetry structures, which the authors address by introducing positional encodings. The paper evaluates the NFNs across various applications, including accuracy prediction, classification of implicit neural representations (INRs), and weight-space editing, demonstrating their effectiveness and promising results compared to baseline models, particularly in tasks involving permutation symmetries.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated, clearly written, and accessible, with effective visualizations.
- The introduction of NF-Layers enhances modeling efficiency and demonstrates significant improvements in various applications.
- The use of permutational symmetries for training on weight and bias inputs is a novel approach, distinct from previously studied groups.
- The experiments are diverse and benchmarked against a reasonable set of baselines, supporting the utility and superiority of NFNs.
- The work facilitates the application of weight-space symmetry techniques to CNNs, addressing a topical issue in neural network learning.

Weaknesses:
- Concerns exist regarding the novelty of the work compared to prior studies, particularly the lack of acknowledgment of similar architectures and theoretical guarantees, especially in relation to concurrent work by Navon et al. (ICML 2023).
- Technical details regarding the generation of INRs and the experimental setup are insufficient, hindering reproducibility.
- The focus on permutational symmetries neglects other potential symmetries related to specific nonlinearities, such as the scaling symmetry of ReLU.
- The proofs in the appendix may be challenging for readers unfamiliar with the topic, and the authors should clarify the relationship between their work and concurrent studies.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proofs in section B.4 to ensure accessibility for all readers. Additionally, it is essential to explicitly discuss the novelty of their work in relation to prior studies, particularly Navon et al., and include them as baselines in the experimental section. The authors should provide more technical details about the INR generation process, including sample sizes and training paradigms, to enhance reproducibility. Furthermore, we suggest including an ablation study on the performance gains from using positional encodings, as well as exploring the application of NFNs to more complex style editing tasks. We also recommend incorporating a discussion on other nonlinearity-dependent symmetries, such as scaling for ReLU, and how these could enhance the architecture's effectiveness. Lastly, it would be beneficial to motivate the choice of classifying implicit neural representations over direct pixel analysis within the main text and to include an experimental comparison with Dupont et al. (ICML'22) to strengthen the paper's contributions.