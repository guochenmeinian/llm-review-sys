ID: P8rTCT6g45
Title: Memory-Efficient LLM Training with Online Subspace Descent
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 5, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a memory-efficient optimization method for large language models (LLMs) that utilizes low-rank projection. The authors derive asymptotic convergence results for optimizers with arbitrary projection matrices and propose an online subspace descent method that updates the projection matrix using online PCA. Experimental results indicate that this method enhances training efficiency while maintaining performance comparable to low-rank baselines using SVD projection.

### Strengths and Weaknesses
Strengths:
1. The paper derives the first convergence result for optimizers with low-rank projection.
2. It addresses the inefficiency of SVD in constructing the projection matrix through online subspace descent.
3. Experimental results demonstrate the advantages of the online subspace descent method over the SVD baseline.

Weaknesses:
1. The proposed method may increase memory overhead, particularly when using Adam-type optimizers for the projection matrix, with unclear implications as model size increases.
2. The motivation for using Hamiltonian descent and Lyapunov analysis for convergence is insufficiently justified, particularly regarding the applicability of results in discrete cases.
3. Comparisons in Figure 2 are misleading, as SVD is not typically updated at every iteration.

### Suggestions for Improvement
We recommend that the authors improve clarity by specifying the model associated with the reported memory overhead figures. Additionally, we suggest providing more motivation for the use of Hamiltonian descent and Lyapunov analysis, including whether similar results can be derived in discrete cases. It would also be beneficial to clarify the comparison methodology in Figure 2. Furthermore, we encourage the authors to conduct additional experiments on standard NLP tasks like GLUE to validate the method's effectiveness and to compute standard deviations to ensure results are not due to noise. Lastly, we recommend addressing the grammatical errors and typos to enhance readability.