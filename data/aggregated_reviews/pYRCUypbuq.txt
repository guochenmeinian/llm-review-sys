ID: pYRCUypbuq
Title: Did You Mean...? Confidence-based Trade-offs in Semantic Parsing
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a system that utilizes calibration of semantic parsers to minimize the annotation required for training models. The authors conduct a simulated annotator-in-the-loop experiment to assess whether calibration scores indicate the need for rephrasing queries, thereby reducing errors in system output. The "DidYouMean" system allows low-confidence parses a second chance by rephrasing questions for user confirmation. The authors demonstrate that token-level confidence scores can balance usability and safety, and a user study indicates high inter-annotator agreement, although safety measures may compromise usability.

### Strengths and Weaknesses
Strengths:
- The work is well-motivated, clearly written, and presents thorough experimentation.
- It effectively addresses the need for expert annotators and innovatively applies calibration to filter high-risk predictions.
- The paper is well-organized, with reasonable results and a thorough limitations section.

Weaknesses:
- The simulated human-in-the-loop experiments rely on an annotator assumed to be 100% accurate, which undermines the study's rigor.
- The choice of three metrics for evaluation lacks clear justification, and the paper does not compare against relevant baselines or previous work.
- The paper's density may hinder comprehension for readers outside the core expertise area.

### Suggestions for Improvement
We recommend that the authors improve the rigor of their study by incorporating real annotators or conducting a disagreement analysis in their simulations. Additionally, we suggest providing a clearer rationale for the selection of the three metrics used in Section 4 and addressing the absence of comparisons with other HITL or automated baselines. To enhance clarity, consider unpacking the experimental setup and results discussion further, and ensure that usability is more clearly defined in the context of the study.