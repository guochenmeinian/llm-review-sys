ID: As4101fOG1
Title: What a MESS: Multi-Domain Evaluation of Zero-Shot Semantic Segmentation
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 8, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark called the Multi-domain Evaluation of Semantic Segmentation (MESS) for evaluating zero-shot semantic segmentation models across diverse domains, including medicine, engineering, earth monitoring, biology, and agriculture. The authors reviewed 120 datasets, developed a taxonomy of task characteristics, and selected 22 representative datasets for the MESS benchmark. They evaluated eight recent models, identifying key characteristics that influence performance, such as semantic similarity of classes and sensor type, while also addressing challenges in applying zero-shot segmentation to domain-specific datasets. Additionally, the authors explore methods to enhance multi-domain performance in semantic segmentation, focusing on domain adaptation and domain-specific fine-tuning. They emphasize the importance of pre-trained vision-language foundation models, noting that models like CAT-Seg, which utilize CLIP fine-tuning, demonstrate superior generalizability, and highlight the potential of SAM in zero-shot semantic segmentation when integrated with CLIP-based models.

### Strengths and Weaknesses
Strengths:  
1. The paper provides a significant contribution by establishing a comprehensive benchmark for zero-shot semantic segmentation models, facilitating a holistic evaluation across various domains.  
2. The statistical analysis of datasets aids in selecting appropriate data, ensuring diversity and balance.  
3. The structure of the paper is clear, with well-presented experimental results and insightful analysis of factors affecting performance.  
4. The manuscript effectively identifies promising methods for improving multi-domain performance, supported by empirical evidence.  
5. The inclusion of the MESS benchmark is a valuable resource for future research.

Weaknesses:  
1. The selection criteria for the 22 datasets in the MESS benchmark lack detailed discussion.  
2. The analysis of model performance is insufficient; a deeper exploration of the pros and cons of existing methods is needed.  
3. The paper does not adequately address the limitations and potential societal impacts of the work.  
4. The paper lacks detailed information on training datasets in the main text, which may hinder understanding.  
5. There is insufficient exploration of specific model components responsible for performance variations.

### Suggestions for Improvement
1. We recommend that the authors improve the discussion surrounding the selection criteria for the representative subset of datasets included in the MESS benchmark.  
2. The authors should analyze the relationships between model features and dataset characteristics, particularly identifying which features contribute to high performance in models like CAT-Seg-L and the limitations of others like ZSSeg-B.  
3. We suggest that the authors summarize the challenges identified in existing zero-shot segmentation research and propose potential solutions or directions for overcoming these challenges.  
4. The authors should include a discussion on the computational costs associated with the models evaluated, as this is crucial for real-world applications.  
5. We recommend that the authors address the societal impacts of their work, particularly concerning biases in the datasets used for training and evaluation.  
6. We encourage the authors to improve the clarity of the training dataset information by including more details in Section 4.3 of the main paper.  
7. We suggest conducting ablation studies on the MESS benchmark to quantitatively assess the impact of specific model features on performance.  
8. To enhance the discussion on societal implications, we encourage the authors to expand on this aspect in the conclusion.