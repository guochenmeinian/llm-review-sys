ID: 3QzTzulZwY
Title: IMTLab: An Open-Source Platform for Building, Evaluating, and Diagnosing Interactive Machine Translation Systems
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified toolkit for evaluating interactive machine translation (IMT) approaches, defining operations (keep, insert, replace, delete, blank filling) and policies (prefix decoding and variations of in-filling). The authors evaluate four different models using these policies, measuring response time and the number of turns to reach final translations. Simulated experiments indicate that a traditional transformer model with prefix-decoding performs best, while a human evaluation shows good correlation with these results. The work also contributes an open-source platform for building and evaluating IMT systems.

### Strengths and Weaknesses
Strengths:
- The unified toolkit for evaluating IMT is likely to facilitate comparisons among different approaches.
- The proposed evaluation metrics and abstractions enhance the understanding of IMT systems.
- The platform allows for fast, automatic evaluation and includes a comprehensive set of evaluations across various metrics.

Weaknesses:
- The paper lacks important details that may hinder replicability.
- The proposed editing costs may not be fairly applied across all IMT policies, potentially skewing results.
- User simulation still requires improvement, and the evaluation does not fully meet its intended purpose.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the editing operations and provide examples for each policy to enhance understanding. Additionally, consider revising the editing cost structure to ensure fairness across IMT policies, possibly by adjusting the costs associated with "keep" and "delete" operations. It would also be beneficial to explore the integration of mouse actions to reduce editing costs further. Lastly, we suggest enhancing the user simulation to better predict human performance and improve the overall evaluation protocol.