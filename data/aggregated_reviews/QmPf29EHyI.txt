ID: QmPf29EHyI
Title: Bifurcations and loss jumps in RNN training
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of piecewise-linear recurrent neural networks (PLRNNs) through the lens of bifurcation theory, categorizing fixed points and bifurcations of 2-dimensional PLRNNs in Section 3.1. The authors employ Jacobian computations and spectrum classifications to identify multi-stability. Theorems 1 and 2 in Section 3.2 establish conditions under which PLRNN gradients approach infinity or zero during degenerate transcritical bifurcations (DTBs) and border collision bifurcations (BCBs). Section 4 introduces the SCYFI algorithm, a heuristic for identifying fixed points and cycles in higher-dimensional PLRNNs, demonstrating faster performance than brute-force methods. The authors also present theoretical results regarding SCYFI's convergence properties, showing that SCYFI almost surely converges with a straightforward proof based on the initialization of $D$-matrices. They provide specific results about SCYFI's convergence speed under certain parameter conditions, indicating linear time convergence when specific criteria are met. Section 5 applies these analyses to neuron spiking predictions, revealing a stable 39-cycle and a DTB-induced loss spike.

### Strengths and Weaknesses
Strengths:  
The paper provides novel insights into the bifurcation properties of PLRNNs, linking them to gradient behavior during training. The theoretical results regarding SCYFI's convergence are clear and backed by rigorous proofs and mathematical formulations, enhancing the credibility of the claims. The writing is clear, and the application of bifurcation analysis is innovative, with experimental validation of the correlation between loss jumps and DTBs.

Weaknesses:  
The SCYFI algorithm's description is ambiguous, hindering acceptance, and the complexity of the mathematical details may hinder accessibility for readers less familiar with the subject. The presentation may be inaccessible to the ML community, lacking clear explanations of dynamical systems concepts. The omission of external inputs $s_t$ is significant, and typing longer theorem proofs in limited comment boxes poses a challenge, potentially affecting clarity. Additionally, the paper's limited discussion of bifurcations may restrict its impact.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the SCYFI algorithm by addressing ambiguities regarding terms like $z_n$ and $D_n$, and by providing explicit definitions and examples for the bifurcations discussed. Additionally, we suggest including a visualization in parameter space for a toy example to enhance understanding. The authors should also trace the relevance of identifying k-cycles and bifurcations more explicitly in the introduction to motivate the study for ML researchers. Furthermore, we encourage the authors to explore the implications of their findings on the training process of PLRNNs and consider including experiments that demonstrate how these insights can influence training. Lastly, we recommend improving the clarity of their proofs by considering alternative formats or breaking them into smaller, more digestible parts, and providing more intuitive explanations or visual aids to enhance accessibility for a broader audience. Addressing the limitations of the algorithm and expanding the discussion on other bifurcation types would also strengthen the paper.