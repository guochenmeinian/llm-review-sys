ID: AthZ2g6VE2
Title: LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 7, 3, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LoCoDL, an algorithm that integrates communication compression with local training, achieving a state-of-the-art (SOTA) convergence rate for strongly convex problems. The authors provide a thorough theoretical analysis and empirical results demonstrating that LoCoDL outperforms existing algorithms like ADIANA. The algorithm's innovative approach of maintaining two local estimates enhances both local training and communication compression.

### Strengths and Weaknesses
Strengths:
1. The combination of communication compression and local training is novel.
2. The convergence results achieve SOTA for large $n$ and nearly SOTA for small $n$.
3. Empirical performance of LoCoDL surpasses that of ADIANA.
4. The algorithm is simple and addresses a general problem setting.
5. The theoretical claims are rigorously proven.

Weaknesses:
1. The experimental settings involve $n$ comparable to $d$, making it crucial to compare LoCoDL with ADIANA when $n$ is significantly smaller than $d$.
2. The datasets used for experiments are relatively small; larger datasets like MNIST are recommended for further validation.
3. The intuition behind the algorithm's development is not clearly articulated, necessitating more detailed explanations.
4. The practical applicability of LoCoDL is unclear, particularly regarding its efficiency in partial participation scenarios and neural networks.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including comparisons with larger datasets and additional tasks beyond logistic regression to demonstrate LoCoDL's efficiency. Clarifying the practical applicability of LoCoDL, especially in partial participation contexts, would strengthen the paper. Additionally, providing more detailed explanations of the algorithm's development and addressing the clarity of certain statements would enhance understanding. Finally, we suggest outlining the limitations of the algorithm explicitly, particularly regarding its applicability to neural networks and partial participation use cases.