ID: cZS5X3PLOR
Title: Data Minimization at Inference Time
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 6, 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework addressing the necessity of soliciting extensive user information at inference time, particularly in privacy-sensitive domains like law, recruitment, and healthcare. The authors propose that individuals may only need to disclose a small subset of their information to achieve accurate predictions, thus enhancing privacy and reducing the verification burden on organizations. They introduce algorithms for selecting appropriate feature subsets and provide theoretical justification and empirical findings, indicating that users might only need to disclose about 10% of their information to maintain prediction accuracy. Additionally, the paper presents a novel approach to data minimization at inference time, focusing on privacy while predicting hard labels for sensitive attributes. The authors argue that when features are independent, it is impossible to infer sensitive attributes from model predictions, and they emphasize that their work does not rely on independence assumptions while aiming to minimize data leakage and improve certainty.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant and underexplored research question, demonstrating that "most users would only need to reveal a small portion of their sensitive data" for accurate predictions.
- It includes a robust framework for quantifying the trade-off between model certainty and privacy loss.
- The proposed algorithms are efficient, and the experiments are well-designed, utilizing random subsets of private features from publicly available datasets.
- The introduction of data minimization at inference time is relevant for privacy in sensitive domains like banking, and the authors provide a clear distinction between hard and soft labels, highlighting the efficiency of using hard labels to minimize privacy risks.

Weaknesses:
- The use of entropy as a proxy for model performance is problematic, as it shifts the focus from predictive performance to prediction entropy, which may not adequately reflect model accuracy.
- The independence assumption underlying the approach is viewed as impractical in real-world scenarios, particularly in banking where the probability of default is crucial.
- Notation and terminology are inconsistent and unclear, particularly regarding "data leakage" and the treatment of unrevealed features.
- The paper lacks a formal definition of the objective to maintain predictive quality with minimal private features, and relevant related work is inadequately addressed.
- The justification for using entropy as a metric for performance is considered insufficient, lacking theoretical or experimental backing.

### Suggestions for Improvement
We recommend that the authors improve the clarity and consistency of terminology, particularly regarding "data leakage," and provide precise definitions for key terms. Additionally, we suggest discussing the choice of using entropy as a performance metric, including theoretical and experimental justification, or clarifying that the focus is on improving certainty rather than overall accuracy. The authors should formalize the objective of maintaining predictive quality with minimal private features and consider a more holistic evaluation of model performance beyond accuracy, such as ROC performance and calibration. Furthermore, we encourage the authors to engage with existing literature on feature selection and local explanation frameworks to contextualize their contributions more effectively. Lastly, including new experiments to demonstrate the ineffectiveness of classical reconstruction attacks on minimized inference data would strengthen the paper.