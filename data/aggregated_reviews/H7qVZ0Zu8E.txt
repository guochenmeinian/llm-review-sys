ID: H7qVZ0Zu8E
Title: Achieving Linear Convergence with Parameter-Free Algorithms in Decentralized Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a parameter-free algorithm for decentralized learning based on forward-backward splitting and variable metrics, targeting convex locally smooth functions. The authors propose that their method converges linearly to the optimal solution without the need for hyperparameter tuning, providing a convergence guarantee under milder conditions than previous studies. The paper also discusses adaptive parameter determination and local min-consensus strategies, which are significant contributions to the field.

### Strengths and Weaknesses
Strengths:
1. The paper introduces the first parameter-free decentralized training algorithm that integrates line search and splitting techniques.
2. It provides a convergence guarantee with a favorable rate and analysis under less stringent conditions.
3. The topic of adaptive parameter determination is meaningful and challenging, and the proposed method shows promise in addressing these issues.

Weaknesses:
1. The proposed algorithm is complex, requiring substantial computation at each iteration, raising questions about its computational complexity compared to existing methods.
2. The experimental evaluation of the algorithm is insufficient, particularly regarding comparisons with well-tuned versions of existing methods like EXTRA and NIDS.
3. Certain sections of the paper are difficult to follow, and the exposition could be improved, particularly in the discussion of the saddle point reformulation and the implications of Theorem 5.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by enhancing the exposition, especially in Section 3, and providing more detailed discussions on the implications of Theorem 5. Additionally, we suggest that the authors conduct a more comprehensive experimental evaluation, including comparisons of their method with well-tuned versions of EXTRA and NIDS to substantiate their claims of superior performance. Furthermore, addressing the computational complexity of the proposed algorithm in comparison to existing methods would strengthen the paper.