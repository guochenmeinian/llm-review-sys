ID: tN61DTr4Ed
Title: OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 9, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents OSWorld, a scalable real computer environment designed for evaluating multimodal agents across various operating systems. It introduces a benchmark comprising 369 tasks that reflect real user scenarios, covering diverse applications such as web and desktop tasks, file I/O, and multi-application workflows. The authors conduct execution-based evaluations of advanced large language models (LLMs) and vision-language models (VLMs), revealing significant performance gaps compared to human users. Additionally, the authors propose a POMDP formulation to address the complexities of real-world computer interactions, highlighting challenges such as unobservable system states, including internal application states, external service dependencies, network issues, and hardware states. They reformulate this as a Goal-Augmented POMDP, incorporating a tuple that includes state and observation spaces, goal definitions, and a mapping function from observations to goals. The authors have also responded to reviewer concerns by enhancing their exploration of open-source models, providing a thorough analysis of experimental results, clarifying evaluation criteria, and incorporating suggested benchmarks.

### Strengths and Weaknesses
Strengths:
- OSWorld is the first scalable environment for evaluating multimodal agents across multiple OS platforms.
- The benchmark includes a comprehensive set of 369 tasks with detailed annotations, enhancing reproducibility.
- The execution-based evaluation provides more reliable results than traditional QA-based assessments.
- The environment supports cross-platform task setup and execution, facilitating diverse application scenarios.
- The proposed Goal-Augmented POMDP formulation aligns well with goal-conditioned reinforcement learning literature.
- The authors have addressed previous concerns by including additional experiments and analyses, improving the clarity and depth of the paper.
- The incorporation of detailed case analyses and structural similarity metrics enhances the evaluation process.

Weaknesses:
- The benchmark mixes "Knowledge" and "Execution," potentially leading to misleading results regarding model capabilities.
- There is a lack of a human executor baseline, complicating the assessment of model performance.
- The analysis of experimental results is insufficient, with limited exploration of open-source models and comparisons to similar works.
- The initial task formulation lacked clarity, which may have hindered understanding.
- Some experimental results did not clearly demonstrate the advantages of multimodal approaches over single strategies.

### Suggestions for Improvement
We recommend that the authors improve the clarity of task definitions, particularly regarding the use of POMDP formulations and the role of rewards in the context of their framework. Additionally, we suggest incorporating a human executor baseline to better evaluate model performance. Expanding the analysis of experimental results, particularly in relation to the performance of different models and task types, would enhance the paper's contributions. Furthermore, we encourage the authors to elaborate on the reasons behind the performance trends observed in their experiments, particularly regarding the integration of visual information. Exploring multi-agent scenarios and conducting larger-scale user studies with diverse participants could provide more robust insights into task difficulty and agent capabilities. Lastly, we urge the authors to continue refining their evaluation criteria and consider standardizing accessibility information quality while addressing scalability challenges in future iterations of the benchmark.