ID: UDi51I8K1p
Title: Exploring the trade-off between deep-learning and explainable models for brain-machine interfaces
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 5, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a rigorous comparison of four neural decoders used in brain-computer interfaces (BCIs): Kalman Filter, KalmanNet, tcFNN, and LSTM, evaluated in both offline and online conditions on a non-human primate performing a 2 degree of freedom dexterous finger task. The authors propose KalmanNet, which integrates deep learning techniques by estimating the Kalman Gain through GRUs, achieving performance comparable to fully "black-box" methods like LSTMs while retaining some explainability. The study also analyzes the behavior of KalmanNet's GRUs and demonstrates that deep learning approaches struggle with out-of-distribution inputs, while LSTMs exhibit surprising robustness to input noise.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clearly addresses the trade-off between performance and explainability in BCI decoders.
- It provides a comprehensive evaluation of four decoders, including a novel hybrid model (KalmanNet) that combines traditional and deep learning approaches.
- The authors successfully demonstrate that traditional linear methods can be adapted with deep learning techniques without sacrificing explainability.
- The analysis of KalmanNet's behavior through the explainable Kalman Filter adds depth to the findings.

Weaknesses:
- The interpretation of p-values is misleading; the authors should avoid concluding that there is no significant difference when $p>0.05$ and instead run a proper statistical equivalence test.
- The paper lacks specification of the statistical tests used, and the argument regarding MSE differences between models is not adequately supported.
- There are missing error bars for online metrics in Figure 2, and the variance of MSE in position is notably high.
- The paper does not sufficiently compare KalmanNet with more recent transformer-based models or provide detailed ablation studies.
- The analysis of interpretability lacks concrete examples and a dedicated related work section is absent, hindering contextual understanding.

### Suggestions for Improvement
We recommend that the authors improve the statistical analysis by clearly specifying the tests used and avoiding misleading conclusions based on p-values. It is crucial to run proper statistical equivalence tests to support claims of model performance. Additionally, the authors should include error bars for online metrics in Figure 2 and address the high variance in MSE. We suggest exploring transfer learning techniques to enhance KalmanNet's generalization capabilities and conducting comprehensive ablation studies to clarify the contributions of different components. Finally, a more detailed analysis of interpretability with practical examples and the inclusion of a related work section would strengthen the paper's context and significance.