ID: kjZIIvFtmK
Title: Investigating Layer Importance in Large Language Models
Conference: EMNLP/2024/Workshop/BlackBoxNLP
Year: 2024
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 4, 3, 4

Aggregated Review:
### Key Points
This paper investigates the importance of individual layers in Large Language Models (LLMs) using Shapley-based interpretation and layer ablation methods. The authors find that certain layers, termed 'cornerstone layers', significantly impact model performance, while the removal of non-cornerstone layers does not yield a similar decline. The study focuses on reading comprehension tasks and demonstrates consistent results across various model sizes and benchmarks.

### Strengths and Weaknesses
Strengths:
- The paper presents a clear idea and well-structured execution, validating hypotheses through multiple methods.
- The empirical results are interesting and provide valuable directions for future research on reading comprehension tasks with Llama-style models.
- The interpretability techniques employed are well-motivated, with Shapley values offering insights into layer importance.

Weaknesses:
- The novelty of the work compared to Ghorbani and Zou (2020) regarding Shapley value and layer ablation results is unclear.
- The analysis is limited to reading comprehension datasets, and extending it to generative tasks could enhance the findings.
- The definition of 'cornerstone layers' lacks clarity, and the computation of Shapley values is not sufficiently detailed.
- There are concerns regarding the experimental design, particularly the assumption of independence among layers.

### Suggestions for Improvement
We recommend that the authors clarify the distinction between their work and Ghorbani and Zou (2020) regarding Shapley values and layer ablation. Additionally, we suggest extending the analysis to generative tasks to strengthen the findings. The authors should provide a clearer definition of 'cornerstone layers' and include a detailed algorithm or pseudocode for computing Shapley values. We also encourage the authors to address the assumptions made in their experimental design and consider a random perturbation-based approach for layer importance computation. Lastly, we recommend improving the clarity of figures and revising the paper title to reflect the methods or findings more specifically.