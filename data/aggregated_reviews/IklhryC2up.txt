ID: IklhryC2up
Title: Computational Complexity of Detecting Proximity to Losslessly Compressible Neural Network Parameters
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 4, 7, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 3, 1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical exploration of lossless compressibility in single-hidden-layer hyperbolic tangent networks. The authors introduce the concepts of 'rank'—the minimal number of hidden units required to replicate a function—and 'proximate rank'—the rank of the most compressible parameter vector within a limited L-infinity neighborhood. The paper also establishes that estimating the proximate rank is an NP-complete problem through a geometric reduction from Boolean satisfiability.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant theoretical problem in deep learning, providing interesting results.
- It demonstrates that bounding the approximate rank is an NP-complete decision problem, contributing to the understanding of network compressibility.
- The presentation is clear and well-structured.

Weaknesses:
- The practical importance of the results is questionable, particularly regarding the relevance of infinitesimal errors in representation.
- The algorithm is limited to single-hidden-layer hyperbolic tangent networks, lacking generalizability to more commonly used architectures.
- The connection between the theoretical findings and existing empirical network compression literature is not sufficiently explored.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the practical implications of their findings, particularly addressing the relevance of allowing slight performance drops in compressed networks. Additionally, we suggest that the authors explore a more relaxed definition of "rank" that could be applicable to existing neural network compression methods. It would also be beneficial to provide a roadmap for generalizing their concepts beyond single-hidden-layer networks and to clarify how their notions of rank and proximate rank relate to existing literature on compressibility and generalization in deep learning.