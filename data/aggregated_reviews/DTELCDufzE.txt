ID: DTELCDufzE
Title: Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel backdoor attack method, LLMBkd, which utilizes GPT-3.5 to generate diverse poison samples aimed at misleading sentiment classification models. The authors propose a gray-box poison selection method and a defense mechanism using antidote training examples. The methodology is supported by comprehensive experiments demonstrating its effectiveness, although concerns were raised regarding the model's performance on the AGNews dataset.

### Strengths and Weaknesses
Strengths:  
- LLMBkd achieves a higher attack success rate than baseline methods and generates style-rich poison text.  
- The paper is well-written and organized, with clear motivation and methodology.  
- Comprehensive experiments, including ablation studies and human evaluations, support the claims made.  

Weaknesses:  
- The use of GPT-3.5 to generate mildly positive samples raises questions about the validity of these as adversarial examples, as they may not represent successful attacks.  
- The performance on the AGNews dataset does not surpass baselines, necessitating further analysis.  
- The defense results are not presented across all datasets, limiting the comprehensiveness of the analysis.  

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of the paper, addressing the concerns regarding the classification of generated samples as positive sentiment. Additionally, we suggest providing a more thorough analysis of the AGNews dataset performance and including defense results for all datasets to enhance the robustness of the findings. Finally, we encourage the authors to clarify the choice of evaluation metrics, particularly the use of USE for sentence similarity.