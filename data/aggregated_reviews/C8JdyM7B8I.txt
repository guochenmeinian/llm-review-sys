ID: C8JdyM7B8I
Title: Towards Label-free Scene Understanding by Vision Foundation Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 4, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method that leverages the SAM and CLIP models for label-free understanding of 2D and 3D environments. The authors propose a two-step process where a noisy output from the CLIP model is refined using a SAM network to enhance semantic results. Additionally, a Cross-modality Noise Supervision module is introduced to optimize the training of both 2D and 3D models concurrently. The authors report promising results on datasets like ScanNet and nuScenes, indicating the effectiveness of their approach in capturing visual and semantic aspects of real-world scenes.

### Strengths and Weaknesses
Strengths:
1. The paper advances the field by utilizing label-free training data with pre-existing models that excel in zero-shot tasks.
2. It effectively refines CLIP's predicted labels to achieve superior results on indoor datasets.
3. The writing is generally clear and maintains a logical flow.
4. The proposed method outperforms existing approaches, such as MaskCLIP and CLIP2scene, achieving state-of-the-art results.

Weaknesses:
1. The explanation of the Prediction Consistency Regularization and the random switching of pseudo labels between 2D, 3D, and CLIP outputs is insufficient.
2. An ablation study is needed to assess the impact of random switching and the effects of using individual pseudo labels.
3. The marginal improvement from using SAM features raises questions about their effectiveness; qualitative intermediate results are necessary for justification.
4. The paper lacks clarity on the "calibration matrix" and other terminologies, which need further elaboration.
5. The mIoU scores for both 2D and 3D segmentation remain below 35, indicating room for improvement.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the Prediction Consistency Regularization and provide a detailed ablation study to clarify the impact of random switching among pseudo labels. Additionally, we suggest including qualitative intermediate results to demonstrate how SAM features enhance 2D and 3D features. The authors should also clarify the "calibration matrix" and other ambiguous terms in the text. Furthermore, we encourage the authors to explore the baseline comparisons more rigorously and consider utilizing simpler models for clearer performance evaluations. Lastly, addressing the low mIoU scores and providing a more comprehensive discussion on limitations and broader impacts would strengthen the paper.