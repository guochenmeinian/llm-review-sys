ID: KZjSvE2mJz
Title: MLFMF: Data Sets for Machine Learning for Mathematical Formalization
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 5, 6, 8, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a dataset designed for evaluating recommendation systems that assist in the formalization of mathematics using proof assistants. The dataset comprises over 250,000 entries from four mathematical formalization libraries, making it the largest collection of formalized mathematical knowledge available in a machine-readable format. The authors propose a method for splitting the data into training and testing sets and conduct benchmark comparisons using various baseline methods, notably highlighting the performance of node2vec embeddings in a tree-bagging model. Additionally, the paper evaluates a model against various datasets, including a comparison with the Lean dataset. The authors provide further details on the node2vec baseline and include an illustrative example in the introduction that enhances comprehension. However, discrepancies between accuracy and `minRank` scores in the results raise concerns about the validity of the findings.

### Strengths and Weaknesses
Strengths:  
- The dataset is substantial, with 250,000 entries from two proof assistants, likely supporting future research in automated theorem proving.  
- The paper is well-motivated, with clear explanations of the dataset's purpose and potential applications.  
- The authors have meticulously unified different library formats for compatibility, and the documentation has been improved to aid reproducibility.  
- Detailed responses and updates to the paper enhance clarity.  
- Inclusion of the Lean dataset provides a valuable comparison.  
- The illustrative example aids understanding for readers unfamiliar with the topic.  
- Acknowledgment of errors and clarifications regarding metrics demonstrate responsiveness to feedback.  

Weaknesses:  
- The text is dense and may be challenging for readers unfamiliar with mathematical formalization, lacking clear definitions and illustrative examples.  
- Documentation is still inadequate, particularly regarding the specifics of benchmark model creation and the execution of provided codes.  
- The experiments lack multiple meaningful baselines, and the performance of most methods appears close to random guessing, raising questions about the dataset's difficulty.  
- Significant discrepancies in accuracy and `minRank` scores remain unresolved.  
- Lack of meaningful baselines beyond node2vec limits the robustness of the evaluation.  
- Insufficient ablation studies hinder justification of dataset utility relative to prior work.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing more accessible explanations and illustrative examples, particularly in the introduction. Including a specific entry from the dataset as an example would enhance understanding. Additionally, we suggest expanding the documentation to include detailed information on how benchmark models were constructed and how to execute the supplementary materials effectively. It would also be beneficial to explore additional state-of-the-art baselines and clarify the rationale behind the choice of metrics used for evaluation, such as considering `top-k accuracy` instead of `minRank`. Furthermore, we encourage the authors to address the discrepancies between accuracy and `minRank` scores, possibly through a whisker plot for better visualization. Including at least one other competitive baseline that exceeds random guess accuracy would strengthen the evaluation. Finally, conducting ablation studies to justify the utility of the datasets by observing model performance degradation with reduced training examples would enhance the paper's contributions.