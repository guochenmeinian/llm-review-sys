ID: 0sDieI5GJh
Title: QUADRo: Dataset and Models for QUestion-Answer Database Retrieval
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new resource named QUADRo, a large annotated dataset for question-answer database retrieval (DBQA), which retrieves answers from a database of question-answer pairs based on input questions. The authors emphasize the importance of incorporating answers into the assessment of question-question similarity and report extensive experimentation to validate the dataset's quality and its effectiveness in improving retrieval and ranking in DBQA tasks.

### Strengths and Weaknesses
Strengths:
- The paper introduces a valuable, well-annotated dataset that addresses the limitations of existing resources and meets the needs of DBQA research.
- The methodology for constructing the dataset is clear, and it is larger than previous datasets, utilizing answers as context to enhance annotation quality.
- Empirical results demonstrate that the dataset improves retrieval and ranking compared to baselines and is applicable to new domains.

Weaknesses:
- The experimental sections lack clarity, with insufficient detail in the explanation of tables and figures.
- The paper critiques previous datasets' quality but does not provide robust quantitative evidence supporting the new dataset's quality, relying on a limited manual analysis of only 200 instances.
- The dataset's diversity is questioned, as a significant portion of samples originates from a few sources, potentially limiting heterogeneity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental sections by providing more detailed explanations of tables and figures. Additionally, we suggest presenting stronger quantitative evidence of the new dataset's quality, possibly by expanding the manual analysis beyond 200 instances. In Section 3, consider only including pairs that exceed a minimum established similarity and clarify the values of k tested. In Section 4.1, report standard measures of inter-annotator agreement and evaluate the impact of the strategies used to ensure annotation quality. Lastly, clarify how the sample size of 200 instances in Section 4.2 was determined.