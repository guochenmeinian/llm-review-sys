ID: UhPUR9cnRJ
Title: Disentangling Likes and Dislikes in Personalized Generative Explainable Recommendation
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel dataset and evaluation method aimed at distinguishing users' positive and negative sentiments towards recommended items, contributing valuable resources to the field of explainable recommendation. The authors utilize GPT-4 to extract sentiment features from user reviews, resulting in a high-quality dataset with clear sentiment information. However, inconsistencies between user ratings and extracted sentiments may affect model performance, as the dataset does not address this contradiction. The authors also propose new evaluation metrics to assess sentiment alignment, demonstrating that existing metrics fail to capture this aspect effectively.

### Strengths and Weaknesses
Strengths:  
- The introduction of datasets that disentangle usersâ€™ sentiments provides a fine-grained understanding of user opinions.  
- The proposed sentiment-matching and content similarity metrics enhance evaluation rigor.  
- Extensive experiments benchmark state-of-the-art models and validate the proposed methods.  
- The paper is well-documented, with detailed explanations of dataset construction and metrics.  
- Human evaluations support the reliability of the results.

Weaknesses:  
- The dataset's reliance on review summarization may omit important sentiment features, potentially compromising accuracy.  
- The strict matching of extracted features to review outputs could overlook implicit sentiments.  
- The datasets are relatively small, with a maximum of 20K entities and half a million interactions.  
- The ground-truth explanations are merely outputs from LLMs, raising concerns about their robustness.  
- Some parts of the paper lack clarity and depth in discussion, particularly regarding the computation of the sentiment-matching score.

### Suggestions for Improvement
We recommend that the authors improve the dataset by considering alternative approaches, such as multi-round extraction, to ensure comprehensive sentiment feature capture. Additionally, the authors should clarify the computation of the sentiment-matching score using mathematical notations for better reader understanding. Incorporating users' predicted ratings as input could enhance the performance of the explanation text generator. We also suggest that the authors conduct significance tests on experimental results to strengthen their quantitative analysis. Lastly, addressing the potential limitations of using LLM outputs as ground-truth explanations would enhance the robustness of the findings.