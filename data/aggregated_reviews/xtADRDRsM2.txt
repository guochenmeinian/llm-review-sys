ID: xtADRDRsM2
Title: Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 1, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to enhancing the robustness of Graph Neural Networks (GNNs) against adversarial attacks by leveraging Hamiltonian Energy Conservation principles. The authors analyze the stability and limitations of various neural ODE-based GNNs, leading to the development of the HANG model, which is empirically evaluated against several adversarial attacks on benchmark datasets. The findings suggest that HANG outperforms prior models in terms of adversarial robustness.

### Strengths and Weaknesses
Strengths:  
- The investigation into GNN vulnerability to adversarial perturbations is significant and well-supported by physics principles.  
- The paper is well-written, with clear motivation for the proposed approach and easy-to-follow technical steps.  
- The proposed model demonstrates significant improvements in robustness against various adversarial attacks.

Weaknesses:  
- The writing quality suffers from excessive equations and a lack of clarity in conveying core ideas, particularly in relation to adversarial research language.  
- The comparison with defense baselines is limited, as it does not include more recent and effective methods.  
- The evaluation of robustness relies on potentially inadequate surrogate model-based black box attacks, lacking strong white-box attack assessments.  
- The connection between Hamiltonian mechanics and energy conservation is not sufficiently articulated, and the importance of energy conservation in relation to graph topology is unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing by minimizing technical jargon and focusing on the core insights relevant to adversarial research. Additionally, it would be beneficial to include comparisons with more recent and robust defense methods, such as those referenced in the reviews. We suggest that the authors evaluate their model against strong white-box attacks, including PGD and Carlini and Wagner attacks, to better demonstrate robustness. Furthermore, the authors should clarify the significance of energy conservation in the context of GNNs and address the relationship between Hamiltonian mechanics and adversarial robustness in a more accessible manner.