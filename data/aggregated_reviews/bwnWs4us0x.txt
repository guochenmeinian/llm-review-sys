ID: bwnWs4us0x
Title: Traceback of Poisoned Texts in Poisoning Attacks to Retrieval-Augmented Generation
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the RAGForensics system, which enhances the security of RAG systems by introducing a traceability mechanism to address poisoning attacks on external knowledge databases. The system can accurately trace and identify potentially poisoned texts without relying on model gradients, marking a significant advancement over previous defense mechanisms. Empirical evaluations demonstrate RAGForensics's effectiveness across multiple datasets against various poisoning attacks, suggesting a promising new direction for securing RAG systems.

### Strengths and Weaknesses
Strengths:
+ The application area of poisoning attacks is relatively unexplored, making this work timely and relevant.
+ The proposed approach shows promising performance and effectively identifies poisoned texts.
+ The evaluation considers adaptive attacks and defense strategies, demonstrating the system's robustness.

Weaknesses:
- The experimental scope is limited, with only 50 queries selected for final testing, which may hinder the robustness of the findings.
- The approach relies heavily on assumptions about attack strategies and lacks insights into the underlying mechanisms of its effectiveness.
- The experiments appear more like lab tests, lacking practical discussions on real-world applicability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of expression and address grammatical errors throughout the text. Additionally, the authors should provide a more in-depth analysis of experimental results, particularly regarding the selection process for the final test set and the rationale behind using the top-5 relevant texts in the RAG settings. It would be beneficial to discuss the limitations of the assumptions made about attacks and explore the applicability of the approach in real-world scenarios. Furthermore, we suggest that the authors clarify the motivation for using LLMs in detecting poisoned texts and compare their approach with more advanced chain-of-thought techniques. Lastly, ensure that all abbreviations are defined upon first use for better clarity.