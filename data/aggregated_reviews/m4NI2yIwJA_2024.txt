ID: m4NI2yIwJA
Title: Deep Graph Mating
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for learning-free and label-free model reuse in graph neural networks (GNNs), termed Deep Graph Mating (Grama). The authors aim to generate a child model by reusing knowledge from pre-trained parent models without fine-tuning or retraining. They introduce two initial methods, Vanilla Parameter Interpolation (VPI) and Vanilla Alignment Prior to Interpolation (VAPI), which face challenges such as weight misalignment and topology-dependent complexities. To address these, the authors propose the Dual-Message Coordination and Calibration (DuMCC) framework, which includes a Permutation Matrix Coordination (PMC) scheme and a Calibration of Message Statistics (CMC) scheme. The authors clarify that their approach does not use gradient descent for alignment but transforms the minimization problem into a maximization problem. Extensive experiments across various benchmarks validate the effectiveness of the proposed framework, although the authors acknowledge the need for model-specific variants of their equations and plan to define heterogeneous GRAMA more explicitly in the revised version.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized, presenting a clear problem definition and logical flow from motivation to proposed solutions.
- Grama is a novel approach that enables multi-model reuse without requiring annotations or retraining, enhancing resource efficiency.
- The proposed DuMCC framework is well-justified, with solid theoretical underpinnings and extensive discussions on challenges.
- The methodologies, PMC and CMC, streamline the alignment process without requiring iterative training or ground-truth labels.
- Thorough experiments validate the method's effectiveness across multiple tasks and architectures, with good reproducibility through detailed supplementary material.
- The authors have provided detailed responses to reviewer queries, indicating a commitment to improving the manuscript.

Weaknesses:
- The discussion on the learning-free message normalization layer lacks clarity regarding its interaction with existing normalization layers in pre-trained models.
- The clarification on heterogeneous GRAMA remains vague, particularly regarding scenarios with identical architectures trained on different tasks.
- The authors should include more experiments involving three or more pre-trained models to substantiate claims of multi-model reuse.
- The experimental results section does not adequately connect to previous assumptions and propositions, affecting readability.
- Some references in the bibliography contain incorrect publication years, which could undermine the paper's credibility.
- The limitations section is not clearly delineated, potentially leaving readers unaware of the study's constraints.
- Minor formatting issues, such as equations extending beyond the margin, need correction.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the learning-free message normalization layer by clarifying whether batch statistics are recomputed in CMC when pre-trained models contain normalization layers. Additionally, we suggest improving the clarity of the heterogeneous GRAMA definition, explicitly stating that it includes scenarios with diverse architectures or different domain tasks. We also recommend including more experiments with three or more pre-trained models to strengthen claims regarding multi-model reuse. It would enhance the paper's readability if the authors more closely relate the experimental results to the methods section, specifying which results validate which propositions. Furthermore, we advise enhancing the limitations section to make it more prominent and informative. Lastly, we recommend double-checking the reference list for accuracy, correcting any publication year discrepancies to maintain the paper's integrity, and addressing minor formatting issues, such as ensuring equations fit within the margins.