ID: HUzbEPMd6v
Title: SPT: Learning to Selectively Insert Prompts for Better Prompt Tuning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for selecting optimal prompt layers in prompt tuning by employing a learnable probabilistic gate and a bi-level optimization framework. The authors demonstrate that their approach enhances convergence and overall performance through extensive experiments. The study addresses the meaningful question of how to effectively integrate prompts based on the task, contributing to the parameter-efficient tuning literature.

### Strengths and Weaknesses
Strengths:
- The proposed technique for identifying optimal layers for prompt integration is algorithmic rather than empirical, adding depth to the findings.
- The optimization objective is novel, combining key designs from existing works.
- The experiments validate the effectiveness of the proposed method, with a comprehensive ablation study.

Weaknesses:
- The work is perceived as incremental, focusing primarily on simple classification tasks, which may limit its impact given the strong performance of large language models in in-context learning.
- Some sections, particularly 4.3 and 4.4, are confusing and require clarification regarding the use of layers for prompt insertion.
- There are concerns about the relevance of the proposed method in the context of cutting-edge large language models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of sections 4.3 and 4.4 by adding a subsection that illustrates the use of layers with larger \(a_i\) for prompt insertion. Additionally, addressing the concerns regarding the necessity of multiple prompt generators for each layer would enhance the understanding of the training requirements. It would also be beneficial to discuss the implications of the proposed method on the performance without consistency regularization, potentially highlighting trade-offs in computational costs. Lastly, we suggest revising figures to ensure they accurately depict the described processes and addressing any missing citations or typographical errors in the manuscript.