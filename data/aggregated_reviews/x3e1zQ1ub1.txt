ID: x3e1zQ1ub1
Title: In-Context Demonstration Selection with Cross Entropy Difference
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for selecting in-context examples from training data to minimize test example loss, utilizing the cross-entropy difference (CED) between target and base domain models. The authors propose a demonstration selection strategy that employs parameter-efficient fine-tuning techniques, showing effectiveness across various benchmarks, particularly for larger LLMs. The method is theoretically grounded and validated through extensive experiments, demonstrating improved performance over baseline strategies.

### Strengths and Weaknesses
Strengths:
- The proposed method shows strong performance and is well-supported by theoretical foundations.
- Extensive experimental validation across multiple benchmarks and LLMs.
- The paper is well-written and provides a solid contextualization within the existing literature on meta-gradient learning.

Weaknesses:
- The results are mixed, with inconsistent performance improvements noted in some cases (e.g., Table 1).
- Scalability concerns arise due to the limited training set size of 256 examples, raising questions about the method's robustness.
- Some experimental design choices lack justification, such as the restriction to a single demonstration and the combination of datasets from different tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the CED calculation and provide more details on how to choose training data for parameter-efficient fine-tuning (PEFT). Additionally, we suggest exploring the impact of using multiple demonstrations instead of restricting to one, as well as conducting ablation studies to analyze the effects of different PEFT methods and models used for example selection. Addressing the scalability issue by quantifying the cost of training multiple models and evaluating performance with larger training sets would also strengthen the paper. Finally, we encourage the authors to clarify the robustness of their results across different subsets of training examples.