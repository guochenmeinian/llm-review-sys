ID: 8jyCRGXOr5
Title: Efficient Sketches for Training Data Attribution and Studying the Loss Landscape
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 5, 6, 6, -1, -1, -1
Original Confidences: 4, 2, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents efficient sketching algorithms aimed at overcoming memory constraints in large-scale models, specifically addressing the multiplication of dense projection matrices in existing methods like FJL and FastFood. The authors propose three algorithms: AFFD, AFJL, and QK, providing theoretical guarantees for their effectiveness. The applications of these algorithms include training data attribution, intrinsic dimension estimation, and eigenvalue estimation of the Hessian.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant topic relevant to the NeurIPS community, with potential applications across various domains.
- The problem statement is well-motivated, clearly articulating the need for efficient sketching algorithms.
- The results are comprehensive, demonstrating the effectiveness of the proposed algorithms across multiple use cases.
- The appendix contains sufficient detail for reproducing results, despite the absence of accompanying code.

Weaknesses:
- Some notations are inadequately explained, and certain statements lack precision, leading to ambiguity.
- The limitations of the proposed approach are not sufficiently addressed, with only a brief mention in the conclusion.
- A more thorough comparative analysis with existing sketching algorithms, such as those used in TDA, is needed.
- Presentation issues arise from the complexity of discussing multiple algorithms and applications within a limited page count, making it challenging for readers to follow.

### Suggestions for Improvement
We recommend that the authors improve clarity by explicitly defining acronyms like AFFD, AFJL, and QK, and ensuring consistent mathematical notation throughout the paper. Additionally, the authors should specify that their work focuses on gradient-based TDA methods, such as TracIn and GradDot, rather than the broader term "TDA." In the introduction, clarify the implementation of random projections without dense matrices, and address the unclear statement regarding memory constraints and intrinsic dimension. The claim about influence function computations should be clarified, particularly regarding the inverse Hessian-vector product estimation. We suggest including tabular results in the main paper to support claims about scalability limitations and providing a direct explanation of why explicit sketching offers substantial speed-up. Lastly, improving the resolution of Figure 1 and enhancing the exposition of the different methods in Section 3 would greatly benefit the overall presentation.