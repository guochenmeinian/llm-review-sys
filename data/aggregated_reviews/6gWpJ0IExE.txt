ID: 6gWpJ0IExE
Title: When is Agnostic Reinforcement Learning Statistically Tractable?
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 6, 6, 7, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the sample complexity of agnostic reinforcement learning (RL) in finite horizon Markov Decision Processes (MDPs) by introducing a novel complexity measure called *spanning capacity*. The authors aim to determine the number of samples required to output an $\epsilon$-optimal policy within a given policy class $\Pi$. They analyze two interaction settings: in the generative model, spanning capacity characterizes learning complexity up to an $H \log(|\Pi|)$ factor, while in the online setting, it fails to provide polynomial bounds, necessitating the introduction of a "sunflower" policy class for efficient learning. The authors propose the POPLER algorithm, which scales with spanning capacity under specific conditions.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant problem in RL, contributing novel insights into the sample complexity of agnostic learning.
- The introduction of spanning capacity is original and provides a clean characterization of learning complexity in the generative model setting.
- The exposition is clear, with well-organized proofs and concrete examples enhancing understanding.

Weaknesses:
- The spanning capacity is overly pessimistic as it is a worst-case measure, potentially overlooking scenarios where efficient learning is feasible.
- The sunflower structure assumption appears overly strong and lacks necessary motivation, raising questions about its necessity for agnostic RL.
- The computational efficiency of the proposed algorithms is not adequately addressed, and the complexity of Algorithm 1 is high, requiring traversal of all policies in $\Pi$.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the sunflower structure's necessity and provide more motivation for its use in the online setting. Additionally, we suggest discussing the computational efficiency of the proposed algorithms more thoroughly. It would also be beneficial to explore the possibility of establishing a more instance-dependent complexity measure that accounts for the interaction between the policy class and the underlying MDP dynamics. Lastly, we invite the authors to clarify the relationship between their work and existing measures like the DEC, as well as to address the minor errors and typos noted in the reviews.