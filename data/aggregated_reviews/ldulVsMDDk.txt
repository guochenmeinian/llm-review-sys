ID: ldulVsMDDk
Title: Towards a Better Theoretical Understanding of Independent Subnetwork Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 7, 4, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of Independent Subnetwork Training (IST) for distributed Stochastic Gradient Descent (SGD) optimization, focusing on convergence properties with a quadratic loss function. The authors analyze both homogeneous and heterogeneous scenarios, providing insights into conditions for efficient convergence and cases where convergence is limited to an irreducible neighborhood of the optimal solution. The work includes a convergence analysis under communication compression and discusses the implications of bias in the training process.

### Strengths and Weaknesses
Strengths:
- The paper offers a solid analytical treatment of IST, addressing an important problem in distributed optimization with reduced communication overhead.
- Sections 3 and 4 are well-written, presenting theorems and analyses in a clear and intuitive manner.
- The submission is well-structured and provides a thorough examination of convergence guarantees for the quadratic loss function.

Weaknesses:
- The introduction lacks clarity and could better motivate the problem and technical contributions, making it less accessible to readers.
- The analysis is limited to a specific type of loss function, which may not reflect practical scenarios in model parallelization.
- The absence of experimental results weakens the validation of the theoretical analysis, and the authors should consider extending their work to include commonly used loss functions and models.

### Suggestions for Improvement
We recommend that the authors improve the introduction to provide a clearer motivation for the study of IST and its significance. Additionally, including experimental results would strengthen the validation of their theoretical findings. The authors should consider extending their analysis beyond quadratic models to encompass other prevalent loss functions and architectures, such as ResNet and Graph Convolutional Networks, to demonstrate the generality of their insights. Furthermore, clarifying the assumptions made in the permutation example in Section 3.1 would enhance reader understanding.