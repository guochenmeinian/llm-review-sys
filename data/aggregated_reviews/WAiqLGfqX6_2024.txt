ID: WAiqLGfqX6
Title: Derivative-enhanced Deep Operator Network
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a derivative-enhanced DeepONet that incorporates derivative information to improve predictive accuracy for PDE problems. The authors propose a method that integrates spatial derivative information alongside functional derivatives, differing from previous encoder-decoder architectures. They provide numerical comparisons with other models, highlighting results from dimensionality reduction techniques.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and structured, effectively conveying the methodology and its components.
- The appendix is comprehensive, enhancing understanding and reproducibility, with compelling visualizations for low data scenarios.
- The focus on the improvement of dm prediction for control is significant, addressing a critical aspect often overlooked in neural operators.

Weaknesses:
- The cost of generating dm and dx labels is significantly higher than solving the PDE, raising questions about the tradeoff between data generation and online prediction benefits.
- The findings regarding the minimal benefit of dx information compared to dm training should be emphasized in the main text, as they are crucial for understanding the method's novelty and impact.
- The empirical evaluation does not convincingly demonstrate the proposed method's effectiveness compared to existing methods like DINO, with marginal gains noted.
- The paper lacks a thorough investigation of the proposed method's performance across various scenarios, particularly where it may struggle.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their findings by explicitly discussing the limited benefits of dx regularization in the main text. Additionally, addressing the novelty of their approach compared to DINO is essential. We suggest including total wall-clock time in Figure 2 to facilitate comparisons with data costs. The authors should also clarify the convergence of models trained for only 1,000 epochs and provide convergence plots. Furthermore, a discussion on the computational efficiency of the proposed method in various scenarios would enhance the manuscript's robustness. Lastly, simplifying complex sections for accessibility to a broader audience would improve readability.