ID: JhQP33aMx2
Title: Module-wise Adaptive Distillation for Multimodality Foundation Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 6, 5, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called OPTIMA for reducing the size of pre-trained multimodal models through layerwise distillation. The authors propose a dynamic selection of modules for knowledge distillation, formulated as a multi-armed bandit problem, to optimize performance. Experimental results indicate that OPTIMA outperforms traditional layer-wise distillation methods across various multimodal tasks.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a significant issue in distilling large-scale multimodal foundation models and introduces a novel approach, Module-wise Adaptive Distillation, which balances the distillation of different model components.  
- The proposed method demonstrates superior performance compared to baseline methods, supported by thorough experimental details and ablation studies.  
- The writing is clear and the methodology is well-motivated, highlighting the varying contributions of different modules during the distillation process.

Weaknesses:  
- The application of OPTIMA is limited to CoCa models, raising questions about its generalizability to other multimodal architectures.  
- The results presented in Table 1 show that the DistilVLM12Ã—384 baseline achieves higher scores with fewer parameters, which casts doubt on the significance of the proposed method.  
- The choice to report the median among five repetitions in results is unconventional; a comparison using mean and standard deviation would be more informative.  
- There are concerns regarding the training costs of the proposed method compared to other baselines, including training time and GPU memory usage.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of OPTIMA, particularly regarding its application beyond CoCa models. It would be beneficial to include results demonstrating the method's effectiveness on additional multimodal architectures, such as BLIP2. Additionally, we suggest providing a more comprehensive analysis of training costs associated with OPTIMA compared to baseline methods. Finally, we encourage the authors to clarify the rationale behind their choice of statistical reporting in Table 1, potentially including mean and standard deviation for better clarity.