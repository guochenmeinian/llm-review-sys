ID: VKbEO2eh5w
Title: Test-Time Distribution Normalization for Contrastively Learned Visual-language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 6, 6, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a solution to the misalignment between the pre-training objective of contrastively trained vision-language models and their downstream usage through the introduction of Distribution Normalization (DN). The authors analyze the InfoNCE loss and demonstrate that the conventional dot product used for zero-shot inference is merely a zero-order approximation. By utilizing unlabeled test data for a first-order approximation, the proposed method shows effectiveness across various downstream tasks.

### Strengths and Weaknesses
Strengths:
1. The identification of the misalignment between pre-training and downstream tasks is significant.
2. The proposed DN method is simple, effective, and practical to implement.
3. The experiments conducted across multiple tasks validate the effectiveness of the method, supported by sufficient ablation studies.

Weaknesses:
1. The absence of recent work, specifically TPT, which focuses on learning prompts for vision-language models at test-time, limits the comparison scope. The authors should compare DN with TPT and consider integrating DN into TPTâ€™s framework.
2. The terminology "visual-language" should be corrected to "vision-language" in the next version.
3. DN's reliance on a small amount of unlabeled data means it does not qualify as a zero-shot method, necessitating a refinement of related descriptions, particularly in Line 175.
4. Empirical gains, especially on CLIP models, are nominal, and the authors should clarify the discrepancies in performance across different models and datasets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the section around Eqn 8 (L130 to L133) to enhance understanding. Additionally, the authors should conduct comparisons with existing test-time augmentation methods and consider testing DN on larger and more standard CLIP architectures like ViT-B-16 and ViT-L. It would also be beneficial to provide finetuning experiments on image classification datasets such as ImageNet or Stanford Cars/Flowers. Furthermore, the authors should ensure that the supplementary material is submitted as a separate PDF, as per standard practices.