ID: EVw8Jh5Et9
Title: Dual Defense: Enhancing Privacy and Mitigating Poisoning Attacks in Federated Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 5, 6, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Dual Defense Federated Learning (DDFed), which integrates Fully Homomorphic Encryption (FHE) and a feedback-driven collaborative selection method to enhance privacy protection and mitigate poisoning attacks in federated learning (FL). The authors propose a threat model assuming an honest-but-curious server and emphasize that collusion between clients and the server is outside their scope. They argue that their similarity-based detection methods effectively counter various model poisoning attacks and can accommodate compromised clients from the first training round. The framework aims to securely aggregate model updates without compromising client privacy and addresses Byzantine attacks through a voting mechanism to filter out malicious clients. The authors demonstrate DDFed's effectiveness using standard datasets like MNIST and FMNIST, while also discussing the computational overhead of using homomorphic encryption (HE) and its implications for privacy enhancement in FL. Additionally, the authors acknowledge a mistake in their reported PSNR values and clarify the nature of comparison solutions, planning to support their findings with detailed experimental results in the final version.

### Strengths and Weaknesses
Strengths:
- The paper offers an innovative approach that simultaneously tackles privacy and Byzantine robustness issues in FL.
- The authors provide a clear threat model and articulate the limitations of existing approaches regarding client-server collusion.
- DDFed employs a well-designed collaborative selection method that minimizes the impact of differential privacy noise on aggregated weights.
- The similarity-based detection methodology is shown to yield strong defense outcomes against model poisoning attacks.
- Experimental results on various datasets illustrate the framework's effectiveness, and the authors demonstrate responsiveness to reviewer comments by acknowledging errors and outlining plans for improvement.

Weaknesses:
- The method's practicality is questionable, particularly regarding the higher training costs and communication overhead associated with FHE.
- The assumption of no client-server collusion raises concerns about real-world applicability, especially with the potential for malicious clients.
- DDFed's detection mechanism only considers the last layer, making it vulnerable to attacks targeting other hidden layers.
- The aggregation protocol lacks a formal convergence guarantee, which is critical for validating the robustness of the proposed solution.
- The authors do not actively ensure that clients' local weights are normalized, which could disadvantage the defense mechanism against adversarial attacks.
- The paper currently lacks detailed theoretical proofs and experimental data, which are essential for validating their claims.

### Suggestions for Improvement
We recommend that the authors improve the practicality of DDFed by providing a thorough analysis of the communication overhead and training costs associated with FHE. Additionally, we suggest that the authors address the potential for client-server collusion, possibly by incorporating a threshold analysis for collusion scenarios. It is crucial to extend their experiments to include more complex datasets, such as CIFAR-10, and larger models like ResNet. The authors should also address the limitations of only considering the last layer in the detection process and provide a formal convergence guarantee for their aggregation method. Furthermore, we encourage the authors to clarify their stance on the normalization of clients' local weights, as this could significantly impact the effectiveness of their similarity-based detection methods. Lastly, we advise that the authors conduct further evaluations of the differential privacy performance using inversion attacks to strengthen their claims regarding privacy guarantees and ensure that all experimental results and references discussed during the rebuttal are included in the next version of the main text.