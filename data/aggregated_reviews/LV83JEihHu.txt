ID: LV83JEihHu
Title: Exploring Question Decomposition for Zero-Shot VQA
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 6, 5, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on question decomposition in visual question answering (VQA), demonstrating that large vision-language models (VLMs) equipped with large language models (LLMs) can leverage question decomposition to enhance answering performance. The authors propose a selective decomposition method to determine which questions necessitate decomposition, supported by comprehensive experiments validating the effectiveness of instruction-tuned language models in auto-generating decompositions.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, making it easy to follow.
- The problem addressed is novel and contributes to understanding in-context learning capabilities in large VLMs with instruction-tuned LLMs.
- Comprehensive experiments validate the effectiveness of question decomposition across multiple datasets.

Weaknesses:
- Concerns arise regarding the effectiveness of decompositions, as indicated by low $E_{CR}$ values and high $E_{IC}$ values, suggesting that many decompositions may hinder model performance.
- The limited scope of experiments on only FlanT5 and BLIP2 raises questions about the generalizability of the proposed methods; further testing on diverse zero-shot VQA methods is recommended.
- The paper lacks a thorough analysis of straightforward VQA tasks, which could provide insights into the generalization ability of the proposed methods.
- The novelty of self-generated question-answer pairs is questioned; a comparison with prior methods would enhance understanding.
- Some technical aspects, such as the scrambling method and the number of perturbed words, are inadequately explained.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 4 by addressing the effectiveness of decompositions, particularly in light of the low $E_{CR}$ values. Additionally, we suggest conducting experiments with a broader range of zero-shot VQA models and LLMs, such as Flamingo, Frozen, and the GPT series, to enhance the study's breadth. It would be beneficial to analyze straightforward VQA tasks like VQAv2 or GQA to demonstrate the generalization ability of the proposed methods. Furthermore, we encourage the authors to provide a dedicated section comparing their approach with earlier methods like VQ^2A and Weak VQA to clarify its novelty. Lastly, we recommend a more detailed explanation of the scrambling method and its implications for performance, as well as addressing the various logical gaps and proofreading issues noted in the reviews.