ID: VQyb9LKmUH
Title: A Prompt-Based Knowledge Graph Foundation Model for Universal In-Context Reasoning
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents KG-ICL, a model designed for generalized reasoning over knowledge graphs through in-context learning. KG-ICL extracts relevant example facts from the knowledge graph to create prompt graphs, which are then encoded using a unified tokenizer and message passing neural network to generate relation representations. The model integrates these embeddings with the knowledge graph to score candidate entities. Extensive experiments across 43 knowledge graphs validate the effectiveness of KG-ICL, demonstrating its superiority over baseline models.

### Strengths and Weaknesses
Strengths:  
1. The paper is well-structured and clearly presents its concepts.  
2. Extensive experimental results show that KG-ICL consistently outperforms baseline models.  
3. The provision of code enhances reproducibility.  
4. The approach to obtaining relational representations through few-shot subgraph examples is innovative and timely.  

Weaknesses:  
1. The motivation for the in-context learning setting over knowledge graphs is unclear and appears similar to inductive learning. Results in Figure 3 suggest that the number of examples does not significantly impact performance, contradicting the ICL hypothesis.  
2. The impact of the parameters $M$ and $k$ on model performance is not adequately explained.  
3. The necessity of the unified tokenizer is questionable, as ablation studies indicate only marginal performance decline when excluded.  
4. The different pre-training dataset mixture complicates direct performance comparisons with Ultra, which utilized a more uniform dataset.  
5. Missing experiments on the performance as a function of training graphs in the pre-training mixture limit the analysis of zero-shot inference performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation for the ICL setting and its distinction from inductive learning. Additionally, the authors should provide a more thorough explanation of how KG-ICL generates the prompt graph and clarify the differences between the w/o unified tokenizer and w/o token representation conditions. It would also be beneficial to include a performance comparison of high and low resource relations or entities. Finally, conducting experiments to measure performance as a function of training graphs in the pre-training mixture would enhance the robustness of the findings.