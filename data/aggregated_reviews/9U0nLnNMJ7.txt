ID: 9U0nLnNMJ7
Title: Compact Language Models via Pruning and Knowledge Distillation
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper empirically explores compressing language models through pruning and knowledge distillation, summarizing best practices supported by extensive experiments. The authors investigate various pruning approaches and retraining methods to derive compact models from larger counterparts, providing valuable insights into model compression.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written, with clear explanations of modeling choices, making it accessible for practical applications.
2. It presents structured best practices that offer key insights into effective model compression.
3. Extensive experiments are conducted, demonstrating the pruned model's performance surpassing that of models trained from scratch.

Weaknesses:
1. The novelty of the approach is limited, as it appears to be a straightforward combination of existing techniques. The authors should clarify how their final choices differ from previous work.
2. Figures and tables lack detailed captions, which diminishes their clarity.
3. While benchmark results are valuable, a qualitative study comparing model generations through human evaluation would enhance the analysis.

### Suggestions for Improvement
We recommend that the authors improve the novelty aspect by clearly highlighting the differences between their approach and previous methods. Additionally, we suggest enhancing the captions of figures and tables for better clarity. Incorporating a qualitative study with human evaluations of model outputs would provide a more comprehensive understanding of the models' performance. Lastly, consider including additional experiments, such as retraining the nemotron-8B model on the retraining dataset and exploring further vanilla training or knowledge distillation with nemotron-15B as the teacher.