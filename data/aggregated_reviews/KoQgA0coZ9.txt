ID: KoQgA0coZ9
Title: Distributed Personalized Empirical Risk Minimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 5, 5, 7, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 1, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for personalized empirical risk minimization in federated learning, addressing the challenge of data heterogeneity (non-IID). The authors propose the Personalized Empirical Risk Minimization (PERM) schema, which utilizes optimization relaxations and data shuffling to achieve near-optimal solutions for the personalized federated learning problem. The paper includes a rigorous theoretical analysis and demonstrates the effectiveness of the proposed method through experiments on synthetic data.

### Strengths and Weaknesses
Strengths:  
- The PERM schema is a novel and practical approach to data heterogeneity in distributed learning.  
- The theoretical analysis is insightful, providing rigorous convergence guarantees for the proposed algorithm.  
- The paper is well-organized and clearly written, making complex concepts accessible.  
- Experimental results on synthetic data show significant improvements over existing methods.

Weaknesses:  
- The evaluation lacks comprehensive testing on real-world data, limiting practical applicability.  
- The reliance on strong concavity and smoothness of the loss function poses limitations, particularly for non-convex models.  
- The scalability of the proposed method is questionable, with potential computational bottlenecks as the number of devices increases.  
- The mixing operations are restricted to weighted summation, which may limit the method's advantages.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including experiments on real datasets and diverse neural network architectures to validate the practical usefulness of the proposed method. Additionally, clarifying the implementation details, including specific hyperparameters used in experiments, would enhance reproducibility. Addressing the scalability concerns by providing runtime comparisons with existing methods and discussing potential computational bottlenecks would also strengthen the paper. Finally, we suggest revising the notation for consistency and clarity, particularly regarding the definitions of parameters used in the algorithm.