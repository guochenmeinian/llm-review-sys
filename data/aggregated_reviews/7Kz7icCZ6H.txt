ID: 7Kz7icCZ6H
Title: CALVIN: Improved Contextual Video Captioning via Instruction Tuning
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CALVIN, a specialized video LLM designed for contextual movie scene captioning, emphasizing the emotional narrative rather than mere visual details. The authors train CALVIN on a combination of movie-audio datasets and other relevant datasets, demonstrating its ability to generate improved contextual descriptions by leveraging previous frames. The model shows significant performance gains over existing methods in audio description tasks.

### Strengths and Weaknesses
Strengths:
- The core contribution is a large model specifically for movie scene captioning, which is notably distinct from typical video captioning tasks.
- The paper reports substantial improvements over existing work and includes extensive experiments and ablation studies, enhancing understanding of CALVIN's capabilities.
- The model's tuning strategy effectively utilizes video context, leading to better performance on the MAD-eval and TV-caption datasets.

Weaknesses:
- The technical novelty of CALVIN is questioned, as it appears to closely resemble existing models like VideoLLaMA, with insufficient differentiation discussed.
- Evaluation results may be biased due to varying training data and pre-trained models, necessitating clearer alignment or acknowledgment of data types used in comparisons.
- The paper lacks a detailed discussion of CALVIN's limitations and does not address the localization of captions, which is crucial for audio description tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how CALVIN differentiates from prior models, particularly VideoLLaMA, by providing a thorough discussion of their unique contributions. Additionally, aligning the training data with that of compared methods or at least highlighting the differences in data usage would strengthen the evaluation. We also suggest including an ablation study on the CMD dataset to assess performance differences and addressing the temporal localization of captions more explicitly in the methodology. Lastly, expanding the limitations section to detail the model's constraints would enhance the paper's comprehensiveness.