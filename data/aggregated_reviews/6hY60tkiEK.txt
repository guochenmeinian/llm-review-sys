ID: 6hY60tkiEK
Title: Sparse High Rank Adapters
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new parameter-efficient fine-tuning (PEFT) method, Sparse High Rank Adapter (SHiRA), which fine-tunes only 1-2% of pretrained model weights. The authors demonstrate that SHiRA allows for rapid adapter switching and multi-adapter fusion with lower concept loss compared to LoRA, particularly in larger models. Empirical results indicate that SHiRA achieves lower latency during inference for model embedding dimensions greater than 1024. Additionally, SHiRA significantly outperforms LoRA across various large-scale tasks in both vision and language domains, even when using basic pruning metrics. The authors highlight that SHiRA-Struct is the most effective masking technique for LVM style transfer due to its orthogonality properties, while SHiRA-SNIP excels in LLM commonsense reasoning tasks. They also acknowledge the effectiveness of SHiRA-Random as a strong baseline, which they plan to include in the main paper in future revisions.

### Strengths and Weaknesses
Strengths:
- The work is timely, addressing the need for efficient adapter methods suitable for memory-constrained devices.
- SHiRA demonstrates superior performance compared to LoRA across multiple tasks and models.
- The paper is well-structured, featuring clear figures, tables, and discussions.
- The inclusion of a new “Discussion” section enhances clarity regarding the contributions of the paper.
- The authors show responsiveness to reviewer feedback, indicating a commitment to improving the work.

Weaknesses:
- The novelty of SHiRA is modest, as it closely resembles prior works like Diff Pruning and SFT, which also focus on fine-tuning a small set of parameters. A comparison with SFT would clarify the benefits of SHiRA's static mask selection.
- The focus on latency reduction through the scatter operation lacks comprehensive end-to-end profiling, particularly for smaller embedding dimensions.
- SHiRA appears to require full gradient buffers for pretrained weights, which may lead to higher memory overhead compared to LoRA. A reparameterization suggestion could help mitigate this issue.
- The pruning criteria for mask initialization could be expanded to include modern techniques like Movement Pruning and Wanda, and further analysis on the performance of the SHiRA-Rand mask is warranted.
- The distribution of fine-tuned parameters across layers was not discussed, and exploring allocation strategies could enhance understanding of SHiRA's effectiveness.
- The paper lacks a clear insight into the significance of the random mask, which could enhance its contribution.
- Space limitations during the initial submission prevented the inclusion of important results related to SHiRA-Random.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related works, particularly by including a comparison with SFT to highlight the differences in performance and generalization. Additionally, providing end-to-end latency profiles for both SHiRA and LoRA during online and batched inference would strengthen the paper. Acknowledging the memory overhead differences between SHiRA and LoRA during training and suggesting methods to reduce this overhead would also be beneficial. Expanding the analysis of pruning criteria and discussing the distribution of fine-tuning parameters across layers could further enhance the paper's contributions. We also recommend improving the clarity of the insights regarding the random mask's effectiveness, as this could significantly enhance the paper's impact. Ensuring that the results for SHiRA-Random are included in the main paper will provide a comprehensive view of its performance. Finally, we encourage the authors to further elaborate on the implications of their findings in the abstract, introduction, and conclusion sections.