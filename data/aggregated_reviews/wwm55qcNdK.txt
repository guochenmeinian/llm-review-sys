ID: wwm55qcNdK
Title: SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two main contributions: the introduction of SoulChatCorpus, a new data resource containing 2.3 million samples of multi-turn empathy conversations in Chinese, and the development of SoulChat, a conversational system fine-tuned using this corpus. The authors demonstrate the advantages of fine-tuning with a large dataset through an NLP engineering experiment. Additionally, the paper provides an overview of existing empathy-related conversation datasets.

### Strengths and Weaknesses
Strengths:
- The paper effectively motivates the necessity of SoulChatCorpus by reviewing existing datasets in the field.
- The approach of using ChatGPT to convert single-turn to multi-turn dialogues is innovative.
- An evaluation study with three baseline models is conducted, and the dataset will be published if accepted.

Weaknesses:
- The reliance on the Appendix makes it challenging to fully understand the paper without additional context.
- The study lacks reproducibility due to insufficient details on the evaluation procedure, including the ratio of training to testing examples and the number of generated dialogues.
- The model architecture is considered basic, as it primarily involves fine-tuning an existing model.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by minimizing references to the Appendix and providing essential information within the main text. Additionally, the authors should enhance the reproducibility of the study by detailing the evaluation procedure, including the number of dialogues generated and the training/testing split. It would also be beneficial to include additional evaluation metrics beyond Rouge and BLEU scores, such as METEOR and BERTScore. Lastly, the authors should clarify the quality of the dataset and the extent of human involvement in creating the questions and answers.