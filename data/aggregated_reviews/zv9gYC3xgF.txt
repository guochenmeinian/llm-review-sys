ID: zv9gYC3xgF
Title: Toward Global Convergence of Gradient EM for Over-Paramterized Gaussian Mixture Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the convergence of the gradient EM algorithm for learning mixtures of Gaussians (GMMs) in an overparameterized setting. The authors analyze a scenario where the ground truth is a single Gaussian, demonstrating that gradient EM converges to this distribution at a rate of \(1/\sqrt{t}\), with constants that depend exponentially on the distance between the initialized means and the true mean. The work extends previous results by Dwivedi et al. (2018) to mixtures with more than two components and includes empirical validation on simple synthetic datasets.

### Strengths and Weaknesses
Strengths:  
- The paper advances the understanding of convergence for EM in GMMs, providing the first global convergence results for mixtures with more than two components.  
- It successfully navigates significant technical challenges to generalize prior findings.  
- The writing is clear, with well-structured theorems and explanations.

Weaknesses:  
- The results are limited to the case where the ground truth is a single Gaussian, which may not reflect more complex scenarios.  
- The contributions are primarily technical, lacking a strong conceptual takeaway, as similar results for two-component mixtures are already established.  
- The experimental evaluation is limited and could benefit from a broader range of datasets and comparisons with existing algorithms.

### Suggestions for Improvement
We recommend that the authors improve the generality of their results by exploring scenarios where the ground truth is not merely a single Gaussian. Additionally, we suggest enhancing the experimental section by including diverse datasets and benchmarking against existing algorithms to provide a more comprehensive evaluation of their approach. Furthermore, addressing the clarity around the convergence of KL divergence and MLE consistency would strengthen the paper's rigor. Lastly, we encourage the authors to discuss the implications of their findings in relation to k-component GMMs and consider the potential for extending their analysis to standard EM algorithms.