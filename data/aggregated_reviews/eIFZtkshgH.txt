ID: eIFZtkshgH
Title: AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 5, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on model pre-training for autonomous driving using a large-scale point cloud dataset, enhancing generalization performance through scene- and instance-level distribution diversity. The authors propose a semi-supervised pre-training approach, generating pseudo-labels for unlabeled data, and validate its effectiveness across several benchmark datasets, including KITTI, nuScenes, and Waymo.

### Strengths and Weaknesses
Strengths:
1. The AD-PT pre-training setting is practical, aiming to create a generalized model applicable to various downstream tasks.
2. The semi-supervised strategy employed is somewhat novel compared to fully unsupervised methods.
3. The data diversity enhancement through re-sampling strategies and the unknown-aware instance learning head are reasonable and well-justified.

Weaknesses:
1. The performance improvements on downstream tasks, while notable with limited training data, could be further illustrated with more extensive datasets (e.g., 100% data on Waymo).
2. The claim of providing a general pre-trained backbone is overstated, as the work primarily focuses on a voxel-based pre-trained backbone amidst a variety of detection pipelines and input representations in the field.
3. The improvements in performance are marginal, raising questions about the necessity of the pre-training given the potential for comparable results through extended training schedules.
4. The novelty is limited, as similar pseudo-labeling techniques for LiDAR-based 3D detection have been previously explored.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding the generalizability of the pre-trained backbone by addressing the diversity of detection pipelines and input representations in 3D object detection. Additionally, the authors should provide insights into the implications of using different voxel sizes for pre-training and downstream tasks. It would also be beneficial to include a limitation section discussing potential memory consumption or time efficiency. Finally, we suggest conducting comparisons with current state-of-the-art methods to further validate the generalization ability and robustness of the proposed approach.