ID: 792txRlKit
Title: DataStealing: Steal Data from Diffusion Models in Federated Learning with Multiple Trojans
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 6, 7, 6, 6, -1
Original Confidences: 5, 3, 4, 3, -1

Aggregated Review:
### Key Points
This paper presents a pioneering privacy threat in Federated Learning (FL) associated with diffusion models, termed DataStealing. The authors demonstrate that attackers can exploit Combinatorial Triggers (ComboTs) to extract private data from clients, even under strict data management protocols. They introduce the Adaptive Scale Critical Parameters (AdaSCP) attack, which effectively bypasses distance-based defenses by adaptively scaling critical parameter updates. Extensive experiments validate that AdaSCP can successfully exfiltrate thousands of images, highlighting significant privacy concerns within the FL community. Additionally, the paper studies the privacy risks of diffusion models in FL through data stealing attacks, showcasing the effectiveness of the proposed methods.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a new benchmark (DataStealing) that reveals security vulnerabilities in federated diffusion models, demonstrating how ComboTs can leak substantial amounts of data.
2. AdaSCP is the first method to optimize scaling of critical parameters based on theoretical targets, supported by a simple proof for calculating optimal scaling values.
3. The work provides critical insights into privacy and security issues in federated diffusion models, emphasizing the need for enhanced protective measures.
4. The proposed methods exhibit good performance with low MSE of the recovered images.
5. The paper is well-written, with clear motivation, a coherent technical approach, and extensive experimental support.

Weaknesses:
1. The explanation of why Model Poisoning cannot bypass defenses is limited to Section 3.3; more quantitative analysis and a figure could enhance understanding.
2. The attack settings are problematic, as the assumption that the attacker can access the training dataset of the poisoned client undermines the rationale for using complex methods to steal images.
3. There is insufficient experimentation and explanation regarding the evaluation metrics, particularly the FID metric, which is not clearly defined in terms of whether higher or lower values are preferable.
4. The absence of error bars in experiments is noted; conducting repeat experiments with different seeds for AdaSCP is recommended to improve academic rigor.
5. The experiments are limited to two low-resolution image datasets, lacking evaluation on higher-resolution datasets.
6. Recent studies should be discussed in the related work section to provide context.
7. Minor typographical errors are present, such as inconsistent terminology in Algorithm 2.

### Suggestions for Improvement
We recommend that the authors improve the discussion on potential defense mechanisms against the identified vulnerabilities to enhance the paper's impact. Additionally, a more realistic proportion of malicious clients in experiments should be considered. We suggest that the authors clarify the rationale behind the attack settings, particularly addressing the assumption of access to target images. Expanding the evaluation to include diverse and higher-resolution datasets, such as a subset of ImageNet, would strengthen the empirical foundation. We also encourage the authors to elaborate on the FID metric in their evaluation and clarify its implications. Lastly, simplifying technical explanations and improving the overall flow and readability of the paper would make it more accessible to a broader audience.