ID: 1bTG4sJ7tN
Title: A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extension of proximal policy optimization (PPO) methods in linear mixture Markov decision processes (MDPs) and introduces an optimistic variant PPO algorithm (OPPO+) for both stochastic and adversarial linear MDPs with full information. The authors propose a multi-batched updating rule from bandit literature, updating the policy at regular intervals rather than every episode. The algorithm achieves a regret guarantee of $\tilde{O}(d^{3/4} H^2 K^{3/4})$ and a sample complexity of $\tilde{O}(d^3H^8/\epsilon^4 + d^5H^4/\epsilon^2)$, which is tighter than existing PPO-based algorithms for stochastic linear MDPs.

### Strengths and Weaknesses
Strengths:
- The proposed algorithm is applicable to both stochastic and adversarial linear MDPs with full-information feedback.
- It employs a novel covering theory to reduce the regret dependency on the action space size to a logarithmic scale.
- It provides a tighter regret guarantee compared to existing policy optimization algorithms in both settings.

Weaknesses:
- The computational cost of policy improvement is not discussed in detail.
- The advantages of the proposed policy optimization algorithms over value-based algorithms remain unclear, aside from theoretical aspects.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational cost of policy improvement in OPPO+ compared to value-based algorithms. Additionally, we suggest clarifying the specific advantages of OPPO+ over value-based algorithms, particularly beyond the ability to learn stochastic policies. Furthermore, addressing the similarities between OPPO+ and existing methods like Natural Policy Gradient (NPG) would enhance the understanding of the novelty and contribution of the paper. Lastly, we encourage the authors to explore the balance between batch size and the number of batches, as well as to consider including empirical studies to demonstrate the algorithm's effectiveness in practice.