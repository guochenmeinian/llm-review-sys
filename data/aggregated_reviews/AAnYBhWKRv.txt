ID: AAnYBhWKRv
Title: FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for monolingual token initialization from multilingual language models, specifically through the FOCUS approach, which averages existing learned embeddings to enhance representation for specific languages. The authors conduct experiments on downstream tasks such as NLI, QA, and NER, demonstrating that this method can effectively initialize the embedding matrix for a new tokenizer without requiring bilingual dictionaries or alignment of embedding spaces.

### Strengths and Weaknesses
Strengths:
- The method is simple, clearly described, and effectively reduces model size while improving performance on downstream tasks.
- The paper provides a thorough analysis of the method's strengths and compares it with strong baselines, showing significant improvements in training and inference speed.

Weaknesses:
- The performance improvements are often marginal and not thoroughly discussed, raising questions about the empirical utility of the approach.
- The analysis is limited to one pre-trained language model (XLM-R), which may not generalize to larger models or different contexts.
- Some figures and terminology lack clarity, and there are concerns regarding the validity of comparing loss across different tokenizers.

### Suggestions for Improvement
We recommend that the authors improve the discussion around performance metrics, particularly addressing the similarities with simpler baselines and providing insights into potential reasons for these results. A deeper examination of error patterns through manual evaluation could enhance understanding. Additionally, clarifying the details in Figure 1 and providing a Formal Definitions section would aid comprehension. We also suggest elaborating on the methodology for comparing loss across different tokenizers and addressing the handling of words not found in fastText vocabulary. Finally, consider reducing the number of footnotes and standardizing the mathematical font used in Section 2.