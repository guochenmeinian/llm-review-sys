ID: LdZ0u1FuXb
Title: A Kernel Perspective on Distillation-based Collaborative Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 6, 6, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 1, 1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of a distillation-based collaborative learning algorithm called FedMD from a nonparametric perspective, proposing DCL-KR to achieve a privacy-preserving nearly minimax optimal collaborative learning algorithm with kernel regression in massively distributed statistically heterogeneous environments. The authors also introduce DCL-NN, a neural network variant that extends their approach. Theoretical results demonstrate the nearly minimax optimality of DCL-KR, while extensive experiments validate the efficacy of both DCL-KR and DCL-NN.

### Strengths and Weaknesses
Strengths:
- The authors prove the (nearly) minimax optimality of a privacy-preserving collaborative learning algorithm using kernel regression.
- The paper includes comprehensive experiments across six datasets, demonstrating significant improvements of DCL-NN over existing methods.

Weaknesses:
- The paper's organization and structure are unclear, resembling a combination of different papers without coherent flow.
- Important experimental results are relegated to the appendix, lacking integration into the main body for better insight.
- The writing is not concise, and many assumptions are presented without adequate intuition or explanation.
- There is no formal privacy analysis, making claims of "privacy-preserving" potentially misleading.

### Suggestions for Improvement
We recommend that the authors improve the paper's organization to enhance clarity and coherence. It would be beneficial to summarize key experimental findings in the main body rather than the appendix. Additionally, we suggest providing intuitive explanations for the assumptions listed in Section 3.3 to aid reader understanding. The authors should also consider conducting a formal privacy analysis and avoid using terms like "privacy-preserving" without substantiation. Finally, exploring the computational burden associated with Gram matrix calculations and the implications of kernel choice on performance would strengthen the paper.