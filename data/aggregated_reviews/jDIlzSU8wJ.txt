ID: jDIlzSU8wJ
Title: The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 7, 8, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for predicting optical flow from a pair of images and depth from a single image using diffusion models. It introduces a training pipeline that effectively addresses noisy training data. The proposed method achieves state-of-the-art (SOTA) performance in depth prediction on NYU and KITTI, and in optical flow prediction on Sintel and KITTI zero-shot, with SOTA results after fine-tuning on KITTI. Additional experiments demonstrate that the pretraining pipeline significantly enhances the performance of RAFT and the proposed model, and that the method can generate multimodal outputs in uncertain scenarios.

### Strengths and Weaknesses
Strengths:
- The paper introduces a straightforward method that competes well with SOTA in optical flow and depth tasks.
- A novel data pipeline is presented that significantly enhances the performance of prior SOTA methods like RAFT.
- The method builds on a standard image-to-image diffusion model, requiring minimal modifications to handle flow and depth data.
- The approach effectively manages real data challenges, utilizing its own predictions during late training stages.
- The writing is clear, with well-structured figures and tables that support the findings.

Weaknesses:
- Fine-tuning performance on real data is weaker on Sintel compared to other methods, suggesting a need for further investigation.
- The method's inability to utilize warm-start techniques, common in other methods, may limit its performance on Sintel.
- The primary novel contribution, step-unrolled denoising diffusion training, shows only modest improvements over infilling alone.
- The distinction between pre-training and fine-tuning phases is unclear, as both use similar loss functions and datasets.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the method's performance on Sintel, particularly regarding the potential for warm-start techniques in diffusion models. Additionally, we suggest providing a theoretical analysis of the step-unrolling process and exploring whether combining pre-training and fine-tuning phases could enhance efficiency. Clarifying the contribution of the PALETTE pre-training in the ablation study would also strengthen the paper. Finally, addressing the sensitivity of the proposed architecture to varying image resolutions would be beneficial.