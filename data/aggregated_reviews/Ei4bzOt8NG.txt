ID: Ei4bzOt8NG
Title: Attention Bias as an Inductive Bias: How to Teach Transformers Simple Arithmetic
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 7, 5
Original Confidences: 3, 4, 4

Aggregated Review:
### Key Points
This paper presents adaptations to transformer attention, specifically the use of a mask to limit attention range and the replacement of absolute positional embeddings with cyclic ones (position mod T). The authors suggest that these modifications enable length generalization and facilitate learning of the parity task, which has been deemed challenging for transformers. Despite the interesting results, the paper is criticized for its readability and clarity, particularly regarding the methods and results.

### Strengths and Weaknesses
Strengths:  
- The paper addresses the length generalization problem and proposes novel adaptations to transformer attention.  
- It presents surprising results indicating that attention masks can help models learn parity, challenging previous assumptions about the task's suitability for language models.  

Weaknesses:  
- A significant amount of necessary exposition is relegated to the Appendix, complicating readability.  
- The paper contains numerous acronyms, which may hinder understanding.  
- The methods and results are not clearly highlighted, and the explanation of attention biases, particularly attention bias calibration, is unclear.  
- Existing methods for positional embeddings appear more robust than the proposed approaches, raising questions about the paper's contributions.

### Suggestions for Improvement
We recommend that the authors improve the paper's readability by integrating essential exposition into the main text rather than the Appendix. Clarifying the meaning of terms such as "PE" and reducing the number of acronyms would enhance comprehension. Additionally, we suggest providing quantitative/tabular results to complement the qualitative explanations. The introduction could be shortened to allocate more space for detailing the adaptations and experimental results. Finally, we advise considering the relocation of results on ABC to the appendix or a separate submission to streamline the current paper.