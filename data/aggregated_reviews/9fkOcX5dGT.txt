ID: 9fkOcX5dGT
Title: LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty
Conference: ACM
Year: 2023
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LinkNER, a framework that integrates local named entity recognition (NER) models with large language models (LLMs) to enhance performance in recognizing unseen entities. The authors address the limitations of local models, which struggle with unseen entities due to a lack of knowledge, while LLMs, despite their extensive knowledge, lack specificity for NER tasks. The proposed uncertainty-based linking strategy allows for effective collaboration between these models. The experiments conducted on various datasets demonstrate significant improvements in NER performance, particularly in challenging scenarios involving out-of-vocabulary (OOV) and out-of-distribution (OOD) entities.

### Strengths and Weaknesses
Strengths:
- High-quality research design and methodology, with well-structured experiments that support the claims.
- Clear organization and accessible language enhance reader comprehension.
- Innovative integration of fine-tuned models with LLMs, addressing a notable gap in the literature.
- Substantial improvements in NER performance, especially in OOV/OOD scenarios.

Weaknesses:
- Some experimental settings lack clarity, and the efficiency of linking local models to LLMs requires further exploration.
- The uncertainty threshold appears dataset-dependent, complicating real-world application.
- There are minor technical inaccuracies in the results presented in the Abstract and Conclusion.
- Limited exploration of the framework's generalizability across different domains and languages.

### Suggestions for Improvement
We recommend that the authors improve the accuracy of the values presented in Table 2, specifically ensuring "Ratio@SOTA" reflects 70.95% for CoNLL'03, 56.52% for Onto. 5.0, and 52.57% for JNLPBA. Additionally, the authors should verify the values for "Min $\Delta$ LinkNER vs. GPT-3.5" at Onto 5.0 ID (should be 37.57) and "Max $\Delta$ LinkNER vs. GPT-3.5" at CoNLLâ€™03 OOV (should be 11.00). Corrections are needed in the Abstract regarding improvements over SOTA, replacing Max $\Delta$ LinkNER (LinkGPT3.5) vs. LocalNER with Best LinkNER vs. SOTA, and updating the figures for CoNLL'03. We also suggest adding "Best LinkNER vs. SOTA" to Table 3 for alignment with the Abstract. Furthermore, the authors should clarify the amount of shots (K) leading to the results in Table 3 and explore the long-term sustainability of the LinkNER framework in the evolving landscape of LLMs.