ID: UEx5dZqXvr
Title: Scaling Law for Document Neural Machine Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents experiments analyzing the key factors influencing the scaling law of document-level machine translations, specifically focusing on the number of non-embedding parameters, data size, and optimal sentence length. The authors propose that the optimal sentence length is logarithmically related to the number of parameters and persists with increased data size, while model width becomes more critical than depth as data size grows. The study is conducted on English to German translation using a standard Transformer architecture, revealing that these factors significantly impact translation quality.

### Strengths and Weaknesses
Strengths:  
- Numerous experiments provide robust support for the authors' claims.  
- The investigation of maximum sequence length is crucial and underexplored in document-level MT.  
- The authors offer both qualitative and quantitative insights, including estimates of constant parameters in their Equation (1).  

Weaknesses:  
- The novelty of the findings is limited, as many results validate previously known concepts.  
- The study is restricted to a single language pair, raising questions about the generalizability of the conclusions.  
- The writing requires polishing, and several analyses lack thoroughness, particularly in establishing causation versus correlation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of statistics in Table 4, particularly regarding #Lines and #Tokens. Additionally, the authors should provide more thorough explanations of their findings, such as the estimation method for the optimal sequence length in Equation (1) and the reliability of this model. In Section 5.2, we suggest explicitly defining "error accumulation" and analyzing its implications more deeply. The authors should also clarify the unexpected trends in accuracy presented in Section 5.3 and consider including confidence intervals to strengthen their conclusions. Furthermore, we advise the authors to address the limitations of using (d-)BLEU as a measure of translation quality, as discussed in relevant literature. Lastly, we encourage the authors to move detailed training and inference configurations to the appendix and ensure that all abbreviations are defined upon first use.