ID: 8mJujVetQv
Title: Less than One-shot: Named Entity Recognition via Extremely Weak Supervision
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies named entity recognition (NER) under the extremely weak supervision setting, where only one example per entity type is provided in the structure "The [Class LABEL]; <example>". The authors propose X-NER, which generates high-quality pseudo labels from unlabeled data using a pre-trained language model (LM) and standard sequence tagging methods. The approach involves finding candidate spans, swapping them with the example entity, measuring KL divergence of context distributions, and selecting top-K entities for pseudo labeling. The main contribution is the innovative use of KL divergence instead of cosine similarity for selecting entity mentions. The experiments demonstrate the effectiveness of this method.

### Strengths and Weaknesses
Strengths:
- The proposed approach is novel and effective, outperforming many methods designed for 1-shot or few-shot settings.
- The experiments are solid, with informative analyses and ablation studies providing good insights.
- The paper addresses an important area in NER, contributing to few-shot learning and data augmentation strategies.

Weaknesses:
- The comparisons with existing methods utilizing unlabeled data are insufficient; the paper lacks references to relevant literature.
- Important methodological details are missing, such as how candidate spans are generated and the construction of the unlabeled corpus.
- The argument for using only one example is unconvincing, as providing a few examples is feasible in practice.
- The experiment in Table 3 appears unfair, and the paper does not analyze the impact of different seed spans or provide a similarity threshold for new datasets.

### Suggestions for Improvement
We recommend that the authors improve the comparison with existing methods that utilize unlabeled data, addressing the missing references. Additionally, the authors should provide more details on the generation of candidate spans and the construction of the unlabeled corpus. Clarifying the argument for using only one example and including a discussion on the potential impact of different seed spans would strengthen the paper. Finally, specifying the similarity threshold for new datasets would enhance the methodological rigor.