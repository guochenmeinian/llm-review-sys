ID: ARrwf7Ev2T
Title: Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 5, 6, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on enhancing the compositional reasoning capabilities of CLIP models by addressing their tendency to devolve into "bags of objects." The authors propose a data-first approach that identifies flaws in the datasets used for training these models. They utilize the Conceptual Captions 3M dataset and improve caption quality through several methods: recaptioning with BLIP2 + OPT, knowledge expansion using GPT-Neo-2.7B, segmenting images with a segmentation model, and applying negative text augmentation. The paper evaluates the finetuned CLIP models on various benchmarks, demonstrating improved performance in compositional reasoning tasks.

### Strengths and Weaknesses
Strengths:
- The paper tackles an important issue regarding the robustness of CLIP models in compositional reasoning and variable binding.
- The experimental results and ablation studies indicate that the proposed data augmentation strategies effectively enhance model performance.
- The use of public LLMs for caption enhancement is commendable, and the approach is straightforward and creative.

Weaknesses:
- The complexity of the caption augmentation methods leaves some uncertainty regarding which strategies yield the best results.
- The reliance on CLIP scores for evaluating improvements in caption quality may not adequately reflect the true performance of the proposed methods.
- The evaluation is conducted on a relatively small CLIP model (ViT-B/32), raising questions about the scalability of the results.

### Suggestions for Improvement
We recommend that the authors clarify which caption augmentation strategies are most effective in terms of overall impact. Additionally, we suggest that the authors consider using a larger CLIP model to assess the scalability of their improvements. It would also be beneficial to provide a more detailed analysis of the metrics used for evaluating caption quality, including human judgment, to support claims of superiority over original captions. Furthermore, addressing the potential risks of propagating errors from the augmented data and exploring better methods for knowledge expansion could enhance the robustness of the approach.