ID: 7AWMTPMZES
Title: Discrete Modeling via Boundary Conditional Diffusion Processes
Conference: NeurIPS
Year: 2024
Number of Reviews: 27
Original Ratings: 6, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a discrete diffusion model that operates in the embedding space of discrete tokens, proposing a method that interpolates from \(x_{t_0}\) to \(x_1\), where \(x_{t_0}\) is the intersection point of the line between \(x_0\) and \(x_1\) with the 'discrete boundary' of \(x_0\). The authors also introduce a deterministic reverse diffusion process or ODE with discrete time steps, utilizing a state transfer probability to extend the traditional reverse process. They derive a training objective that simplifies to minimizing the difference between predicted and actual data, and demonstrate the rescaled probability contours of their method, showing adaptability to different boundaries and noising trajectories. The confidence factor is introduced to manage uncertainty in the learned diffusion model, with implications for mode collapse. The authors aim to bridge the gap between discrete data and continuous modeling in diffusion processes, establishing a new state-of-the-art for categorical image generation, while also discussing the optional nature of trajectory alteration in their algorithm.

### Strengths and Weaknesses
Strengths:
- The approach exploits the specific structure of the embedding space, which is novel and interesting.
- The proposed method effectively extends traditional diffusion processes, providing a deterministic framework that is theoretically sound.
- Experimental results demonstrate strong performance, particularly in machine translation and text summarization tasks, and the rescaled probability contours are well illustrated, demonstrating adaptability and consistency with boundary contours.
- The authors address uncertainty in the model through the confidence factor, enhancing the robustness of their approach.
- The paper provides comprehensive empirical analysis and theoretical explanations, addressing reviewer concerns effectively.

Weaknesses:
- The paper lacks clarity and precision in describing the method, particularly in justifying the training objective and sampling regime, leading to confusion about its implementation and effectiveness.
- The connection between the proposed method and established flow matching paradigms is unclear, especially regarding the definition of the conditional vector field \(\tilde{u}_t(\tilde{x}_t | x_0)\).
- The derivation of the loss function lacks clarity, raising questions about its justification and alignment with standard generative modeling objectives.
- The discussion on the confidence factor is vague, raising concerns about its implications for model stability and performance.
- The experimental comparisons lack fairness, particularly in Tables 1 and 3, where the evaluation metrics may not provide a clear sense of the method's relative performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity and precision of their method's description, particularly in justifying the training objective and sampling regime. It would be beneficial to explicitly define the conditional vector field \(\tilde{u}_t(\tilde{x}_t | x_0)\) and clarify how it fits within the flow matching framework. Additionally, we suggest revisiting the probability contour intuition presented in the introduction to better link it with the method in Section 3. The authors should also provide a more comprehensive discussion of the confidence factor and its implications for model stability. Furthermore, we recommend improving the clarity of the loss function derivation, explicitly addressing how it aligns with standard generative modeling objectives. A more detailed explanation of the sampling design in Algorithm 2 is necessary to enhance understanding. Lastly, we encourage the authors to clarify the narrative around their experimental results, particularly in relation to simpler baselines, to better highlight the benefits of their method, and ensure fairness in experimental comparisons by re-ranking the transformer baseline in Table 1 and providing clearer metrics in Table 3.