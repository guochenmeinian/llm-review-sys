ID: gwd3MQufGP
Title: KptLLM: Unveiling the Power of Large Language Model for Keypoint Comprehension
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the problem of Semantic Keypoint Comprehension to assess the capabilities of Multimodal Large Language Models (MLLMs) in fine-grained perception and comprehension. The authors define three tasks: (1) keypoint semantic understanding, (2) Visual prompt-based keypoint detection, and (3) Textual prompt-based keypoint detection. The proposed model, KptLLM, utilizes an identify-then-detect strategy to capture semantics before detecting keypoint positions, employing attention to fuse features from image and keypoint prompts. The model is evaluated on the MP-100 and AP-10K datasets, demonstrating superior performance compared to state-of-the-art methods.

### Strengths and Weaknesses
Strengths:
- The paper introduces new tasks for studying the semantic comprehension of MLLMs at a fine-grained level.
- The proposed MLLM-based method outperforms existing keypoint detection methods by notable margins across datasets and tasks.
- The authors conduct ablation studies on their detection strategy and architectural choices.

Weaknesses:
- The paper lacks discussion on the runtime, memory, and scalability of the MLLM-based approaches compared to existing methods.
- Insufficient details on the methods chosen for cross-comparison hinder evaluation of the contribution.
- The Visual Prompt-based and Textual Prompt-based Keypoint Detection tasks appear to be trained separately, which raises questions about potential benefits from joint training.
- Some claims, such as the novelty of equipping LLMs with point-level visual perception capabilities, are not adequately substantiated.

### Suggestions for Improvement
We recommend that the authors improve the discussion on runtime, memory, and scalability aspects of the proposed method. Additionally, clarifying whether the visual encoder is trained or frozen would enhance understanding. We suggest exploring the possibility of jointly training the Visual Prompt-based and Textual Prompt-based Keypoint Detection tasks to assess potential synergies. Furthermore, we encourage the authors to provide clearer explanations of keypoint identifiers in Tables 2 and 3, and to address the claims regarding novelty more rigorously. Lastly, conducting an ablation study on the model's performance with visually different support images would be beneficial to understand its robustness.