ID: WcoX8eJJjI
Title: Low Tensor Rank Learning of Neural Dynamics
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 4, 7, 5, -1, -1, -1, -1
Original Confidences: 2, 1, 4, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a low tensor rank recurrent neural network (ltrRNN) architecture, constraining the tensor formed by stacking RNN weight matrices across trials to have low tensor rank. The authors empirically demonstrate that ltrRNNs can effectively model neural recordings during a motor learning task, achieving lower unexplained variance than baseline methods. They also show that an RNN trained for the same task exhibits dynamics that can be well-represented by ltrRNNs. Additionally, the authors provide a theoretical analysis of gradient-based optimization leading to low rank, establishing upper bounds on matrix and tensor ranks for RNNs.

### Strengths and Weaknesses
Strengths:
1. The paper reads relatively well and presents a novel technique for modeling task learning processes through low tensor rank weights across iterations.
2. The investigation of low-rank RNNs as a model for real-world neural data is insightful and may contribute to understanding implicit regularization in gradient descent.
3. The theoretical analysis of gradient dynamics and the bounds on singular values of RNN weights are valuable contributions.

Weaknesses:
1. Empirical evidence is limited, with experiments conducted on only two datasets, raising questions about the generalizability of the low tensor rank dynamics.
2. Some terminology is non-standard, which may hinder understanding for readers unfamiliar with the specific jargon used in the paper.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation by applying the ltrRNN to additional datasets to assess the universality of the low tensor rank property. Clarifying non-standard terms, such as “trial,” “task condition,” and “chaotic regime,” in footnotes or an appendix would enhance accessibility for a broader audience. Additionally, providing more detailed mathematical definitions and algorithmic descriptions of the ltrRNN training process would strengthen the presentation and clarity of the methodology.