ID: MlrFYNo1yc
Title: Minimum norm interpolation by perceptra: Explicit regularization and implicit bias
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 4, 5, 6, 5, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 2, 1, 4, 2, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the convergence properties of shallow ReLU networks, specifically how they interpolate between known regions as the number of data points and parameters approaches infinity. The authors prove that minimizers of regularized risk functionals converge to minimum norm interpolants of a target function in an infinite parameter limit. They provide both theoretical results, particularly Theorem 2.1, and empirical insights into the implicit bias of neural network optimizers, emphasizing the role of optimization algorithms, initialization scales, and regularization. Additionally, the paper discusses the necessity of a second limit in equation (1) to prevent catastrophic overfitting, particularly in the absence of label noise, while acknowledging that the reasoning for its occurrence without noise remains unclear. The use of $\Gamma$ convergence tools is highlighted, with a suggestion to reference relevant literature.

### Strengths and Weaknesses
Strengths:
- The paper presents rigorous theoretical proofs and significant contributions, particularly in understanding convergence properties and generalization bounds.
- The technical contribution of Theorem 2.1 is recognized as significant and worthy of publication.
- The authors summarize empirical insights into the implicit bias of neural network optimizers, providing a concise overview of the behaviors exhibited by various algorithms.
- The authors demonstrate an understanding of the complexities involved in the problem, particularly regarding the representation of target functions by neural networks.

Weaknesses:
- The practical effectiveness of the findings for real-world neural network optimization, such as CNNs and Transformers, is not demonstrated.
- The paper applies to a limited setting, focusing on one-layer ReLU networks, and lacks strong connections between theoretical and empirical sections.
- The explanation of the second limit in equation (1) lacks clarity, particularly in the context of overfitting without label noise.
- The overall exposition of the paper is deemed unclear, necessitating substantial rewriting to enhance its impact.
- Important related works are not cited, and there is insufficient contextualization of results within the existing literature.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing, particularly in the abstract and conclusion, to better convey the main contributions and insights of the paper. Additionally, we suggest enhancing the discussion on the trade-off between fit and regularization, and briefly addressing potential practical applications of their findings to motivate further research. We also recommend that the authors clarify the explanation regarding the second limit in equation (1), particularly addressing the concerns about catastrophic overfitting without label noise. Incorporating references to the paper on $\Gamma$ convergence tools would strengthen the discussion. A substantial rewriting of the paper is essential to deliver a clearer and more impactful message. Lastly, it would be beneficial to include a more comprehensive review of related works, particularly those concerning implicit bias and minimum norm interpolants, to strengthen the paper's contextual foundation.