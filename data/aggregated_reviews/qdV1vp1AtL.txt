ID: qdV1vp1AtL
Title: Universal Sample Coding
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the problem of efficiently encoding a sequence of \( n \) iid realizations \( X_i \sim P \) using minimal bits, differing from one-shot channel simulation by involving multiple communication rounds. The authors propose a scheme that achieves the optimal lower bound for this task within a multiplicative constant, leveraging universal source coding techniques. Their experiments demonstrate significant improvements in federated learning scenarios, particularly when only a subset of clients participates in each communication round.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and presents a clear motivation and problem setup.
- The proposed solution, while seemingly straightforward, contains important subtleties, particularly in the choice of probability estimators, which provide valuable insights into efficient sample communication.
- The experimental results in federated learning illustrate the practical benefits of the authors' method.

Weaknesses:
- The sample complexity and runtime of the proposed solution are unclear, particularly regarding the runtime of Algorithm 1 in relation to the subroutine's runtime.
- The relationship between universal source coding, universal sample coding, and channel simulation requires better clarification, especially regarding the differences in communication settings.
- The assumption of a uniform initial distribution is not explicitly stated, and the implications of using different initial distributions are not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the sample complexity and runtime of their proposed solution, particularly by calculating the total runtime in terms of the subroutine's runtime and stating it alongside Theorem 5.1. Additionally, the authors should clarify the distinctions between universal source coding, universal sample coding, and channel simulation, emphasizing the differences in their communication settings. It is also crucial to explicitly state the assumption of a uniform initial distribution and discuss the effects of alternative initial distributions on the results. Furthermore, we suggest providing more details on the experimental setup in federated learning, including how parameters like \( \mu \) were set and the rationale behind the choice of sample sizes. Lastly, addressing minor issues in notation and writing would enhance the overall presentation of the paper.