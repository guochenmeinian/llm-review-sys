ID: XjaWEAyToL
Title: Scientific Document Retrieval using Multi-level Aspect-based Queries
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 5, 7, 8, 7, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the DORIS-MAE dataset for document retrieval, specifically targeting complex, multifaceted queries in the computer science domain. The dataset comprises 100 human-authored queries, each linked to approximately 100 relevant documents. The authors propose a hierarchical structure for queries based on semantically distinct aspects and introduce an annotation framework, Anno-GPT, which utilizes ChatGPT for generating relevance scores. The paper benchmarks various information retrieval methods on this dataset, revealing that existing models perform significantly worse compared to their performance on traditional datasets. The dataset aims to fill a research gap in the domain of multi-aspect queries, particularly in machine learning and computer science literature.

### Strengths and Weaknesses
Strengths:
- The dataset addresses a critical gap in the research community by focusing on complex, multifaceted queries, which is an important area in information retrieval.
- The expansion to 100 complex queries enhances its utility for model performance evaluation.
- The clear documentation of the annotation process and the thorough evaluation of ChatGPT-generated annotations compared to human annotations are commendable.
- The hierarchical aspect-based structure of queries allows for innovative approaches in multi-turn question answering systems.
- The authors provide a clear framework for query formation and decomposition, which is well-documented.

Weaknesses:
- The dataset's scale, while improved, may still be limited for broader research on multi-level aspect-based query ranking and retrieval.
- The reliance on ChatGPT for annotation raises concerns about potential biases, as the annotation process involved only three graduate students.
- The potential for selection bias in candidate pool creation is acknowledged but requires more detailed discussion.
- The paper lacks a clear limitations section, and the discussion of the candidate pool's construction and its implications is insufficient.
- The reliance on LLMs for annotations may not align perfectly with human judgments, raising questions about the dataset's reliability.

### Suggestions for Improvement
We recommend that the authors improve the dataset's scale by including a more diverse set of queries across various scientific domains to enhance its generalizability. Additionally, we suggest providing more detailed information on the candidate pool's construction and the specific instructions given to annotators for generating queries. Clarifying the training details for the models used in the benchmarks and addressing the limitations of both the query decomposition and Anno-GPT methods would strengthen the paper. We also encourage the authors to expand the dataset further or explore the use of decomposition strategies on full-text articles to enhance the complexity and realism of queries. Lastly, we recommend including a clear task description section that outlines the various tasks possible with the dataset, independent of specific models, and clarifying the differences between DORIS-MAE and similar datasets, such as CSFCube, to emphasize the unique contributions of this work.