ID: HF6bnhfSqH
Title: On quantum backpropagation, information reuse, and cheating measurement collapse
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 5, 6, 7, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of the efficiency of training parameterized quantum models, particularly in relation to backpropagation scaling. The authors propose an algorithm that achieves backpropagation scaling in quantum resources while reducing classical computational costs, leveraging recent developments in shadow tomography and the assumption of accessing multiple copies of a quantum state. The findings provide insights into the reusability of quantum information, which is significant for the future of quantum machine learning.

### Strengths and Weaknesses
Strengths:
- The investigation of training efficiency in quantum machine learning is timely and relevant.
- The novel approach utilizing shadow tomography contributes meaningfully to the study of quantum neural networks.
- The proposed algorithm effectively matches backpropagation scaling in quantum resources and reduces classical auxiliary costs.
- The connection between quantum backpropagation and shadow tomography is well-articulated and technically sound.

Weaknesses:
- The analysis is limited to variational quantum circuits, restricting the scope of the paper.
- The application of results to general quantum machine learning algorithms lacks convincing demonstration.
- The paper does not provide a clear, well-motivated example to illustrate the practical implications of the proposed methods.
- The algorithm's requirement for exponential classical resources limits its practicality for large systems.

### Suggestions for Improvement
We recommend that the authors improve the scope of their analysis by considering a broader range of quantum neural networks beyond variational circuits. It would be beneficial to provide a clearer demonstration of how the results apply to general quantum machine learning algorithms. Additionally, we suggest including a well-motivated example to illustrate the practical implications of the proposed methods. Clarifying the relationship between this work and existing literature on quantum neural network gradients, particularly in relation to Proposition 7, would enhance the paper's contribution. Finally, addressing the exponential classical resource requirement and its implications for scalability would strengthen the overall impact of the work.