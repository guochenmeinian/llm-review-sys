ID: b1ylCyjAZk
Title: Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 4, 6, 4, 6, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a systematic method for evaluating the reasoning capabilities of large language models (LLMs) by focusing on the concepts of necessity and sufficiency within logical reasoning. The authors introduce a framework that computes the probability of necessity (PN) and the probability of sufficiency (PS) through reasoning graphs, comparing actual values derived from factual and counterfactual datasets with those generated by LLMs. The study assesses LLMs' performance on various reasoning tasks, revealing trends in reasoning capabilities across models in the GPT family.

### Strengths and Weaknesses
Strengths:
- The introduction of a novel method for evaluating LLMs' reasoning capabilities is a critical contribution to the field.
- The paper effectively illustrates the differences in reasoning abilities among various GPT models through clear examples and diagrams.
- The examination of necessity and sufficiency is well-founded, making it suitable for measuring reasoning abilities.

Weaknesses:
- The reliance on a single example, the divisibility problem, limits the generalizability of the findings; additional examples from diverse domains are needed.
- The framework lacks a comprehensive evaluation of multiple LLMs, as it primarily focuses on the GPT series without considering other architectures or training datasets.
- The probabilistic interpretations of necessity and sufficiency are limited to specific mathematical problems, neglecting more complex reasoning scenarios.
- The paper does not adequately address the influence of LLMs' prior knowledge on reasoning performance, nor does it explore prompt tuning effects on results.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by including additional examples from various domains to demonstrate the robustness of their framework. It would be beneficial to expand the evaluation to include a wider range of LLMs with different architectures and training datasets. We also suggest that the authors clarify how they control for the influence of prior knowledge in their evaluations and consider discussing the impact of prompt tuning on LLM performance. Additionally, providing a more rigorous theoretical analysis and elaborating on the correlation between PN/PS values and reasoning capabilities would enhance the paper's persuasiveness.