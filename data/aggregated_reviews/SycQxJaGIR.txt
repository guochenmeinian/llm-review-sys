ID: SycQxJaGIR
Title: Learn to Follow: Lifelong Multi-agent Pathfinding with Decentralized Replanning
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 4, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a decentralized approach to the multi-agent pathfinding (MAPF) problem, integrating a heuristic sub-goal decider with a reinforcement learning (RL) policy for achieving short-term goals. The authors demonstrate that their method outperforms both decentralized learnable competitors and centralized planners across various experimental setups. The framework is designed to operate without agent-to-agent communication, utilizing a congestion-based heuristic alongside an A*-planner.

### Strengths and Weaknesses
Strengths:
1. The method is straightforward and clear, making it easy to understand.
2. The combination of heuristic search and reinforcement learning shows potential for effective decentralized control in MAPF.
3. The paper includes extensive comparisons with existing methods and insightful ablation studies, verifying the necessity of each component.

Weaknesses:
1. The hierarchical reinforcement learning framework has been extensively studied, limiting the novelty of the sub-goal selection design.
2. The heuristic changes in the A* planner lack strong substantiation, and the need for hyperparameter tuning may hinder practical application.
3. The RL policy's handling of collisions and deadlocks is unclear, raising concerns about its robustness in complex scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding how the RL policy addresses collisions and deadlocks, particularly in scenarios where agents may choose actions leading to conflicts. Additionally, we suggest providing more concrete qualitative examples to demonstrate the learning-based policy's effectiveness in avoiding congestion. It would also be beneficial to clarify the implications of hyperparameter tuning and the heuristic's performance guarantees. Lastly, addressing the concerns about the generalizability of the reward functions and the assumptions regarding global map access would strengthen the paper's contributions.