ID: waQ5X4qc3W
Title: Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 3, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to quantized autoencoders built on VQGAN, aiming to enhance both recognition performance for linear probing and the latent space's suitability for generative modeling. The authors argue that modern autoencoders often overlook their application in generative tasks, focusing primarily on reconstruction. They propose integrating recognition losses into the encoder, utilizing DINOv2 features discretized via k-means, and training a translation model into VQ-GAN decoder features. The model demonstrates reasonable generative capabilities while maintaining a latent space conducive to linear probing classification.

### Strengths and Weaknesses
Strengths:
- The paper achieves impressive results in the discrimination/generation tradeoff, as evidenced by figure 1.
- The finding that DINOv2 can be discretized via k-means to train a strong generative model is intriguing.
- It addresses the significant issue of understanding modern autoencoders more rigorously.

Weaknesses:
- The equivalence between linear autoencoders and PCA is a well-known fact and should not be claimed as a novel contribution.
- The assertion that a reconstructive autoencoder does not guarantee an advantageous latent space for generative models is also widely recognized.
- The proposed stability metric lacks clarity regarding its correlation with downstream generative model performance.
- Proposition 2.4 is vague and diverges from its rigorous counterpart in the supplementary material.
- The FID metrics for VQGAN on ImageNet are significantly higher than those reported in the original paper.
- Comparing the developed model's performance to those trained from scratch is misleading, as the developed model utilizes strong pre-trained models.
- The paper provides only 16 random samples for image generation, which is insufficient for a comprehensive evaluation; more samples should be included.
- The rationale for including DiT-XL/2 without its guided variants and the omission of more recent papers like EDMv2 is unclear.
- Logical transitions in the paper are often ambiguous, particularly regarding the implications of the proposed training on D^G and the definition of stability.

### Suggestions for Improvement
We recommend that the authors improve the clarity and rigor of the writing, particularly in defining key terms such as "stability" and "latent space." The motivation for using self-supervised models as encoders should be articulated more explicitly. We suggest consolidating the quantitative comparisons into a single, clear table to enhance readability. Additionally, the authors should provide more qualitative samples to strengthen their findings and clarify the definition and computation of the proposed stability metric. Finally, addressing the logical transitions and ensuring that theoretical insights directly inform the proposed methods will enhance the paper's coherence.