ID: SKhR5CuiqQ
Title: Diffusing Differentiable Representations
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 4, 5, 5, -1, -1, -1, -1
Original Confidences: 3, 2, 1, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a zero-shot method to sample neural representations using pre-trained diffusion models by pulling back the measure over the data space through the representation. The authors express the PF ODE in the parameter space, allowing for direct parameter sampling. The method is capable of generating random samples with higher diversity compared to baselines, maintaining performance, and is compatible with RePaint to ensure implicit constraints. The authors also address scenarios involving coupled images, where the PF ODE can be solved by expectation.

### Strengths and Weaknesses
Strengths:
- The introduction of a zero-shot method for sampling neural representations is innovative and has potential for challenging downstream tasks.
- The method generates samples with significantly higher diversity and ensures consistency using RePaint, enhancing global coherence and quality.
- The approach of approximating each ODE step through an optimization task is clever.

Weaknesses:
- The runtime of the method raises concerns, particularly regarding the number of optimization iterations required for each data point and Euler step, and the lack of reported runtimes for various experiments and baselines.
- The technical aspects of the paper are difficult to follow, especially the arguments in Section 3.1 and the relevance of certain equations.
- There is insufficient discussion of related works and comparisons with state-of-the-art methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the technical content, particularly in Section 3.1, by providing more intuitive explanations and detailing the derivation of key equations. Additionally, it would be beneficial to include a comprehensive discussion of related works and to compare the runtime of their method with baselines across all experiments. We also suggest that the authors clarify the limitations of their approach, particularly regarding runtime and stochasticity in Monte Carlo estimates, and consider providing further experimental validation to strengthen their claims.