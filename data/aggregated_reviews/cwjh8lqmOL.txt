ID: cwjh8lqmOL
Title: GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 5, 5, 6, 6, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GPT4Tools, a self-instruction approach aimed at teaching large language models (LLMs) to utilize tools for vision tasks. The authors generate an instruction-following dataset using ChatGPT, combining images with 23 image-related tools, and augment this dataset to include scenarios of non-usage and multi-turn tool usage. They apply Low-Rank Adaptation (LoRA) to open-source LLMs like LLaMa and Vicuna, and propose an evaluation method measuring success rates from multiple aspects. Experimental results indicate that GPT4Tools effectively enables tool usage and adapts to new tools.

### Strengths and Weaknesses
Strengths:
- The paper introduces an innovative method for teaching LLMs to use tools for visual tasks through self-instruction.
- The impact of enabling LLMs to utilize tools is substantial and could have widespread applications.
- The proposed evaluation metric is logical and establishes a standard for future research in this area.

Weaknesses:
- The novelty of the proposed method is limited, as similar self-instruction techniques have been previously explored in works like tool-llama and tool-alpaca.
- The evaluation method lacks robustness, as both training and testing data are generated from the same tool set, which may not convincingly demonstrate model performance.
- Clarity issues persist in the descriptions of variables and processes, and some crucial details regarding data extraction and dataset filtering are omitted.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contributions by clearly distinguishing their approach from existing methods like tool-llama and tool-alpaca. Additionally, enhancing the evaluation method by including comparisons with standard benchmarks, such as GQA, would strengthen the paper. We also suggest clarifying the variables P_t, X_C, and Y, possibly by referencing corresponding parts in figures or providing examples in the appendix. Furthermore, addressing the clarity of the evaluation metrics, particularly regarding the success rate of thought, and providing more details on the dataset construction process would be beneficial. Lastly, correcting typos and ensuring clear distinctions in figures would enhance the overall presentation.