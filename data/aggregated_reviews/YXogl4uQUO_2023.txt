ID: YXogl4uQUO
Title: PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 7, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PlanBench, a benchmark designed to evaluate the planning capabilities of large language models (LLMs) in the established domains of BlocksWorld and Logistics. The authors propose a prompt design that includes a domain description, example instances, and a query instance, emphasizing the importance of the domain description for customization. They assert that their benchmark is the first to specifically assess planning capabilities, distinguishing it from other reasoning benchmarks. The paper breaks down planning tasks into subtasks such as plan verification and reasoning about plan execution, converting these tasks from PDDL into natural language prompts. The authors provide experimental results comparing the performance of GPT-4 and Instruct-GPT (davinci-002) across various tasks, noting the impact of obfuscation on LLM performance while acknowledging the absence of environmental feedback in their approach.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a novel benchmark for evaluating LLM planning capabilities, filling a gap in existing research.  
- It offers a comprehensive taxonomy for assessing LLMs' planning abilities and includes a large number of prompts for each problem category, enhancing the robustness of the evaluation.  
- The prompt design is well-structured, with clear components that facilitate understanding of the task.  
- Experimental results effectively demonstrate the performance differences between GPT-4 and Instruct-GPT, along with initial analysis highlighting the effects of obfuscation on performance.

Weaknesses:  
- The discussion on prompt design lacks depth, particularly regarding alternative design choices and their impacts.  
- The benchmark currently covers only two domains, limiting its diversity and generalizability.  
- The lack of environmental feedback may restrict the assessment of LLM planning abilities.  
- The analysis of results is not comprehensive, lacking insights into failure cases and the complexity of proposed problems.  
- The evaluation methods for plan generation and verification require further justification, particularly concerning omitted instances of errors.

### Suggestions for Improvement
We recommend that the authors improve the discussion on alternative design choices for prompts, detailing the rationale behind their selections and the potential impacts of different configurations. Additionally, including a comparison table with other planning benchmarks would enhance clarity. The authors should consider incorporating environmental feedback into their evaluation, as it is crucial for understanding LLM planning capabilities. Furthermore, we suggest expanding the analysis of results to include a summary of failure cases and common attributes, which would provide deeper insights into the complexity of the proposed problems. Finally, justifying the evaluation criteria for plan generation and verification would strengthen the paper's methodological rigor and clarify the motivation for focusing on planning, making it more broadly appealing to a wider audience beyond domain experts.