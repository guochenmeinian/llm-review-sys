ID: lYNSvp51a7
Title: Lift Yourself Up: Retrieval-augmented Text Generation with Self-Memory
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 5, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework called Selfmem, designed to enhance retrieval-augmented text generation by creating an unbounded memory pool through iterative use of a retrieval-augmented generator. The authors demonstrate the effectiveness of Selfmem across three text generation tasks: machine translation, summarization, and dialogue generation, achieving state-of-the-art results. The paper includes thorough analyses of the framework's components, revealing potential bottlenecks.

### Strengths and Weaknesses
Strengths:
- The motivation behind the proposed method is intriguing, supported by preliminary experiments that validate its rationale.
- Selfmem shows promising results across various text generation tasks, indicating practical applicability.
- The paper is clearly written, with a lucid presentation of the problem setting and proposed approach.

Weaknesses:
- The paper does not adequately address recent work utilizing LLMs for knowledge generation, which should be discussed in the related work section.
- There is insufficient empirical evaluation regarding the diversity of retrieval memory from Selfmem, raising concerns about its effectiveness in knowledge-intensive tasks.
- The computational cost of Selfmem appears higher than direct retrieval methods, yet there is no detailed discussion on time computation.
- The novelty of the framework is somewhat limited, relying on existing paradigms for its dual structure.

### Suggestions for Improvement
We recommend that the authors improve the related work section by incorporating discussions on recent advancements in LLMs for knowledge generation. Additionally, further clarification and empirical evaluation on the diversity of retrieval memory should be provided to assess Selfmem's effectiveness in knowledge-intensive tasks. We suggest including a quantitative analysis of computational costs and time computation to address concerns regarding efficiency. Finally, the authors should consider conducting experiments beyond the current tasks to demonstrate the framework's versatility and explore the adaptation of Selfmem for use with learnable retrievers.