ID: kHXUb494SY
Title: Nesterov acceleration despite very noisy gradients
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 7, 3, 7, 6, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Accelerated Gradient Descent with Noisy Estimators (AGNES), a modification of Nesterov's accelerated gradient method (NAG) designed for smooth and strongly convex optimization problems under a multiplicative noise model. The authors demonstrate that while NAG diverges when the noise level exceeds a certain threshold, AGNES maintains convergence across all noise levels. The method employs two distinct step size sequences, achieving optimal convergence rates for both convex and strongly convex problems. Theoretical guarantees and numerical experiments substantiate the effectiveness of AGNES compared to existing methods.

### Strengths and Weaknesses
Strengths:  
- The manuscript clearly articulates the algorithmic contributions and the necessity of specific parameter choices for optimal convergence rates.  
- The authors effectively identify the limitations of NAG and propose a straightforward modification to address these issues under multiplicative noise.  
- The theoretical results are robust, and the numerical experiments provide practical validation of AGNES's performance.

Weaknesses:  
- The distinction between AGNES and NAG should be emphasized, clarifying that AGNES is a simple modification rather than a completely new framework.  
- The equivalence to existing methods, particularly the work of Liu & Belkin, is not sufficiently highlighted, leading to a perception of AGNES as a novel contribution rather than an analysis of prior frameworks.  
- The results for general convex cases may lack novelty given existing adaptive methods, which could potentially handle similar noise scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manuscript by explicitly stating that AGNES is a modified version of NAG, focusing on the two-step size sequences. Additionally, we suggest including a dedicated section comparing AGNES with related works to clarify its contributions and strengths. The authors should also consider moving the literature review from the appendix to the main text for better accessibility. Finally, enhancing the presentation of numerical experiments, including more comprehensive hyperparameter sweeps and clearer labeling of axes, would strengthen the empirical claims made in the paper.