ID: A7wC1CTkYl
Title: Efficient Lifelong Model Evaluation in an Era of Rapid Progress
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 5, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the concept of Lifelong Benchmarks to address model overfitting at both the individual and community levels. The authors propose the Sort & Search (S&S) framework to manage the increasing costs associated with benchmarking, which arise from evaluating numerous models and samples. S&S reduces complexity by identifying the smallest subsets of new samples and models, allowing for extrapolation of evaluations from these subsets.

### Strengths and Weaknesses
Strengths:  
- The originality of introducing lifelong benchmarks to mitigate overfitting is noteworthy.  
- The technical soundness of the submission is supported by extensive evaluations and appropriate methodologies.  
- The paper is well-written, with clear examples and theoretical proofs, demonstrating significant computational reductions in benchmark efficiency.

Weaknesses:  
- The source and growth mechanism of new samples and models are unclear, raising questions about the practical implementation of the benchmarks.  
- The evaluation section is condensed, lacking sufficient discussion and clarity, particularly in section 3.  
- The assumptions regarding the generalization of difficulty rankings to new models may not hold in practice, and the robustness of the method under such conditions is not adequately addressed.  
- The reported improvements in MSE are minimal, and the necessity of the sorting algorithm remains questionable without comparative analysis against simpler methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology by restructuring the evaluation section and including a figure that outlines the framework steps. Additionally, the authors should explicitly state the assumptions regarding data samples and models within the main text. To address concerns about the generalization of difficulty rankings, we suggest providing empirical observations on the robustness of the proposed method. Furthermore, a comparative analysis with a sample-only approach could clarify the necessity of the sorting algorithm. Lastly, discussing the storage overhead introduced by the framework and its applicability to tasks beyond classification would enhance the paper's scope.