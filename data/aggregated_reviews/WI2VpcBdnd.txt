ID: WI2VpcBdnd
Title: Provable and Efficient Dataset Distillation for Kernel Ridge Regression
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 4, 7, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents provable algorithms for computing distilled sets in the context of dataset distillation for kernel ridge regression (KRR) and provides a mathematical analysis of the sensitivity of the Moore-Penrose inverse under the Frobenius norm. The authors demonstrate that having \( m = k \) (the number of classes) is sufficient to recover the performance of the original model and propose methods to initialize distilled points from the data. They extend results from linear ridge regression (LRR) to KRR, addressing how to construct distilled sets in the original space from mapped points. Additionally, the paper discusses the implications of choosing different values for the variable \( Z \) and clarifies the conditions under which certain theorems hold. The authors also explore the privacy properties of distilled datasets, applying the Gaussian mechanism to ensure privacy in deterministic outputs derived from these datasets, and provide experimental validation of their theoretical claims.

### Strengths and Weaknesses
Strengths:
- The paper provides important provable guarantees and algorithms for dataset distillation, filling a significant gap in theoretical understanding.
- It enhances the understanding of distillation mechanisms, addressing long-standing questions in the field.
- The mathematical rigor in bounding the Frobenius norm is commendable, providing a clear framework for understanding sensitivity.
- The application of the Gaussian mechanism is well-articulated, demonstrating a solid grasp of privacy concerns in data analysis.
- The proposed methods are efficient, which is beneficial for practical applications.

Weaknesses:
- The framing and guarantees regarding the parameters \( \lambda_s \) and \( \lambda \) are unclear, raising questions about their formulation and practical implications.
- The clarity of mathematical writing is lacking, particularly regarding quantifiers and definitions, which may confuse readers.
- Certain statements, such as those in Theorem C.1 and Corollary 4.1.1, are perceived as tautological or misleading, detracting from the overall coherence of the arguments.
- The experimental results, particularly in Table 4, lack clarity and do not align with findings from similar studies, necessitating further explanation.
- Related work is inadequately cited, missing crucial references that could contextualize the contributions of this paper.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the framing around the distillation problem, specifically addressing the relationship between \( \lambda_s \) and \( \lambda \) and providing clearer definitions of the main results. Additionally, we suggest enhancing the clarity of the mathematical writing, particularly in Theorem C.1 and Corollary 4.1.1, to avoid tautological statements and ensure definitions are explicitly stated. It would also be beneficial to clarify the relationship between \( \lambda \) and \( \lambda_S \) to eliminate any confusion. Furthermore, we recommend enhancing the experimental section to clarify discrepancies with existing literature and to justify the choice of architectures used in experiments. Finally, ensure that the conditions regarding \( Z \) are rigorously defined to avoid ambiguity in the interpretation of results, and incorporate missing citations in the related work section to better position the contributions of this paper within the existing body of research.