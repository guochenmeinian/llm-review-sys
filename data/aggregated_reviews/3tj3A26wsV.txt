ID: 3tj3A26wsV
Title: A theoretical case-study of Scalable Oversight in Hierarchical Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of scalable oversight within goal-conditioned hierarchical reinforcement learning (HRL). It addresses the challenge of limited human feedback by proposing a hierarchical structure that allows for efficient learning from bounded feedback. The authors introduce two algorithms: one that learns from low-level feedback and another that incorporates human preferences over high-level trajectories. The paper also provides regret bounds for these algorithms, demonstrating the potential for effective learning in both cardinal and ordinal feedback scenarios. Additionally, the authors have effectively addressed previous feedback, enhancing the overall quality of the paper.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant problem in the field, contributing to the theoretical understanding of HRL and the integration of human feedback.
- The theoretical analysis and the design of sub-MDP rewards for Hierarchical-UCB-VI are novel, particularly the extension to ordinal feedback with the Hierarchical-REGIME algorithm.
- The writing is generally clear, with a useful running example and intuitive discussions about theoretical results.
- The authors have incorporated additional details and responded well to prior critiques, enhancing the overall quality of the paper.

Weaknesses:
- The assumption of a well-defined goal function is strong and may not be realistic, although the authors acknowledge this limitation.
- The clarity of Algorithm 1 is lacking, making it difficult to follow the core insights regarding bounds on lower-level goals.
- The regret analysis could benefit from clearer separation of high-level and low-level errors, and the reliance on the REGIME algorithm is not well-explained.
- There are several typographical errors that detract from the overall presentation.
- No specific weaknesses were identified in the reviews regarding the improvements made.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Algorithm 1 to enhance understanding of its components and insights. Additionally, providing a more detailed discussion of the constants introduced in Section 4 would help contextualize their significance. It would also be beneficial to empirically demonstrate Algorithms 1 and 2 to validate their effectiveness. Furthermore, we suggest addressing the assumptions regarding the goal function and exploring the implications of clustering functions more thoroughly, as this is central to the paper's contributions. Lastly, we encourage the authors to utilize automated tools to check for spelling and grammar errors throughout the manuscript, while continuing to refine their work by ensuring clarity and depth in all sections to further strengthen the paper's impact.