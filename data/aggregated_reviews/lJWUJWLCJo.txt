ID: lJWUJWLCJo
Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 5, 6, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Unlimiformer, a method that utilizes k-nearest neighbor (kNN) search to extend the context length of encoder-decoder transformers without modifying the underlying architecture. The authors demonstrate that their approach allows for processing input sequences longer than 500K tokens, effectively addressing the limitations of traditional transformers. Empirical results indicate that Unlimiformer enhances performance on benchmark tasks, particularly in long text summarization, by efficiently integrating retrieval mechanisms into the cross-attention phase. Additionally, the authors provide updated memory consumption results after re-evaluating memory profiling and switching to `fp16`, showing that while memory consumption increases slightly with one layer, it remains constant across additional layers, showcasing its scalability. The authors also conducted experiments on four additional QA and NLI tasks from the SCROLLS benchmark, addressing reviewers' concerns regarding model performance.

### Strengths and Weaknesses
Strengths:
- The proposed method is a straightforward enhancement to existing pre-trained models.
- Extensive experiments validate that Unlimiformer performs well with pretrained architectures like BART and Longformer.
- The computational cost increases sublinearly with input size, which is advantageous for practical applications.
- The authors provided clear and updated memory consumption data, demonstrating the efficiency of Unlimiformer.
- The scalability of Unlimiformer is highlighted, with constant GPU memory consumption across layers.
- Additional experimental results from the SCROLLS benchmark enhance the paper's robustness.

Weaknesses:
- The concept of using kNN for token retrieval has been previously explored in Reformer, raising questions about the novelty of the proposed method.
- There is ambiguity regarding the time complexity of the encoder's forward pass as token numbers increase, which was not adequately addressed in the figures.
- The evaluation is primarily focused on text summarization, limiting the generalizability of the findings to other tasks.
- Clarification is needed regarding the source of additional memory when transitioning from the baseline to one layer of Unlimiformer.
- The discussion on memory requirements could be further elaborated in the next revision.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing, particularly regarding the encoding process and the overlap of chunks, as discrepancies exist between the text and figures. Additionally, please provide a comparison of using approximate versus exact nearest neighbors in the kNN search. It would be beneficial to include a quantitative analysis of memory and speed trade-offs when applying Unlimiformer across different layers. Furthermore, we recommend that the authors improve the clarity of the memory consumption analysis, specifically addressing the source of additional memory when using Unlimiformer. We encourage the authors to refine the additional experimental results and provide a detailed discussion of memory requirements in the next paper revision. Finally, consider expanding evaluations beyond summarization to include other NLP tasks and possibly computer vision applications to strengthen the paper's impact and relevance.