ID: rvBabL7DUu
Title: Face2QR: A Unified Framework for Aesthetic, Face-Preserving, and Scannable QR Code Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 5, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a pioneering approach to integrating face identity with QR codes, proposing a novel pipeline that generates customized QR codes with embedded facial identification. The authors introduce three main components: ID-refined QR Integration (IDQR) for seamless background styling, ID-aware QR ReShuffle (IDRS) to resolve conflicts between facial ID and QR patterns, and ID-preserved Scannability Enhancement (IDSE) to optimize scannability while maintaining aesthetic quality. The experimental results demonstrate a successful balance between face identity, visual appeal, and scannability.

### Strengths and Weaknesses
Strengths:
- The integration of face identity with QR codes is innovative and addresses real-world social connection needs.
- The IDRS module effectively resolves conflicts between control signals, enhancing the customization of QR codes.
- The IDSE module improves scannability using adaptive loss while preserving face identity.
- Experimental results validate the preservation of face identity and aesthetic quality, with minimal interference from QR patterns.
- The paper is well-organized, thoroughly discussing motivation and related work, supported by rigorous experiments.

Weaknesses:
- The paper lacks examples of failures due to generative model limitations, which would help clarify the algorithm's constraints.
- Certain technical details, such as the definition of error rate and a comparison of image difference visualization $D$ between adaptive and uniform losses, require further elaboration.
- There is no comparison of computational resource requirements with other methods, making it difficult to assess practical feasibility.
- Typos and minor errors exist, such as the incorrect learning rate mentioned in line 174.

### Suggestions for Improvement
We recommend that the authors improve the paper by including examples of bad cases caused by generative model failures to provide a clearer understanding of limitations. Additionally, the authors should define the error rate and compare the image difference visualization $D$ between adaptive and uniform losses to substantiate their claims. A comparison of computational resource requirements with other methods would also enhance the assessment of the algorithm's practicality. Finally, we suggest correcting typos, such as ensuring the learning rate in line 174 is accurately stated.