ID: XxSME6GE1G
Title: TAIA: Large Language Models are Out-of-Distribution Data Learners
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TAIA, a novel method for enhancing the performance of large language models (LLMs) in data-scarce domains with domain-mismatched data. The authors propose that during fine-tuning, only attention parameters significantly contribute to downstream task performance when training and test sets' distributions do not align. TAIA retains updates to attention parameters while discarding updates to feed-forward network parameters, effectively improving performance on out-of-distribution (OOD) tasks. Extensive experiments validate TAIA's superior performance across various datasets and model configurations, demonstrating its robustness and generalizability.

### Strengths and Weaknesses
Strengths:
- The TAIA method is innovative in selectively retaining beneficial parameter updates during fine-tuning.
- The paper provides a thorough analysis of the roles of self-attention and feed-forward networks in LLMs.
- Comprehensive experiments validate the proposed method across various datasets and models, showcasing strong empirical results.

Weaknesses:
- The presentation lacks clarity; for instance, Figure 2's caption is uninformative, and Figure 3 raises questions about the necessity of using other methods given the base model's performance.
- There is insufficient empirical evidence supporting the claim that FFN parameters encapsulate significant pre-trained knowledge, and the lack of mathematical formulations in the method section complicates understanding.
- Concerns about the inconsistency between training and inference stages due to the use of different parameters are not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, particularly by enhancing the captions of figures to better convey their significance. Additionally, including a comparative experiment that utilizes only FFN parameters during inference could validate the effectiveness of TAIA. We suggest that the authors provide more detailed implementation information on applying TAIA to full fine-tuning and consider adding theoretical proof to support their claims. Finally, addressing the inconsistency between training and inference stages with more justification and additional experiments could strengthen the paper's contributions.