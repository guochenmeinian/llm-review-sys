ID: Ce0dDt9tUT
Title: Explore Positive Noise in Deep Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 5, 3, 7, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the impact of adding three types of noise—Gaussian, salt-and-pepper, and linear transform noise—to images or their latent representations on the performance of CNNs and ViTs. The authors propose a classification of noise into positive and harmful categories based on their effects on deep learning models. They provide both theoretical and empirical evidence supporting the notion that certain types of noise can enhance model performance, challenging the conventional view that noise is detrimental. Additionally, the paper presents a novel method that achieves impressive results on ImageNet, raising questions about the reproducibility of these findings. The authors propose a theoretical framework based on information quantities related to task complexity, although some definitions and logical connections within this framework are unclear.

### Strengths and Weaknesses
Strengths:
1. The paper is well-motivated, presenting the novel concept of "positive noise," which contrasts with the traditional belief that noise is harmful.
2. Extensive theoretical analysis and comprehensive experiments support the authors' claims, demonstrating significant performance improvements in image classification tasks.
3. The proposed method demonstrates significant performance on ImageNet, suggesting its potential impact on the field.
4. The authors acknowledge the importance of experimental results as a primary contribution, aligning with reviewer feedback.
5. The clarity of presentation and the potential applicability of positive noise across various deep learning architectures are notable.

Weaknesses:
1. The theoretical analysis contains fundamental errors and logical inconsistencies, raising doubts about the validity of the theorems presented and the relationship between different entropy measures.
2. The experimental results, particularly regarding Gaussian and salt-and-pepper noise, lack sufficient support, as existing literature suggests these types of noise can be beneficial.
3. The paper does not adequately differentiate its contributions from existing noise-based regularization techniques, limiting its novelty.
4. The code provided has issues, such as missing directories and functions, complicating reproducibility.
5. Some definitions and logical connections within the theoretical framework are unclear, particularly regarding the definitions in Eq. (6) and Eq. (8).

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework by addressing the identified errors, particularly regarding the definitions and relationships of task complexity and noise types. Additionally, please ensure that the code is complete and includes all necessary components for successful execution, specifically addressing the missing function OptimalQ. We suggest conducting more comprehensive experiments to validate their claims, especially comparing their method against established noise-based regularization techniques like SGD and dropout. Clarifying the connection between their proposed method and existing stochastic methods would enhance the paper's depth. Furthermore, we recommend providing clearer explanations of the noise injection process and its implications for model performance, as well as addressing the limitations of the proposed method in greater detail. Lastly, we encourage the authors to emphasize the experimental results more prominently in the paper, as they are crucial to the work's contribution.