ID: NhucGZtikE
Title: Deep Learning Through A Telescoping Lens: A Simple Model Provides Empirical Insights On Grokking, Gradient Boosting & Beyond
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 3, 6, 7, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel analytical model of neural networks that extends traditional linearized approximations by employing a sequence of first-order approximations, termed a telescoping sum. The authors aim to analyze the learning trajectory of neural networks and connect their model to various empirical observations, including phenomena such as double descent, grokking, and linear mode connectivity. The applicability of this model is demonstrated through case studies that explore the effects of different optimizers and architectures. Additionally, the authors provide a detailed analysis of the telescoping model as a superior approximation compared to the lazy model, emphasizing that its design allows for smaller approximation terms, leading to significant performance improvements. They argue that while matching performance is necessary for a model to be considered a reliable proxy, it is not sufficient on its own, supported by evidence of minimal bias in the telescoping model's approximation.

### Strengths and Weaknesses
Strengths:
- The introduction of a telescoping model provides a bridge between theoretical and empirical research, yielding new insights into well-known phenomena like double descent.
- The paper covers a wide range of topics and offers a detailed analysis, contributing significantly to the understanding of neural network behavior.
- The authors effectively connect their framework to existing empirical observations, enhancing its relevance.
- The authors address reviewer concerns by providing additional experiments and analysis, offering insightful novel evidence likely to interest the wider research community.
- Empirical results demonstrate a clear categorical improvement in the telescoping model's performance compared to the lazy model.

Weaknesses:
- The telescoping model incurs increased computational costs, which may limit its practical applicability for large networks and datasets.
- The quality of the approximation is sensitive to the learning rate and optimizer choice, raising concerns about generalizability.
- The writing lacks clarity in several sections, particularly in the case study part, which may confuse readers.
- Limited empirical validation across diverse datasets and architectures weakens the claims made in the paper.
- There are concerns regarding limited insights when dealing with modern optimizers and architectures, which may need further clarification.
- Some claims in the paper may be perceived as overly ambitious and could benefit from being toned down.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in the abstract and case study sections, to enhance reader comprehension. Additionally, we suggest providing more rigorous empirical validation of the proposed model across various datasets and architectures to strengthen the claims. The authors should also consider simplifying the complexity introduced by the telescoping model and discuss its limitations in theoretical analysis more thoroughly. Furthermore, we recommend improving the main text by integrating discussions on the derivation of $\mathbf{s}_{\theta_T}(x)$ to enhance self-containment. We encourage the authors to clarify the limitations regarding insights into modern optimizers and architectures to address reviewer concerns more thoroughly. Finally, we suggest that the authors consider toning down some of the more ambitious claims to align expectations with the current state of research in the field.