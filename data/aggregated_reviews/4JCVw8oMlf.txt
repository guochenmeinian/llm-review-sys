ID: 4JCVw8oMlf
Title: Effectively Learning Initiation Sets in Hierarchical Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 3, 7, 5, 3, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 2, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach for learning initiation sets in hierarchical reinforcement learning (HRL) and tracking initiation sets in reinforcement learning (RL). The authors introduce the Initiation Value Function (IVF), which predicts the probability of successful option execution from a given state, and clarify that the initiation set of the current policy differs from that of the optimal policy. The method is evaluated across various environments, including grid worlds and robotics, demonstrating improved performance compared to existing methods. The authors address challenges such as data non-stationarity, temporal credit assignment, and the implications of using privileged information in comparisons with existing methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, effectively motivating the technical challenges in learning initiation sets.
- The evaluation is comprehensive, with detailed experimental setups and thorough comparisons against existing methods.
- The originality of the approach is notable, particularly in combining classification with temporal structure in reinforcement learning.
- The authors provide a clear explanation of how accuracy is calculated and the rationale behind their approach to tracking initiation sets over time.
- The method demonstrates higher success rates compared to baseline approaches, even without privileged information.
- The authors are responsive to reviewer feedback and express willingness to include additional experiments and analyses in the final paper.

Weaknesses:
- The technical soundness is questionable; the problem setting is somewhat confusing, particularly regarding the internal option reward function and its relationship to the success condition cumulant.
- There is confusion regarding the interpretation of results in certain figures, particularly concerning the impact of different horizon values on performance.
- The paper lacks qualitative visualizations of learned initiation sets and IVF, which would help in understanding the proposed changes.
- The authors' reliance on sampling for the initiation set in some tasks may not be optimal, especially in simpler environments like FourRooms.
- The absence of a comparison with methods using privileged information may limit confidence in the results.
- There is insufficient discussion on the limitations and weaknesses of the method, and the clarity of some comparisons and figures is lacking.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the problem setting, particularly in relation to the internal reward function and its implications. Including qualitative visualizations of the learned initiation sets and IVF would enhance understanding. Additionally, a more thorough discussion of the limitations and weaknesses of the proposed method is necessary. We suggest that the authors improve clarity by incorporating the suggested references into the background section and elaborating on the calculation of accuracy in more detail. Including a separate plot with the old setting (x,y) that compares their method with DSC would enhance confidence in the results shown in Figure 5. Furthermore, clarifying how the approach integrates with option discovery methods is currently unclear, and addressing the hyperparameter effects, particularly the horizon \(H_0\), in the context of learning initiation sets should be included in the discussion.