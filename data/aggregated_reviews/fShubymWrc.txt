ID: fShubymWrc
Title: Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 8, 8, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of feature learning in three-layer neural networks, where the first layer is fixed, the second layer is trained for one step, and the third layer undergoes further training. The authors demonstrate that this architecture achieves better learning bounds for functions of the form \(g(x^TAx)\) compared to two-layer networks, establishing a depth-separation between the two. The results indicate a significant reduction in sample complexity relative to kernel methods, particularly for quadratic feature models, which cannot be approximated by two-layer networks.

### Strengths and Weaknesses
Strengths:
- The results are novel and solid, particularly the characterization of the feature learning capability of the three-layer network and its ability to learn quadratic functions.
- The paper is well-written, presenting the main intuitions clearly.
- It provides a general approach for studying feature learning capabilities and includes a significant optimization-based separation result.

Weaknesses:
- The limitations discussed are common in deep learning theory literature, such as the restrictive nature of layer-wise training.
- The paper lacks experiments comparing the proposed method with more common settings, such as training all parameters together.
- The definition of "Hierarchical function" is not introduced early enough, which could clarify the problem being solved.

### Suggestions for Improvement
We recommend that the authors improve the discussion on other possible features that this approach can learn beyond quadratic features. Additionally, the authors should consider commenting on potential generalizations for multi-step training of the middle layer, as the one-step training may be limiting. It would also be beneficial to include a discussion on the generalizability of their findings to deeper networks and convolutional networks. Finally, addressing the tightness of the bounds and their applicability to practical deep learning scenarios would enhance the paper's contribution.