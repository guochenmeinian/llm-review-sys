ID: d85pPNBHLt
Title: Meta-AdaM: An Meta-Learned Adaptive Optimizer with Momentum for Few-Shot Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 6, 7, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Meta-AdaM, a meta-learned adaptive optimizer that incorporates momentum and utilizes an LSTM network to predict the inner learning rate and momentum parameters based on previous gradient and momentum steps. The optimization process features a double look-ahead strategy and a dynamic class weighting scheme to assign weights to each class according to loss changes. The optimizer is evaluated on mini-Imagenet, tiered-Imagenet, and CIFAR100 datasets using two backbones (conv4 and Resnet12). The authors also employ an adaptive learning rate strategy with momentum, utilizing optimizers like Adam or RMSProp, and provide results demonstrating strong performance even without dynamic class weighting (DCW). They acknowledge the need for additional results on regression tasks, asserting that the method's efficacy should extend to regression, similar to ALFA.

### Strengths and Weaknesses
Strengths:
- The originality of the approach lies in the use of an LSTM to predict momentum and learning rates, enhancing the optimizer's adaptability.
- The proposed method demonstrates strong performance across various benchmarks, indicating its effectiveness in few-shot learning scenarios and robustness without DCW.
- The authors clarify misunderstandings regarding the gradient descent formula and the adaptive learning rate strategy.
- The inclusion of regression task results enhances the paper's contribution.

Weaknesses:
- The clarity of the paper suffers due to a lack of formal definitions and vague explanations of key concepts, such as the loss function in Equation 5 and the temperature in Equation 6.
- The dynamic class weighting scheme is inadequately explained and appears disconnected from the main optimization strategy.
- The ablation study lacks clarity regarding the momentum component, and the gradient descent formulas deviate from standard practices.
- The absence of regression task results remains a concern, as it could further validate the method's applicability.
- There is a lack of clarity on how weight decay can be integrated into the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing formal definitions for key concepts and directly integrating algorithm equations into the text. Additionally, the authors should clarify the role of the dynamic class weighting scheme in the optimization process and consider including results without this weighting to strengthen the evaluation. Addressing the questions regarding the necessity of the LSTM, the impact of the base learning rate, and the dataset inconsistencies would also enhance the paper's robustness. We further suggest that the authors provide additional results on regression tasks to strengthen their contribution and include a discussion on how weight decay can be incorporated from Eq. (3) to enhance clarity. Finally, we encourage the authors to expand the comparison with non-MAML baselines to provide a more comprehensive evaluation of their method.