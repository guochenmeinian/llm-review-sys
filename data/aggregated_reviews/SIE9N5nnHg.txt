ID: SIE9N5nnHg
Title: Adversarial Robustness through Random Weight Sampling
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 3, 7, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for enhancing adversarial robustness in neural networks by randomizing weights during inference. The authors propose sampling weights from a distribution $\mathcal{N}(\mu,\sigma)$, where $\mu$ is learned and $\sigma$ is constrained within a bounded region $[A, B]$. They evaluate this approach on CIFAR10/100 and ImageNet using ResNet architectures, reporting improved robustness over several baselines. The theoretical foundation includes two key lemmas, with particular emphasis on Lemma 1, which requires the condition $\alpha < 0$ to clarify its low confidence guarantee. The authors argue that this low confidence does not undermine the lemma's effectiveness, as it plays a crucial role in the algorithm's performance. Empirical evidence shows that without the lower bound constraint in Lemma 1, the probability $P(cos(W^{r}[1], W^{r}[2]) \leqslant \epsilon_{r})$ approaches $0$, indicating that random weights struggle to achieve a lower cosine similarity. The results demonstrate that with Lemma 1, this probability remains significant, allowing the algorithm to maintain state-of-the-art performance.

### Strengths and Weaknesses
Strengths:
- The empirical results demonstrate significant improvements in robustness compared to baselines.
- The paper extends previous work by proposing a method to learn the noise distribution during training.
- The theoretical analysis offers valuable insights into the relationship between noise parameters and network performance.
- The authors effectively justify the relevance of Lemma 1 in the algorithm's design, and empirical data supports the theoretical claims regarding the impact of constraints on cosine similarity probabilities.

Weaknesses:
- The concept of randomizing weights for robustness is not novel and has been previously explored, limiting the perceived contribution.
- The theoretical results, particularly Lemma 1, lack soundness and clarity, leading to confusion regarding their implications.
- The low confidence guarantee indicated by $P < 0.5$ may raise concerns about the robustness of the algorithm.
- The reliance on specific conditions may limit the generalizability of the findings.
- The presentation suffers from numerous inaccuracies and unclear statements, making it difficult to follow the arguments.

### Suggestions for Improvement
We recommend that the authors improve the clarity and precision of their theoretical results, particularly in the statements of Lemma 1 and Lemma 2. It is crucial to explicitly define all terms and assumptions used in these lemmas. Additionally, we suggest conducting experiments that evaluate the proposed algorithm against various fixed values of $\sigma$ within the constrained interval to better illustrate its effectiveness. Furthermore, addressing the inaccuracies in the presentation, such as the misuse of terms and symbols, will enhance the overall readability of the paper. We also recommend that the authors improve the clarity of the implications of the low confidence guarantee in Lemma 1 and discuss the potential limitations of relying on the specific conditions outlined in the lemma to enhance the robustness of the algorithm. Lastly, we encourage the authors to include evaluations under black-box attacks to provide a more comprehensive assessment of their method's robustness.