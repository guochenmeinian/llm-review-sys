ID: PWzB2V2b6R
Title: Does Video-Text Pretraining Help Open-Vocabulary Online Action Detection?
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to open vocabulary video action detection in an online setting, utilizing a dual-encoder architecture with pre-trained visual language models. The authors propose a model comprising a transformer decoder that cross-attends between recent and past video frames, and an action clustering block that employs slot attention for frame grouping and classification. The model is trained using a combination of contrastive image-text loss, multi-label contrastive video-text loss, and mask loss, demonstrating improved performance over CLIP baselines in this novel setting. Additionally, this paper is a revised version that has addressed previous concerns raised by reviewers, leading to an increased rating to weak accept. The authors express gratitude for the valuable feedback and insights provided by the reviewers, indicating their eagerness to incorporate suggested changes.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a realistic problem in online-streaming action recognition, relevant for applications like home security, and achieves impressive inference speeds (292 fps).  
- It includes extensive ablation studies validating various design choices, contributing to the robustness of the model.  
- The writing is clear and well-organized, facilitating comprehension of the proposed methods.  
- The authors demonstrate responsiveness to reviewer feedback, which has positively influenced the evaluation of the paper.

Weaknesses:  
- Claims regarding the model's ability to learn clusters of similar video frames lack direct evidence or examples.  
- The naming of the Object-Centric Decoder is misleading, as it groups frames rather than focusing on specific objects.  
- The model's comparison is limited to CLIP variants; additional baselines, particularly video-text models, should be explored.  
- Some explanations in the paper could be more concise, and certain technical terms and operations require clarification.  
- No specific weaknesses are noted in the reviews of the revised version.

### Suggestions for Improvement
We recommend that the authors improve the evidence supporting claims about clustering by providing examples of correctly clustered frames based on action semantics. The naming of the Object-Centric Decoder should be reconsidered for clarity. Additionally, the authors should include comparisons with more recent video-text models and clarify the differences between "open vocabulary" and "zero-shot" action recognition. We suggest presenting experimental results for higher values of neighboring frames and explaining the operator "$\circ$" in Equation 1 to avoid misunderstandings. Finally, enhancing the clarity of figures and providing ablation results for the Object-Centric Decoder and final transformer encoder would strengthen the paper. Furthermore, we recommend that the authors improve the clarity of their revisions to ensure that all changes are explicitly highlighted for the reviewers' consideration.