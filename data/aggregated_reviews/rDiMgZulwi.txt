ID: rDiMgZulwi
Title: Learning better with Daleâ€™s Law: A Spectral Perspective
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 5, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a simulation study investigating gradient-based optimization of recurrent neural networks (RNNs) that adhere to Dale's Law, focusing on networks with strictly excitatory or inhibitory neurons. The authors disentangle the effects of enforcing Dale's Law during training and the initial spectral properties resulting from the initialization of separate excitatory and inhibitory populations. They conclude that the performance gap between RNNs that do and do not obey Dale's Law is influenced not only by the enforcement of sign constraints but also by the initialization and parameterization of the networks.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important question regarding the performance gap between RNNs that obey Dale's Law and those that do not, providing convincing evidence that this gap is influenced by initialization and parameterization.
- The introduction of Normalised SVD Entropy as a predictor of network performance before training is a novel contribution.
- The paper is well-structured, clearly written, and includes detailed explanations of simulation methods, enhancing accessibility.

Weaknesses:
- The learning rate of DANNs is scaled independently for excitatory and inhibitory weights, which may confound results, especially since ColEI networks do not use a similar scaling scheme.
- The analysis is limited to a specific initialization scheme for ColEI networks, lacking insight into whether other initialization methods could yield different spectral properties.
- The application of gradient clipping during training may obscure the effects of spectral properties, potentially confounding results.
- Comparisons between DANNs and ColEIs may be unfair due to differences in the number of free parameters.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how the learning rate scaling for DANNs influences simulation results, particularly regarding changes in the excitatory/inhibitory ratio and network size. Additionally, we suggest providing insights into whether alternative weight initialization schemes for ColEI networks could lead to different spectral properties. It would be beneficial to clarify the impact of gradient clipping on the results and to ensure that comparisons between DANNs and ColEIs account for the differences in free parameters. Finally, addressing the minor issues such as improving figure clarity and correcting typos would enhance the overall presentation.