ID: luyXPdkNSN
Title: K-Nearest-Neighbor Local Sampling Based Conditional Independence Testing
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for conditional independence (CI) testing that utilizes a conditional mutual information (CMI) estimator based on a classifier over 1-NN samples. The authors propose a hypothesis testing procedure that iterates over CMI estimates derived from K-NN samples. Theoretical analysis supports the method's asymptotic control of type I and II errors, while empirical results indicate low type I error rates and high power, even as the number of dimensions increases for conditioning variables.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and technically sound, with an original contribution to CI testing.
- The proposed method demonstrates promising theoretical and empirical results, achieving balanced detection of true positives and true negatives in real-world datasets.
- The use of a single classifier for CMI estimation is advantageous, as CI tests are typically executed multiple times for graph detection.

Weaknesses:
- The paper lacks a comparison between the proposed testing approach and methods employing a binning strategy, as noted in the claim regarding the $k$-nearest-neighbor local sampling strategy.
- The performance sensitivity to the selection of $k$ raises concerns about the appropriateness of using a fixed $k=7$ across all experiments, particularly for real data.
- The empirical evaluation could be enhanced by including a distillation procedure to clarify the contributions of different components of the approach.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the differences between the $k$-nearest-neighbor local sampling strategy and the binning strategy, highlighting the advantages of the former. Additionally, the authors should provide insights into the optimal value of $k$ for real data applications. To strengthen the empirical evaluation, we suggest incorporating comparisons against two-classifier approaches and testing with different classifiers, such as DNNs or Random Forest methods. Furthermore, we encourage the authors to expand on the runtime analysis to substantiate claims regarding computational efficiency.