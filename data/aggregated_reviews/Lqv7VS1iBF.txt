ID: Lqv7VS1iBF
Title: StEik: Stabilizing the Optimization of Neural Signed Distance Functions and Finer Shape Representation
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 8, 7, 7, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 5, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical framework for analyzing the optimization of neural signed distance functions (SDFs) and addresses the instability associated with the eikonal loss. The authors propose a novel Laplacian normal regularization to stabilize the optimization without causing oversmoothing. Additionally, they introduce quadratic layers in neural SDFs to enhance expressivity and capture finer details. Empirical results demonstrate the effectiveness of their approach across three benchmark datasets.

### Strengths and Weaknesses
Strengths:
- The paper provides a clear theoretical analysis of the stability issues in neural SDF optimization, enhancing understanding of the eikonal loss and regularizers.
- The introduction of a novel regularizer that effectively stabilizes the optimization process is well-supported by empirical results.
- The use of quadratic layers is a significant contribution, improving the expressivity of the SDFs.
- The writing is clear and the experimental comparisons with state-of-the-art methods are comprehensive.

Weaknesses:
- The ablation studies do not clarify the relative contributions of the quadratic layers versus the new regularization term, particularly in the context of the Surface Reconstruction Benchmark (SRB).
- Increased training time due to quadratic layers is a notable drawback, and the paper lacks exploration of alternative architectures that could mitigate this issue.
- More context on the Von Neumann analysis and additional ablation studies on ShapeNet would strengthen the paper.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the ablation studies by conducting them on ShapeNet to better distinguish the contributions of the Laplacian normal regularization and quadratic layers. Additionally, providing more context for the Von Neumann analysis would enhance reader comprehension. Exploring alternative neural architectures that do not rely on quadratic layers could also be beneficial, potentially offering a balance between performance and computational efficiency. Finally, including a sensitivity analysis of the regularization term's hyperparameters would provide valuable insights into its effectiveness.