ID: bnzeOG0yey
Title: Revealing Distribution Discrepancy by Sampling Transfer in Unlabeled Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 4, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel concept called Importance Divergence (I-Div) aimed at measuring the discrepancy between training and test distributions without requiring test sample class labels. The authors propose that I-Div utilizes importance samples, density ratios, and likelihood ratios to assess the applicability of hypotheses across different datasets. The method enhances the generalization capability of machine learning models, particularly in scenarios where training and testing distributions are inconsistent. The paper validates I-Div through experiments on various datasets, including CIFAR10 and SVHN, demonstrating its effectiveness in quantifying distribution discrepancies.

### Strengths and Weaknesses
Strengths:  
1. The introduction of I-Div provides a novel method for evaluating distribution discrepancies without needing test sample class labels, addressing a significant challenge in the field.  
2. The study explores diverse experimental scenarios, including semantically dissimilar and adversarial data, offering a comprehensive perspective on the robustness and generalization of machine learning algorithms.  
3. The paper demonstrates the effectiveness of I-Div across various datasets and conditions, enhancing its credibility.

Weaknesses:  
1. The reliance on accurate estimation of density and likelihood ratios may limit the practical applicability of I-Div, particularly in complex datasets.  
2. The baseline comparisons are outdated, particularly the use of MMD-D from 2020, which diminishes the paper's novelty. More recent benchmark algorithms should be included.  
3. The choice of backbone architecture does not align with modern developments in computer vision, such as Transformer-based models like ViT and CLIP, which could enhance the algorithm's applicability.

### Suggestions for Improvement
We recommend that the authors improve the baseline comparisons by including more recent benchmark algorithms introduced after 2021, such as ViT and CLIP, to enhance the paper's relevance. Additionally, we suggest exploring more simplified or optimized algorithmic processes to reduce computational costs and improve the practical feasibility of I-Div. It would also be beneficial to provide a more detailed explanation of the experimental results, particularly regarding the performance of I-Div in complex datasets and the implications of achieving high accuracy in practical applications. Finally, we encourage the authors to analyze the sensitivity of hyperparameters, such as $\sigma$ and $\gamma$, to provide deeper insights into the method's robustness.