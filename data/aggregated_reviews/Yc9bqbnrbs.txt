ID: Yc9bqbnrbs
Title: Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 6, 6, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the Regret-Optimal Best Arm Identification (ROBAI) problem in multi-armed bandits, proposing three algorithms that achieve regret minimization and best arm identification simultaneously. The algorithms are designed for Gaussian bandits with known and unknown suboptimality gaps, utilizing upper confidence bound (UCB) exploration and lower confidence bound (LCB) commitment. The authors claim that these algorithms achieve tight regret bounds and sample complexities, with guarantees on cumulative regret and stopping times. Additionally, the paper introduces an algorithm that requires only a lower bound of $\Delta$ to define the parameter $T_c$, enhancing practicality compared to strategies that necessitate the exact value of $\Delta$. The authors argue that their algorithm maintains performance for any problem where $\epsilon$ exceeds $\Delta$, attributing lower regret to an adaptive arm sampling rule, while acknowledging that commitment time remains predetermined. However, concerns are raised regarding the practical applicability of the framework, particularly in predetermined stopping time settings.

### Strengths and Weaknesses
Strengths:
- Originality: The paper addresses a novel problem in bandit theory, providing significant insights into UCB algorithms and over-exploration.
- Quality: The theoretical results appear sound, and the presentation is clear, facilitating understanding.
- Contribution: The introduction of new techniques for bounding regret and sample complexity is noteworthy, potentially advancing the field.
- Practicality: The algorithm's reliance on a lower bound of $\Delta$ enhances its applicability.
- Adaptive Sampling: The adaptive arm sampling rule contributes to lower regret compared to fixed-design settings.
- Generalizability: The authors provide general results for bandits with more than two arms and finite-time bounds for all algorithms.

Weaknesses:
- Significance: The practical implications of the ROBAI framework are unclear, especially in predetermined settings where the necessity of knowing the minimum gap is questioned.
- Motivation: The focus on constant-level optimal regret while allowing for rate-level optimal sample complexity raises concerns about the paper's theoretical motivation.
- Applicability: Two algorithms require knowledge of the minimal gap, which is often impractical in real-world scenarios.
- Comparison Fairness: The comparison with the ETC strategy may not be entirely fair due to differing requirements for $\Delta$.
- Commitment Time: The pre-determined commitment time may limit adaptability in certain scenarios.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the practical applications of their framework, particularly addressing the limitations of requiring knowledge of the minimum gap in predetermined settings. Additionally, we suggest providing a clearer justification for the emphasis on constant-level optimal regret over rate-level optimal sample complexity. It would also be beneficial to include more diverse experimental results and comparisons with existing algorithms to substantiate the claimed advantages of the proposed methods. Furthermore, we recommend that the authors enhance the clarity of their comparisons with existing strategies, particularly the ETC strategy, to address potential misunderstandings. Lastly, further elaboration on the implications of the pre-determined commitment time in adaptive settings could enhance the paper's robustness.