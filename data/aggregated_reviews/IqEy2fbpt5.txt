ID: IqEy2fbpt5
Title: Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents **SOCKET**, a benchmark designed to evaluate social knowledge across 58 NLP tasks categorized into five groups: Humor & Sarcasm, Offensiveness, Sentiment & Emotion, Social Factors, and Trustworthiness. The authors benchmark various popular LLMs, revealing limited social capabilities and analyzing correlations between tasks, indicating potential for social knowledge transfer. The paper emphasizes the systematic nature of the benchmark and provides empirical analyses that highlight LLMs' weaknesses in social tasks.

### Strengths and Weaknesses
Strengths:  
- Comprehensive experiments with a diverse set of tasks collected from 35 datasets.  
- Clear presentation and logical writing, supported by well-formatted tables and figures.  
- In-depth analysis of experimental results, contributing to understanding social knowledge transfer.  
- Excellent literature review contextualizing the challenges of social factors in NLP.  

Weaknesses:  
- Limited task selection may not sufficiently cover the breadth of social knowledge.  
- The technical contribution regarding task transfer is not novel, as it has been explored in prior works.  
- Recent state-of-the-art models like GPT-3.5 and GPT-4 were not evaluated.  
- Potential misleading framing of the benchmark's relevance to social knowledge.  
- Aggregation of results may obscure meaningful performance differences across tasks.

### Suggestions for Improvement
We recommend that the authors improve the theoretical grounding of SOCKET by clearly articulating the theories behind the selected tasks and their relevance to social knowledge. Additionally, addressing the aggregation of results to provide a finer-grained analysis of model performance would enhance interpretability. Clarifying the evaluation methods for regression tasks and ensuring that all metrics are well-defined and consistently applied is crucial. Finally, we suggest including evaluations of more recent LLMs and addressing the concerns regarding task transfer and its implications for social knowledge understanding.