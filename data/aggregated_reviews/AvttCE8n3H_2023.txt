ID: AvttCE8n3H
Title: A Massive Scale Semantic Similarity Dataset of Historical English
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 8, 5, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the HEADLINES dataset, a large-scale semantic similarity dataset comprising nearly 400,000 semantic similarity pairs from U.S. local newspapers spanning 70 years (1920-1989). The authors automated the extraction of articles and utilized deep neural techniques to associate headlines with their corresponding articles. The dataset is publicly available and aims to facilitate research in semantic similarity modeling and historical language analysis. Additionally, the authors conduct a comprehensive analysis of the dataset, addressing aspects such as data validation, limitations, copyright issues, and benchmarking against existing datasets. They propose validation exercises to ensure data quality, including evaluations of article association, clustering, and OCR accuracy, and benchmark their dataset against the Massive Text Embedding Benchmark (MTEB), providing detailed statistics and comparisons.

### Strengths and Weaknesses
Strengths:
1. The dataset is innovative, large, and covers a significant historical period.
2. It is publicly accessible, providing valuable resources for the NLP community.
3. The authors have implemented a rigorous methodology for data processing, including a high-precision rule-based approach for article association.
4. Thorough validation exercises enhance the credibility of the dataset.
5. Inclusion of a "Limitations" section and a discussion on copyright issues adds depth to the analysis.
6. Benchmarking against MTEB demonstrates the dataset's quality and relevance in comparison to existing datasets.
7. The supplementary materials provide detailed information for reproducibility.

Weaknesses:
1. The paper lacks benchmarking of the dataset in the initial submission, which negatively impacts its usability.
2. There is insufficient discussion on data quality control and the potential impact of OCR errors.
3. Key details regarding dataset construction and limitations are not adequately addressed, making reproducibility challenging.
4. The comparison between the dataset and existing datasets lacks clarity and could benefit from more explicit connections.
5. The authors do not adequately address geographic variation in headline sources, which remains an unexplored area.

### Suggestions for Improvement
We recommend that the authors improve the paper by benchmarking the dataset to quantify its usability and address the impact of OCR errors. Expanding the section on data quality control would provide users with essential insights. Additionally, including basic statistical facts about the dataset, such as average word and sentence counts, would enhance clarity. The authors should also provide a pseudo code description of the rule-based method used for article association. Furthermore, we suggest splitting Section 4 into subsections for better clarity and including a detailed discussion on the limitations of the dataset, particularly regarding historical bias and topic diversity. To enhance clarity, we recommend including a table that explicitly contrasts key features between their dataset and existing datasets. We also suggest exploring methods to quantify geographic variation in the semantics of headlines, as this could provide valuable insights into the dataset's diversity. Lastly, we encourage the authors to ensure that all aspects of reproducibility are clearly articulated in the supplementary materials to facilitate further research.