ID: Jw0KRTjsGA
Title: CODA: Generalizing to Open and Unseen Domains with Compaction and Disambiguation
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 8, 6, 8, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a two-stage framework for Open Test-Time Domain Generalization (OTDG), addressing the challenges of domain shift and unknown classes during testing. The authors propose a methodology called Compaction and Disambiguation (CODA), where the training phase involves introducing virtual unknown classes to compact the embedding space of known classes. The test phase employs a prototype-based adaptation pipeline to ensure consistency between predicted class distributions and estimated class conditionals, as well as semantic consistency between source and target models. Experimental results demonstrate the effectiveness of the proposed approach across various benchmarks.

### Strengths and Weaknesses
Strengths:
1. The paper tackles a practical problem of open test-time domain generalization, which has been underexplored, providing clear comparisons with related settings.
2. The CODA framework is novel and technically sound, with an easy implementation and minimal computational overhead. The organization of the paper effectively highlights CODA's advantages.
3. Comprehensive experiments validate the method, with explicit comparisons to various baselines and thorough ablation studies.

Weaknesses:
1. There are inconsistencies in terminology, such as the use of 'adaptivity gap' and 'optimality gap,' which need clarification. The appropriateness of the term 'test-time domain generalization' is also questioned due to significant improvements during source training.
2. The superiority of Equation (3) over Equation (2) requires further discussion, particularly regarding the benefits of multiple virtual prototypes.
3. The prototype-based classification process for disambiguation raises questions about its effectiveness in the presence of open classes, and the potential for extending CODA to classify open classes into clusters is unclear.
4. Details on the experimental setup, including train-validation splits and model selection, need to be clarified to ensure fairness in comparisons.

### Suggestions for Improvement
We recommend that the authors improve the clarity of terminology by explicitly defining the distinctions between 'adaptivity gap' and 'optimality gap.' Additionally, the authors should provide concrete examples or case studies to illustrate the practical relevance of OTDG. Further discussion on the advantages of using multiple virtual prototypes versus a single prototype is necessary. We also suggest that the authors clarify how disambiguation is achieved with only source-learned virtual prototypes and explore the potential for classifying open classes into clusters. Lastly, the authors should enhance the transparency of their experimental methodology by detailing aspects such as train-validation splits and model selection processes.