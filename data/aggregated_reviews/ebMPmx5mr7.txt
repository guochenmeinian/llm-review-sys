ID: ebMPmx5mr7
Title: Semantic HELM: A Human-Readable Memory for Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 4, 7, 4, 7, 4, 6, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Semantic History-Embedded Language Model (SHELM), an interpretable memory mechanism for reinforcement learning (RL) agents designed to address partial observability. The authors propose a method that converts visual observations into human-readable language tokens, enhancing interpretability and resilience against representation collapse. The extensive empirical evaluation across multiple environments demonstrates the effectiveness of SHELM, although it raises questions regarding its applicability in more complex scenarios.

### Strengths and Weaknesses
Strengths:
- The method is straightforward to implement and shows broad applicability due to the generalizability of the foundational model (CLIP).
- The paper provides a thorough review of foundational knowledge, enhancing readability for diverse audiences.
- Extensive experiments validate the effectiveness of the Semantic HELM approach, with robust results across different environments.

Weaknesses:
- The proposed method may be overly simplistic, potentially only suitable for environments that rely on recognizing specific objects, limiting its effectiveness in more complex scenarios.
- Clarity regarding the differences between SHELM and HELMv2 is lacking, particularly in the method's explanation and its interpretability claims.
- The reliance on a limited set of predefined tokens for memory may restrict the method's applicability and interpretability, especially in environments requiring nuanced understanding.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the method by expanding section 2.2 and including a detailed depiction of the differences from HELMv2. Specifically, they should:
- Expand figure 2 to highlight key changes from HELMv2.
- Provide a detailed explanation of the token-retrieval mechanism and its implications for interpretability.
- Discuss the design of the memory mechanism and how it can be adapted for environments where past events significantly influence current decisions.
- Include a more thorough analysis of the interpretability of the method, addressing how the memory's contents can be understood in practical applications.
- Consider using a more generalist caption model like BLIP-2 to mitigate limitations in the predefined token set.