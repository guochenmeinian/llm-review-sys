ID: vFIC8Le5UH
Title: Auditing Empirical Privacy Protection of Private LLM Adaptations
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 7, 6
Original Confidences: 4, 4

Aggregated Review:
### Key Points
This paper presents a thorough empirical investigation into privacy risks during the personalization fine-tuning of large language models (LLMs), specifically evaluating the effectiveness of membership inference attacks under various data adaptation strategies, including parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) and prefix tuning. The authors find that privacy leakage risks decrease when adaptation data closely resembles pre-training data. The study also discusses the influence of different datasets and model architectures on privacy leakage.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a significant research gap in LLM privacy protection, proposing innovative evaluation methods and privacy protection strategies with high originality and practical application value.
2. The experimental design is rigorous, systematically assessing privacy risks through a combination of multiple datasets and fine-tuning strategies.
3. The analysis of experimental results is thorough, detailing how factors such as data distribution and fine-tuning techniques affect privacy leakage.

Weaknesses:
1. The theoretical analysis is relatively weak, lacking a deep explanation of the mechanisms behind privacy leakage.
2. The study primarily focuses on membership inference attacks, neglecting other types of privacy attacks like model inversion or attribute inference attacks.
3. The research is limited to specific datasets and models, which may restrict the generalizability of the conclusions.

### Suggestions for Improvement
1. We recommend that the authors extend the theoretical analysis by incorporating support from information theory or computational complexity theory to better explain the impact of different adaptation strategies and data distributions on privacy leakage.
2. We suggest diversifying the privacy attack assessments by introducing a wider variety of models, such as model inversion and attribute inference attacks, to comprehensively evaluate the privacy protection effectiveness.
3. We encourage the authors to enhance generalizability studies by conducting experiments across datasets in multiple languages and domains, and utilizing LLMs with various architectures.
4. We recommend adding case studies of practical application scenarios, particularly in sensitive areas like healthcare or finance, to demonstrate the effectiveness and challenges of the proposed methods in real-world operations.