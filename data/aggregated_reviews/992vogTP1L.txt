ID: 992vogTP1L
Title: Generalization bounds for neural ordinary differential equations and deep residual networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 7, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation of the generalization performance of functions represented as solutions of time-dependent parametrized ordinary differential equations (ODEs), including Neural ODEs and their discretized variants, such as Residual Networks. The authors derive generalization bounds of $O(1/n^4)$ for the general case and $O(1/n^2)$ for the time-independent case, where $n$ is the sample size. They conduct numerical experiments to explore the relationship between the smoothness of weights across layers and generalization performance, proposing a learning method that incorporates the difference in weights as a regularization term.

### Strengths and Weaknesses
Strengths:
- The results align with classical statistical learning theory, particularly demonstrating that setting $K_\Theta=0$ yields the classical $O(1/n^2)$ rate.
- Numerical experiments substantiate the connection between weight smoothness and generalization performance, consistent with theoretical predictions.
- The paper provides novel generalization bounds for depth-independent models, distinguishing it from prior works.
- The writing is clear, well-organized, and accessible, enhancing comprehension of the main points.

Weaknesses:
- The imposition of smoothness between weights in discretized ResNet models is uncommon, and the practical prediction accuracy of these models remains unverified, complicating claims of theoretical guarantees.
- The implications and applications of the results are not sufficiently articulated for the reader's understanding.
- Some expressions lack clarity, and certain results have overly stringent conditions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of definitions for symbols such as $R_X$, $R_Y$, and $n$ in Theorem 1 to avoid ambiguity. Additionally, consider providing a more thorough discussion of the relationship between the generalization bounds and existing studies on continuous-time NNs, as well as addressing the strong conditions in Corollary 1. We also suggest conducting further experiments to illustrate the correlation between the generalization gap and the maximum Lipschitz constant of the weights, and to clarify any anomalous results observed in the numerical experiments. Finally, adding citations for key theorems mentioned in the text would enhance the paper's scholarly rigor.