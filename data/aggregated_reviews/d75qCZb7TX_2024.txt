ID: d75qCZb7TX
Title: Context-Aware Testing: A New Paradigm for Model Testing with Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 29
Original Ratings: 3, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Context-Aware Testing (CAT) and the SMART framework, which utilizes context as an inductive bias to enhance the identification of meaningful model failures in machine learning (ML) models. Unlike traditional data-only methods, CAT integrates contextual information to reduce high false positive and negative rates. The authors propose that SMART does not require feature descriptions, relying instead on interpretable feature names and minimal task descriptions, making it cost-effective and applicable in various scenarios. The self-falsification mechanism within SMART is highlighted as a robust method for hypothesis validation, demonstrating improved identification of significant model failures compared to conventional methods. However, concerns about the practical applicability of the method in scenarios lacking feature descriptions and questions regarding the novelty of the approach are noted.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and clearly written, making it accessible and engaging.
- CAT represents a significant advancement in ML testing, offering a fresh perspective that transcends traditional data-only approaches.
- The SMART framework effectively utilizes LLMs and self-falsification, showcasing practical applications of CAT.
- Empirical evaluations indicate that SMART identifies more relevant and impactful failures than traditional methods.
- The authors clarify that SMART does not depend on feature descriptions, enhancing its applicability.
- The framework is presented as cost-effective, making it accessible for practical use.
- The authors effectively identify and address limitations in data-only testing methods, providing both theoretical and empirical support for their claims.
- The framework and method proposed are model-agnostic, broadening their applicability across different ML models.

Weaknesses:
- The effectiveness of CAT is contingent on the quality and relevance of contextual information, which may not always guarantee meaningful hypothesis generation.
- The use of LLMs for hypothesis generation could introduce biases, potentially affecting the reliability of results.
- The paper lacks a systematic comparison of the practicality of testing subgroups identified by SMART against data-only methods.
- The evaluation primarily focuses on outdated tabular models, neglecting more contemporary deep learning architectures.
- The method's effectiveness is limited in scenarios where feature names are absent or corrupted, raising concerns about its reliability in real-world applications.
- The novelty of the approach is perceived as constrained, primarily introducing LLMs for identifying model failures without substantial insights.
- There is a perceived misalignment between the reviewers' initial comments and their later critiques regarding the novelty and effectiveness of the proposed methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how contextual information is defined and utilized, particularly in the first step of hypothesis generation. Providing concrete examples, similar to those in Table 1, would enhance understanding. Additionally, we suggest including a systematic comparison between SMART and data-only methods regarding the practicality of subgroup testing. Expanding the evaluation to include more recent deep learning models would also strengthen the contribution. Furthermore, we recommend that the authors improve the clarity regarding the limitations of SMART in settings without feature names, emphasizing this critical aspect in the manuscript. Including the ablation study conducted in the rebuttal would provide empirical evidence of the method's performance under varying conditions. Finally, we suggest enhancing the discussion on the novelty of the approach to better articulate its contributions and insights beyond merely employing LLMs for failure identification, ensuring alignment with reviewer feedback.