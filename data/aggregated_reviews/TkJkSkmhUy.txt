ID: TkJkSkmhUy
Title: Injecting structural hints: Using language models to study inductive biases in language learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the effects of pretraining transformers on formal languages to enhance natural language learning, focusing on three inductive biases: recursion, context-sensitive language, and a Zipfian distribution. The authors find that pretraining on the CROSS language, which incorporates crossing dependencies, outperforms NEST, a recursive Dyck language, and that a Zipfian vocabulary distribution also contributes positively to performance. The experiments involve three natural languages: English, Japanese, and Basque, and measure perplexity as a primary metric.

### Strengths and Weaknesses
Strengths:
- The methodology of using formal languages to impart inductive biases is straightforward and compelling.
- The findings regarding the superiority of CROSS over NEST and the benefits of a Zipfian distribution are both interesting and unexpected.
- The experiments are well-structured and presented clearly, with a focus on three typologically diverse natural languages.

Weaknesses:
- The framing of the study's implications regarding language learnability is unclear, as it does not define criteria for successful language learning.
- The evaluation is limited to perplexity, lacking deeper analyses such as error analysis or qualitative insights into the model's performance.
- The introduction is disorganized, mixing arguments with related work, and lacks a clear motivation for the experiments.

### Suggestions for Improvement
We recommend that the authors improve the framing of their study by clearly defining what constitutes successful language learning. Additionally, we suggest incorporating error analysis or qualitative assessments to elucidate how pretraining on each formal language contributes to model performance. It would be beneficial to test the hypothesis regarding the role of copying or "induction heads" in the repetition language finding. Furthermore, we advise reorganizing the introduction for clarity and providing a more focused discussion on the implications of their findings, particularly regarding the surprising performance of CROSS compared to NEST. Lastly, consider including results for Japanese and Basque in the main text rather than the appendix to enhance the paper's presentation.