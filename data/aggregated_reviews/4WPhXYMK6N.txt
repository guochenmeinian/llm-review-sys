ID: 4WPhXYMK6N
Title: Learning Sample Difficulty from Pre-trained Models for Reliable Prediction
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a difficulty-aware uncertainty regularization approach that utilizes pre-trained large models, such as CLIP, to estimate the difficulty of training samples. The authors propose using the relative Mahalanobis distance (RMD) to quantify sample difficulty, which is then incorporated into the training loss to improve model calibration. The evaluation demonstrates that this method enhances uncertainty calibration across various datasets and model architectures.

### Strengths and Weaknesses
Strengths:
- The problem of uncertainty calibration is significant in deep learning, and the paper is well-presented and easy to follow.
- The innovative use of large models to estimate sample difficulty adds flexibility to the training process, allowing for various model architectures.
- Empirical results support the effectiveness of the proposed method, showing improvements in calibration and accuracy.

Weaknesses:
- The originality of the work is limited, as it closely resembles existing methods that utilize example weighting and RMD for outlier detection.
- The reliance on CLIP raises concerns about its universality, particularly in specialized domains like medical imaging, where its effectiveness may be uncertain.
- The theoretical justification for RMD over other metrics, such as K-Means, is insufficiently addressed, and the paper lacks a discussion of the limitations of the proposed approach.

### Suggestions for Improvement
We recommend that the authors improve the theoretical support for the RMD metric and clarify why it outperforms simpler methods like K-Means. Additionally, we suggest conducting ablation studies to explore the necessity of large models for estimating sample difficulty and to evaluate the impact of the hyperparameters $\alpha$ and $T$ on the robustness of the method. It would also be beneficial to include error bars in the results to provide insight into the variability of the findings and to discuss the limitations of the proposed approach more thoroughly.