ID: G522UpazH3
Title: Transferability Bound Theory: Exploring Relationship between Adversarial Transferability and Flatness
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the relationship between the transferability of adversarial examples and their flatness, demonstrating that flatness alone does not guarantee transferability. The authors propose an optimization method for adversarial examples, termed Theoretically Provable Attack (TPA), which enhances transferability and is empirically evaluated against various baselines. The paper also derives an upper bound for transferability loss and introduces a new loss function based on this bound. Additionally, the authors discuss the second derivative of $\log F(x+\delta)$ with respect to $x+\delta$, highlighting the complexity of proving its negativity for nonlinear functions $F$. They propose that commonly used proxy models, such as ResNet or DenseNet with ReLU activation functions, exhibit linear characteristics, which simplifies the analysis.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and presents a sound theoretical motivation and result.
- The assumptions are reasonable and clearly stated.
- There is a comprehensive empirical evaluation, including real-world applications.
- The experimental results on benchmarks are impressive, providing interesting insights into the link between flatness and transferability.
- The authors provide a clear explanation of the behavior of proxy models under ReLU activation, enhancing the understanding of their theoretical framework.
- The use of specific examples effectively illustrates the mathematical concepts discussed.

Weaknesses:
- The theoretical claims lack a strong and important assumption, particularly in Theorem 3.1, which is not explicitly stated and may not be practical across different models.
- The proof of Theorem 3.1 could be clearer, as it contains potential typos and lacks sufficient insights into the design of the TPA method.
- The paper does not discuss how its insights can be applied to improve defense mechanisms.
- Attack success rates in experiments are not reported with error bars or standard deviations, and the methodology for crafting adversarial examples is unclear.
- The evaluation relies on a single volunteer, which raises concerns about reliability.
- The explanation regarding the negativity of the second derivative for nonlinear $F$ is deemed unconvincing, necessitating further proof.
- Assumptions underlying the analysis of the second derivative are not explicitly stated, which may lead to confusion.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proof for Theorem 3.1 and explicitly state the strong assumptions underlying their theoretical claims. Additionally, it would be beneficial to justify the theoretical assumptions and discuss their limitations, such as the assumption on the smoothness of the target distribution. We also suggest providing a performance comparison of the TPA method with existing methods that constrain the gradient norm and reporting attack success rates with error bars. Furthermore, clarifying the methodology for crafting adversarial examples and ensuring a more robust evaluation process would enhance the paper's reliability. Lastly, we strongly suggest that the authors provide a proof that the second derivative is always negative for nonlinear $F$ and explicitly state and explain all assumptions in the revised version to enhance the manuscript's comprehensibility.