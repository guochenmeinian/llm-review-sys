ID: 6iouUxI45W
Title: The Exact Sample Complexity Gain from Invariances for Kernel Regression
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 7, 7, 8, 5, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the impact of encoding invariances into kernel functions on sample complexity within kernel ridge regression. The authors demonstrate that for finite groups, sample efficiency is enhanced by the size of the group, while for groups of positive dimension, improvements arise from reducing the effective dimension and volume of the input manifold. The results generalize previous work by allowing any compact manifold and smooth compact Lie group, significantly contributing to understanding model invariance's effect on generalization performance.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant issue in machine learning, providing a theoretical framework that enhances sample complexity through invariance.
- It is well-written and presents a differential geometric perspective that adds depth to learning theoretic discussions.
- The results are more general than prior work, offering better bounds and demonstrating minimax optimality.

Weaknesses:
- The paper occasionally appears overly mathematical, lacking concrete examples or empirical results to ground the theoretical findings.
- The assumptions regarding the target function f* and the group G limit practical applicability, as they require precise knowledge of these elements.
- A broader discussion of limitations, including applicability to deep learning scenarios and empirical predictive power, is needed.

### Suggestions for Improvement
We recommend that the authors improve the paper by including more concrete examples of invariances with positive dimension and their effects on generalization bounds. It would be beneficial to provide examples for equations (5) and (7) that illustrate differences between finite and infinite groups. Additionally, we suggest running experiments, even synthetic ones, to validate the predictive power of the theoretical bounds presented. A discussion on the implications of approximate knowledge of group G and the effects of changing measures on Theorem 3.1 would also enhance the paper's robustness.