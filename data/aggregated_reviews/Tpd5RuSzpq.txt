ID: Tpd5RuSzpq
Title: PUNR: Pre-training with User Behavior Modeling for News Recommendation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an unsupervised pre-training method for news recommendation based on user behaviors, introducing two tasks: user behavior masking to recover masked behaviors and user behavior generation to enhance user representations. The authors propose a pretraining model, PUNR, which employs BERT-like masked behavior modeling and GPT-like autoregressive behavior generation. Comprehensive experiments demonstrate that PUNR outperforms existing news recommendation baselines.

### Strengths and Weaknesses
Strengths:  
- The problem addressed is interesting and important, contributing novelty to the field.  
- The proposed pretraining methods are reasonable and align with established techniques in language modeling.  
- The experiments are extensive and demonstrate significant improvements.  
- The architecture is straightforward and easy to understand.  

Weaknesses:  
- Writing flaws exist, such as missing punctuation after equations.  
- The overlap of pretraining and finetuning data raises concerns about data contamination and generalization.  
- The novelty of the approach is limited, and the gradual improvement in results is a drawback.  
- Advanced news recommendation baselines are not sufficiently compared, and there are inconsistencies in equations.  
- The necessity of an additional decoder for user behavior generation is questioned, and initializing the generative decoder with BERT is deemed irrational.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing by correcting punctuation errors after equations. Additionally, the authors should address the overlap between pretraining and finetuning data to avoid data contamination, and consider testing the model on other datasets to enhance generalizability. It is also advisable to compare their method against more recent advanced news recommendation baselines. The authors should clarify the rationale behind using an additional decoder for user behavior generation and reconsider the initialization of the generative decoder with BERT's architecture. Lastly, we suggest that the authors provide case studies to illustrate the efficiency of their proposed structure compared to other baselines.