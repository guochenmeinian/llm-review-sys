ID: w7TyuWhGZP
Title: Interpretable Reward Redistribution in Reinforcement Learning: A Causal Approach
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 5, 4, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 5, 1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm called Generative Return Decomposition (GRD) aimed at addressing the challenge of identifying state-action pairs that contribute to delayed rewards in reinforcement learning. GRD models causal relationships among variables and utilizes a factored representation to derive a Markovian reward function. The authors provide theoretical proof of the identifiability of the Markovian reward function and demonstrate GRD's effectiveness through empirical experiments, showing superior performance and interpretability compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The paper excels in presenting an innovative method that enhances interpretability in reinforcement learning by explicitly modeling causal contributions.
- The theoretical foundation is solidified by proving the identifiability of the unobservable Markovian reward function and causal structure.
- Experimental results indicate that GRD outperforms state-of-the-art methods across various tasks, and visualizations enhance interpretability.

Weaknesses:
- The technical contribution appears limited, with similarities to existing frameworks, particularly regarding the learned generative model.
- The experiments do not consistently demonstrate significant performance improvements over baselines, suggesting a need for stronger experimental validation.
- Certain sections, particularly the explanation of methods and experimental results, lack clarity and require improvement for better comprehension.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the explanations in sections 4 and 5.1, possibly by including a figure to illustrate how causal masks are applied. Additionally, we suggest expanding the experimental evaluation to include other policy optimization algorithms beyond SAC and testing GRD in delayed reward environments like Atari. To enhance the interpretability of results, we encourage the authors to provide visualizations of the learned causal structures for all environments, not just Ant. Furthermore, addressing minor typographical errors and improving the presentation of equations and notations will enhance the overall quality of the paper.