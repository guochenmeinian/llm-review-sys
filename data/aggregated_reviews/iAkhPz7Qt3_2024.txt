ID: iAkhPz7Qt3
Title: Scaling Retrieval-Based Language Models with a Trillion-Token Datastore
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 6, 7, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MASSIVEDS, a substantial 1.4 trillion-token datastore designed for retrieval-in-context language models (RIC-LM) and provides a comprehensive analysis of datastore expansion with diverse data sources to enhance performance in Retrieval-Augmented Generation (RAG). The authors outline a unique construction pipeline that prioritizes indexing and retrieval before merging and filtering results. They empirically selected general web data and domain-specific data to optimize the datastore for specific tasks, such as scientific benchmarks, and argue that maximizing the likelihood of having useful information in the datastore is critical for performance improvements. The evaluation demonstrates that RIC-LM utilizing MASSIVEDS surpasses standalone language models in knowledge-intensive tasks, and the authors analyze the datastore's scaling behavior across various configurations. They also suggest exploring automatic data selection methods for future work.

### Strengths and Weaknesses
Strengths:  
1. The exploration of datastore configurations' impact on retrieval-based language models is significant and intriguing, contributing valuable insights to the field.  
2. The paper is well-written and accessible, with thorough explanations of each implementation step.  
3. The proposed datastore has the potential to advance related research in retrieval-based language models.  
4. The authors provide a clear rationale for their data source selection strategy, emphasizing the importance of useful information in the datastore.  
5. Empirical results demonstrate the effectiveness of their approach, particularly in enhancing performance on targeted tasks.  
6. The authors are responsive to reviewer feedback and show a willingness to clarify and improve their work.

Weaknesses:  
1. The experimental scope lacks robustness, failing to address critical questions regarding the consistency of findings across different data sources and tasks, as well as the interplay between retrievers and language models.  
2. There is insufficient comparative analysis with existing datastores, which undermines claims of superiority or uniqueness.  
3. Several major assertions lack adequate justification, such as the claim regarding computational overhead reduction through initial indexing and concerns about data leakage due to overlap between the datastore and evaluation datasets.  
4. There is a lack of clarity regarding the computational overhead associated with the method, particularly in relation to GPU resources.  
5. The theoretical implications of computational challenges are not fully explored, leaving questions about their origins.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their experimental design by addressing the unanswered questions regarding the consistency of findings across various data sources and tasks. Additionally, conducting a comparative analysis with established datastores would help substantiate claims of MASSIVEDS's uniqueness. We also suggest reevaluating major assertions, particularly those related to computational overhead and data leakage, to ensure they are well-supported. Furthermore, we recommend improving the clarity regarding the computational overhead, specifically addressing whether it is related to GPU resources or the choice of index type and third-party libraries. Incorporating the discussion on the scaling trend into the main paper would be beneficial, as it is well justified and resolves several concerns. Lastly, we encourage the authors to further explore and clarify the theoretical implications of the computational challenges associated with their method.