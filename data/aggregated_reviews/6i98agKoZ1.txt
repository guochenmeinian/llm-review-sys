ID: 6i98agKoZ1
Title: Self-Improvement of Non-autoregressive Model via Sequence-Level Distillation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called Sequence-Level Self-Distillation (SLSD) aimed at enhancing non-autoregressive translation models (NAT). The core concept involves generating distilled training data from the NAT model itself and selecting high-quality samples using an n-gram-based scoring function. This approach addresses the multi-modality problem and is evaluated across four prominent NAT models—CMLM, GLAT, CTC, and DAT—on WMT14 En-De and WMT16 En-Ro, demonstrating consistent and significant improvements in performance.

### Strengths and Weaknesses
Strengths:
- The method is simple and effective, applicable to multiple NAT models with minimal tuning.
- It mitigates the multi-modality issue more effectively than distillation methods using autoregressive models.
- The paper includes detailed analyses and ablation studies to elucidate the effects of self-distillation.

Weaknesses:
- The method relies on a well-initialized NAT model for generating quality candidates, which may not be applicable to Vanilla NAT, as acknowledged in the limitations.
- The selection function's design is not well-founded, as the n-gram-based metric is known for its high variance in sequence-level evaluations.
- The improvements observed are relatively modest for advanced NAT methods, indicating limited effectiveness for state-of-the-art models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how the proposed method addresses the multi-modality issue. Additionally, the authors should evaluate the method across a broader range of tasks to strengthen the findings. Finally, we suggest refining the selection function to enhance its principled basis, potentially exploring alternative metrics that may offer more stability in evaluations.