ID: rvSv8Df4AB
Title: Identifying Risky Vendors in Cryptocurrency P2P Marketplaces
Conference: ACM
Year: 2023
Number of Reviews: 6
Original Ratings: -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on online safety in cryptocurrency peer-to-peer (P2P) marketplaces, specifically analyzing data from Paxful and LocalCoinSwap over one year. The authors investigate reputation mechanisms and account suspensions, employing machine learning (ML) models to predict account suspensions with a reported accuracy of 93% AOC. The findings highlight issues such as reputation manipulation and the inadequacy of current feedback systems in assessing risky vendors. The dataset is noted for its comprehensiveness and potential interest beyond cryptocurrency contexts.

### Strengths and Weaknesses
Strengths:
- The dataset is robust and extensive, collected through public APIs, ensuring data completeness.
- The literature review is well-constructed, covering P2P crypto marketplaces and reputation mechanisms effectively.
- The findings are significant, revealing potential manipulation of reputation systems and the effectiveness of ML models in predicting account suspensions.
- The coding for manual inspection of negative feedback categories is appropriate, supported by a good Kappa score.
- The comparison of different ML models and the online evaluation of vendors are commendable.

Weaknesses:
- The machine learning methods employed are basic and lack innovation.
- The writing quality is poor; research questions and findings should be clearly listed for better readability.
- There is a lack of detailed descriptions of the implementation of machine learning methods and a framework diagram to outline the study's content and steps.
- The proposed solution shows limited generality and transferability across platforms.

### Suggestions for Improvement
We recommend that the authors improve the depth of their analysis by exploring how vendors overcome the cold-start problem in untrusted settings. Additionally, statistical tests should be included in Section 4 to confirm significant differences in feedback rates and intervals between "non-suspended" and "suspended" accounts. Clarification is needed regarding the 30-day evaluation results, specifically whether the 80% of users not suspended followed the predictions. The authors should consider alternative testing methods that do not require the removal of overlapping users to satisfy independence assumptions. Furthermore, we suggest avoiding claims of being "first" in the field due to the breadth of existing literature. Lastly, we encourage the authors to provide a more detailed description of their machine learning methods and the rationale behind selecting the seven models used in their analysis.