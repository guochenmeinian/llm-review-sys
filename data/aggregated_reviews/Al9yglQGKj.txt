ID: Al9yglQGKj
Title: Phase diagram of early training dynamics in deep neural networks: effect of the learning rate, depth, and width
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the effects of depth, width, and learning rate on the early training dynamics of deep neural networks (DNNs). The authors identify four phases of training dynamics, particularly focusing on the first two phases. In the first phase, they categorize learning rates into three ranges based on their impact on training loss and sharpness, noting that smaller learning rates lead to monotonic decreases in both loss and sharpness, while larger rates result in non-monotonic behavior. The second phase reveals that sharpness stabilizes at a value that depends on the learning rate and width. The authors also explore a simple model to validate their findings.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important problem by analyzing the training dynamics as influenced by width, depth, and learning rates, contributing new insights into these regimes.
- The clarity and depth of analysis are commendable, with thorough experimentation across various architectures.
- The identification of new training regimes is a notable contribution to the field.

Weaknesses:
- The definition of numerous regimes based on subtle behavioral changes may obscure correlations with other relevant quantities, such as generalization and feature learning.
- The paper's empirical findings lack a comprehensive theoretical framework, particularly for general neural networks beyond the simplified model.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their conclusions by highlighting key findings more prominently throughout the paper. Additionally, we suggest including more experimental results that explore a wider range of widths and depths to strengthen the support for their conclusions. It would also be beneficial to provide a more detailed analysis of the relationship between early training dynamics and eventual generalization performance. Lastly, we encourage the authors to address the impact of normalization layers and the differences in behavior between MSE and cross-entropy losses in their revised version.