ID: BRW0MKJ7Rr
Title: Action Gaps and Advantages in Continuous-Time Distributional Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into continuous reinforcement learning (RL) with high decision frequency, focusing on the action gap in distributional RL. The authors demonstrate that distributional RL agents struggle to accurately estimate action values as decision frequency increases, paralleling challenges faced by traditional RL agents. They introduce the concept of superiority, a distributional analogue of the action gap, and propose a superiority-based algorithm to address this issue, validated through numerical simulations and extensive experiments in high-frequency option trading.

### Strengths and Weaknesses
Strengths:
* The writing is rigorous, with clear definitions and theorems, making the paper easy to follow.
* The introduction of superiority distributions is both interesting and intuitively justified, supported by solid theoretical foundations.
* The empirical studies are extensive, covering both illustrated and comparative settings, and the proposed algorithms outperform the baseline QR-DQN.

Weaknesses:
* The motivation behind the action gap in continuous-time MDPs and the implications for high-frequency decision-making could be better articulated.
* Clarity in presentation could be improved; the main conclusions and the significance of findings should be emphasized more, and some rigorous theorems might be better placed in the appendix.
* The theoretical framework requires substantial background knowledge, and the explanation of the algorithmic part is dense and potentially confusing.

### Suggestions for Improvement
We recommend that the authors improve the motivation section to clarify the significance of studying distributional RL in high-frequency settings. Additionally, enhancing clarity by emphasizing main conclusions and simplifying complex explanations would benefit readability. We suggest providing more heuristic rules for the choice of the tuning parameter \( q \) and addressing the non-monotonic performance of certain methods in simulation studies. Furthermore, expanding the appendix to include more background on stochastic differential equations and breaking down Algorithm 1 into sub-procedures or higher-level abstractions would aid comprehension. Lastly, including pointers to the proofs of each theorem would allow readers to verify them more easily.