ID: OF48FkZdlQ
Title: Analog Gradient Calculation of Optical Activation Function Material
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 7, 6
Original Confidences: 3, 2

Aggregated Review:
### Key Points
This paper presents a method for computing the derivative of the activation function with respect to its input in artificial neural networks using optical physical devices. The authors propose an analog implementation of backpropagation in optical neural networks (ONNs) that utilizes small-signal modulation and a lock-in amplifier. The method is applicable to networks with negative and complex-valued weights, showing good agreement between measured and analytic gradients.

### Strengths and Weaknesses
Strengths:  
- The proposed method effectively handles networks with negative and complex-valued weights.  
- There is a strong correlation between the measured gradient and both the analytic and numpy gradients.  
- The paper discusses alternative implementations of backpropagation, highlighting the advantages of the proposed method over existing approaches.

Weaknesses:  
- The connection between computing the derivative of the activation function and its role in the broader context of machine learning systems is unclear.  
- The example of applying ReLU to a weight matrix is inappropriate, as the activation function should be applied to the matrix-vector product.  
- The references did not compile correctly, and there are several typographical errors.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how the proposed method integrates into the overall neural network training process, particularly regarding the handling of backpropagation of the error. Additionally, we suggest revising the example involving ReLU to accurately reflect its application. The authors should also ensure that all references compile correctly and address the identified typographical errors.