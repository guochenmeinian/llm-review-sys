ID: oDtyJt5JLk
Title: Directional diffusion models for graph representation learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 6, 5, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 5, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Directional Diffusion Model (DDM) for unsupervised graph and node representation learning, incorporating data-dependent, anisotropic, and directional noises in the diffusion process. The authors demonstrate that DDM outperforms existing models on various benchmark datasets, effectively capturing meaningful representations. The study also explores the impact of different types of noise on the learning process, highlighting the advantages of directional diffusion over standard isotropic noise.

### Strengths and Weaknesses
Strengths:
- The authors provide a compelling motivation and thorough explanation of the challenges in graph representation learning, supported by effective visualizations.
- The proposed method is straightforward and demonstrates promising results in addressing anisotropic structures, enhancing the quality of node embeddings.
- Extensive empirical evaluations across multiple datasets validate the effectiveness of DDM, with robust evaluation methodologies.

Weaknesses:
- The writing quality is inconsistent, particularly in the mathematical formulations, which contain numerous mistakes.
- The paper lacks theoretical foundations, limiting its generalizability and leaving questions about the effectiveness of the proposed constraints.
- There is insufficient detail regarding the implementation and reproduction of results, particularly concerning the calculation of means and deviations during inference.

### Suggestions for Improvement
We recommend that the authors improve the clarity and quality of the writing, particularly in the mathematical sections, to enhance readability. Additionally, we suggest including a brief introduction to diffusion models in the context of representation learning and providing theoretical proofs to support the claims made about the effectiveness of DDM. Furthermore, we encourage the authors to clarify the implementation details, particularly regarding the calculation of means and deviations, and to explore comparisons with other non-isotropic noise processes and different noise scheduling techniques as baselines.