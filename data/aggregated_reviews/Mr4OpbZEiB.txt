ID: Mr4OpbZEiB
Title: Task-Robust Pre-Training for Worst-Case Downstream Adaptation
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 4, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel concept called task-robust pre-training, aimed at ensuring uniformly good performance across various downstream tasks. The authors introduce a minimax loss and a corresponding algorithm, supported by convergence analyses in a convex setting. Their experiments on large-scale natural language processing and computer vision datasets demonstrate the effectiveness of the proposed method, complemented by theoretical explanations of its benefits.

### Strengths and Weaknesses
Strengths:
1. The concept of task-robust pre-training is innovative and generalizes distributionally robust optimization. The softmax weighted gradient descent method is a new minimax optimization algorithm tailored for this concept.
2. Theoretical analyses on the convergence rate are provided.
3. Impressive experimental results are achieved, considering the complexity of the tasks.
4. The paper is well-organized and clearly written.

Weaknesses:
1. The convergence rate for the new minimax optimization algorithm is $O(1/\sqrt{K})$, comparable to subgradient descent, which is known to be slow in practice. The authors should provide convergence comparisons with existing methods, either theoretically or empirically.
2. The paper lacks comprehensive comparisons to more recent multi-task learning optimization methods, limiting the evaluation of its effectiveness.
3. The assumptions regarding the relationship between upstream and downstream tasks are unclear and should be articulated more explicitly.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the assumptions regarding upstream and downstream tasks, particularly in Section 5. Additionally, the authors should enhance the experimental comparisons by including more recent multi-task learning methods, such as PCGrad, CaGrad, and GradDrop, to provide a fairer baseline. Furthermore, we suggest that the authors clarify the motivation behind their experimental setup in Section 4.2 and consider collecting worst-case results across various downstream tasks to demonstrate clear improvements in worst-case downstream adaptation.