ID: qlJoo2y3gY
Title: Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 5, 8, 6, 7, 6, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 3, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Bayesian nonparametric non-renewal (NPNR) process to model variability in neural spike trains with covariate dependence. The method generalizes modulated renewal processes using sparse variational Gaussian processes. The authors validate NPNR on synthetic data, mouse head direction cell data, and rat hippocampal place cell data, demonstrating its superiority in capturing interspike interval statistics and predictive power.

### Strengths and Weaknesses
Strengths:
- The NPNR process effectively models variability in neural spike trains.
- The approach is validated on both synthetic and real datasets, showcasing solid performance.
- The paper is well-structured, with clear mathematical derivations and a comprehensive literature review.

Weaknesses:
- The inference's sensitivity to the dimension of \(x_t\), the number of unmeasured neurons, and the amount of data required for efficient inference is not adequately explored.
- The criteria for what constitutes good predictions in terms of expected log-likelihood (ELL) are unclear.
- Some figures, particularly Figure 2, are difficult to interpret.
- The literature review lacks a comprehensive examination of prior work, leading to potential biases in the discussion.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of the inference process, particularly regarding the impact of the dimension of \(x_t\) and the number of samples on performance. Clarifying the thresholds for ELL that indicate good predictive performance, especially for k-step ahead forecasts, would enhance the experimental investigation. Additionally, we suggest improving the clarity of visualizations in Figure 2. A more exhaustive literature review is necessary to provide an unbiased context for the contributions of this work. Finally, addressing the scalability of the method to larger datasets and incorporating latent variables could enhance the model's robustness and applicability.