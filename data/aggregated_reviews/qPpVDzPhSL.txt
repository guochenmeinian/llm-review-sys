ID: qPpVDzPhSL
Title: Source Code Foundation Models are Transferable Binary Analysis Knowledge Bases
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 4, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for Human-Oriented Binary Reverse Engineering (HOBRE) tasks, specifically focusing on binary function name recovery, utilizing a probe-and-recover framework that integrates a binary-source encoder-decoder model with Large Language Models (LLMs). The authors propose that their approach, which leverages pre-trained Source Code Foundation Models (SCFMs) to generate context-rich code fragments, significantly enhances the accuracy of binary analysis. Experimental results indicate notable improvements in binary summarization and function name recovery tasks, achieving substantial gains in established metrics. The authors clarify that the reported statistics for function name recovery, including precision, recall, and F1 scores, are averages over all function names, and they acknowledge the need for clearer presentation.

### Strengths and Weaknesses
Strengths:
- The paper addresses a critical area in software engineering, focusing on the urgent need for effective binary code analysis.
- The proposed probe-and-recover framework is novel and demonstrates promising performance across multiple tasks.
- The authors conducted extensive experiments with various models, enhancing the soundness of their findings.
- The authors demonstrate a willingness to address reviewer concerns and clarify their methodology.

Weaknesses:
- The presentation lacks clarity in several areas, making it difficult to follow; for instance, the positioning of tables and the explanation of metrics require improvement.
- There are inconsistencies in the reported F1 scores, leading to trust issues regarding the contribution.
- There is insufficient human evaluation, particularly in the main body of the text, which limits the impact of the user study included in the appendix.
- The current presentation lacks sufficient detail about the metrics and user studies, which may hinder understanding.
- The selection and justification of evaluation metrics, particularly the exclusive use of ROUGE-L without commonly used metrics like BLEU and METEOR, need further elaboration.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manuscript by revisiting the placement of tables and enhancing the explanations of metrics discussed. Specifically, we suggest that the authors explicitly state that the F1 scores are averaged across individual function predictions. Integrating all relevant metrics (SymLM P, R, F1, charBLEU, METEOR, ROUGE-L, charROUGE-LSum, and CHRF) into a single table for better comparison would also be beneficial. Additionally, we suggest that the authors provide a more detailed discussion on the rationale behind their choice of evaluation metrics, including the exclusion of BLEU and METEOR, and clarify how the properties of samples from the prober enhance LLM performance. The inclusion of a user study in the main body, rather than relegating it to the appendix, would strengthen the paper's impact. Finally, we encourage the authors to elaborate on how the prober can be made more knowledgeable and flexible without introducing noise, potentially including a cost analysis in future work. Addressing these points will enhance the overall credibility and comprehensibility of the work.