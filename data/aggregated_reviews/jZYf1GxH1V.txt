ID: jZYf1GxH1V
Title: Design from Policies: Conservative Test-Time Adaptation for Offline Policy Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 5, 7, 6, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DROP, an iterative bi-level offline reinforcement learning (RL) algorithm that separates optimization into an "inner level" for contextual behavior policy training and an "outer level" for reward maximization. The authors propose a novel model-based optimization (MBO) approach, incorporating task decomposition and task embedding for flexible test-time adaptation. The DROP+CVAE implementation is introduced to stabilize learning by avoiding one-hot encoding and directly learning task embeddings conditioned on task statistics. Experimental results demonstrate state-of-the-art performance on D4RL benchmarks, showing competitive advantages over OneStep, although improvements over previous methods like APE-V and CCVL are not clearly articulated.

### Strengths and Weaknesses
Strengths:
- The proposed method integrates model-based optimization into offline RL, enabling test-time adaptation.
- Each methodological component is well justified, and the reformulation of offline RL as model-based optimization is novel and valuable.
- The DROP+CVAE implementation effectively addresses instability issues and simplifies hyperparameter selection.
- Systematic evaluations reveal statistically significant improvements over existing methods.

Weaknesses:
- The writing, particularly in the abstract and introduction, is unclear, with confusing terminology like "inner level" and "outer level," and vague framing of questions.
- The improvements of DROP over APE-V and CCVL lack clarity, and the complexity of DROP, particularly regarding hyperparameter tuning, raises concerns about practical application.
- The paper does not adequately address the hyperparameter tuning process for subtasks, which is critical in offline settings where online data collection is limited.
- Clarity issues persist regarding the representation of task embeddings and the distinction between different variants of DROP.
- Some experimental results lack clarity, such as missing entries in Table 2 and the inference time comparisons in Figure 5.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing by replacing confusing terms like "inner level" and "outer level" with more intuitive alternatives such as "value estimation" and "policy extraction." Additionally, we suggest that the authors clarify the improvements of DROP over APE-V and CCVL, possibly through a direct comparison, and provide a more thorough investigation into the robustness of DROP concerning hyperparameter choices. It would also be beneficial to report the best hyperparameters for each environment and clarify the definition of trajectory quality in the context of the rank decomposition rule. Furthermore, we encourage the authors to enhance the discussion on the relationship between DROP and related works, particularly APE-V and CCVL, and to provide a more in-depth analysis of how different behavior policies influence performance. Lastly, we recommend addressing the concerns regarding potential overestimation more robustly and ensuring that all results in Table 2 are complete and clearly labeled.