ID: nYgu408UIo
Title: Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a truth-teoretic account of why LLMs hallucinate, establishing logical preconditions for a truthful language model, which include perceptual, extensional, and intensional learning. The authors propose a "Learn-Babble-Prune" algorithm to constrain LMs to output only truthful sentences. The framework draws on analytic philosophy, aiming to provide a theoretical procedure for generating truthful outputs.

### Strengths and Weaknesses
Strengths:
- Addresses a crucial and timely issue in LLMs, specifically systematic hallucination (R1, R2, R3).
- Develops a cogent theoretical framework that connects current issues with underused philosophical literature (R2).
- The paper is well-written and accessible to non-philosophers (R1).

Weaknesses:
- Lacks empirical evidence and concrete implementation strategies for the proposed framework (R1, R3).
- Heavy reliance on abstract concepts from analytic philosophy limits accessibility to a broader NLP audience (R1).
- Questions regarding the mathematical soundness of certain definitions and claims remain (R2).

### Suggestions for Improvement
We recommend that the authors improve the paper by providing concrete suggestions on how to implement the "Learn-Babble-Prune" algorithm, addressing its feasibility and applicability in real-world scenarios. Additionally, we suggest clarifying the connections between their ideal truthful LMs and contemporary NLP/AI models, particularly regarding the nature of perceptual and extensional learning. Lastly, we encourage the authors to enhance the mathematical exposition to ensure clarity and soundness, particularly in definitions and claims that have raised concerns.