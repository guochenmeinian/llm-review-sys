ID: qf4CWnrvZa
Title: VTaC: A  Benchmark Dataset of Ventricular Tachycardia Alarms from ICU Monitors
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 6, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive annotated dataset for ventricular tachycardia (VT) alarms, derived from 5,759 waveform recordings across multiple ICUs in two major U.S. hospitals. The dataset includes 10,939 labeling decisions made by six experts, ensuring robustness through multiple independent annotations. The authors provide a benchmark of various machine learning models, demonstrating their performance on this dataset, which is noted as the largest open-access VT alarm dataset available. Additionally, the paper introduces a federated, anonymized, multi-center, multi-vendor dataset aimed at enhancing algorithm development for patient monitoring alarms. The authors clarify discrepancies in statistics between Table 1 and Table 4, explain their choice of a 10-second window for alarm onset input based on ANSI/AAMI standards, and detail the handling of Unresolved Conflicts in labeling decisions.

### Strengths and Weaknesses
**Strengths:** 
- The dataset's size and diversity enhance its utility for ongoing research in VT detection, addressing a significant challenge in critical care.
- High agreement among annotators suggests the quality of the labels is robust.
- The paper is well-written and effectively compares the proposed dataset with existing ones.
- The authors demonstrate a commitment to creating a valuable resource for researchers and proactively address review comments.
- Clarifications regarding statistical discrepancies and the rationale for the chosen window size reflect thoroughness.

**Weaknesses:** 
- Critical omissions include a lack of demographic information about the patient population, which is essential for understanding potential biases.
- The absence of a required checklist and insufficient documentation regarding study design and participant demographics are noted as significant drawbacks.
- The inability to release specific vendor identities limits transparency.
- The complexity surrounding the handling of Unresolved Conflicts may confuse readers.

### Suggestions for Improvement
We recommend that the authors improve the paper by adding visual representations of the dataset to enhance clarity. Additionally, providing detailed information on the bedside monitors used, including their identifiers, would allow for better assessment of classifier performance and improve transparency. It would also be beneficial to justify the choice of time windows for real and retrospective events in the analysis. Furthermore, addressing the unbalanced data issue more thoroughly, particularly for traditional and CNN models, is advised. We suggest enhancing clarity in the explanation of Unresolved Conflicts to ensure readers fully understand the rationale behind their labeling decisions. Lastly, we encourage the authors to correct the typo in the caption of Table 1 and update the statistics in the final version to reflect the expanded annotation effort, as well as including a statement on patient consent and clarifying the rationale for including unresolved conflict events in the dataset.