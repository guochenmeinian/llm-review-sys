ID: IdtoJVWVnX
Title: Teach Better or Show Smarter? On Instructions and Exemplars in Automatic Prompt Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 5, 6, 5, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the performance impact of instruction optimization (IO) and exemplar selection (ES) in automatic prompt optimization (APO). The authors demonstrate that optimizing ES can yield greater performance gains than IO, emphasizing the need to investigate ES and its synergy with IO. They provide empirical evidence showing that their simpler two-stage optimization algorithm performs comparably or better than more complex methods like PromptBreeder. The choice of seed instruction, specifically "Letâ€™s think step by step," is justified by its common usage in prior works, and experiments demonstrate that the two-stage IO-ES method is robust to variations in seed instructions. The authors acknowledge the importance of joint optimization between IO and ES and reference Algorithm 1 in Appendix B.8 as a method to explore this further. Empirical results across various tasks, including BBH and MMLU, support the conclusion that combining IO and ES can enhance APO performance.

### Strengths and Weaknesses
Strengths:
- The paper provides comprehensive experiments that illustrate how combining IO with ES can improve APO performance.
- The authors conduct extensive evaluations of various combinations of IO and ES approaches.
- The presentation of results, particularly in tables and figures, is clear and well-organized.
- The paper effectively demonstrates the practical relevance of using a smaller query budget.
- Empirical results support the performance claims of the proposed two-stage optimization method.
- The authors provide a clear rationale for their choice of seed instruction and show robustness to variations in seed prompts.

Weaknesses:
- The study is limited to only two models, PaLM 2 and Gemini Pro, which may not generalize to other models like ChatGPT or GPT-4.
- Key settings are inadequately explained, such as the choice of exemplars for ES and the selection criteria for $\mathcal{D}_c(I^*)$.
- The exploration of joint optimization between IO and ES is acknowledged but not fully developed in the main text.
- Some state-of-the-art IO methods are not mentioned, and comparisons with these methods are lacking.
- Some reviewers express concerns about the clarity and fairness of comparisons between IO and ES.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by including additional models, such as GPT-3.5 Turbo, in their experiments. It is crucial to clarify the rationale behind the selection of exemplars for ES and to consider the potential benefits of joint optimization of ES and IO. We suggest that the authors improve the clarity of discussions surrounding joint optimization in the main text, emphasizing the potential benefits of interdependent optimization strategies. Furthermore, we recommend that the authors compare their results against more state-of-the-art methods in IO to provide a more comprehensive evaluation. Lastly, addressing the evaluation criteria to include metrics beyond accuracy would enhance the robustness of their findings.