ID: Dn68qdfTry
Title: Almost Surely Asymptotically Constant Graph Neural Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of GNN-based probabilistic classifiers, demonstrating that their outputs converge almost surely to a constant across various random graph models, including Erdős-Rényi and Barabási-Albert. The authors propose a formal term language to express GNN architectures, which aids in understanding their convergence behavior. Empirical studies validate the theoretical findings, showing alignment with the proposed asymptotic results.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and presents a solid theoretical study.
- The introduction of a formal language for GNNs is novel and useful for future theoretical work.
- Empirical validation of theoretical results on both synthetic and real datasets is commendable.

Weaknesses:
- The term language is abstract and inadequately explained, making it difficult for readers to grasp its implications; more straightforward examples are needed.
- The discussion on the impact of asymptotic behavior on real-world GNN applications is insufficient.
- The paper lacks clarity on the definition of "probabilistic classifier" and the implications of non-i.i.d. features on results.
- The expressiveness analysis is unclear, particularly regarding how the results contribute to generalization performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the term language by providing detailed explanations and simple numerical examples, such as on explicit small graphs. Additionally, the authors should discuss the implications of their findings on real-world applications of GNNs more thoroughly. A rigorous definition of "probabilistic classifier" should be included, along with a discussion on how non-i.i.d. features may affect the results. Finally, the authors should clarify the differences between their work and previous studies, particularly regarding proof techniques and contributions to expressiveness analysis.