ID: pcpjtYNJCH
Title: Initialization-Dependent Sample Complexity of Linear Predictors and Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 6, 7, 3, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel analysis of sample complexity for neural networks using Rademacher complexity, revealing that sample complexity is influenced by initialization. The authors provide bounds for learning functions of the form \( g(x) = f(Wx) \), where \( W \) is a matrix and \( f \) is a Lipschitz function. They establish upper bounds based on Rademacher complexity and lower bounds through fat-shattering dimensions, demonstrating that size-independent bounds are achievable only when the initialization matrix \( W_0 \) is zero. The work also highlights scenarios where uniform convergence fails despite successful learning via SGD.

### Strengths and Weaknesses
Strengths:  
- The mathematics is rigorous, with clear definitions and thorough discussions of results.  
- The contribution is novel, particularly in demonstrating the dependency of sample complexity on initialization.  
- The paper effectively addresses open questions in the literature and provides interesting implications for neural networks.

Weaknesses:  
- The results appear somewhat incremental, primarily benefiting one-layer networks with multiple outputs.  
- There is a lack of practical experiments to validate the theoretical results, particularly regarding the relationship between sample complexity and Rademacher complexity.  
- The manuscript does not adequately address the implications of using typical optimization algorithms like SGD in relation to the global optimality of the trained models.  
- The distinction between results in Sections 3.1 and 3.2 is unclear, and further discussion is needed.

### Suggestions for Improvement
We recommend that the authors improve the practical validation of their theoretical claims by conducting experiments that demonstrate the consistency between sample complexity and Rademacher complexity. Additionally, it would be beneficial to clarify the implications of typical optimization algorithms like SGD on achieving global optimal solutions. We suggest enhancing the discussion surrounding the results in Sections 3.1 and 3.2 to elucidate their differences. Furthermore, providing a more detailed comparison with related works, particularly in the introduction, would strengthen the manuscript. Lastly, addressing the limitations of uniform convergence and exploring alternative bounding methods for sample complexity would add depth to the analysis.