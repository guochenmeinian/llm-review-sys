ID: sYZvdIh9ro
Title: AdvTG: An Adversarial Traffic Generation Framework to Deceive DL-Based Malicious Traffic Detection Models
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AdvTG, an adversarial traffic generation framework that utilizes Large Language Models (LLMs) and Reinforcement Learning (RL) to deceive deep learning-based detection models analyzing malicious payloads. AdvTG generates adversarial traffic while maintaining semantic coherence and protocol compliance by introducing perturbations in non-functional fields. The framework operates in three stages: training detection models, fine-tuning an LLM with traffic-specific data, and employing RL for optimizing adversarial traffic generation. The experiments demonstrate that AdvTG can produce more stealthy malicious traffic compared to existing techniques.

### Strengths and Weaknesses
Strengths:
- The paper clearly articulates the limitations of current malicious traffic generation methods and effectively positions AdvTG within the literature.
- The integration of LLMs and RL for adversarial traffic generation is innovative, and the experimental results show significant improvements in attack efficacy against detection models.

Weaknesses:
- There is a lack of critical experimental comparisons with existing content attack methods specifically targeting malicious traffic detection.
- The requirement for fine-tuning the LLM with RL raises concerns about the number of queries needed for effective fine-tuning, which may impact the feasibility of the approach.
- The paper does not adequately address the transferability of the fine-tuned LLM across different detection models.
- The impact of parameter-efficient fine-tuning on the overall framework is not evaluated, leaving its relevance unassessed.
- Presentation issues, including missing details on data generation, unclear prompts for fine-tuning, and undefined symbols, detract from the clarity of the paper.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by including comparisons with existing adversarial traffic attack methods specifically designed for malicious traffic detection. Additionally, the authors should clarify the number of queries required for fine-tuning the LLM using RL and discuss the transferability of the generated traffic across different detection models. It is essential to evaluate the impact of parameter-efficient fine-tuning on the framework's performance. Furthermore, the authors should address presentation issues by providing detailed explanations of data generation, specific examples of prompts used for fine-tuning, and ensuring that all symbols and parameters are clearly defined.