ID: orxQccN8Fm
Title: Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to aligning Large Language Models (LLMs) with human preferences by integrating Inverse Reinforcement Learning (IRL) into the supervised fine-tuning (SFT) stage. The authors propose two algorithms: one that explicitly learns a reward model and another that implicitly learns the reward within the policy. The evaluation includes theoretical analysis and empirical testing across various benchmarks, demonstrating improved performance in fine-tuning smaller language models compared to traditional SFT methods.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel IRL-based method for the SFT stage, providing a fresh perspective on LLM alignment.
2. The writing and motivation are clear, supported by a strong theoretical basis for the proposed algorithms.
3. The evaluation encompasses a wide variety of benchmarks, ensuring the validity of the results.

Weaknesses:
1. The experimental results show only marginal gains (1-2%) on the Open LLM leaderboard, raising concerns about the significance of these improvements.
2. There is insufficient discussion regarding the computational costs and scalability of the proposed methods, as well as the sensitivity of results to hyperparameter choices.
3. The paper lacks thorough exploration of limitations, particularly regarding the complexity of the training procedure and its impact on generalization.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental settings, particularly regarding the use of Algorithm 1 and the selection of data samples. Additionally, it would be beneficial to include a comprehensive analysis of computational costs associated with the proposed methods. We suggest conducting further experiments to assess the robustness of the learned reward model and the impact of hyperparameter settings on performance. Finally, presenting SFT results alongside IRFT and SPIN comparisons would provide a clearer context for evaluating the proposed methods.