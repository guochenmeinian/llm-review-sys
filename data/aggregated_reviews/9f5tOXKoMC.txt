ID: 9f5tOXKoMC
Title: A Bayesian Approach to Data Point Selection
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Bayesian approach to Data Point Selection (DPS), proposing a method that assigns weights to training data points and derives the posterior joint probability of these weights and model parameters. The authors utilize stochastic gradient Langevin dynamics for iterative sampling of parameters and weights. The effectiveness of the proposed method is demonstrated through experiments on datasets such as MNIST, CIFAR, and WebNLG, showing that it outperforms or matches existing methods like bi-level optimization (BLO) in terms of accuracy and efficiency.

### Strengths and Weaknesses
Strengths:
- The novel framing of DPS within a Bayesian context introduces a fresh perspective and enhances the theoretical foundation of the method.
- The paper is well-written, making the methodology and motivations clear and accessible.
- Empirical results indicate strong performance, particularly in instruction fine-tuning for large language models (LLMs).
- The proposed method appears to have lower memory and computational requirements compared to BLO.

Weaknesses:
- The explanation of the weight network's implementation lacks detail, particularly regarding its dependency on model embeddings and the implications for the causal graph.
- The paper's portrayal of BLO's performance is misleading, as it does not adequately address BLO's competitive accuracy with longer training times.
- The theoretical analysis is limited, particularly concerning convergence guarantees and the implications of hyperparameter sensitivity.
- Key equations, such as Eq. 4, are not derived, raising questions about their assumptions and implications.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the weight network's implementation, particularly how it interacts with model embeddings and its impact on the causal graph. Additionally, it would be beneficial to include a discussion of BLO's performance with longer training times in the main text. We suggest providing a more thorough theoretical analysis, including convergence guarantees and the effects of hyperparameters on performance. Finally, we encourage the authors to derive key equations, such as Eq. 4, to clarify their assumptions and implications for the proposed method.