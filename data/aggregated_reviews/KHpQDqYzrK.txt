ID: KHpQDqYzrK
Title: Multi-Task Neural Network Mapping onto Analog-Digital Heterogeneous Accelerators
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 7, 6
Original Confidences: 3, 2

Aggregated Review:
### Key Points
This paper presents a framework for Multi-Task Learning (MTL) that integrates analog in-memory computing (AIMC) to optimize model sharing across tasks. The authors propose a model that employs a mixture of task-specific and multi-task losses, trained with hardware-aware techniques for acceleration on analog devices. The largest connected part of the network is identified for sharing, resulting in a reported reduction of total parameters by approximately 3x while maintaining performance within 1% of task-specific models across various datasets.

### Strengths and Weaknesses
Strengths:  
- The approach effectively reduces the parameter count, which is beneficial for edge devices where chip size is critical.  
- The integration of AIMC components enhances energy efficiency, particularly for synchronous task inferences.  
- The results demonstrate a plateau in performance despite a significant reduction in shared parameters, indicating a robust model design.

Weaknesses:  
- The paper lacks a clear translation of results into overall compute efficiency gains, raising concerns about the practical benefits of the proposed method in light of Amdahl's law.  
- The argument regarding reduced overfitting through shared models may not hold, as the method appears to underfit compared to task-specific models.  
- Clarity is needed on the retraining process of the final model architecture and the selection of weights.  
- The paper does not provide estimates of energy savings, which would strengthen its impact.

### Suggestions for Improvement
We recommend that the authors improve the discussion on overall compute efficiency gains to address concerns related to Amdahl's law. Additionally, providing estimates of energy savings would enhance the practical relevance of the study. Clarifying the retraining process for the final model architecture and explaining the observed performance coincidence in Figure 4 would also strengthen the paper's arguments.