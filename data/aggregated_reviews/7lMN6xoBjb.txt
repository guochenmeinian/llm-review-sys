ID: 7lMN6xoBjb
Title: Improving Visual Prompt Tuning by Gaussian Neighborhood Minimization for Long-Tailed Visual Recognition
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel optimization strategy, Gaussian neighborhood minimization prompt tuning (GNM-PT), aimed at enhancing visual prompt tuning (VPT) under long-tailed distributions. GNM-PT, inspired by the SAM optimizer, seeks a gradient descent direction within a random parameter neighborhood, independent of input samples, thereby mitigating the effects of class imbalance. Theoretical evidence supports GNM-PT's ability to achieve a tighter upper bound on the loss function and improve performance across both head and tail classes, with reduced computational costs compared to SAM.

### Strengths and Weaknesses
Strengths:
1. The proposed method is simple, effective, and versatile.
2. GNM-PT demonstrates improved performance and accelerates training compared to SAM.
3. Solid theoretical foundations support the approach.
4. The empirical results are thorough and convincing, with a well-written presentation.

Weaknesses:
1. The rationale for applying GNM to VPT rather than other PEFT methods or CNN-based training is unclear.
2. Existing optimization strategies under long-tailed distributions should be directly compared with GNM-PT, including discussions on their performance and training costs.
3. The impact of loss reweighting or logit adjustment on flat minima for tail classes needs exploration.
4. The analysis of hyper-parameter influence on performance is lacking, despite experiments being conducted.

### Suggestions for Improvement
We recommend that the authors clarify the choice of GNM for VPT over other methods and provide comparative experiments with existing strategies under the same training paradigm. Additionally, we suggest exploring loss reweighting or logit adjustment to address flat minima issues. The authors should include a detailed analysis of hyper-parameter effects on performance and consider providing more quantitative metrics regarding the loss landscape, such as measuring convexity. Finally, we encourage the inclusion of experimental results for the comparative method LDAM and additional visualizations for other datasets.