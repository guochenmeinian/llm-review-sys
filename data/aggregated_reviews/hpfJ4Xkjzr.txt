ID: hpfJ4Xkjzr
Title: Event-based backpropagation on the neuromorphic platform SpiNNaker2
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 7, 7
Original Confidences: 4, 4

Aggregated Review:
### Key Points
This paper presents the first on-chip training of a Spiking Neural Network (SNN) on the SpiNNaker2 platform using EventProp, a learning technique that conserves memory by storing only spiking times. The authors demonstrate successful training on the Yin-Yang dataset, showing that the model outperforms a shallow classifier and that on-chip results align with off-chip experiments in terms of voltage and gradient performance. The paper details the SpiNNaker2 architecture, the types of programs executed, mini-batching methods, and future work.

### Strengths and Weaknesses
Strengths:
- The paper is clear and well-structured, with neat figures.
- It presents a pioneering hardware experiment on SpiNNaker2, performing all computations on-chip.
- Impressive alignment of voltages and gradients between on-chip and off-chip experiments.
- The system supports mini-batching.

Weaknesses:
- The relevance of using a static dataset with spikes is unclear; a temporal task may be more appropriate.
- The higher variance in performance for the off-chip experiment compared to the on-chip experiment is unexpected and requires explanation.
- The claim regarding memory savings from event-based backpropagation lacks grounding; a comparison of maximal architecture sizes for both SNNs and standard feedforward networks would be beneficial.
- Better benchmarking on the same chip with a feedforward network trained by backpropagation is needed.

### Suggestions for Improvement
We recommend that the authors clarify the relevance of the static dataset and consider using a temporal task. Additionally, an explanation for the unexpected variance in performance between off-chip and on-chip experiments should be provided. To strengthen the claim about memory savings, we suggest including a comparison of the maximal architecture size that could fit on the chip using standard backpropagation. Lastly, we encourage better benchmarking of a feedforward network trained by backpropagation on the same chip to enhance the paper's contributions.