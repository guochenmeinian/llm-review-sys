ID: 9QEVJ9qm46
Title: Robust Learning with Progressive Data Expansion Against Spurious Correlation
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 5, 8, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Progressive Data Expansion (PDE), a novel method for training models that are robust to spurious features. The authors propose a two-phase training approach: a warmup phase with a balanced dataset to prevent learning spurious features, followed by an expansion phase that finetunes the model with the full dataset. The authors provide theoretical insights into feature learning and demonstrate strong empirical performance across benchmark datasets.

### Strengths and Weaknesses
Strengths:  
**S1.** The authors provide a theoretical analysis that offers intuition for the method.  
**S2.** The proposed method is simple and computationally efficient.  
**S3.** The authors achieve strong empirical performance across multiple benchmark datasets.  
**S4.** The method is well-motivated theoretically and is easy to implement and adapt to other algorithms.  

Weaknesses:  
**W1.** The theoretical analysis is limited, focusing primarily on binary classification and using a simplified model.  
**W2.** The method requires careful tuning of several hyperparameters, and its robustness to these choices is unclear.  
**W3.** The paper does not adequately address the impact of overparameterization on the learning process.  
**W4.** The empirical results show a tradeoff between improving worst-group performance and overall accuracy, which raises concerns about the method's effectiveness compared to existing approaches.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis by addressing the effects of overparameterization and providing a more detailed investigation of the inductive bias of the proposed method. Additionally, the authors should clarify the relationship between PDE and existing methods like DFR, particularly regarding computational efficiency and hyperparameter sensitivity. It would also be beneficial to include discussions on the role of specific hyperparameters, such as the length of the expansion training phase and learning rates, to enhance understanding of their impact on performance. Lastly, we suggest that the authors provide more comprehensive empirical results that demonstrate improvements across all classes, not just the worst group.