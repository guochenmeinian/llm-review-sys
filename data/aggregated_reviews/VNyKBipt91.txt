ID: VNyKBipt91
Title: Federated Learning via Meta-Variational Dropout
Conference: NeurIPS
Year: 2023
Number of Reviews: 5
Original Ratings: 5, 6, 6, 5, -1
Original Confidences: 4, 4, 5, 4, -1

Aggregated Review:
### Key Points
This paper presents a novel Bayesian meta-learning approach, metaVD, for federated learning (FL) that predicts client-dependent dropout rates through a hypernetwork, addressing model personalization and limited non-i.i.d. data issues. The approach also compresses model parameters, alleviating overfitting and reducing communication costs. The authors demonstrate the effectiveness of metaVD through experiments on various datasets, including CIFAR-10 and CelebA.

### Strengths and Weaknesses
Strengths:
- The proposed method is novel and well-motivated, effectively combining various established strategies in FL.
- Empirical results show consistent improvements over existing baselines, indicating strong practical merits.
- The paper is clearly written, well-organized, and includes rigorous ablation studies.

Weaknesses:
- The method lacks theoretical guarantees and analysis, with many aspects remaining unaddressed.
- There is insufficient discussion of related works in Bayesian federated learning, with relevant methods not compared.
- The integration of well-known strategies lacks technical novelty, as the components behave as expected without significant challenges.
- Potential data leakage risks are present in the server-side optimization process.
- The experiments do not include error bars or standard deviations, and the performance versus communication rounds is not adequately demonstrated.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of the proposed method to provide guarantees for its effectiveness. Additionally, the authors should include a comprehensive discussion of related works in Bayesian federated learning and compare their results with state-of-the-art personalized FL algorithms. To enhance clarity, we suggest revising the presentation of results to include error bars and a performance versus communication rounds analysis. Furthermore, the authors should address the potential data leakage issue in the optimization process and clarify the implications of their aggregation method in equation (5). Lastly, we encourage the authors to provide more details on the hypernetwork's computational overhead and its benefits compared to directly learning client-specific dropout rates.