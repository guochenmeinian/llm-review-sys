ID: p8eUitex7p
Title: ImageNet3D: Towards General-Purpose Object-Level 3D Understanding
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 7, 8, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ImageNet3D, a substantial enhancement of the traditional ImageNet dataset, featuring 2D and 3D annotations aimed at improving general-purpose object-level 3D understanding in vision models. The dataset includes 200 object categories and over 86,000 instances, addressing the limitations of existing datasets by providing diverse annotations that facilitate better generalization across unseen object categories. Key contributions include cross-category 3D alignment annotations, natural language captions integrated with 3D data, and new benchmark tasks such as probing object-level 3D awareness and open-vocabulary pose estimation. Experimental results indicate that models trained on ImageNet3D can effectively infer 3D information across various rigid objects, showcasing the dataset's potential to enhance 3D understanding in vision models.

### Strengths and Weaknesses
Strengths:
1. ImageNet3D significantly expands the scope of object categories and instances, surpassing previous datasets in scale and diversity.
2. The dataset includes a variety of annotations, including 6D poses and cross-category alignments, which facilitate multi-modal research.
3. Unique features of the dataset enable exploration of new research challenges, potentially advancing the field of 3D object understanding.
4. Experimental results demonstrate the versatility of models trained on ImageNet3D in inferring 3D information across diverse rigid object categories.

Weaknesses:
1. The paper lacks a detailed discussion on the computational efficiency required for training models on ImageNet3D, which is critical given the dataset's size and task complexity.
2. The increase in dataset size compared to previous works is modest, raising questions about its sufficiency to significantly advance the field.
3. The experimental findings do not present significant novel insights, particularly in linear probing of object-level 3D awareness, which diminishes the overall impact.
4. Potential applications in 3D-LLM integration are mentioned but not thoroughly explored, leaving uncertainty about practical benefits.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by including a figure that outlines the overall pipeline for dataset creation, as the current presentation may confuse readers. Additionally, the authors should provide more details in Section 4.1 regarding the input to foundation models and how viewpoints are encoded. To enhance the dataset's utility, we suggest fine-tuning models like DeIT and MAE on a 2D-3D dataset such as Objaverse before evaluating 3D awareness. The authors should also clarify the advantages of ImageNet3D over Objaverse, particularly regarding canonical alignment, and demonstrate its effectiveness in training models. Lastly, an evaluation of the quality of natural captions should be included to assess their reliability.