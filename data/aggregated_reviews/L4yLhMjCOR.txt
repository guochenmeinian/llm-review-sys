ID: L4yLhMjCOR
Title: 3DCoMPaT200: Language Grounded Large-Scale 3D Vision Dataset for Compositional Recognition
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 6, 6, 7, -1, -1
Original Confidences: 3, 2, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents 3DCoMPaT200, a large-scale dataset designed for compositional 3D understanding of object parts and materials, featuring 200 object categories, 1,031 fine-grained part categories, and 293 distinct material classes. The authors propose a novel Compositional Shape Retrieval task to evaluate 3D shape models, enhancing the understanding of part-level and material-level characteristics. Extensive evaluations demonstrate the dataset's potential to advance 3D object understanding techniques.

### Strengths and Weaknesses
Strengths:
1. The dataset significantly expands upon its predecessor, 3DCoMPaT, with a fivefold increase in object categories and detailed annotations for parts and materials.
2. Comprehensive evaluations across multiple tasks, including object classification and part/material segmentation, highlight the dataset's utility.
3. The integration of 3D shapes, 2D images, and text descriptions supports multi-modal learning approaches.

Weaknesses:
1. The source and quality of CAD shapes used for 3DCoMPaT200 are not clearly articulated, raising questions about their comparison to other datasets like Objaverse.
2. The selection of evaluation models (PCT, PointNet++, CurveNet) lacks justification amidst numerous available 3D deep learning networks.
3. Concerns about data sparsity arise, as many fine-grained part categories have fewer than 50 occurrences, potentially limiting generalization.

### Suggestions for Improvement
1. We recommend that the authors clarify the source and quality of CAD shapes used for 3DCoMPaT200, providing a comparison with datasets like Objaverse or Objaverse-XL.
2. We suggest including a rationale for the choice of evaluation models, considering the breadth of available 3D deep learning networks.
3. We encourage the authors to address the quality of the 304M rendered images and explore their applicability for training 2D part segmentation models.
4. We recommend that the authors enhance the explanation of the Compositional Shape Retrieval task, including its practical applications and how it compares to prior datasets.
5. We suggest adding human-generated descriptions to the dataset to complement the language model-generated captions, and discussing the relationship between color and material representation in the context of shape retrieval.