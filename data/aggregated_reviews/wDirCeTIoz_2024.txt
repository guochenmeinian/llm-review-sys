ID: wDirCeTIoz
Title: Communication Efficient Distributed Training with Distributed Lion
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 5, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Distributed Lion, a variant of the Lion optimizer designed for distributed training environments. It innovatively reduces communication costs by utilizing binary or low-precision vectors instead of high-precision floating-point vectors. The authors provide theoretical convergence properties and empirical results demonstrating Distributed Lionâ€™s robustness and efficiency across various tasks, worker counts, and batch sizes, achieving performance comparable to standard Lion or AdamW optimizers while significantly lowering communication bandwidth.

### Strengths and Weaknesses
Strengths:  
+ Innovation in Communication Efficiency: The use of binary or low-precision vectors for communication significantly reduces bandwidth requirements, which is critical in distributed training.  
+ Theoretical Validation: A solid theoretical foundation confirms the convergence properties of Distributed Lion.  
+ Empirical Evidence: Extensive experiments demonstrate the robustness and efficiency of Distributed Lion across various tasks, supporting its practical applicability.  

Weaknesses:  
- Incompatibility with Allreduce: The conversion of gradients to binary or low-precision vectors prevents the use of Allreduce for gradient synchronization, raising concerns about communication efficiency in real-world distributed systems with many workers.  
- Computation Overhead: The overhead of converting updates to binary or low-precision vectors may offset some communication gains in certain scenarios. Reporting end-to-end training throughput comparisons would be beneficial.  
- Lack of Baselines: Important baselines like QSGD and SignSGD are missing, and the performance on CIFAR-10 is notably low compared to expected benchmarks.  
- Sensitivity to Hyperparameters: The performance of Distributed Lion may be sensitive to hyperparameter choices related to communication and aggregation strategies.  
- Reproducibility Issues: The absence of provided code weakens the reproducibility of the experiments.  
- Wall-clock Time: The paper does not report wall-clock time for training, which is crucial for assessing the overall impact of communication reduction on training time.  

### Suggestions for Improvement
We recommend that the authors improve the paper by addressing the compatibility of Distributed Lion with Allreduce and providing a detailed analysis of communication efficiency in real-world distributed systems. Additionally, including comparisons with important baselines like QSGD and SignSGD would strengthen the contribution. We suggest reporting wall-clock time alongside loss/accuracy metrics to better illustrate the training efficiency. Furthermore, clarifying the hyperparameter settings for fair comparisons and providing the code for reproducibility would enhance the study's credibility. Lastly, exploring the algorithm's performance in non-i.i.d. settings and federated learning scenarios could be valuable for future work.