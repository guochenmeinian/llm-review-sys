ID: EDuKP7DqCk
Title: Text Embeddings Reveal (Almost) As Much As Text
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the information stored in text embeddings generated by text encoders, focusing on recovering original full texts from these embeddings. The authors propose a multi-step method, Vec2Text, which formulates the task as controlled text generation and demonstrates that most short texts (e.g., 32 tokens) can be reconstructed with high accuracy. The findings raise concerns about potential privacy issues related to the leakage of private information from training data through text embeddings.

### Strengths and Weaknesses
Strengths:
- Novelty: The paper introduces a unique approach to extracting raw texts from text embeddings, highlighting significant privacy implications.
- Clarity: The motivation, methodology, and experimental settings are clearly articulated.
- Effective Method: Vec2Text significantly outperforms standard decoding methods and includes a simple defense against inversion attacks by adding random noise.
- Comprehensive Evaluations: The evaluations cover various tasks and domains, providing insightful case studies.

Weaknesses:
- Method Efficiency: The multi-step refinement approach may raise efficiency concerns, though this does not detract from the paper's main contributions.
- Lack of Baseline Comparisons: The paper does not adequately compare Vec2Text with other embedding inversion methods, limiting the assessment of its effectiveness.
- Insufficient Analysis: There is a lack of detailed analysis and ablation studies, particularly regarding performance variations with token length and the impact of Gaussian noise.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the main contributions by providing more detailed analyses and ablation studies. Specifically, they should explain the performance differences observed with varying token lengths and enhance the discussion surrounding Tables 4 and 5. Additionally, we suggest including comparisons with other embedding inversion methods to better contextualize the effectiveness of Vec2Text. It would also be beneficial to run experiments multiple times to report mean and standard deviation, along with conducting statistical significance tests to validate the improvements over baselines. Finally, consider exploring alternative methods for defending against inversion attacks, such as using less accurate embedding models, and include this in the discussion.