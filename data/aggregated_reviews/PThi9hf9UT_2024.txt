ID: PThi9hf9UT
Title: Mutual Information Estimation via $f$-Divergence and Data Derangements
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 4, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for mutual information estimation using a discriminative training approach based on f-divergences. The authors address the limitation of exponential sample complexity in strong dependence limits and highlight the significant impact of marginal sample drawing methods, introducing "data derangements" to improve estimates. The proposed estimator demonstrates lower variance compared to existing methods like MINE, particularly in high-dimensional settings.

### Strengths and Weaknesses
Strengths:
- The f-divergence approach avoids partition function estimation elegantly.
- The surprising effects of negative samples are well-explained.
- A robust selection of results is provided across various settings, including high-dimensional image data.
- The proposed derangement strategy enhances estimation accuracy.

Weaknesses:
- The theoretical analysis assumes the optimum discriminator is known, lacking insights on mis-estimation effects.
- Clarity in the results section could be improved, particularly regarding the architecture, which may benefit from schematic illustrations.
- The conclusions in the Variance Analysis need clearer articulation.
- The existence of a permutation in Theorem 3.1 is unclear and may imply strong independence assumptions that require further clarification.
- The empirical results do not consistently support claims of an excellent bias-variance trade-off, particularly against competing methods.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis by addressing the effects of mis-estimation of the discriminator. Clarifying the existence of the permutation in Theorem 3.1 is crucial; providing proof or examples would enhance understanding. Additionally, we suggest enhancing the results section's clarity, possibly through schematic illustrations. To better support claims regarding bias-variance trade-offs, we encourage the authors to quantify bias and variance in the empirical results and consider presenting relevant information from the appendix in the main text. Lastly, referencing non-neural network methods with strong theoretical rates and experimental results would strengthen the paper's context.