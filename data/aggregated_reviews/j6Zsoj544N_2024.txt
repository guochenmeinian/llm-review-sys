ID: j6Zsoj544N
Title: Does Worst-Performing Agent Lead the Pack? Analyzing Agent Dynamics in Unified Distributed SGD
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 6, 7, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an asymptotic analysis of Unified Distributed SGD (UD-SGD) under heterogeneous agent dynamics and various communication topologies. It establishes that, under specific assumptions—such as the regularity of the gradient, ergodicity of Markovian sampling, decreasing step sizes, stability of model parameters, and contraction properties of the communication matrix—each agent's parameter converges to a common minimum value. The authors also introduce a new central limit theorem that accounts for individual agent strategies, enhancing the understanding of linear speedup and asymptotic network independence in federated learning.

### Strengths and Weaknesses
Strengths:
1. The work provides a general framework for asymptotic analysis in distributed learning, extending beyond specific algorithms and communication topologies.
2. The introduction of UD-SGD offers a clear definition that aids in understanding the universality of the proposed properties.
3. The authors derive the exact form of the limiting covariance matrix for agents, enhancing theoretical clarity.
4. Numerical experiments support the theoretical claims, demonstrating the framework's applicability.

Weaknesses:
1. The claim regarding the uniqueness of convergence to $\theta^*$ is questionable in non-convex scenarios, as $\mathcal{L}$ may contain multiple local minima.
2. The reliance on Assumption 2.2, which requires ergodic Markov chains, raises questions about practical sampling strategies that meet this criterion.
3. The experimental section lacks comparisons with i.i.d. sampling and shuffling strategies, limiting the breadth of empirical validation.
4. Some theoretical claims, such as linear speedup, lack empirical justification, and the experiments on neural networks appear somewhat simplistic.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the uniqueness of convergence to $\theta^*$ in non-convex settings. Additionally, the authors should provide examples of sampling strategies that satisfy the ergodicity condition and discuss their advantages over i.i.d. methods. We also suggest including comparisons with i.i.d. sampling and shuffling strategies in the experimental section to broaden the empirical analysis. Finally, we encourage the authors to enhance the complexity of the neural network experiments to provide a more robust evaluation of their theoretical claims.