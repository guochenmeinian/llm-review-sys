ID: lckAdnVzsT
Title: Coherent 3D Scene Diffusion From a Single RGB Image
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 5, 5, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for 3D scene reconstruction from single RGB images using diffusion models. The authors propose a method that jointly predicts 3D object poses and shapes while capturing global scene context through a multi-head attention module. A surface alignment loss based on one-sided Chamfer Distance is introduced, allowing the model to utilize depth maps as supervision. Extensive experiments demonstrate the efficacy of the proposed method across various datasets.

### Strengths and Weaknesses
Strengths:
1. The method effectively addresses the ill-posed nature of 3D reconstruction by leveraging powerful generative priors from diffusion models.
2. The surface alignment loss enables the use of partial supervision from depth maps, enhancing model training.
3. Comprehensive experiments yield promising results in both scene and object reconstruction, showcasing improvements over previous methods.
4. The overall presentation and clarity of writing are commendable.

Weaknesses:
1. The model architecture lacks clarity, particularly regarding the integration of the independent diffusion models for object poses and shapes.
2. The reliance on one-sided Chamfer Distance may not adequately address errors in shape estimation, as seen in qualitative results.
3. The joint learning of poses may not generalize well to unseen object categories, raising concerns about the stability of estimated shape Gaussians.
4. The paper may overstate its contributions by implying full 3D scene reconstruction, as it primarily focuses on foreground objects without addressing background elements.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the model architecture in Section 3.7, explicitly detailing how the independent diffusion models are integrated. Additionally, addressing the limitations of one-sided Chamfer Distance in the context of shape estimation errors would strengthen the paper. We suggest including discussions on the generalization of the model to unseen categories and providing more insights into the training process for the shape decoder diffusion model. Finally, we encourage the authors to refine the writing for better precision, particularly in mathematical descriptions and notations.