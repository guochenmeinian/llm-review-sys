ID: KwRLDkyVOl
Title: Noise Contrastive Alignment of Language Models with Explicit Rewards
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 7, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a general framework for language model (LM) alignment that integrates Noise Contrastive Estimation (NCE) to effectively handle reward datasets annotated with scalar evaluations. The authors propose two algorithms, Noise Contrastive Alignment (NCA) and InfoNCA, which allow for the direct extraction of LM policies from both reward and preference data. The work addresses the limitations of Direct Policy Optimization (DPO) by demonstrating how NCA mitigates the "decreasing likelihood trend" associated with DPO, showing improved performance particularly in complex reasoning tasks.

### Strengths and Weaknesses
Strengths:
1. The integration of NCE with DPO provides a novel theoretical framework for LM alignment.
2. The proposed methods, NCA and InfoNCA, outperform existing preference-based methods by effectively utilizing both optimal and suboptimal responses from reward datasets.
3. The paper is well-written, with clear explanations and a solid empirical foundation supporting the claims.

Weaknesses:
1. The motivation for the paper is weak, particularly the assertion that DPO is limited to pairwise preference data, which contradicts existing literature.
2. The main experiments lack relevant baselines, such as comparing with PPO under the "Reward" annotation type.
3. The assumption that one of the scored responses is from the optimal policy is questionable, as most preference data may not reflect optimal responses.

### Suggestions for Improvement
We recommend that the authors improve the motivation section by accurately representing DPO's capabilities regarding multi-response preference data. Additionally, we suggest including relevant baseline comparisons, such as PPO and PRO, to strengthen the empirical evaluation. It would also be beneficial to address the fundamental assumption regarding the optimal policy and consider incorporating human-written responses into the preference data to enhance the robustness of the proposed methods. Furthermore, providing more detailed training information, including hyper-parameter search space, would improve the clarity of the methodology.