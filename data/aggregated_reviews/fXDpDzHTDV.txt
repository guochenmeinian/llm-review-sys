ID: fXDpDzHTDV
Title: DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 6, 7, 7, -1, -1
Original Confidences: 3, 5, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents StackFormer, a new architecture that effectively integrates fine-grained visual tokens from the CLIP vision transformer into the early layers of LLaVA-1.5 and LLaVA-Next language models without increasing the sequence length of visual tokens. The authors propose a method that enhances the resolution of the visual component of visual language models (VLMs) by splitting images into patches, applying ViT-CLIP to these patches, and using residual connections to embed the resulting high-resolution feature map into the LLM. StackFormer demonstrates improved performance on various benchmarks, including VQAv2, GQA, and Zero-shot Video QA.

### Strengths and Weaknesses
Strengths:  
- The proposed method is novel and diverges from the commonly used stringing method for visual tokens.  
- StackFormer shows significant performance improvements on traditional VQA datasets.  
- The approach is simple, effective, and well-documented, making it reproducible.  

Weaknesses:  
- Figures 1 and 2 lack details on the implementation of StackFormer, specifically regarding the splitting of high-resolution images into patches and the organization of visual tokens.  
- The paper does not provide specific indicators of overhead costs (flops, parameters, latency, memory consumption) for StackFormer compared to other VLMs.  
- There are numerous typographical errors, and the writing quality needs improvement.  
- Limited improvement is observed for LLaVA-Next on MLLM benchmarks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figures 1 and 2 by detailing how high-resolution images are split into patches and how visual tokens are organized with spatial dilation. Additionally, we suggest providing numerical indicators (flops, parameters, latency, memory consumption) to substantiate claims of StackFormer achieving the best trade-off between performance and effectiveness compared to other VLMs. Furthermore, we advise the authors to proofread the paper to correct typographical errors and ensure consistent terminology regarding multi-modal language models. Lastly, an explanation for the lack of significant improvement for LLaVA-Next on MLLM benchmarks should be included.