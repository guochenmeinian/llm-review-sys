ID: ZVRG3toCTT
Title: Beyond Confidence: Reliable Models Should Also Consider Atypicality
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 7, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 1, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of uncertainty quantification in classification through the lens of atypicality during recalibration. The authors propose measures of atypicality to enhance the understanding of classifier reliability, particularly in distinguishing between aleatoric and epistemic uncertainties. They argue that conventional confidence scores may conflate these uncertainties and introduce atypicality-aware recalibration as a solution. The paper includes extensive empirical evaluations across various tasks, demonstrating the potential benefits of the proposed approach.

### Strengths and Weaknesses
Strengths:
1. The paper raises a pertinent question regarding the interpretation of confidence scores in classifiers, addressing a significant gap in the literature.
2. The operationalization of atypicality is straightforward and supports the authors' arguments effectively.
3. Comprehensive empirical evaluations are conducted across diverse scenarios, enhancing the paper's credibility.

Weaknesses:
1. There are errors and deficiencies in definitions, which undermine the reliability of the work.
2. The critical section, Sec. 4.2, lacks necessary theoretical analysis, particularly regarding the validity and necessity of Eq. 2.
3. The presentation is cluttered, making it difficult to follow, and some concepts, such as the relationship between atypicality and calibration, require clearer articulation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of definitions, particularly for accuracy (Bm) and RMSE, ensuring that all terms are formally defined. Additionally, we suggest that the authors provide a more thorough theoretical analysis of Eq. 2 and explore alternative methods to the proposed approach. The authors should also clarify the rationale behind the atypicality definitions for NLP tasks and the computation of expected calibration error (ECE) on large-scale models. Finally, we advise expanding the discussion in Section 3.2 to draw insights from the theoretical results and enhance the overall presentation for better readability.