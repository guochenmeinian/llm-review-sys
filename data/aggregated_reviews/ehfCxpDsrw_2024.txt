ID: ehfCxpDsrw
Title: LinNet: Linear Network for Efficient Point Cloud Representation Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 8, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "LinNet: Linear Network for Efficient Point Cloud Representation Learning," which introduces a linear network aimed at efficient point cloud representation learning. The authors propose a novel disassembled set abstraction (DSA) module and a linear sampling strategy, enhancing computational efficiency and scalability. By mapping 3D point clouds onto 1D space-filling curves, the method allows for parallelized downsampling and neighborhood queries on GPUs with linear complexity, achieving state-of-the-art performance across various benchmarks.

### Strengths and Weaknesses
Strengths:
- The linear sampling strategy is elegant.
- The extensive comparison across multiple datasets and approaches is commendable.
- The authors plan to open their code upon acceptance.
- Well-conducted ablation studies clearly demonstrate the contribution of each proposed module.
- The reasoning behind DSA is well-motivated.
- The paper is well-written, with clear illustrations.
- Numerous assessments show the innovative nature of the approach, achieving state-of-the-art results.

Weaknesses:
- The significant memory footprint of the approach is a concern.
- Improvements in performance, while consistent, are relatively small (1-2%), raising questions about practical significance.
- The writing may be overly complex, making it difficult to follow core ideas.
- Some figures could be improved for clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing to make the core ideas and contributions more accessible. Additionally, conducting a small experiment to measure the memory footprint of the approach would be beneficial. It would also be interesting to include cross-validation tests to assess the generalization of the technique. Furthermore, we suggest analyzing the reasons behind the performance improvements compared to transformer-based methods, as well as including results from concurrent methods like PointTransformer v.3 for completeness. Lastly, addressing the potential issues with grid-based discretization artifacts in practical applications would enhance the robustness of the findings.