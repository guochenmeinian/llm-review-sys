ID: hD9TUV4xdz
Title: Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 5, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the relationship between optimal learning rate and batch size scaling for adaptive optimizers, specifically Sign SGD and Adam. Building on McCandlish (2018), the authors reveal a non-monotonic relationship where the optimal learning rate initially increases, peaks, and then decreases, leading to a saturation phenomenon termed the surge phenomenon. Theoretical predictions are supported by experiments in image classification and NLP tasks.

### Strengths and Weaknesses
Strengths:
- The paper provides new insights into the relationship between optimal learning rate and batch size for Adam optimizers, particularly the novel observation of a decrease in optimal learning rate after the peak.
- Prior results on square root scaling for Adam are reproduced, reinforcing the findings.

Weaknesses:
- The paper assumes familiarity with McCandlish (2018), which may hinder understanding; reiterating prior results would enhance motivation for the analysis.
- Empirical results lack convincing clarity; for instance, the surge phenomenon is not appreciably demonstrated in Figure 4 without a finer learning rate search.
- The theoretical results are derived for Sign SGD, raising questions about their generalizability to Adam, given the importance of Adam's beta1 and beta2 parameters.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their empirical results, particularly by conducting a finer learning rate search to better demonstrate the surge phenomenon. Additionally, it would be beneficial to provide more context for readers unfamiliar with McCandlish (2018) by reiterating key prior results. The authors should also clarify how the theoretical results can be generalized to Adam optimizers, considering the significance of the beta parameters. Lastly, addressing the practical implications of the decrease in optimal learning rate after the peak and proposing guidelines for scaling learning rate and batch size would enhance the paper's applicability.