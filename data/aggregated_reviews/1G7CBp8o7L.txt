ID: 1G7CBp8o7L
Title: Adapting Neural Link Predictors for Data-Efficient Complex Query Answering
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 7, 7, 5, 7, -1, -1, -1, -1
Original Confidences: 1, 5, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called CQD$^\mathcal{A}$ for answering complex queries on incomplete knowledge graphs. The authors address the challenge of missing knowledge by recalibrating neural link prediction scores through a parameter-efficient adaptation model. This model, which keeps the neural link predictor frozen, significantly enhances accuracy and data efficiency, achieving higher Mean Reciprocal Rank values compared to state-of-the-art methods while requiring less training data. The empirical evaluations demonstrate CQD$^\mathcal{A}$'s robustness and effectiveness across various benchmarks.

### Strengths and Weaknesses
Strengths:
- The paper introduces a unique approach that effectively addresses the limitations of existing methods in complex query answering.
- CQD$^\mathcal{A}$ shows improved accuracy and data efficiency, achieving competitive results with only 1% of the training data.
- The writing is clear and well-structured, making the proposed approach easy to follow.
- Extensive evaluations confirm the model's robustness and its ability to handle negations.

Weaknesses:
- The contribution appears incremental, as it builds upon existing work without substantial novelty.
- There is a lack of discussion regarding the limitations of the proposed method.
- The interpretability of results could be better illustrated with concrete examples.
- The paper does not adequately analyze the impact of reducing training data on performance across different query types.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the limitations of their work, including potential societal impacts. Additionally, providing concrete examples to illustrate the interpretability of results would enhance clarity. A more detailed analysis of how reducing training data affects model performance across various query types and datasets should be included. Finally, discussing potential future directions for research, such as exploring CQD$^\mathcal{A}$'s applicability in different domains, would strengthen the paper.