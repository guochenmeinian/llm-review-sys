ID: YkJ5BuEXdD
Title: Preference Learning Algorithms Do Not Learn Preference Rankings
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 6, 3, 5, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the effectiveness of preference learning algorithms, specifically RLHF and DPO, in training LLMs to rank human-preferred outputs. The authors challenge the assumption that these algorithms can effectively teach models to rank outputs according to human preferences, revealing that many state-of-the-art models achieve low ranking accuracy, often below 60%. The study identifies a significant alignment gap due to the limitations of the DPO objective and provides both theoretical and empirical analyses, concluding that a reevaluation of these algorithms is necessary.

### Strengths and Weaknesses
Strengths:  
- The paper offers a thorough empirical analysis, demonstrating the limited effectiveness of RLHF and DPO in improving ranking accuracy, which is a significant contribution to the field.  
- It provides theoretical insights into the failure of these algorithms to correct ranking errors, enhancing understanding of preference learning issues for LLMs.  
- The extensive empirical study covers various state-of-the-art preference-tuned models across multiple datasets, providing robust evidence of the alignment gap between idealized and learned policies.

Weaknesses:  
- The motivation for studying ranking accuracy is weak and not well justified, with concerns about whether completely discarding the reference model is reasonable.  
- The analysis of ranking accuracy is somewhat superficial, lacking depth in exploring the implications of using ranking accuracy versus win rate as performance measures.  
- The paper does not sufficiently address the potential trade-offs between performance on instruction-following benchmarks and academic benchmarks, nor does it explore the soft ranking accuracy versus hard ranking accuracy.

### Suggestions for Improvement
We recommend that the authors improve the justification for studying ranking accuracy, possibly by including experimental results on the gap in reward accuracy between the learned and ideal policies. Additionally, we suggest that the authors explore the implications of using ranking accuracy versus win rate more explicitly, including potential discrepancies and how they might be reconciled. It would also be beneficial to discuss techniques or algorithmic adjustments that could alleviate the heavy reliance on the reference model. Finally, we encourage the authors to consider reporting both soft and hard ranking accuracy to provide a more comprehensive evaluation of their findings.