ID: RGmQOhSGp0
Title: Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a two-stage framework for detecting harmful memes, utilizing the reasoning capabilities of Large Language Models (LLMs) to enhance multimodal fusion and fine-tuning for harmfulness prediction. The first stage involves distilling multimodal reasoning from LLMs, while the second stage focuses on harmfulness inference. The authors validate their approach through experiments on three datasets, demonstrating its effectiveness compared to existing methods. This work is notable for being one of the initial explorations of LLMs in a multimodal context, contributing to the understanding of harmful meme detection.

### Strengths and Weaknesses
Strengths:
- The research topic is significant, addressing the pressing issue of harmful content on social media.
- The integration of LLMs is straightforward yet effective, with comprehensive experiments supporting the findings.
- The paper is well-structured and presents clear case studies, enhancing understanding.

Weaknesses:
- The techniques employed are largely existing and well-known, with the primary contribution being the introduction of LLMs to this specific task.
- Baseline comparisons lack fairness, as they do not utilize LLMs, leaving uncertainty about their performance if equipped with such models.
- The write-up quality requires improvement, and the evaluation of the proposed approach across diverse datasets is insufficient.

### Suggestions for Improvement
We recommend that the authors improve the clarity and quality of the write-up significantly. Additionally, the authors should expand the evaluation to include more diverse task categories, such as emotions and offensiveness, to establish better generalizability. It is crucial to address the potential variability in reasoning quality from LLMs by implementing a filtering mechanism. Furthermore, the authors should clarify how multimodal information from images is integrated into the reasoning process and discuss the limitations of the datasets used, considering the dynamic nature of memes. Lastly, we suggest incorporating a broader range of recent SOTA models in the evaluation to enhance the robustness of comparisons.