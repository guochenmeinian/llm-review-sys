ID: nYbOG9EaxD
Title: A Question Answering Framework for Decontextualizing User-facing Snippets from Scientific Documents
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for decontextualizing scientific documents by dividing the task into question generation, answering, and rewriting. The authors conduct a human study to analyze user needs and challenges in decontextualization, utilizing various combinations of large language models (LLMs) for the pipeline. The evaluation includes ablation studies that demonstrate the performance gap between gold-standard and model-generated outputs.

### Strengths and Weaknesses
Strengths:
- The experimental section is well-structured, providing insightful analysis.
- The problem addressed is both challenging and relevant, appealing to a broader audience.
- The paper is scientifically sound, with a comprehensive state-of-the-art review and convincing evaluations.

Weaknesses:
- Missing error analysis and significance tests weaken the findings.
- The English language quality requires improvement, affecting readability.
- The dataset is small, which may introduce biases and limit reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by rewriting sections for better readability, particularly the abstract, which was confusing. Specific phrases, such as "Many real-world applications..." and "Yet, snippets divorced from their origin..." should be rephrased for clarity. Additionally, consider splitting long sentences for easier comprehension and providing examples for each step in the framework to enhance understanding.

We also suggest including a comparison with established baselines, such as a fine-tuned seq2seq system from Choi et al. 2021, to strengthen the evaluation. Furthermore, we recommend discussing the implications of using zero-shot versus few-shot settings in LLM prompts, as this could significantly impact performance. Lastly, addressing potential biases related to the dataset and the use of commercial models would enhance the paper's robustness.