ID: 5r3e27I9Gy
Title: Composing Parameter-Efficient Modules with Arithmetic Operation
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 3, 7, 7, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for composing parameter-efficient fine-tuning (PEFT) modules directly in weight space, utilizing linear combinations to achieve effective performance in distribution generalization, multitasking, detoxification, and domain transfer. The authors evaluate their approach on both T5 and LLaMA models, demonstrating that their compositions are less sensitive to initialization compared to adapters. The study includes extensive experiments to support their claims.

### Strengths and Weaknesses
Strengths:
- The paper is clear, well-structured, and easy to follow.
- The method is straightforward and well-tested across diverse settings, making it interesting for practitioners.
- The introduction of the negation operator allows for more complex operations in module composition.

Weaknesses:
- The idea is not novel, as parameter ensemble methods have been previously proposed, and important references are missing.
- The analysis lacks depth, making it difficult to draw strong conclusions from the findings.
- There is insufficient comparison with existing approaches, particularly regarding the performance of the addition operator alone.
- The choice of the hyperparameter lambda needs clearer explanation, as its tuning may complicate practical application.

### Suggestions for Improvement
We recommend that the authors improve the novelty discussion by explicitly comparing their work with existing literature, particularly addressing the missing references. Additionally, the authors should delve deeper into their analysis to provide a more comprehensive understanding of the benefits of parameter averaging. Clarifying the procedure for selecting lambda and including comparisons to full dataset fine-tuning results would enhance the paper's robustness. Finally, we suggest incorporating more automatic metrics for evaluating detoxification performance and exploring the implications of using GPT-4 for model evaluation to mitigate potential biases.