ID: rVSc3HIZS4
Title: Multi-turn Reinforcement Learning with Preference Human Feedback
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel mirror-descent-based policy optimization algorithm for multi-turn preference-based reinforcement learning (RL) in a tabular setting, demonstrating convergence to Nash equilibrium. The authors evaluate the algorithm's performance in the Education Dialogue environment, where a teacher agent guides a student. The work extends the RLHF paradigm to the multi-turn context, proposing methods such as Multi-turn RLHF, MTPO, and MTPO-$\tau$, and provides proofs of convergence. Additionally, the authors propose a methodology for implementing a single-turn algorithm that requires preference feedback between two immediate responses in each turn. This methodology generates partial conversations using the SFT policy, from which two independent answers are derived. Preference feedback is obtained through two methods: Single-turn-reward, which utilizes a modified preference prompt to evaluate responses based on their conversational impact, and Single-turn-value, where the SFT policy continues the conversations to completion before assessing preference. The input for the algorithm consists of datasets of partial conversations, immediate responses, and their preferences, enabling the learning of a reward model through standard RLHF techniques.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and easy to understand.  
- It extends the RLHF paradigm to the multi-turn setting, capturing long-term effects of actions.  
- The theoretical discussions and proofs are comprehensive, particularly regarding mirror descent policy optimization.  
- The methodology for generating preference feedback is clearly articulated, detailing the generation of preference feedback and the use of SFT policy.  
- The dual approach for obtaining preference feedback (Single-turn-reward and Single-turn-value) is innovative and well-justified.  

Weaknesses:  
- The experiments utilize a small T5 model, limiting generalizability to larger-scale models prevalent in current LLMs.  
- Evaluation relies solely on reward-based judgments, lacking human evaluation, which may not fully reflect human preferences in complex conversations.  
- The definition of Nash Equilibrium may be inaccurately presented, and the paper lacks citations of foundational game theory literature.  
- The explanation of the modified preference prompt lacks sufficient detail, which may hinder reproducibility.  
- The paper could benefit from a more comprehensive discussion on the implications of the chosen methods on overall model performance.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by conducting experiments with diverse small-scale models, such as those around 2B parameters. Additionally, incorporating human evaluations would enhance the assessment of conversation quality. We suggest clarifying the definition of Nash Equilibrium and including relevant citations to foundational works. Furthermore, the authors should provide a more detailed explanation of the "anchor policy" concept and the derivation from Equation (1) to Equation (2) to improve clarity. We also recommend improving the clarity and detail of the modified preference prompt to enhance reproducibility. Lastly, incorporating a more thorough discussion on how the proposed methods impact the performance of the model would provide insights into their effectiveness and potential limitations.