ID: Kxta8IInyN
Title: CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called Clave, which integrates multiple LLMs to evaluate the values of texts, alongside a dataset named ValEval that standardizes the value evaluation process. The framework utilizes a strong LLM for extracting value concepts and a smaller LLM that can be fine-tuned for various value systems. The authors benchmark 12 popular LLM evaluators using this approach, demonstrating its adaptability and generalizability.

### Strengths and Weaknesses
Strengths:
- The authors unify three value-specific datasets into a single evaluation method.
- The Clave framework effectively splits responsibilities between a strong LLM and a weaker, cost-effective LLM.
- The ValEval dataset offers a robust foundation for fine-tuning and evaluating LLMs across different ethical value systems.
- The paper clearly articulates the generalizability and adaptability challenges in evaluating values in LLMs.

Weaknesses:
- The marginal contribution compared to existing datasets could be better articulated.
- The discussion of limitations is insufficient, particularly regarding the trade-offs between adaptability and accuracy.
- Writing clarity could be improved, particularly in explaining generalizability and adaptability.
- Some language anthropomorphizes the model, leading to potential overclaims.

### Suggestions for Improvement
We recommend that the authors improve the engagement with related work, specifically citing "Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties" (Sorensen et al., 2023), and benchmark against it to clarify similarities and differences. Additionally, the authors should enhance the discussion of limitations, including methodological decisions and the implications of using weaker LLMs. We suggest tightening the writing to clarify concepts and providing clearer examples to motivate the framework. Finally, including confidence intervals or error bars in Figure 3 would aid in understanding the significance of the results.