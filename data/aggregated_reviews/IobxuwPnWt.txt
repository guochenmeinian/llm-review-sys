ID: IobxuwPnWt
Title: SLM: A Smoothed First-Order Lagrangian Method for Structured Constrained Nonconvex Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 4, 4, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a smoothed first-order Lagrange method (SLM) designed to address structured nonconvex functional constrained optimization (FCO) problems, particularly in the context of bilevel optimization (BO) with nonconvex lower-level problems. The authors establish theoretical convergence rates, demonstrating that SLM converges to $\epsilon$-KKT points with rates of $\mathcal{O}(\epsilon^{-1})$ for FCO problems and $\mathcal{O}(\epsilon^{-2})$ for BO problems under the P≈Å condition. The method is empirically validated against existing algorithms, showing superior performance in both a toy example and a data hyper-cleaning problem.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel algorithm for a class of structured FCO and nonconvex BO problems.
- The convergence analysis provided is thorough and supports the algorithm's effectiveness.

Weaknesses:
- The absence of formal definitions for $\epsilon$-KKT points and $\epsilon$-stationary points complicates understanding.
- Assumption A4 lacks clarity, particularly regarding its validity when $\delta=0$.
- Numerous errors in the proofs undermine the rigor of the theoretical claims, including issues with gradient Lipschitz constants and the proof of Lemma 6.
- The authors need to clarify the theoretical novelty of their work relative to existing literature, particularly reference [1].

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions for $\epsilon$-KKT points and $\epsilon$-stationary points to enhance the paper's accessibility. Additionally, further elaboration on Assumption A4, especially its applicability when $\delta=0$, would strengthen the argument. We urge the authors to meticulously review the proofs to correct identified errors, such as the missing $p$ in the gradient Lipschitz constant and the justification for the maximizer in the objective function. Lastly, the authors should explicitly delineate how their contributions differ from those in reference [1] to clarify the novelty of their approach.