ID: 5cPz5hrjy6
Title: Replicability in Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 7, 6, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical study of replicability in reinforcement learning (RL), focusing on discounted tabular Markov Decision Processes (MDPs) with generative models. The authors define replicability in terms of an algorithm's ability to return the same policy across different random draws, with a specified probability $\rho$. They establish a sample complexity lower bound for a $\rho$-replicable $(\varepsilon, \delta)$-optimal algorithm and propose an algorithm achieving a sample complexity of $\tilde O(N^3)$. Additionally, they explore a weaker formulation called approximate-replicability, which allows for a reduced sample complexity of $\tilde O(N)$. The results provide both upper and lower bounds, contributing novel insights to the field.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel framework for studying replicability in RL, addressing a previously unexplored area in the literature.
2. It provides a comprehensive analysis of the problem, presenting tight upper and lower bounds for sample complexity.
3. The writing is clear and accessible for readers familiar with theoretical RL concepts.

Weaknesses:
1. The motivation for the importance of replicability in RL is insufficiently articulated, as the focus on identical policies may not align with the primary goal of maximizing rewards.
2. Certain definitions, such as Definition 2.6 and Definition 2.9, lack clarity regarding the algorithm's ability to request specific state-actions, leading to potential ambiguities.
3. The notation used for randomness ($\bar{r}$) may cause confusion due to existing symbols for rewards.
4. The paper could benefit from additional explanations of the algorithm, particularly the replicable rounding procedure, to enhance understanding.

### Suggestions for Improvement
We recommend that the authors improve the motivation for why replicability is significant in RL, particularly addressing the distinction between policy replicability and reward maximization. Clarifying Definitions 2.6 and 2.9 to explicitly state the algorithm's capabilities regarding state-action requests would enhance comprehension. Additionally, we suggest revisiting the notation for randomness to avoid confusion with reward symbols. Finally, providing more detailed explanations of the algorithm, especially the replicable rounding procedure, would be beneficial for readers unfamiliar with these concepts.