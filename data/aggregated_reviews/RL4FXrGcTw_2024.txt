ID: RL4FXrGcTw
Title: Gradients of Functions of Large Matrices
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 8, 7, 6, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new matrix-free method for automatically differentiating functions of matrices, addressing computational challenges in scenarios where matrix dimensions scale with dataset size. The authors propose an algorithm that efficiently computes exact gradients of the forward pass in linear time and memory complexity, utilizing Lanczos and Arnoldi iterations to evaluate functions of large matrices. The method allows for differentiation via adjoints of these iterations, with experimental results demonstrating significant speedups over existing methods in Gaussian processes, physics-informed PDEs, and Bayesian neural network tuning.

### Strengths and Weaknesses
Strengths:
- The paper addresses a critical computational bottleneck in hyperparameter tuning for complex models in scientific and machine learning applications.
- The proposed method is general, accommodating any matrix functionals without requiring explicit gradient derivations.
- Experimental results on relevant problems showcase impressive performance improvements.

Weaknesses:
- The motivation for the method's exact gradient claims is unclear, particularly when the Lanczos and Arnoldi iterations do not yield exact factorizations.
- Empirical results lack convincing evidence of significant improvements over existing methods, particularly in terms of RMSE and performance comparisons.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind their method, explicitly discussing the benefits of their approximation compared to previous work. Additionally, we suggest providing more robust empirical results that convincingly demonstrate the advantages of the proposed method over existing techniques, particularly in terms of RMSE. A comparative analysis with reverse-mode automatic differentiation could further clarify the distinctions and advantages of the adjoint method. Finally, addressing the concerns regarding the performance metrics used in the experiments, particularly in Table 3, would enhance the paper's impact.