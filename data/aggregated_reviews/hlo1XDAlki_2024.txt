ID: hlo1XDAlki
Title: Can Knowledge Editing Really Correct Hallucinations?
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 7
Original Confidences: 4, 4

Aggregated Review:
### Key Points
This paper presents a novel benchmark, **HalluEditBench**, designed for real-world hallucination correction, offering a comprehensive evaluation across five dimensions: **Efficacy**, **Generalization**, **Portability**, **Locality**, and **Robustness**. The dataset is well-tagged for various topics and includes questions aimed at assessing model knowledge and reasoning abilities. However, the analysis lacks synthesis and generalized insights, presenting merely a collection of observations without explaining their implications or underlying reasons.

### Strengths and Weaknesses
Strengths:
- **Novel Benchmark**: Introduction of **HalluEditBench** for hallucination correction.
- **Holistic Evaluation**: Comprehensive assessment across multiple dimensions.
- **Good Motivation**: Well-structured breakdown in section 2.2 and effective methodology illustration in Figure 1.

Weaknesses:
- **Weak Problem Definition**: Ambiguity regarding the limitations of previous datasets and the novelty of the proposed framework.
- **Limited Model Scope**: Evaluations restricted to smaller models (**Llama2-7B**, **Llama3-8B**, **Mistral-v0.3-7B**), lacking diversity.
- **Lack of Domain Explanation**: Insufficient rationale for the selection of **9 domains**.
- **Lack of Method Interpretation**: No clear interpretation of knowledge editing method performance, particularly regarding domain and LLM dependencies.
- **Synthetically Generated Questions**: Evaluation relies solely on synthetically generated QA pairs, lacking human validation for relevance and quality.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the introduction by defining key terms such as “efficacy” and “generalization” before presenting results. Additionally, the authors should clarify the use of the term “hallucination” to encompass all cases, including those due to lack of knowledge. Including error bars on plots would enhance the understanding of variance in model responses. The authors should also statistically validate observed differences across domains and provide explanations for these patterns. Addressing the validation process for the dataset questions, particularly for portability, is essential. Lastly, we suggest revising Figure 2 for readability and ensuring the abstract highlights the main novel findings more explicitly.