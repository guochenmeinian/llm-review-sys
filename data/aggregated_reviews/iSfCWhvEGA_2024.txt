ID: iSfCWhvEGA
Title: Learn To be Efficient: Build Structured Sparsity in Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called Learning-to-be-efficient (LTE) aimed at enhancing structured sparsity in large language models (LLMs) during training. The authors propose a new training loss that encourages fewer neuron activations while preserving performance, alongside a threshold-based sigmoid routing strategy for adaptive expert selection. The method is implemented with a custom CUDA kernel, demonstrating effectiveness across various models and NLP tasks, outperforming existing baselines in terms of FLOPs and latency reduction.

### Strengths and Weaknesses
Strengths:
- The method exhibits good generality, applicable to various activation functions.
- The designed separability loss is intuitive and effective.
- Experiments across diverse tasks validate the method's effectiveness.
- The custom CUDA kernel implementation accelerates computational time.

Weaknesses:
- The overall speedup is limited due to the dense models' FFN to attention ratio; higher FFN proportions may yield better acceleration.
- The two-step training process complicates implementation and increases computational demands.
- Some experimental and methodological details require further discussion, particularly regarding expert selection and threshold principles.
- The novelty of the approach is somewhat limited, as similar concepts exist in the MoE literature.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the implementation of Dejavu in the context of LTE, clarifying how it aligns with the proposed training method. Additionally, we encourage the authors to explore the potential for further improvement in clustering W_1 and to provide insights into the limitations of Softmax routing. It would also be beneficial to compare LTE against a broader range of sparsity baselines, including structured weight pruning, to strengthen the paper's contributions. Finally, we suggest that the authors consider continued pretraining across tasks to validate the generalization capability of the proposed method.