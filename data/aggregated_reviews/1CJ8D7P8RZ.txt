ID: 1CJ8D7P8RZ
Title: PoET: A generative model of protein families as sequences-of-sequences
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 5, 8, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Protein Evolutionary Transformer (PoET), an autoregressive generative model designed to generate protein families. The model utilizes a novel Transformer architecture that incorporates a TieredTransformerDecoder to manage order-dependence within sequences and order-independence across sequences. PoET can generate new sequences and assess mutation effects without relying on multiple sequence alignments (MSAs), achieving significant performance improvements on protein variant fitness tasks. The experiments demonstrate its effectiveness, particularly on the ProteinGym benchmark. Additionally, the paper analyzes the sensitivity of PoET to the order of sequences by comparing held-out sequence perplexities and joint log likelihoods across three different orderings: random, shortest-to-longest, and longest-to-shortest. The findings indicate that sequence ordering has minimal impact on next sequence perplexity and joint log likelihoods, with only a slight average change of 0.00753 for perplexity. Furthermore, PoET outperforms baseline models regardless of the ordering used.

### Strengths and Weaknesses
Strengths:
- The proposed model achieves remarkable performance on zero-shot fitness prediction tasks, demonstrating its effectiveness.
- The architecture is simple yet effective, and the ablation studies provide valuable insights into optimal prompt engineering strategies.
- The related work section is well-documented, clarifying the authors' contributions.
- The experiments demonstrate that PoET's performance is robust against variations in sequence ordering, which is beneficial for practical applications.
- The authors have responded promptly to reviewer feedback and provided additional analyses that enhance the manuscript's clarity.

Weaknesses:
- The exploration of sequence generation capabilities is limited, with only one example provided in the appendix.
- The autoregressive decoding method raises questions about soundness, particularly regarding order dependence in output probabilities.
- The paper lacks a thorough discussion of limitations, and some claims are not sufficiently substantiated.
- The DMS-level performance of PoET alone has not been adequately addressed, particularly in relation to variance across assays compared to other models.

### Suggestions for Improvement
We recommend that the authors improve the exploration of sequence generation capabilities by including more examples beyond the single case in the appendix. Additionally, we suggest addressing the soundness of the autoregressive decoding method by clarifying how order dependence is managed during inference. It would also be beneficial to add a dedicated "Limitations" section to discuss potential shortcomings and societal impacts of the work. Furthermore, we recommend that the authors improve the manuscript by adding PoET's performance data to Fig S1/S2 to provide insights into its DMS-level performance relative to other models. Including the results of the sensitivity analysis in the revised version would further strengthen the paper. Finally, we encourage the authors to provide more comprehensive comparisons with existing fitness prediction baselines and to report averages over multiple runs for the ablation results to strengthen the observed trends.