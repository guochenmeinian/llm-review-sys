ID: 9yQ2aaArDn
Title: Probabilistic Inference in Reinforcement Learning Done Right
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 7, 3, 6, 6, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a model-based reinforcement learning (RL) algorithm, VAPOR, which treats reinforcement learning as a Bayesian variational inference problem. It approximates the posterior probability of state-action optimality and computes an optimal occupancy measure and policy. The authors analyze the Bayesian regret of VAPOR, demonstrating a sub-linear bound, and evaluate its performance in various environments, including GridWorld, DeepSea, and Atari. Additionally, the paper introduces the VAPOR optimization objective, which approximates the optimal policy while accounting for epistemic uncertainty, proposing to optimize in the space of occupancy measures to target the optimal policy's occupancy measure. However, the framing of the paper is contentious, particularly regarding the characterization of VAPOR as Bayesian inference, which is debated due to the lack of traditional observations or posteriors.

### Strengths and Weaknesses
Strengths:  
- The paper provides a rigorous Bayesian treatment of state-action optimality, contributing a novel variational optimization approach.  
- The motivation and theoretical foundations of VAPOR are well-explained, with clear performance analysis and comparisons to existing methods.  
- The writing is clear, and the paper is well-organized, making it accessible.  
- The introduction of a tractable optimization objective, VAPOR, which has the potential to contribute to the field, is a notable strength.  
- The approach of optimizing occupancy measures is a novel insight.

Weaknesses:  
- The abstract is misleading, suggesting that the authors address issues with RL as inference when they primarily utilize a model for action computation.  
- The paper lacks sufficient discussion of existing literature on Bayesian model-based RL, particularly regarding its relationship to other approaches like Thompson sampling and K-learning.  
- There are inconsistencies in the definition and role of the hyper-parameter $\sigma$, which could lead to errors in the algorithm's analysis.  
- The evaluation of VAPOR-lite in complex environments is limited, with insufficient details on its implementation and performance.  
- The presentation is unclear, with insufficient discussion of existing literature and limitations.  
- The claims regarding VAPOR's capabilities are overstated, particularly concerning its applicability and the assumptions made.

### Suggestions for Improvement
We recommend that the authors improve the abstract to accurately reflect the contributions of the paper and clarify the relationship of their work to existing literature on Bayesian model-based RL. Additionally, the authors should provide a more thorough discussion of the hyper-parameter $\sigma$, including whether it is constant or follows a decay process, and correct any inconsistencies in its treatment. Expanding on the implementation details and performance results for VAPOR-lite in complex environments would also enhance the paper's contribution. We suggest that the authors improve the clarity of the paper by explicitly stating the limitations of VAPOR throughout, especially in the abstract. The discussion of existing literature should be expanded to include a coherent analysis of the shortcomings identified. Furthermore, VAPOR-lite should be given a more prominent position in the main body of the paper, with adequate analysis to support its applicability outside the tabular setting. Finally, we recommend that the authors provide a thorough analysis of the posterior concentration and clarify their assumptions regarding priors and likelihoods to enhance accessibility and understanding.