ID: bUgqyyNo8j
Title: Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper examines reparameterization (RP) gradient estimators for policy gradient methods (PGM) in reinforcement learning, focusing on the chaotic optimization landscape and exploding gradient variance. The authors theoretically analyze the impact of the smoothness of function approximators on gradient estimator quality and propose using spectral normalization (SN) to enforce smoothness in neural networks. Their empirical results indicate that this modification effectively mitigates gradient issues, leading to improved performance in various control tasks. Additionally, the authors respond to reviewer feedback by addressing the introduction of deterministic policies (DP) and stochastic policies (DR), acknowledging the use of stochastic policies while suggesting that exploring other environments with stochastic transitions could enhance the experiments' comprehensiveness. They emphasize the importance of the step size $h$ in relation to model accuracy and the tradeoff between model and critic error.

### Strengths and Weaknesses
Strengths:  
- The paper provides valuable insights into the relationship between function approximator smoothness and RP gradient performance, contributing significantly to the field.  
- The theoretical foundations are robust, and the empirical results support the proposed spectral normalization method, enhancing the clarity and organization of the findings.  
- The authors effectively addressed the tautology issue in the introduction and acknowledge the use of stochastic policies, showing openness to exploring additional environments to improve comprehensiveness.  
- The writing is clear and accessible, making complex concepts easier to understand.

Weaknesses:  
- Clarity issues arise from low-quality figures, which hinder readability and comprehension. Key figures lack sufficient detail, and the absence of hyperparameter reporting raises reproducibility concerns.  
- The application of spectral normalization is not novel, and its potential limitations on model capacity and convergence speed are not thoroughly explored.  
- The authors did not fully resolve concerns regarding the optimal step size $h$ and the necessity of using SN when good performance can be achieved without it.  
- The theoretical claims, particularly regarding convergence, lack practical guidance and do not adequately address the implications of the assumptions made. Additionally, the language used regarding the application of SN may imply it is applied post-training, which could misrepresent its role during training.

### Suggestions for Improvement
We recommend that the authors improve the clarity of figures by increasing font sizes and moving some figures earlier in the paper for better visibility. Additionally, please provide detailed hyperparameter settings to enhance reproducibility. It would be beneficial to investigate the impact of setting the spectral norm to values lower than 1 and to explore the effects of longer rollout horizons on sample complexity. We also suggest discussing the trade-offs between gradient variance and bias more comprehensively, particularly in relation to the limitations of spectral normalization. Lastly, please clarify the assumptions regarding Lipschitz continuity and the implications of model updates on these assumptions. Furthermore, we recommend improving the clarity regarding the application of spectral normalization during training to avoid misinterpretation and further clarifying the rationale behind the choice of step size $h$ and its implications for model accuracy, particularly in relation to the necessity of SN. Exploring environments with stochastic transitions should also be prioritized in future work to enhance the comprehensiveness of the experiments.