ID: J3w0AXtEhp
Title: Uniform Last-Iterate Guarantee for Bandits and Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 6, 5, 6, -1, -1
Original Confidences: 4, 3, 2, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a new metric for sample efficiency in online learning called Uniform Last Iterate (ULI), which is shown to be stronger than existing metrics such as regret and uniform-PAC. The authors propose algorithms that achieve near-optimal ULI in various settings, including multi-armed bandits, linear bandits, and tabular MDPs. The paper highlights that algorithms based on action elimination outperform optimistic algorithms in achieving near-optimal ULI. Additionally, ULI encapsulates instantaneous performance, making it relevant for high-risk applications.

### Strengths and Weaknesses
Strengths:
- The introduction of ULI as a metric is valuable, capturing both cumulative and instantaneous performance.
- The algorithms presented achieve near-optimal ULI, providing significant insights into the effectiveness of action elimination versus optimistic approaches.
- The paper is well-written, clearly explaining the new concept, its motivation, and its differences from existing metrics.

Weaknesses:
- The potential impact of the paper appears limited, as the insights regarding action elimination versus optimistic algorithms have been previously observed, and optimistic algorithms are rarely utilized in practice.
- The paper's structure could be improved; important concepts should be included in the main text rather than relegated to the appendix, and proof sketches for theorems would enhance reader understanding.
- The computational efficiency of the RL algorithm is inadequate, raising questions about the claim of achieving near-optimal ULI in MDPs, especially since efficient algorithms already exist for tabular MDPs.

### Suggestions for Improvement
We recommend that the authors improve the paper's structure by integrating key insights and proof sketches into the main text to enhance clarity. Additionally, addressing the computational inefficiency of the RL algorithm is crucial; the authors should clarify why existing efficient algorithms do not achieve near-optimal ULI. Finally, we encourage the authors to explore potential future work that builds on their findings to enhance the paper's impact.