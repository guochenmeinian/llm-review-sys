ID: mtyy3Myyhz
Title: S2HPruner: Soft-to-Hard Distillation Bridges the Discretization Gap in Pruning
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 6, 5, 6, -1, -1
Original Confidences: 5, 3, 5, -1, -1

Aggregated Review:
### Key Points
This paper presents "S2HPruner: Soft-to-Hard Distillation Bridges the Discretization Gap in Pruning," which introduces a framework to tackle the discretization gap in neural network pruning. The authors propose a method that utilizes a soft network with differentiable masks and a hard network with binary masks, aiming to enhance pruning performance without post-pruning fine-tuning. The method was evaluated on datasets such as CIFAR-100, Tiny ImageNet, and ImageNet across various models.

### Strengths and Weaknesses
Strengths:
1. The S2HPruner framework innovatively addresses the discretization gap, ensuring that the hard network retains performance similar to the soft network.
2. The writing is clear and the work is easy to follow.
3. The experiments and investigation into gaps and gradients are well-developed.

Weaknesses:
1. Figures, particularly Figure 2, require improvement to better illustrate the overall pipeline, specifically by showing the soft and hard networks rather than just their outputs.
2. The abstract incorrectly mentions 'bidirectional' without clarification; it should specify that S2HPruner only uses sparsity in forward flows.
3. The method section lacks a description of how S2HPruner controls network sparsity, which raises questions about its effectiveness.
4. The formats of the tables are inconsistent, and Table 4 should indicate training epochs due to varying training settings.
5. The baselines in Table 4 are outdated and should include comparisons with recent channel pruning methods like SCOP and CHEX.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 2 by explicitly showing the soft and hard networks. Additionally, please clarify the term 'bidirectional' in the abstract and provide justification for using Kullback Leibler divergence as a gap measure, possibly with corresponding experiments. We also suggest including a detailed description of how S2HPruner controls network sparsity in the method section. Furthermore, ensure consistent formatting across tables and update Table 4 with the number of training epochs and comparisons to the latest channel pruning methods. Lastly, justify the choice of baseline accuracy for ResNet-50 used in the comparisons.