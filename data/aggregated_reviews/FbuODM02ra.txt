ID: FbuODM02ra
Title: Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 7, 5, 6, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of the challenge posed by noisy rationales in large language models (LLMs), introducing the NoRa dataset specifically designed to evaluate LLMs' robustness to such noise. The authors reveal a significant vulnerability among LLMs to noisy rationales, despite advancements in in-context learning, and propose the CD-CoT method to enhance denoising-reasoning capabilities by contrasting noisy rationales with clean ones. Comprehensive evaluations demonstrate that CD-CoT significantly improves LLM performance by rectifying noisy rationales, contributing to the formalization of the noisy rationales problem and the construction of the NoRa dataset. Additionally, the paper analyzes the performance of the SD method in symbolic equal and commonsense tasks, identifying inconsistencies in SD's performance, particularly in how it handles noise in these tasks.

### Strengths and Weaknesses
Strengths:
1. The manuscript addresses an under-explored challenge in LLMs, focusing on noisy rationales in chain-of-thought prompting, which is relevant across various domains.
2. The NoRa dataset serves as a comprehensive testbed for evaluating LLMs' robustness, covering diverse reasoning tasks and enhancing reliability through controlled noise ratios.
3. The thorough evaluation reveals LLMs' intrinsic vulnerabilities and motivates the development of robust methods.
4. The authors provide a detailed examination of the empirical results, highlighting specific behaviors of the SD method in different task settings.
5. The inclusion of representative examples effectively illustrates the issues encountered with SD's rationale generation.

Weaknesses:
1. The manuscript lacks extensive comparisons of CD-CoT with existing denoising or reasoning enhancement techniques, which would clarify its effectiveness.
2. There is insufficient analysis of the underlying causes of LLMs' vulnerability to noisy rationales, limiting understanding and potential research directions.
3. The authors do not provide adequate details on the mechanisms of CD-CoT, particularly regarding the "Rationale Selection" step for answer matching.
4. Q3 remains unaddressed, specifically regarding why w/SD outperforms in commonsense/symbolic equal tasks despite using irrelevant and inaccurate rationales compared to clean ones.
5. The analysis does not sufficiently explore the implications of SD's performance inconsistencies on overall task accuracy.

### Suggestions for Improvement
We recommend that the authors improve the comparative analysis of CD-CoT with other existing methods to better illustrate its advantages. Additionally, a deeper exploration of the reasons behind LLMs' vulnerabilities to noisy rationales should be included to enhance understanding. The authors should also clarify the specific techniques employed in the "Rationale Selection" step of CD-CoT to provide a clearer understanding of its operational mechanisms. Furthermore, we recommend that the authors explicitly address the reasons behind the performance differences of w/SD in commonsense and symbolic equal tasks. Lastly, we suggest that the authors further investigate the implications of the observed inconsistencies in SD's rationale generation on the accuracy of task outcomes.