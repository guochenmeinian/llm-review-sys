ID: rDoPMODpki
Title: KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 4, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents KG-FIT, a framework designed to enhance the expressiveness of existing Knowledge Graph Embedding (KGE) models by integrating large language models (LLMs). KG-FIT involves four key steps: generating entity descriptions with an LLM, constructing a semantically coherent seed hierarchical structure, refining this structure using real-world entity knowledge from the LLM, and fine-tuning knowledge graph embeddings by merging the hierarchical structure with textual embeddings. Experiments on benchmark datasets demonstrate the effectiveness of KG-FIT in improving link prediction accuracy.

### Strengths and Weaknesses
Strengths:
1. The motivation for KG-FIT is clearly articulated, and the paper is well-structured.
2. Extensive experimental results validate that KG-FIT enhances the performance of most KGE baseline models.
3. The authors provide code for reproducibility and detailed experimental descriptions.

Weaknesses:
1. The performance of KG-FIT is heavily dependent on the LLM used; if the LLM lacks comprehensive knowledge, the embeddings may be suboptimal, particularly in domains with limited coverage.
2. The paper lacks clarity in organization, making it difficult to understand, and does not include a sensitivity study for hyperparameters in the loss function.
3. The rationale for selecting agglomerative hierarchical clustering and the silhouette score is not justified, and additional ablation studies are needed.
4. The paper does not consider recent LLM-based methods or analyze time and space complexity, which are crucial for assessing model efficiency.

### Suggestions for Improvement
We recommend that the authors improve the organization of the paper to enhance clarity and understanding. Additionally, the authors should justify their choice of agglomerative hierarchical clustering and the silhouette score, and include more recent LLM-based methods in their comparisons. Incorporating a sensitivity analysis for hyperparameters and providing an analysis of time and space complexity would also strengthen the paper. Finally, we suggest that the authors reconsider the motivation behind their approach, particularly regarding the necessity of using KG embedding methods alongside LLMs for knowledge discovery.