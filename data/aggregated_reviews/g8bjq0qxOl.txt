ID: g8bjq0qxOl
Title: Where Did I Come From? Origin Attribution of AI-Generated Images
Conference: NeurIPS
Year: 2023
Number of Reviews: 25
Original Ratings: 9, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an origin attribution method aimed at distinguishing whether an image is generated by a specific model or is a real image. The authors propose a model-agnostic approach based on reverse-engineering, utilizing reconstruction loss to infer the image's origin. They clarify that their method does not require repeated training of large models, which is resource-intensive. The authors provide experimental results demonstrating high detection accuracy across various configurations, including different model architectures and image editing methods, and assert that reverse-engineering is more effective for images belonging to a specific model than for those generated by others. The incorporation of calibration and hypothesis testing is highlighted as a technical innovation that enhances detection accuracy, supported by a theoretical foundation (Theorem 4.2) and empirical results from an ablation study.

### Strengths and Weaknesses
Strengths:
1. The introduction of an "alteration-free and model-agnostic origin attribution" algorithm is notable.
2. The method effectively distinguishes images generated by a specific model, achieving high detection accuracy in various experimental settings.
3. The authors provide comprehensive experimental results, including comparisons with existing methods, demonstrating superior performance.
4. The analysis of reconstruction loss differences between generated and non-generated images is insightful.
5. The incorporation of calibration and hypothesis testing is a notable technical innovation, with a theoretical analysis explaining the method's effectiveness.

Weaknesses:
1. The paper lacks a detailed description of computational costs and their implications.
2. The high-level presentation of the algorithm raises concerns about reproducibility without available code.
3. Some reviewers express concerns regarding the theoretical underpinnings of the method, particularly in relation to existing literature on model convergence and reconstruction error.
4. The paper lacks a detailed exploration of the technical innovations, such as calibration and hypothesis testing, which are crucial for understanding the method's effectiveness.
5. The assumption of having belonging images to a specific model may not hold true in practical scenarios, and the comparison with concurrent work highlights limitations in model-agnostic applicability.

### Suggestions for Improvement
We recommend that the authors improve the description of computational costs and include a detailed mapping to Table 3, addressing uncertainties and resource utilization during runs. Additionally, we suggest enhancing the reproducibility of the algorithm by providing more detailed steps or making the code available. The authors should also improve the theoretical discussion surrounding the method, particularly by providing a detailed explanation of the significance of calibration and hypothesis testing in their approach. Addressing the concerns regarding the assumptions made in comparison to existing methods, such as those by Albright et al. and Zhang et al., would strengthen the paper. Clarifying the differences in threat models and the implications of these differences on the method's performance is also advised. Lastly, we encourage the authors to include a discussion of the limitations of their method in relation to the referenced concurrent work, particularly regarding its applicability to non-invertible activation functions and modern model architectures.