ID: MzZcXPeqcU
Title: CORL: Research-oriented Deep Offline Reinforcement Learning Library
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 8, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CORL (Clean Offline Reinforcement Learning), an open-source library that provides single-file implementations of offline and offline-to-online reinforcement learning algorithms. CORL emphasizes minimal dependencies and lightweight abstractions, facilitating understanding and modification of algorithms. The library includes built-in benchmarking on D4RL datasets, ensuring quality and reproducibility. The authors aim to support newcomers in offline RL and assist researchers in developing new algorithms. They argue that while other libraries like d3rlpy may offer optimized performance, their approach is more suited for researchers needing control over every component. The paper also includes empirical comparisons of existing offline RL algorithms, aiming to highlight the strengths of CORL.

### Strengths and Weaknesses
Strengths:
- CORL enables accurate, quick, and reproducible implementation and empirical comparison of various offline RL algorithms.
- The codebase is well-organized, easy to read, and integrates experiment tracking with Weights & Biases.
- The library's single-file design isolates algorithmic details, enhancing clarity and modification ease.
- The authors have improved the visibility and accessibility of results in tables and figures.
- A detailed documentation website has been created, enhancing clarity for users.
- The library promotes transparency and ease of use for researchers in offline reinforcement learning.

Weaknesses:
- The visibility of values in result tables is poor, making them difficult to interpret.
- Some figures and tables remain difficult to read due to small text size.
- There is uncertainty regarding the ease of implementing new algorithms on top of CORL, which necessitates clearer demonstrations.
- The purpose of Section 4 could be clearer in demonstrating how CORL stands out against existing packages.
- The paper lacks explicit limitations and does not sufficiently compare CORL with other offline RL libraries.
- The documentation lacks certain critical details about hyperparameter sensitivity.

### Suggestions for Improvement
We recommend that the authors improve the visibility of result tables and enhance the readability of figures by increasing text size. Additionally, we suggest providing clearer demonstrations of implementing and comparing new algorithms with existing methods, as well as clarifying the objectives of Section 4 to better emphasize CORL's advantages over other libraries. Enhancing the documentation to include critical hyperparameter sensitivities and improving code comments would increase accessibility for first-time users. We also suggest that the authors consider a more modular structure to reduce code duplication while maintaining readability. Finally, updating the codebase to the latest library versions and addressing inconsistencies in training lengths across seeds would enhance reliability.