ID: 8z4isrqbcf
Title: CV-VAE: A Compatible Video VAE for Latent Generative Video Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CV-VAE, a 3D VAE that utilizes latent space regularization with a 2D VAE decoder, enabling integration with 2D VAE-based diffusion models. The framework effectively compresses latent space dimensionality through temporal downsampling, achieving performance comparable to existing models while enhancing video generation capabilities. The authors also propose a bidirectional VAE alignment method, facilitating effective performance in both 2D and 3D contexts.

### Strengths and Weaknesses
Strengths:
1. The method is novel and effectively compresses video latents while maintaining latent distribution.
2. The experimental results indicate that CV-VAE performs comparably to SD VAE in image/frame encoding and shows minimal performance drop in video encoding.
3. The writing is fluent, and the experimental design is robust.

Weaknesses:
1. CV-VAE exhibits more severe flickering issues than the original SVD, potentially due to latent distribution discrepancies. The authors should provide convincing evidence that these issues can be mitigated through further fine-tuning.
2. The paper lacks critical training details, such as batch size, augmentations, and whether the CV-VAE was trained on video and image data separately or jointly.
3. Comparisons with recent video autoencoders are outdated, and the authors should address the pros and cons of their approach relative to newer models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of training details, including batch size, augmentations, and the training process for CV-VAE. Additionally, we suggest providing metrics for SVD in Table 3 and Figure 6, specifying whether decoder-based regularization is employed with image inputs, and including a comparison of metrics between SVD and SVD + CV-VAE for text-to-video generation. Furthermore, we encourage the authors to provide theoretical proof regarding the minimization of distribution distance between video latents of CV-VAE and corresponding image latents encoded by the 2D VAE.