ID: vWSll6M9pj
Title: Unified Speech Recognition: A Single Model for Auditory, Visual, and Audiovisual Inputs
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified architecture and training methodology for auditory, visual, and audiovisual speech recognition (ASR, VSR, AVSR). The authors propose a semi-supervised pseudo-labeling method to utilize unlabeled audio-visual data and a self-supervised pre-training approach to enhance model performance. The model achieves competitive state-of-the-art results across tasks, leveraging a single model for all three modalities while maintaining performance.

### Strengths and Weaknesses
Strengths:
- The work introduces a novel and effective model and training procedure for unifying auditory and visual speech content recognition, marking a significant contribution to the field.
- Comprehensive ablation studies validate the model's characteristics and the effectiveness of the training paradigm, providing robust experimental results.
- The paper is well-organized, with clear descriptions and illustrations of the complex USR system, facilitating reader comprehension.
- The authors provide extensive experimental details, enhancing reproducibility and impact.

Weaknesses:
- The technical novelty is limited, as many techniques employed are well-known or straightforward.
- The model architecture lacks clarity, particularly regarding the use of the transformer decoder during fine-tuning and inference.
- Performance sensitivity to configurations, such as pseudo label ratios, is not adequately analyzed, and the paper does not discuss failure cases in detail.

### Suggestions for Improvement
We recommend that the authors clarify the initialization of the teacher model's weights in self-supervised pre-training and compare their method with masked-autoencoding training of audio and visual features. Additionally, investigating the effects of different masking ratios would be beneficial. We suggest providing a rationale for incorporating all three features during fine-tuning and exploring the use of advanced video encoders. Lastly, addressing the sensitivity of performance to hyperparameters and discussing potential mitigation strategies would strengthen the paper.