ID: XYxNklOMMX
Title: Benchmarking Distribution Shift in Tabular Data with TableShift
Conference: NeurIPS
Year: 2023
Number of Reviews: 31
Original Ratings: 3, 7, 6, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the TABLESHIFT benchmark, which addresses the need for robustness evaluation in tabular machine learning models under distribution shift. It comprises 15 tasks across various domains, including finance and healthcare, and provides an accessible Python API for dataset access along with implementations of multiple models and domain generalization methods. The authors conduct a large-scale study revealing a linear relationship between in-distribution (ID) and out-of-distribution (OOD) accuracy, the effects of domain robustness methods on shift gaps, and a connection between shift gaps and label distribution shifts. The authors also analyze the impact of multiple random seeds on model performance, employing a rigorous hyperparameter tuning protocol and computing 95% Clopper-Pearson confidence intervals for both ID and OOD accuracy. The results indicate strong consistency across models, suggesting that the main findings are robust against variations in random seeds.

### Strengths and Weaknesses
Strengths:
- The paper effectively highlights the importance of benchmarking distribution shifts in tabular data, which is highly relevant to the research community.
- The inclusion of user-friendly datasets and APIs enhances usability and encourages wider adoption.
- The work is well-written and addresses a timely topic in distribution shift robustness.
- The dataset construction and evaluation methods are sound, with a diverse range of datasets included.
- The use of multiple random seeds enhances the reliability of the results, and the rigorous hyperparameter tuning contributes to the robustness of the findings.
- Clear presentation of results with confidence intervals enhances the reliability of findings, and the inclusion of a "Limitations" section acknowledges the scope and constraints of the study.

Weaknesses:
- The paper lacks extensive exploratory data analysis (EDA) and clarity regarding dataset selection and types of shifts represented.
- The presentation of results could be improved for better comparison and understanding, with some discrepancies existing between reported metrics in figures and tables.
- The current table format may obscure the understanding of performance metrics, particularly for datasets with similar results.
- The evaluation is anchored to a single seed, which limits comprehensiveness, and the conflation of methods and model evaluations complicates interpretation and comparison.
- The benchmark primarily focuses on binary classification, despite the broader implications suggested by the name "TableShift."

### Suggestions for Improvement
We recommend that the authors improve the paper by incorporating comprehensive exploratory data analysis (EDA) to detail the presence of shifts, including visual tools like violin plots and native visualizations such as KDE plots. Clarification on the dataset selection criteria and the types of shifts represented is necessary, especially regarding domain generalization methods. The authors should refine the report presentation by including standard deviation values in tables and providing a detailed explanation of the evaluation metric used. Additionally, clarification on the combinations of models and methods tested, along with the rationale behind hyperparameter choices, would enhance comprehensibility. 

We also suggest expanding the related work section to better acknowledge existing benchmarks and consider integrating them into TABLESHIFT. To address reproducibility issues, the authors should ensure that the codebase is thoroughly reviewed and that clear instructions for downloading datasets and benchmarking new algorithms are provided. We recommend that the authors improve clarity by explicitly stating the binary nature of all prediction tasks in the manuscript and consider presenting results for each dataset in separate tables to enhance clarity. Furthermore, we encourage the authors to run multiple trials for hyperparameter optimization to align with standard practices in domain generalization studies. Lastly, addressing the limitations of the research explicitly and discussing the computational costs of methods would further strengthen the paper.