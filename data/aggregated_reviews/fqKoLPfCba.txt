ID: fqKoLPfCba
Title: Large-Scale and Multi-Perspective Opinion Summarization with Diverse Review Subsets
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called SUBSUMM for large-scale and multi-perspective opinion summarization. It introduces a review sampling strategy based on sentiment analysis and contrastive information valuation to select high-quality review subsets. The authors propose a two-stage training scheme, where the model learns from both sub-optimal and optimal review subsets, with the second stage utilizing contrastive learning to enhance summary quality. Experiments on the AmaSum and Rotten Tomatoes datasets demonstrate that SUBSUMM outperforms previous state-of-the-art models in generating pros, cons, and verdict summaries.

### Strengths and Weaknesses
Strengths:
1. The paper addresses the significant problem of summarizing large volumes of opinion text, which is relevant for real-world applications.
2. SUBSUMM achieves new state-of-the-art results on benchmark datasets, supported by in-depth analysis of its components.
3. The empirical analysis is robust, with thorough ablation studies justifying the final system design.

Weaknesses:
1. The problem setup is somewhat artificial, as summarizing a very large set of reviews may not reflect typical real-world scenarios.
2. The framework lacks a specific component for modeling aspect information, relying primarily on ROUGE scores.
3. The two-stage training technique appears ad-hoc, and there are no mechanisms to reduce repetition or ensure diversity in generated summaries.
4. There is a lack of comparison with long-document summarization methods, which limits the comprehensiveness of the evaluation.

### Suggestions for Improvement
We recommend that the authors improve the framework by incorporating a component that models aspect information to enhance opinion summarization. Additionally, we suggest conducting experiments with varying values of K and N to better understand the method's performance under different settings. It is also crucial to include comparisons with long-document summarization models, such as Longformer, to strengthen the analysis. Furthermore, we encourage the authors to clarify the selection process for review subsets and the categorization of summaries in the paper. Lastly, addressing the concerns regarding the unfairness of certain baselines and providing clearer parameter settings would enhance reproducibility and overall rigor.