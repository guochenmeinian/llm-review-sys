ID: Y4L8GQXZZO
Title: Federated Learning from Vision-Language Foundation Models: Theoretical Analysis and Method
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 6, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to finetune the vision-language model CLIP through prompt learning, optimizing a global textual prompt via FedAvg while each client optimizes a local textual prompt. The authors propose a combination of existing methods, CoOp and PromptFL, and demonstrate that their approach outperforms both. The theoretical analysis provides insights into the performance of prompt-based federated learning, particularly in balancing generalization and personalization.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and offers a thorough theoretical analysis of the proposed method.
- The connection between prompt mixing and financial portfolio theory is novel and insightful.
- The results indicate that combining global and local prompts can enhance accuracy.

Weaknesses:
- The novelty of the proposed method, primarily a combination of CoOp and PromptFL, needs further emphasis.
- The investigation is limited to simpler classification tasks, raising questions about the necessity of using a larger foundation model.
- The method's performance on unseen distributions and its generalization to different tasks, such as image captioning, remain unclear.
- The evaluation is simplistic, focusing on a limited number of users and tasks, which may not reflect typical federated learning scenarios.

### Suggestions for Improvement
We recommend that the authors improve the novelty section by clearly highlighting the unique contributions of their method compared to CoOp and PromptFL. Additionally, the authors should expand their experimental analysis to include comparisons with other prompt-based tuning methods for federated learning, such as Pfedprompt and other recent works. Incorporating more ablation studies and evaluating the method on a broader range of tasks, including unseen distributions, would strengthen the paper. Furthermore, clarifying the privacy implications of sharing prompts and addressing the dimensionality of prompts used in experiments would enhance the overall rigor of the study.