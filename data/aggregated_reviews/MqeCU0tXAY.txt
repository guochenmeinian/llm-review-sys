ID: MqeCU0tXAY
Title: CLIPCEIL: Domain Generalization through CLIP via Channel rEfinement and Image-text aLignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 6, 4, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CLIPCEIL, a method aimed at improving domain generalization for vision-language models like CLIP by refining visual feature channels to ensure they are domain-invariant and class-relevant. The authors employ techniques such as Channel rEfinement and Image-text aLignment, along with a self-attention fusion module, to enhance performance on unseen test datasets with domain shifts. The study clarifies that in their domain generalization (DG) setting, the model is trained exclusively on source domains, with the target domain remaining entirely unseen during training. The authors utilize a leave-one-domain-out strategy using datasets like PACS to evaluate model performance. While the method achieves state-of-the-art results on various benchmarks, concerns about its novelty, computational efficiency, and the adequacy of the datasets used for DG are raised.

### Strengths and Weaknesses
Strengths:
1. The paper is well-structured and clearly presented, making it easy to follow.
2. The motivation for the method is solid, focusing on excluding domain-sensitive features, which aids understanding.
3. Extensive ablation studies clarify the importance of each component in the proposed method.
4. The authors effectively address most reviewer concerns and provide a clear explanation of their methodology.
5. They acknowledge the limitations of the CLIP model and the potential for improvement in generalizability.

Weaknesses:
1. The novelty of the approach is questioned, as it resembles existing works like DomainDrop, and lacks sufficient discussion of related methods.
2. The method's performance gains diminish when fine-tuning is applied, with significant improvements observed only when backbones are frozen.
3. The adapter design is specific to CLIP with ViT, limiting its applicability to other architectures like ResNet.
4. There remains confusion regarding the differences between few-shot learning and domain generalization, particularly in the context of fine-tuning labeled data.
5. Some reviewers express skepticism about the adequacy of the datasets used for DG, questioning whether they are truly out-of-distribution (OOD) for CLIP due to its extensive pre-training.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related works, particularly DomainDrop and other CLIP adapters, to clarify how CLIPCEIL differs. Additionally, the authors should provide comparisons of their method against popular baselines like CoOP, especially regarding computational efficiency. It would also be beneficial to explore the model's performance on various ViT backbones and clarify the term "multi-scale information" for better understanding. Furthermore, we suggest that the authors improve clarity regarding the distinction between few-shot learning and domain generalization, particularly in their responses. We also recommend that the authors reconsider and refine their definition of "domain adaptation" to ensure it accurately reflects the nuances of their methodology. Finally, addressing the concerns about the representativeness of the datasets used in DG, particularly regarding the balance of realistic versus stylized images, would strengthen their argument. An analysis of the weights for the loss terms in Eq. 7 should also be included to determine optimal configurations across datasets.