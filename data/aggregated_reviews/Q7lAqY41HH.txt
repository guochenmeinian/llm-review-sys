ID: Q7lAqY41HH
Title: CRAG - Comprehensive RAG Benchmark
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 7, 7, 7, -1, -1
Original Confidences: 4, 4, 2, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents CRAG, a comprehensive benchmark for evaluating retrieval-augmented generation (RAG) systems, featuring 4,409 question-answer pairs across five domains and various question types. The authors propose a novel evaluation methodology that includes a scoring system penalizing hallucinations and an automatic evaluation method using large language models (LLMs). The benchmark aims to provide insights into the performance of both baseline LLMs and integrated RAG systems, revealing significant limitations in handling dynamic information and complex reasoning tasks.

### Strengths and Weaknesses
Strengths:
- CRAG addresses a critical gap in RAG research by offering a robust platform for testing and comparing modern solutions.
- The dataset includes diverse domains, question types, and temporal dynamics, enhancing its applicability to real-world scenarios.
- The framework allows for quick evaluation and insightful analyses of system performance across different domains.
- The evaluation mechanism effectively distinguishes between hallucinated and missing answers, adding value to the research community.

Weaknesses:
- The benchmark does not fully evaluate the retrieval component, limiting its assessment of the entire RAG system.
- The small retrieval candidate pool (up to 50 web pages per question) may not adequately represent real-world retrieval challenges.
- The paper lacks a clear description of the validation process for question/answer pairs and does not report inter-annotator agreement, raising concerns about reliability.
- The dataset is licensed for non-commercial use, which may limit its accessibility compared to other RAG datasets.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by expanding the retrieval candidate pool to better reflect real-world scenarios and by providing a clearer evaluation of first-stage candidate pool construction. Additionally, we suggest including the sizes of other datasets in Table 1 and offering a discussion on the implications of the results to guide users on the strengths and weaknesses of different retrieval systems. It would also be beneficial to release the human annotator instructions and prompts for automated evaluation, as well as to explore the potential for fine-tuning RAG models using the dataset. Finally, we advise ensuring consistent terminology throughout the paper regarding the tasks and considering hosting the dataset in a permanent repository for long-term access.