ID: OiivS2mqQf
Title: Augmentation-Aware Self-Supervision for Data-Efficient GAN Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AugSelf-GAN, an enhanced approach for self-supervised GANs that incorporates label augmentation (SSGAN-LA) and links the arithmetic-harmonic mean (AHM) divergence to improve stability in self-supervised adversarial tasks, particularly in data-limited scenarios. The authors propose a discriminator that predicts continuous augmentation parameters, allowing for a more nuanced representation of data distribution and addressing overfitting issues. They acknowledge that SSGAN-LA occasionally outperforms AugSelf-GAN(+) on certain datasets but argue that metrics like Inception Score (IS) may not accurately reflect image quality. The authors provide a theoretical analysis supporting their method and are conducting further experiments on the AFHQ dataset to validate their approach. Experimental results indicate that AugSelf-GAN improves GAN performance in limited data scenarios, as evidenced by metrics like Fr√©chet Inception Distance (FID) and IS.

### Strengths and Weaknesses
Strengths:
- The proposed method shows significant improvements in quantitative results despite its simplicity.
- The theoretical foundation linking AHM divergence to improved training stability is well-articulated.
- The paper provides comprehensive empirical and theoretical evidence supporting the effectiveness of the approach.
- The technique of self-supervision for predicting augmentation types demonstrates notable improvements across various baselines.
- The clarification regarding the use of separate self-supervised discriminators enhances understanding of the method.
- The authors demonstrate responsiveness to feedback and willingness to conduct additional experiments.

Weaknesses:
- The claim regarding augmentation leaking in SSGAN-LA and ADA lacks empirical support or citations, necessitating justification as it is central to the paper's motivation.
- The novelty of the proposed method is limited, primarily differing from SSGAN-LA in the discriminator's prediction of continuous rather than discrete parameters.
- The empirical evidence supporting the advantages of AHM divergence remains limited.
- The paper does not compare its method with previous works like ContraD and SSGAN-LA, which would be essential for contextualizing its contributions.
- The performance of AugSelf-GAN compared to SSGAN-LA on specific datasets may not fully capture its potential.
- Insufficient experiments are conducted to substantiate claims about the discriminator's representation learning capabilities, with evaluations limited to CIFAR-10/100.

### Suggestions for Improvement
We recommend that the authors improve the justification for the claim regarding augmentation leaking by providing empirical evidence or citations. Additionally, we suggest including comparisons with previous methods such as ContraD and SSGAN-LA to better contextualize the contributions of AugSelf-GAN. To strengthen the representation learning claims, we advise conducting more extensive experiments beyond CIFAR-10/100 and across various datasets to substantiate the claimed advantages of AugSelf-GAN. Furthermore, clarifying the robustness of AHM divergence through more detailed experimental results and providing convergence plots for FID would enhance the paper's rigor.