ID: dGQtja9X2C
Title: Thinking Forward: Memory-Efficient Federated Finetuning of Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SPRY, a federated learning (FL) algorithm aimed at finetuning large language models (LLMs) on resource-constrained devices by mitigating the high memory demands of traditional backpropagation methods. SPRY employs Forward-mode Auto-Differentiation (AD) and distributes trainable weights among clients, enabling accurate gradient computation with reduced memory usage. Theoretical analysis confirms that SPRY's global gradients are unbiased for homogeneous data and outlines a convergence rate influenced by FL rounds and data heterogeneity. Empirical results indicate that SPRY reduces memory usage by 1.4-7.1Ã— and achieves faster convergence and higher accuracy than existing methods, making LLM finetuning feasible on mobile and edge devices.

### Strengths and Weaknesses
Strengths:
* The proposed SPRY algorithm effectively addresses a challenging problem, supported by both empirical and theoretical results.
* The paper is well-written, with helpful illustrations.
* Empirical experiments are conducted across multiple datasets, models, and hardware configurations, demonstrating SPRY's superiority over previous works.
* An anonymous code repository for SPRY is provided.

Weaknesses:
* The scalability of SPRY with a larger number of clients is not thoroughly investigated, raising concerns about communication and computational overheads in large-scale deployments.
* The empirical evaluation is limited in dataset and model variety, focusing primarily on specific language tasks and a narrow range of LLMs, which may affect generalizability.
* Although SPRY outperforms existing zero-order methods, it converges slower in terms of time compared to traditional gradient-based methods like FedAvg.

### Suggestions for Improvement
We recommend that the authors improve the analysis of SPRY's scalability by discussing the communication and computational overheads associated with increasing the number of clients. Additionally, expanding the empirical evaluation to include a broader range of datasets and model architectures, as well as exploring the impact of SPRY on tasks beyond natural language processing, would strengthen the findings. We also suggest providing a detailed comparison of computational overheads in the experimental section and clarifying the implications of theorem 4.2, particularly regarding the variance of the gradient estimator. Finally, addressing the questions regarding dynamic layer distribution and communication overhead metrics would enhance the manuscript's clarity and depth.