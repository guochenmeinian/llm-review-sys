ID: siPdcro6uD
Title: OneRef:  Unified One-tower Expression Grounding and Segmentation with Mask Referring Modeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 5, 6, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified one-tower referring framework, introducing the Mask Referring Modeling (MRefM) paradigm to capture referential relationships between vision and text. The authors propose a Mask Referring Modeling (MRefM) approach that includes referring-aware mask image modeling and referring-aware mask language modeling, demonstrating its effectiveness across REC, PG, and RES tasks. The framework, named UniRef, aims to streamline architecture by eliminating the need for separate modality-specific encoders and complex interaction modules, achieving state-of-the-art performance on multiple datasets.

### Strengths and Weaknesses
Strengths:  
- The proposed method is structurally simpler than previous methods and achieves higher grounding and RES performance.  
- The introduction of the MRefM paradigm effectively captures the referential relationship between visual and linguistic features, enhancing model robustness and accuracy.  
- Extensive experimental results across five datasets validate the effectiveness of the proposed approach and its components.  

Weaknesses:  
- Adapting BEiT to downstream grounding tasks does not present a particularly interesting finding, and the rationale behind introducing two types of relation scores in MIM and MLM lacks clarity.  
- The technical contribution appears insufficient, primarily adapting traditional Masked Autoencoders into a 'Referring MAE' without sufficient novelty.  
- Section 3.2 is intricate and challenging to follow, requiring thorough proofreading and revision for clarity.  
- There is a lack of discussion regarding related works, particularly the differences between the proposed referring-aware mask language modeling and masked contrastive learning.  
- The manuscript does not analyze computational costs compared to other methods, including parameters, FLOPs, and speed.  

### Suggestions for Improvement
We recommend that the authors improve the clarity and readability of Section 3.2 through thorough proofreading and revision. Additionally, we suggest discussing the differences between the proposed referring-aware mask language modeling and the masked contrastive learning in the relevant literature. It would be beneficial to include a computational cost analysis comparing parameters, FLOPs, and speed with other methods. Furthermore, the authors should clarify the rationale behind the design of the four masks: x-, y-, w-, and h-masks, and provide insights into the real-time applicability of the approach by detailing its computational demands and processing speed in practical scenarios.