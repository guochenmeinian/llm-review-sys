ID: aIp5EZeO3f
Title: ZGUL: Zero-shot Generalization to Unseen Languages using Multi-source Ensembling of Language Adapters
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called ZGUL aimed at enhancing zero-shot transfer between languages using ensembles of adaptors. The methodology integrates train-time ensembling through token-based attention and language vectors, alongside test-time ensembling via EMEA. The authors evaluate their approach on 15 languages across 4 language families and 2 tasks, demonstrating improved performance. The incorporation of unlabeled target language data during training is particularly noteworthy, and the authors plan to release their code to enhance reproducibility.

### Strengths and Weaknesses
Strengths:
- The paper proposes a novel approach to extend multilingual models to unseen languages, leveraging language adaptors from related languages.
- It performs thorough ablation studies and qualitative analyses, solidifying the utility of the proposed components.
- The results show significant improvements over baseline models, and the methodology is built on established techniques like AdapterFusion.

Weaknesses:
- The approach is somewhat incremental, primarily combining existing methods without introducing substantial novelty.
- The limitations section lacks critical insights into potential failure conditions and future directions.
- The classification of language families is inaccurate, which undermines the paper's credibility.

### Suggestions for Improvement
We recommend that the authors improve the limitations section by detailing specific cases where the approach may fail and suggesting future research directions, such as testing on unseen scripts and generative tasks. Additionally, we urge the authors to correct the linguistic family classifications presented in Table 1. Clarifying the training objective of the train-time ensemble in section 3.1 is also essential. Furthermore, we suggest enhancing the clarity and presentation of figures and tables, including increasing font sizes, providing legends, and ensuring consistent terminology throughout the paper.