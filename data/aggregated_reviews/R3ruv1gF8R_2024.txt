ID: R3ruv1gF8R
Title: The Reliability of OKRidge Method in Solving Sparse Ridge Regression Problems
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 8, 3, 8, 8, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 2, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical error analysis of the Scalable Optimal K-Sparse Ridge Regression (OKRidge) method, addressing the estimation error by reframing it as a Primary Optimization (PO) problem and simplifying it into an Auxiliary Optimization (AO) problem using the Convex Gaussian Min-Max Theorem (CGMT). The authors provide a rigorous theoretical error analysis that supports the reliability of OKRidge and its applicability in sparse ridge regression contexts. Additionally, the authors define a linear model as $$ \pmb{y}=\pmb{X} \pmb{\beta}^* + \pmb{\epsilon} $$ with the constraint $\Vert\pmb{\beta}^* \Vert_0 \leq k$, clarifying that $k$ is a function of $d$ and emphasizing that the sparsity value $k/d$ should remain constant once the model is selected. The theoretical results indicate that $\Delta(\hat{\lambda})$ is influenced by $\lambda$ and the selection of model (1), while treating the sparsity $k/d$ as a known constant.

### Strengths and Weaknesses
Strengths:  
- The paper fills a significant gap by providing the first theoretical analysis of the estimation error of OKRidge, enhancing its applicability in large-scale scenarios.  
- Clarity in presentation is notable, with well-defined symbols and detailed proofs, contributing to the paper's readability.  
- The application of CGMT to simplify the estimation error into an AO problem is innovative and substantiates the reliability of OKRidge through theoretical and empirical validation.  
- The authors provide a clear explanation of the relationship between sparsity and the theoretical results, enhancing the understanding of their model.  
- They acknowledge previous misunderstandings and demonstrate a willingness to clarify their concepts in revisions.

Weaknesses:  
- Several formulas lack punctuation, which detracts from the overall presentation quality.  
- The distinction between GMT and CGMT is not adequately clarified, and the advantages of CGMT over GMT remain unexplained.  
- The experimental scope is limited, primarily focusing on synthetic data with Gaussian noise, which raises questions about the generalizability of the findings.  
- The presentation of the theoretical results may still lack clarity for readers unfamiliar with the underlying assumptions, particularly regarding the implications of sparsity on the model's performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their presentation by ensuring all formulas are punctuated correctly. Additionally, we suggest that the authors clarify the differences between GMT and CGMT, explicitly stating the advantages of CGMT. Expanding the experimental section to include a wider variety of sparsity levels and real-world datasets would enhance the robustness of the findings. Furthermore, we recommend that the authors improve the clarity of their theoretical results by explicitly detailing the implications of the sparsity value on model performance. Finally, further emphasis on the relationship between $k$, $d$, and $\Delta(\hat{\lambda})$ in the context of the selected model would enhance reader comprehension. Addressing the Gaussian assumption more thoroughly and exploring potential extensions to non-Gaussian settings would significantly strengthen the paper's applicability.