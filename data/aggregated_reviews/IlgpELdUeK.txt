ID: IlgpELdUeK
Title: Axiomatic Preference Modeling for Longform Question Answering
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an innovative axiomatic framework aimed at enhancing reward models (RMs) in reinforcement learning from human feedback (RLHF) by better aligning them with human preferences in long-form question answering. The authors propose five principles to guide RMs, demonstrating the effectiveness of their preference model (PM) through extensive experiments, achieving performance that surpasses larger models like GPT-4 with only 220M parameters. The methodology involves constructing training data based on human-desired axioms such as usefulness, relevance, groundedness, truthfulness, and thoroughness, utilizing diverse data from community QA forums.

### Strengths and Weaknesses
Strengths:
- The axiomatic framework and PM are well-defined and focused on effectively capturing human preferences.
- Promising empirical results, with the PM outperforming GPT-4 in aligning with human-annotated preference labels.
- Comprehensive experiments lend credibility to the study.

Weaknesses:
- The contribution primarily revolves around data generation, with a need for more exploration of training methods for PM models.
- Some preference signals may not be necessary or could negatively impact model performance, requiring further analysis.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the reliability of augmenting human preference data with LLM-generated pairs, as the current justification appears unconvincing. Additionally, we suggest providing a thorough evaluation of out-of-distribution (OOD) data to clarify the model's generalization capabilities. It would also be beneficial to include a comparison with baseline models, such as ChatGPT, and to elaborate on the selection of the threshold tq mentioned in line 243. Lastly, consider adding an Ethical Concern section detailing the origins and compensation of human annotators involved in the study.