ID: nFEQNYsjQO
Title: Label Correction of Crowdsourced Noisy Annotations with an Instance-Dependent Noise Transition Model
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 6, 5, 6, 3, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for modeling input-dependent label noise in classification tasks, utilizing a hierarchical Bayesian framework to account for noise introduced by multiple annotators. The authors propose a novel label correction algorithm based on an instance-dependent noise transition model, supported by a posterior-concentration theorem that ensures the model's consistency. The paper includes a comprehensive analysis of label inference in the context of noisy annotations, with extensive experimental results demonstrating the method's effectiveness on benchmark datasets with both synthetic and real annotation noise. The authors conduct experiments varying the number of annotators (10, 30, 50, 100) and compare their approach against several baseline methods, including "Multi-AnT," "EM," "MBEM," "Co-teaching," and "Co-teaching+." The results indicate high accuracy of the inferred labels and highlight the advantages of the proposed method.

### Strengths and Weaknesses
Strengths:
- The approach realistically models labeling noise as a function of both input and annotator, addressing a significant gap in existing literature.
- The authors provide a sound theoretical justification for their algorithm, including a concentration theorem and an information-theoretic bound on error.
- Extensive experiments validate the proposed method's efficacy across various datasets, showing superior performance in label inference compared to baseline methods, especially under varying noise conditions.

Weaknesses:
- The empirical analysis lacks depth, particularly regarding the importance of different components of the annotation model and sensitivity to prior specifications.
- The performance on datasets with human annotations, such as CIFAR-10N and LabelMe, does not show significant advantages over existing methods, and the authors fail to adequately explain these results.
- The theoretical assumptions, particularly regarding anchor points, may require further clarification and justification.
- The experimental scope is currently limited to the CIFAR10 dataset, with plans for additional experiments on CIFAR100 not yet realized.
- The assumption that an instance-dependent noise model can be learned with limited training data is questionable, and the conditions for the proposed theorems are not clearly articulated.

### Suggestions for Improvement
We recommend that the authors improve the analytical discussion of experimental results, particularly for datasets with human annotations, and consider including additional datasets to strengthen their validation. The authors should also address the claim regarding the challenges of learning an instance-dependent noise transition model by citing relevant prior work. Furthermore, clarifying the motivation behind the use of a sparse Bayesian approach and providing a more detailed analysis of computational complexity would enhance the paper's rigor. We suggest improving the theoretical justification of the anchor point assumption by including detailed proofs in the revised manuscript. Additionally, expanding the experimental analysis to include results from the CIFAR100 dataset would strengthen the generalizability of the findings. Finally, clarifying the role of inferred labels in training the transition matrix model would enhance the understanding of the methodology.