ID: E58gaxJN1d
Title: Learning from Visual Observation via Offline Pretrained State-to-Go Transformer
Conference: NeurIPS
Year: 2023
Number of Reviews: 26
Original Ratings: 5, 5, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to learning from expert visual observation (LfVO) without action labels, utilizing a State-to-Go Transformer (STG) that predicts next-step embeddings through causal masking. The method employs adversarial training inspired by WGAN and temporally-aligned regressor objectives. The authors demonstrate improved performance of the STG on four Atari and four Minecraft tasks compared to existing baselines. Additionally, the paper introduces a new representation learning technique aimed at enhancing reinforcement learning in various domains, particularly video games and robotics, while addressing the challenges of sparse rewards in robotics and contrasting these with video game dynamics. The authors acknowledge the need for clearer differentiation from prior work, particularly regarding terminology such as "state-to-go."

### Strengths and Weaknesses
Strengths:
- The incorporation of sequence modeling with a causal transformer and temporally-aligned regressor into LfVO is original and intriguing.
- The proposed method demonstrates promising experimental results, particularly in the context of video games.
- The paper is well-written and accessible, with clear illustrations aiding understanding.
- The authors are responsive to reviewer feedback, indicating a willingness to clarify and improve the manuscript.
- The STG shows superior performance in Atari and Minecraft tasks compared to baselines, indicating its effectiveness.

Weaknesses:
- The results of STG without the temporally-aligned regressor are nearly identical to those of ELE, suggesting that the main contribution may stem from the off-the-shelf module.
- The term "State-to-Go" is potentially misleading, as its role differs significantly from that of Return-to-Go in Decision Transformer.
- The paper assumes a high level of prior knowledge from readers, which may hinder accessibility and understanding.
- There is a significant disparity in the number of demonstrations used for Minecraft (50,000) versus Atari (50), with no explanation provided for this difference.
- The significance of the proposed small-scale per-task module for surrogate reward is unclear, especially in light of existing large-scale models like MineDojo and CLIP4MC.
- The comparison with other representation learning techniques is insufficiently addressed, leaving questions about the necessity of such comparisons.
- Empirical evaluations lack comparisons with relevant baselines such as BCO with IDM and enhanced GAIfO, which could better highlight the innovation of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve clarity by reconsidering the naming of "State-to-Go" to avoid confusion with established terms. Additionally, we suggest including an ablation study that examines the impact of dataset size on performance. It would also be beneficial to provide a more thorough analysis of the importance of the discriminator trained on offline samples and to compare the proposed method against relevant baselines like BCO and enhanced GAIfO. Furthermore, we encourage the authors to clarify the unique challenges addressed by their approach in comparison to prior methods, especially regarding the use of MuJoCo as a physics engine. Lastly, addressing the questions regarding the choice of PPO for the online RL phase and the optimization of the STG transformer during online interactions would strengthen the paper.