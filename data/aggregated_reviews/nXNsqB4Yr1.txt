ID: nXNsqB4Yr1
Title: Aggregating Capacity in FL through Successive Layer Training for Computationally-Constrained Devices
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 5, 6, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel training methodology for federated learning (FL) called Successive Layer Training (SLT), aimed at reducing memory usage on constrained devices. The authors propose partitioning the model into three categories: frozen layers, fully trained layers, and partially trained layers. SLT allows for a progressive training approach, where the training process expands to include more layers over time. The authors evaluate SLT against various baselines and demonstrate its effectiveness in terms of accuracy and convergence speed across different datasets and neural network architectures.

### Strengths and Weaknesses
Strengths:
- The SLT technique is innovative and offers flexibility in adapting to varying memory constraints and heterogeneous client environments.
- The paper is well-written, with clear descriptions and a comprehensive experimental analysis that includes appropriate baselines.
- The evaluation covers multiple datasets and architectures, showing that SLT achieves higher accuracy and faster convergence in both iid and non-iid settings.

Weaknesses:
- The paper lacks a discussion on the underlying reasons for SLT's performance gains over baselines, particularly in the Heterogeneous Memory Constraints setup.
- There is insufficient exploration of the parameter *N*, which dictates the number of configurations during the FL process, and its impact on training.
- The originality of the proposed method is questioned, as it appears to be a straightforward combination of existing techniques without sufficient theoretical justification for the design hyperparameters.
- The paper does not address other resource constraints, such as computation and communication, which are critical in practical FL scenarios.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the performance gains of SLT by providing insights into the mechanisms behind its effectiveness compared to baselines. Additionally, the authors should clarify the selection and impact of parameter *N* on the training process. It would be beneficial to include theoretical justification for the design hyperparameters and explore the implications of SLT on computation and communication costs. Lastly, we suggest modifying the title to better reflect the focus on memory constraints, such as "Aggregating Capacity in FL through Successive Layer Training for Memory-Constrained Devices."