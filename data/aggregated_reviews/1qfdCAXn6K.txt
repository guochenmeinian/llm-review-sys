ID: 1qfdCAXn6K
Title: Wasserstein Distance Rivals Kullback-Leibler Divergence for Knowledge Distillation
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm for knowledge distillation that replaces KL-divergence loss with Wasserstein distance loss. The proposed method consists of two components: (1) Logits distillation using Wasserstein distance loss, implemented via entropy regularized linear programming; (2) A feature distillation approach assuming Gaussian distribution, where the Wasserstein distance loss is solved using parametric techniques. Experiments on ImageNet and CIFAR100 demonstrate significant improvements over baseline methods such as KD, DKD, and NKD.

### Strengths and Weaknesses
Strengths:  
1. The paper is clearly written and easy to follow.  
2. It includes sound ablation studies that explore combinations of the proposed method with previous techniques.  
3. Strong performance is reported compared to recent work, with extensive comparisons and detailed ablation studies.

Weaknesses:  
1. The hyperparameter $\lambda$ in Eq. (3) and Eq. (5) raises questions about whether they share the same value in implementation, and the ablation for $\lambda$ in Eq. (3) is missing.  
2. The number of hyperparameters to tune is excessive, including $\lambda$ in Eq. (3), $\lambda$ in Eq. (5), $k$ for calculating IR, $\gamma$ for the trade-off between D_mean and D_cov, weight between WKD-L and WKD-F, and temperature. Consistency in hyperparameters across experiments is unclear.  
3. The choice of layer features for WKD-F is not specified, raising concerns about the computational efficiency, as WKD-F (207ms) is faster than the original KD (215ms).  
4. The claim that Wasserstein distance rivals KL-divergence is questionable, as results in Table 2(a) and Table 2(b) show KL-div achieving better performance in certain settings.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the hyperparameter $\lambda$ in Eq. (3) and provide an ablation study for it. Additionally, an explanation and analysis of the sensitivity of all hyperparameters should be included. The authors should clarify how to calculate $\mu^T$ and $\mu^S$ in their implementation for Eq. (9) and investigate the performance of the proposed method in self-KD settings where the student and teacher share the same architecture. Furthermore, a detailed analysis of the computational complexity of the proposed WD-based methods compared to traditional KL-Div methods would enhance the paper's contribution.