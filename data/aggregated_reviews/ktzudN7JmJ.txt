ID: ktzudN7JmJ
Title: ROBBIE: Robust Bias Evaluation of Large Generative Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an adversarial benchmark suite, ROBBIE, aimed at evaluating the performance of large language models (LLMs) across 12 demographic axes and 6 datasets. The authors propose extending prompt-based metrics to the intersection of demographic groups and provide insights into the pre-training of model corpora in relation to model bias. The study also investigates toxicity and undesired bias in generative language models, evaluating five recent models and three bias mitigation techniques, revealing trends such as larger models' reliance on toxic features.

### Strengths and Weaknesses
Strengths:
- The proposed ROBBIE benchmark suite offers a comprehensive analysis of LLMs across multiple demographic axes.
- The paper is well-structured, with clear motivation and justification for experimental settings.
- The volume of experiments conducted is impressive, and limitations are transparently discussed.
- The AdvPromptSet dataset enables evaluations over combinations of demographic groups, which is significant for the field.

Weaknesses:
- The term "bias" is used broadly; the authors should specify the harms being measured.
- Important experimental details are relegated to appendices, making it difficult to follow the main text.
- The rationale for excluding instruction-tuned models is unconvincing, as such models may influence toxicity inclinations.
- The paper lacks clear conclusions regarding the covariates of LMs' toxicity, and the organization could be improved for clarity.

### Suggestions for Improvement
We recommend that the authors improve the definition of "bias" by explicitly stating the harms they aim to measure. Additionally, we suggest moving details about dataset creation from the appendix to the main text for better accessibility. The authors should also consider prioritizing key experiments for detailed discussion in the main body, rather than relying on appendices. Furthermore, we advise the authors to clarify their rationale for excluding instruction-tuned models and to provide clearer conclusions regarding the implications of their findings. Lastly, we encourage the authors to make the source code available to enhance reproducibility.