ID: XmpthbaJql
Title: Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 6, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified framework called Res-Tuning for combining various efficient tuning methods. It introduces an unbinding form that integrates existing methods, allowing for combination flexibility, and proposes a memory-efficient variant, Res-Tuning-Bypass. The authors conduct experiments on both discriminative tasks (CIFAR-100 and VTAB-1K) and generative tasks (text-to-image generation on COCO), demonstrating the framework's flexibility and efficiency.

### Strengths and Weaknesses
Strengths:
1. The paper is well-organized, with clear presentation and concise illustrations.
2. The formulation of the unified and unbinding form is sound, novel, and allows for combination flexibility.
3. Extensive empirical evaluations show superior performance compared to existing approaches.
4. The proposed Res-Tuning-Bypass demonstrates clear advantages in memory efficiency.

Weaknesses:
1. The comparisons with previous works, particularly in NLP tasks, are insufficient, as the paper primarily focuses on vision tasks.
2. The novelty of the unbinding formulation is questioned, as similar parallel structures exist in prior works.
3. Several detail errors and inconsistencies in the manuscript detract from clarity.
4. Limitations regarding the generalizability and societal impacts of the proposed method are not adequately discussed.

### Suggestions for Improvement
We recommend that the authors improve the comparisons with previous works by including evaluations on NLP benchmarks, as this is crucial for establishing the framework's relevance across domains. Additionally, we suggest clarifying the relationship between Res-Tuning and Res-Tuning-Bypass, and providing a more detailed explanation of the derivation processes in the manuscript. Correcting detail errors and inconsistencies, such as those noted in the equations and figures, will enhance the clarity of the paper. Finally, addressing the limitations regarding generalizability and potential societal impacts will strengthen the overall contribution of the work.