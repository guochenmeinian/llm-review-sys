ID: 64sZtFSOh6
Title: ClevrSkills: Compositional Language And Visual Reasoning in Robotics
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 9, 7, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a dataset suite for developing large vision language model-based robot control, which can be utilized for training robots on multi-modal prompts for specific tasks. The authors propose an environment suite with 33 tasks across three levels to benchmark compositional understanding in robotic models. Additionally, the work introduces the ClevrSkills benchmark for robot compositional manipulation tasks, including datasets generated with scripted oracle policies, language, and visual annotations.

### Strengths and Weaknesses
Strengths:  
- The proposed dataset is timely and addresses the increasing need for multi-modal behavior models in current research.  
- It provides unique features such as compositionality and includes numerous examples in the appendix.  
- The benchmark is comprehensive, encompassing various annotations and keyframe data, which aids in understanding compositional reasoning tasks.  
- The construction of the dataset and benchmark enhances insights into the limits of current vision language models (VLMs) in robotic manipulation.

Weaknesses:  
- There are concerns regarding the real-world applicability of the multi-modal behavior model and the lack of video demonstrations to showcase trajectory quality.  
- The diversity of trajectories generated by scripted solvers is unclear, and the high failure rates of baselines on L1 and L2 tasks raise questions about their solvability.  
- The clarity of visualizations and the explanations of zero-shot generalization and fine-tuning processes require improvement.

### Suggestions for Improvement
We recommend that the authors improve the clarity of visualizations, particularly in Figures 1 and 3, to better illustrate their innovations. Additionally, the authors should provide more analysis and examples regarding zero-shot generalization to L1 and L2 tasks and the fine-tuning process. It would be beneficial to include video demonstrations to showcase the benchmark's trajectory quality. Furthermore, the authors should clarify the diversity of trajectories produced by the scripted solvers and consider benchmarking goal-conditioned policies using keyframes to specify subgoals for L1 and L2 tasks.