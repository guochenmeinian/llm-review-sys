ID: b2WpR0Fymj
Title: On the Universal Approximation Properties of Deep Neural Networks using MAM Neurons
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 3, 3, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents universal approximation results for Multiply-And-Max/min (MAM) neurons, which are similar to ReLU neurons but operate on the maximum and minimum of weighted inputs plus a bias. The authors demonstrate that any real-valued continuous function defined on a compact set can be approximated to any arbitrary degree of precision by a network primarily consisting of MAM neurons. Additionally, the paper shows that networks with MAM neurons can maintain universal approximation properties under various norms for target functions with differing smoothness. Furthermore, the authors discuss the pruning properties of MAM neurons in neural networks, highlighting their differences from existing methods like GroupSort. They propose that the absence of linear combinations in MAM, except at the last layer, allows for aggressive pruning, which is a significant distinction from prior works.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and presents important universal approximation properties, which are crucial when introducing new activation functions.  
- The constructive proofs and the introduction of localized hyper-rectangles may inspire further research in approximation problems.  
- The novelty of the theoretical guarantees on the universal approximation properties of MAM neurons is clear.  
- The authors clarify the significance of the lack of linear combinations in MAM neurons, which may contribute to aggressive pruning.  
- The manuscript addresses reviewer concerns effectively, demonstrating the authors' engagement with the feedback.  

Weaknesses:  
- The contribution is perceived as incremental due to the existence of many universal approximation results in the literature.  
- The authors do not adequately explore the advantages of MAM neurons, particularly regarding aggressive pruning, which weakens the significance of their findings.  
- The theorems and proof processes lack investigation into pruning properties, raising questions about their novelty.  
- There are issues with mathematical formatting and notation, leading to ambiguity in the proofs.  
- The requirement for target functions to be twice continuously differentiable is seen as a limitation, and the relationship between the two theorems presented is unclear.  
- Many related works are absent, leading to skepticism regarding the paper's innovation and contributions.  

### Suggestions for Improvement
We recommend that the authors improve the clarity and rigor of their proofs, particularly in Lemma 1, by removing ambiguous statements and ensuring that all assumptions are clearly defined. Additionally, we suggest that the authors provide a simpler proof for their results, potentially by adapting existing techniques for universal approximation. It would also be beneficial to discuss the practical implications of MAM neurons more thoroughly, including numerical experiments to demonstrate their efficiency. Furthermore, we encourage the authors to address the limitations of their results, particularly regarding the parameter n and the implications for layer width. Lastly, we recommend that the authors improve the depth of their literature review on neural network universal approximation properties to provide a more comprehensive context for their findings, and include a discussion on how the lack of linear combinations at the first layer contributes to the pruning capabilities of MAM neurons.