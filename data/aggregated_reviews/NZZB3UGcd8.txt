ID: NZZB3UGcd8
Title: Editing Large Language Models: Problems, Methods, and Opportunities
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a systematic analysis and comparative evaluation of various techniques for editing large language models (LLMs). The authors address the absence of comprehensive comparative analysis under uniform experimental conditions and propose a new evaluation metric called Portability to assess the generalization ability of different model editing methods. The main contributions include an exhaustive overview of current model editing methods, identification of the lack of comparative analysis, proposals for comprehensive benchmark evaluations on portability, locality, and efficiency, controlled experiments for impartial comparison, and guidance for selecting appropriate editing methods based on comparative analysis.

### Strengths and Weaknesses
Strengths:
- The topic is highly relevant and timely, addressing the important challenge of efficiently editing LLM behavior.
- The manuscript is well-structured and clearly written, effectively motivating the gap it aims to fill.
- Comparing editing models under uniform experimental conditions is a valuable contribution, and the proposed benchmarks enable robust evaluation.
- The focus on practical implications for method selection is significant for real-world applications.

Weaknesses:
- The scope is limited to factual knowledge editing; expanding the discussion to other editable attributes could enhance impact.
- Conclusions regarding MEND's performance on GPT-J lack convincing experimental results for other models like OPT-13B and GPT-NEOX-20B.
- The absence of sample results from comparative experiments limits the illustration of insights gained, and qualitative analysis is lacking.
- The paper could benefit from analyzing patterns in failures and providing intuitive explanations for performance degradation.

### Suggestions for Improvement
We recommend that the authors expand the discussion to include other editable attributes beyond factual knowledge editing. Additionally, we suggest providing experimental results for MEND on models like OPT-13B and GPT-NEOX-20B to support conclusions. Including sample results from comparative experiments would enhance the illustration of insights gained. Furthermore, we encourage the authors to analyze failure patterns and provide intuitive explanations for performance degradation to deepen understanding of the conditions under which certain editing techniques fail. Lastly, please ensure that salient points regarding encoder models and discussions of editing failures are documented in the paper.