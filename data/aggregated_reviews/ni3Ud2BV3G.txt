ID: ni3Ud2BV3G
Title: On the Impacts of the Random Initialization in the Neural Tangent Kernel Theory
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the impact of standard random initialization of deep neural networks within the neural tangent kernel (NTK) regime. The authors prove that under standard random initialization, the network function converges uniformly to the NTK predictor in the finite wide limit. They establish upper and lower bounds for the generalization errors of deep neural networks (DNNs) in the NTK regime, utilizing the real interpolation space of the reproducing kernel Hilbert space (RKHS). The paper also discusses the shortcomings of kernel gradient flow (KGF) in predicting neural network performance, particularly highlighting the effects of initialization.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a significant problem regarding the role of standard random initialization in neural network training and convergence.
- It is generally well-written and presents clear arguments, with compelling numerical results based on artificial data.
- The theoretical contributions, particularly Theorem 4.2, are novel and provide interesting insights into the behavior of neural networks under standard initialization.

Weaknesses:  
- Some technical results, such as those in Theorems 3.3 and 4.1, are not particularly surprising and their proofs appear standard.
- The positioning of Theorem 4.2 within existing literature on kernels and DNNs lacks clarity, necessitating further effort from the authors to enhance the paper's message.
- The paper contains several typographical and grammatical errors that detract from its overall presentation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by explicitly differentiating between the results of standard and mirror initializations in the introduction. Additionally, we suggest that the authors clarify which theorems are novel versus applications of known results, particularly regarding the decay rates. It would be beneficial to expand the discussion on how their findings fit into the broader context of NTK theory and to provide more detailed explanations of their experimental results in the main text. Furthermore, we advise correcting the identified typographical errors and ensuring consistent notation throughout the paper. Lastly, consider running experiments comparing mirror and standard initializations on real-world datasets to strengthen the empirical findings.