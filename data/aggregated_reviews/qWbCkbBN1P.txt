ID: qWbCkbBN1P
Title: Reducing Spurious Correlations in Aspect-based Sentiment Analysis with Explanation from Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a 2-step framework that utilizes large language models (LLMs) to augment datasets for aspect-based sentiment analysis (ABSA) with the aim of improving performance. The first step involves generating explanations for (sentence, aspect, polarity) triples using an LLM, while the second step employs these explanations either to augment training data or in a distillation setup. The authors propose that this approach mitigates spurious correlations associated with sentiment labels.

### Strengths and Weaknesses
Strengths:
- The framework demonstrates significant overall improvement in ABSA across multiple benchmark datasets.
- Robust experiments provide strong empirical evidence and deep analysis from various perspectives, including text length and robustness.
- The novelty of leveraging LLMs for data augmentation in ABSA is commendable.
- The methods are intuitive and the proposed training approaches enhance the quality of the results.

Weaknesses:
- The improvements achieved are relatively limited compared to other data augmentation baselines, raising concerns about cost-effectiveness given the resource-intensive nature of the framework.
- There is a lack of analysis on the extent of spurious correlations in the datasets, and the paper does not provide an error analysis to validate claims.
- Relying on potentially biased LLM-generated explanations necessitates caution, as they may not consistently yield accurate or comprehensive results.
- The paper lacks transparency regarding the snapshot version and parameters of the LLMs used, which affects reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the analysis of spurious correlations in the datasets to substantiate their claims. Conducting an error analysis would also strengthen the paper. Additionally, we suggest investigating the quality of LLM-generated explanations and implementing necessary quality control measures. Clarifying the snapshot version and parameters of the LLMs used will enhance reproducibility. Finally, a comparison of performance with a simple baseline that does not involve additional training data would provide clearer insights into the contributions of the proposed methods.