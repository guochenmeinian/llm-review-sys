ID: YKKcbwztwH
Title: Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a translation-based alignment method aimed at addressing multilingual disparities in vision-language models, specifically extending the monolingual CLIP to a multilingual context. The authors propose three main differences from the previous Multilingual-CLIP (M-CLIP): using independent captions for each language, aligning within English space, and employing language-specific parameters with parameter-efficient fine-tuning techniques. Empirical results demonstrate that this approach outperforms M-CLIP across various scenarios.

### Strengths and Weaknesses
Strengths:
- Improved average performance and reduced disparities across languages compared to M-CLIP.
- Comprehensive experiments and insightful analysis of translation-based alignment and parameter-efficient fine-tuning methods.
- The paper benchmarks several methods and provides recommendations for model trainers.

Weaknesses:
- Limited novelty, as the method combines existing ideas and lacks new insights specific to the task.
- Few-shot and full-dataset evaluations are conducted on only one dataset, insufficient for robust conclusions.
- Some sections, such as Section 5.4, resemble a technical report rather than an academic paper.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by demonstrating new insights specific to the multilingual vision-language task rather than relying on existing methods. Additionally, we suggest conducting evaluations across multiple datasets for few-shot and full-dataset scenarios to strengthen their conclusions. Merging Table 1 with Table 2 or providing a more detailed Table 1 in the appendix would enhance clarity. Lastly, we advise clarifying the claim regarding the alignment of the multilingual CLIP's encoder to English representations, potentially by conducting translate-test/train experiments with other languages as pivot languages.