ID: N12B6wvA55
Title: Mirror and Preconditioned Gradient Descent in Wasserstein Space
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 9, 7, 6, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of the mirror descent method in Wasserstein-2 spaces, focusing on constructing Bregman divergence functionals to define a mirror descent direction that generalizes classical Wasserstein gradient descent. The authors analyze the convergence of the Wasserstein mirror descent algorithm and provide numerical examples to demonstrate its effectiveness. Additionally, the integration of mirror descent and preconditioned gradient descent within the framework of Wasserstein distances is explored, supported by a thorough theoretical background and convergence analysis.

### Strengths and Weaknesses
Strengths:  
- The authors define and apply Wasserstein mirror descent directions effectively in sampling algorithms, showcasing a natural and novel approach.  
- The mathematical foundations are solid, with rigorous derivations of convergence behaviors for mirror descent and preconditioned gradient descent.  
- The paper is relevant to conference themes and provides a comprehensive background necessary for understanding the proposed methodologies.  

Weaknesses:  
- The analysis does not address the mirror descent of KL divergences for non-Gaussian target distributions, which could be an intriguing area for exploration.  
- Results presentation could be improved with better visualizations, such as particle evolution snapshots and contour plots using MATLAB's `surf` function.  
- Some sections lack clarity, particularly regarding definitions of convergence and the presentation of results, which could benefit from explicit definitions and enhanced visual aids.  
- The contribution paragraph does not effectively clarify the paper's unique offerings, and the computational complexity of the proposed methods requires more thorough discussion.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by providing more detailed visualizations and explicit definitions of key concepts, such as convergence and global convergence, in the appendix. Additionally, the authors should consider exploring the mirror descent of KL divergences for non-Gaussian distributions and clarify the main difficulties in implementing the Wasserstein-Bregman proximal gradient method in practice. Addressing the computational complexity and real-life runtime comparisons with benchmark methods would also enhance the paper's accessibility and impact. Finally, we suggest revising the "Contributions" paragraph to better articulate the unique contributions of the paper.