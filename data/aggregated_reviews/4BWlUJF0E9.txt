ID: 4BWlUJF0E9
Title: Towards Dynamic Message Passing on Graphs
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a dynamic message passing method, $N^2$, which utilizes pseudo nodes to facilitate communication between graph nodes, allowing for immediate interaction among non-adjacent nodes while maintaining linear complexity. The authors empirically demonstrate that this approach alleviates oversquashing and oversmoothing issues, achieving superior performance across various benchmarks for both node and graph classification tasks.

### Strengths and Weaknesses
Strengths:
- The writing is clear, with well-defined concepts and an easily understandable background.
- The methodology is innovative, with a sensible message passing design.
- Strong experimental results across multiple datasets, showcasing the method's applicability to both graph and node-level predictions.
- Effective visualizations enhance understanding of the results.

Weaknesses:
- The paper lacks theoretical explanations for the method's effectiveness against oversquashing and oversmoothing.
- Comparisons with graph structure learning or graph rewiring methods are insufficient, as the baselines primarily consist of GNNs and graph transformers.
- The motivation for using recurrent layers and proximity measurements over distance metrics is not intuitive and requires further clarification.

### Suggestions for Improvement
We recommend that the authors improve the theoretical foundation of their method by providing explanations for its effectiveness against oversquashing and oversmoothing. Additionally, a more comprehensive comparison with graph structure learning and rewiring methods should be included. The authors should clarify the rationale behind using recurrent layers and proximity measurements, possibly through an ablation study that examines the impact of removing the recurrent layer. Furthermore, details regarding hyperparameter tuning for both the proposed model and baseline methods should be explicitly stated to enhance reproducibility. Finally, we suggest increasing the font size of tables and figures to improve readability.