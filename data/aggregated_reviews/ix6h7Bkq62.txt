ID: ix6h7Bkq62
Title: Standardizing Distress Analysis: Emotion-Driven Distress Identification and Cause Extraction (DICE) in Multimodal Online Posts
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel task of Unified Distress Identification and Cause Extraction, addressing challenges in understanding multimodal models involving images and texts. The authors propose a multi-task framework, build a relevant dataset (DCaM), and benchmark various baselines. The study aims to enhance discussions on distress identification in multimodal posts, contributing a well-designed method and a valuable dataset.

### Strengths and Weaknesses
Strengths:
- The constructed dataset DCaM is comparable to existing datasets and provides insights into stressed samples.
- The model DICE, particularly the Inter-modal Fusion module, is a general approach with good reusability for multimodal tasks.
- The paper is easy to read and presents a well-designed methodology.

Weaknesses:
- Some descriptions are inconsistent, potentially leading to misunderstandings, particularly regarding the distinction between hate speech and distress content.
- Illustrations of the model DICE lack clarity, with symbols and modules in figures not aligning with the text.
- Existing experiments do not sufficiently demonstrate the effectiveness of DICE, lacking ablation studies and necessary baseline comparisons.
- Important references on multimodal hate detection are missing, and some sections lack clarity, such as metric definitions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of descriptions, particularly in distinguishing between hate speech and distress content. Additionally, the authors should enhance the illustrations of the model DICE to ensure that all symbols and modules correspond clearly to the text. We suggest including ablation experiments to validate the contributions of various modules and incorporating necessary baseline comparisons, such as CLIP and BERT+ResNet+MLP. Furthermore, the authors should provide clearer definitions for metrics and elaborate on experimental settings to facilitate reproducibility.