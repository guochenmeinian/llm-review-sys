ID: H5bUdfM55S
Title: LVD-2M: A Long-take Video Dataset with Temporally Dense Captions
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the LVD-2M dataset, a large-scale collection of long-take videos with temporally dense captions, aimed at addressing the need for high-quality video-text data for video generation models. The authors propose a sophisticated pipeline for filtering and annotating videos, which includes the use of MLLMs for quality assessment and a two-stage captioning process. The dataset is shown to be effective through experiments with T2V and I2V models, demonstrating its potential for various video-related tasks.

### Strengths and Weaknesses
Strengths:
1. The LVD-2M dataset fills a significant gap in high-quality video-text pairs for long-form video generation.
2. The technical methodology is clear and well-structured, making it replicable for future research.
3. Extensive qualitative and quantitative evaluations support the dataset's quality and its applications in video generation.

Weaknesses:
1. Clarity issues remain regarding the use of MLLMs for filtering low-quality videos and the rationale behind the choice of an image captioner over existing video captioners.
2. The experiments lack sufficient quantitative comparisons to validate the dataset's effectiveness against previous datasets.
3. Common filters, such as OCR and aesthetics scores, are not applied, raising questions about the dataset's overall quality.

### Suggestions for Improvement
We recommend that the authors improve clarity on how MLLMs filter low-quality videos and provide a detailed rationale for using an image captioner instead of video captioners. Additionally, we suggest including more comprehensive quantitative evaluations of the finetuning experiments to substantiate the dataset's claimed effectiveness. A discussion comparing LVD-2M with recent datasets like ShareGPT4Video should be added, along with an updated link to the dataset documentation. Finally, incorporating objective evaluation metrics for caption quality and exploring the dataset's applicability to various video tasks beyond generation would enhance the paper's contributions.