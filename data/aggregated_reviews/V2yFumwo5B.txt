ID: V2yFumwo5B
Title: Effective Human-AI Teams via Learned Natural Language Rules and Onboarding
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 5, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an HAI system called IntegrAI, designed to assist users in making decisions about when to rely on AI predictions, make their own predictions, or combine both. The authors perform semantic clustering using LLMs to identify disagreements between AI and human decisions, distilling these into actionable insights. The IntegrAI-describe algorithm involves creating clusters in a joint embedding space, generating candidate descriptions, searching for counterexamples, and iteratively updating clusters. Key claims include that IntegrAI learns optimal integration decisions, that rules generated by the oracle (GPT-3.5) are understandable, and that onboarding helps calibrate human expectations about AI performance, ultimately improving teaming performance. Additionally, the paper provides a detailed analysis of the statistical methods used to calculate accuracies for the "Onboard (ours)" dataset, highlighting that the data was twice-duplicated, resulting in an array of 100 accuracies instead of 50, while clarifying that the means and standard deviations of the original and duplicated datasets remain the same and explaining the implications of this duplication on the p-value and test statistic in the context of a two-sample t-test.

### Strengths and Weaknesses
Strengths:
- The IntegrAI framework is novel and intriguing.
- The IntegrAI-describe algorithm effectively leverages LLMs for semantic clustering.
- The user study is extensive and convincingly set up.
- The authors provide a clear explanation of the statistical implications of data duplication, ensuring that the mean and standard deviation remain unchanged, which enhances understanding of the results.

Weaknesses:
- Several main claims lack empirical substantiation, particularly the understandability of rules and the effectiveness of onboarding and recommendations.
- The significance testing for results in Table 3 appears questionable due to large standard errors.
- Initial concerns regarding the reporting of key claims and results indicate potential confusion in the methodology prior to clarification.
- The framework may not demonstrate its full potential in the traffic light setting, which may not be complex enough.

### Suggestions for Improvement
We recommend that the authors improve empirical validation for claims, particularly regarding the understandability of rules and the utility of onboarding and recommendations. It would strengthen the paper to evaluate the framework in more complex settings, such as multi-class scenarios with greater ambiguity. Additionally, we suggest that the authors clarify the significance testing issues noted in Table 3 and discuss the amount of paired data required for the proposed approach. Furthermore, we recommend improving the clarity of their statistical methodology in the initial presentation to prevent misunderstandings, ensuring that the distinction between population and sample standard deviation is explicitly stated in the context of their results. Lastly, enhancing the clarity of figures and text throughout the paper would improve overall presentation.