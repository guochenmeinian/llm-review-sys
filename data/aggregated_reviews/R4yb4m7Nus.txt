ID: R4yb4m7Nus
Title: Model-tuning Via Prompts Makes NLP Models Adversarially Robust
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel prompt-based tuning strategy, termed Model-tuning via Prompts (MVP), aimed at enhancing adversarial robustness in NLP models. The authors propose that MVP, which involves fine-tuning on prompts and ensembling results during inference, demonstrates superior robustness compared to traditional fine-tuning methods. Experimental results indicate that MVP outperforms previous state-of-the-art methods, particularly in adversarial settings.

### Strengths and Weaknesses
Strengths:  
- The idea of using prompts for robustness is compelling and shows effective results across various datasets.  
- Strong experimental results validate the efficacy of the proposed method, particularly in adversarial contexts.  
- Clear analysis and sound ablation studies support the claims made in the paper.  

Weaknesses:  
- The experimental evaluation is limited to simple classification tasks, lacking diversity in dataset complexity.  
- There is insufficient discussion on the inference latency as more templates are added for MVP.  
- The paper does not provide a direct comparison between MVP and LPFT regarding out-of-distribution generalization and adversarial robustness.

### Suggestions for Improvement
We recommend that the authors improve the experimental scope by evaluating MVP on more complex datasets and non-classification tasks. Additionally, a direct comparison between MVP and LPFT should be included to elucidate the relative advantages of each approach. Further analysis on the selection and performance of prompts, as well as the impact of prompt tuning beyond MVP+Adv, would enhance the paper's depth. Lastly, addressing the inference latency associated with the use of multiple prompts would provide a more comprehensive understanding of the method's scalability.