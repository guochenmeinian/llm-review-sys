ID: zgh0ChWocO
Title: Learning the Optimal Policy for Balancing Short-Term and Long-Term Rewards
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 4, 5, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 1, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to balancing multiple rewards by utilizing Pareto Policy Learning to optimize each reward up to the tradeoff frontier. The authors argue that this method is more practical than linear weighting, which applies constant weights regardless of conflicts between objectives. The empirical results indicate that the proposed method outperforms linear weighting on two synthetic tasks and some real data. The authors also formulate the problem as a multi-objective optimization problem using the Lagrange algorithm and preference vectors to achieve Pareto optimization.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant and well-motivated question regarding the combination of multiple rewards, with broad implications.
- The proposed method is mathematically sound, providing a theoretical basis for interpreting input parameters as worst-case values for each objective.
- The authors effectively highlight the limitations of linear weighting and demonstrate the superiority of their method through empirical evidence.

Weaknesses:
- The experimentation is limited, primarily relying on partial real data and synthetic generation of rewards. More realistic scenarios, such as robotic planning, would enhance the evaluation.
- The proposed method appears to favor short-term rewards over long-term ones, raising concerns about its long-term effectiveness.
- There is insufficient explanation regarding the optimality of the proposed method and the selection of preference vectors, as well as the definitions of several mathematical symbols and terms used in the paper.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including more realistic scenarios and additional comparison methods beyond linear weighting. Clarifying the mathematical definitions and providing necessary explanations for symbols and terms would enhance the paper's accessibility. Additionally, we suggest that the authors address the potential short-sightedness of their method in balancing long-term rewards and consider conducting a sweep over different configurations for preference vectors to validate their approach.