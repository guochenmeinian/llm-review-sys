ID: dNQETBrNcD
Title: Direct Feedback Alignment for Recurrent Neural Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 3, 6
Original Confidences: 5, 4

Aggregated Review:
### Key Points
This paper presents a method for training recurrent neural networks (RNNs) using direct feedback alignment (DFA) without propagating error signals back in time through the step-to-step Jacobians of the RNN dynamics. The authors derive update rules for parameters and compare the performance of recurrent networks (vanilla RNN and GRU) trained with this update rule against backpropagation through time (BPTT) on small-scale datasets. However, the paper lacks citations of relevant literature and comparisons with state-of-the-art methods, which is a significant oversight.

### Strengths and Weaknesses
Strengths:
- The methodological derivation is logical and convincing.
- DFA is presented as a promising alternative to BPTT, particularly for physical implementations.

Weaknesses:
- The paper does not cite or compare against relevant methods from the neuromorphic computing community, which is a major flaw.
- The experimental results are unclear and inconsistent, particularly regarding the performance metrics presented in tables versus figures.
- The evaluation does not meet the standards of the machine learning community, as it ignores the dynamics of recurrent networks.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including relevant works from the neuromorphic computing community and comparing against established baselines such as the e-prop algorithm. It is essential to benchmark the proposed method on standardized tasks, particularly those requiring long-range dependencies, such as the Long Range Arena benchmark. Additionally, we suggest clarifying the discrepancies between the results shown in the tables and figures, ensuring consistent hyperparameter settings across models, and addressing the performance gap between DFA and BPTT. Finally, the authors should provide a detailed explanation of the restrictions of the DFA-RNN scheme proposed by Han et al. (2020) and its applicability to gated architectures.