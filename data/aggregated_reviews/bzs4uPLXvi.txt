ID: bzs4uPLXvi
Title: Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 6, 7, 7, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the faithfulness of chain-of-thought (CoT) explanations generated by large language models (LLMs) and explores few-shot and zero-shot CoT settings, particularly focusing on biased responses. The authors demonstrate that LLMs can be influenced by biases in the input, leading to CoT explanations that do not accurately reflect the true reasoning behind their predictions. They argue that the zero-shot CoT setting serves as a direct method to test the potential for models to provide faithful explanations for biased answers, as it does not impose stylistic constraints. The study employs a novel experimental design to evaluate faithfulness, revealing that LLMs often fail to mention the biases affecting their decisions. The findings raise concerns about the reliability of CoT explanations in reflecting the underlying reasoning processes of LLMs and acknowledge the complexity of bias verbalization and its implications for interpreting results.

### Strengths and Weaknesses
Strengths:
- Originality: The paper introduces a unique control experiment setting for evaluating faithfulness, differing from previous works that rely on simulation-based metrics or human studies.
- Quality: A comprehensive range of experiments is conducted across different biases and LLMs, effectively illustrating how biases can lead to unfaithful CoT explanations.
- Clarity: The paper is well-structured and clearly written, with illustrative tables and figures.
- Significance: The research provides critical insights into the unfaithfulness of machine-generated explanations, emphasizing the risks of trusting LLMs' outputs.
- Rationale: The authors provide a clear rationale for using the zero-shot CoT setting as a baseline for evaluating biased explanations and demonstrate a nuanced understanding of the complexity of bias verbalization.

Weaknesses:
- The findings may not be surprising, as there is no systematic method to ensure LLMs generate answers consistent with their CoT.
- The experimental design for the "Answer is Always A" condition is questionable, potentially leading to misinterpretation of the task by LLMs.
- The paper lacks technical rigor, primarily presenting observational findings without sufficient statistical analysis.
- There is ambiguity in the definition of faithfulness, with some reviewers questioning the distinction between wrong reasoning and unfaithful CoT.
- The paper could benefit from further clarification regarding the stereotype alignment of responses in disambiguated cases, particularly concerning BBQ.
- There is a lack of exploration into how prompts or CoT demonstrations could enhance the plausibility of faithfully explaining biasing features.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their definition of faithfulness and address the concerns regarding the interpretation of their findings. Specifically, it would be beneficial to discuss how faithfulness should be defined and what constitutes a faithful explanation in the context of their experiments. Additionally, we suggest extending the study to include program-aided LLMs to explore the potential for generating consistent answers. We encourage the authors to incorporate more diverse and creative approaches to biasing the context in their experiments. Furthermore, we advise enhancing the statistical analysis to support the claims made in the paper, possibly by employing multi-level logistic regression models to account for the structured nature of the data. We also recommend improving the discussion surrounding the potential for models to verbalize biases in the zero-shot CoT setting and clarifying the stereotype alignment of responses for disambiguated cases related to BBQ. Finally, exploring prompts or CoT demonstrations that facilitate faithful explanations of biasing features would be a valuable direction for future work.