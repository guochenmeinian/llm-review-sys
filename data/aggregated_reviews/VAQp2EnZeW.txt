ID: VAQp2EnZeW
Title: Training Neural Networks is NP-Hard in Fixed Dimension
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 8, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the complexity of training two-layer neural networks with ReLU or linear threshold activations. The authors demonstrate that training such networks to achieve zero training error is NP-hard when the input dimension \(d \geq 2\). They establish that there is no polynomial algorithm with respect to the number of data points \(n\) and hidden nodes \(k\) (where \(k < n\)). Additionally, they prove \(W[1]\)-hardness with respect to \(d\) when \(k = 4\) and provide a branching algorithm for learning these networks, which exhibits exponential time complexity concerning \(k\) and \(d\).

### Strengths and Weaknesses
Strengths:  
The paper effectively addresses open questions regarding the complexity of training two-layer ReLU networks, presenting significant results. The motivation and background are well-articulated, and the geometric insights, particularly regarding the concept of a levee, are informative.

Weaknesses:  
The setting considered is detached from practical generalization, as the sample complexity for fixed dimensions is trivial. The variable \(\ell\) is overloaded, causing potential confusion. Additionally, the authors overlook substantial literature on efficient learning thresholds, and the practical implications of their results seem limited, as training two-layer ReLU networks is typically efficient.

### Suggestions for Improvement
We recommend that the authors improve the connection between their theoretical results and practical applications by discussing assumptions on architectures and regularizations that could lead to efficient training algorithms. Clarifying the use of the variable \(\ell\) and addressing the existing literature on learning thresholds would enhance the paper's depth. Furthermore, we suggest removing the sentence about assuming basic knowledge of computational complexity theory, as it may be unnecessary. Lastly, providing insights into extending their results to networks with multiple layers would be beneficial.