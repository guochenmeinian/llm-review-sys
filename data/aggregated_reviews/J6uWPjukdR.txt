ID: J6uWPjukdR
Title: Data Similarity is Not Enough to Explain Language Model Performance
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the correlation between semantic similarity to pretraining data and model performance across various tasks. The authors test this hypothesis using both coarse-grained and fine-grained similarity metrics on multilingual evaluations and English benchmarks, specifically text classification, (X)NLI, and BigBench tasks, with models pretrained on the Pile and C4 corpora. The findings reveal that while similarity to pretraining data correlates with performance in multilingual contexts, this correlation does not hold for finer-grained evaluations within English. The paper challenges the assumption that data similarity is a primary determinant of model performance, suggesting that this relationship is more complex than previously thought.

### Strengths and Weaknesses
Strengths:
- The paper presents an innovative challenge to the widely accepted "similarity hypothesis," providing a fresh perspective in NLP research.
- It includes a thorough evaluation using diverse similarity metrics and benchmarks, enhancing the robustness of the findings.
- The experiments are well-designed, offering valuable insights into the intricate relationship between data similarity and model performance.
- The counterintuitive results prompt a reevaluation of established beliefs, potentially guiding future research directions.

Weaknesses:
- The limited scope of datasets may restrict the generalizability of the findings to other tasks or datasets.
- The similarity measures employed may not fully capture the complexity of the relationship between pretraining and task data.
- The assumption of linearity in the relationship between similarity and performance may oversimplify the findings.
- The treatment of task difficulty as a constant factor may overlook its multifaceted nature.

### Suggestions for Improvement
We recommend that the authors improve the discussion on multilinguality to align with existing literature, particularly regarding the presence of non-English text in pretraining datasets. Additionally, it would be beneficial to elaborate on why the chosen similarity measures are not correlated with each other and to clarify what each metric measures, potentially moving Table 2 to the main body for better context. Addressing the questions posed, such as the exclusion of MAUVE scores in Table 2 and the rationale behind evaluating only five languages in the multilingual XNLI dataset, would also strengthen the paper. Lastly, we suggest emphasizing that while the correlation between similarity and performance is inconsistent, it does not negate the potential influence of data similarity in certain contexts.