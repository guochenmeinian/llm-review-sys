ID: bPOaHf8OcX
Title: Vivid-ZOO: Multi-View Video Generation with Diffusion Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "Vivid-ZOO," an innovative diffusion model for Text-to-Multi-view-Video (T2MVid) generation, addressing the challenges of creating high-quality multi-view videos from textual descriptions. The authors propose a method that factorizes the T2MVid generation problem into viewpoint-space and time components, enabling the reuse of layers from pre-trained multi-view image and 2D video diffusion models. They claim this is the first study to apply diffusion models to T2MVid generation. The authors also introduce alignment modules to bridge domain gaps and provide a captioned multi-view video dataset to support their research and the wider community.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel diffusion-based approach for generating multi-view videos from text prompts, addressing a relatively unexplored area in the literature. The factorization into viewpoint-space and time components is an innovative solution to the complexity of multi-view video generation.
2. The proposed method achieves impressive visual results, with performance metrics showing improvements over naively combining MVDream and AnimateDiff.
3. The introduction of alignment layers effectively reduces parameters and training time while addressing domain gaps, making sophisticated multi-view video generation more accessible.

Weaknesses:
1. The visual quality of the generated videos may not match state-of-the-art single-view image generation models, indicating a need for improvement in visual fidelity and realism.
2. The model's assumption of point light sources may limit realism; enhancements in simulating complex lighting conditions are necessary.
3. The authors' claim of being the first in T2MVid generation overlooks existing related works in 4D generation, necessitating a broader comparison in Table 1.

### Suggestions for Improvement
We recommend that the authors improve the visual fidelity and realism of the generated multi-view videos. Enhancements in simulating complex lighting conditions should be considered to increase the model's applicability. Additionally, we suggest that the authors clarify their position within the existing literature on 4D generation and include relevant works for comparison in Table 1. Expanding the evaluation section with more generated videos and addressing failure cases would strengthen the justification of their method. Lastly, providing details on the dataset used for computing CLIP and FVD scores, as well as ensuring the prompts are separate from the training set, would enhance transparency in their evaluation process.