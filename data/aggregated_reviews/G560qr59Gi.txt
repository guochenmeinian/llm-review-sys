ID: G560qr59Gi
Title: Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of convergence and implicit bias in two-layer (leaky) ReLU networks trained with gradient descent, focusing on cases with small initialization scales and step sizes, and nearly orthogonal training data. The authors demonstrate that for leaky ReLU networks, the weight matrices converge to stable rank 1, while for ReLU networks, the stable ranks are bounded by a constant, potentially reaching approximately 2. They also provide convergence rate analyses for both cases.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant challenge in understanding the implicit bias of gradient-based algorithms, particularly in practical settings by focusing on gradient descent rather than gradient flow.
- The theoretical results are detailed and well-supported by extensive appendices and experimental data, including synthetic data and MNIST.

Weaknesses:
- The claim regarding ReLU networks having superior learning ability compared to leaky ReLU networks lacks clarity and justification based on the results.
- The relationship between low stable rank and learning ability is not straightforward, as the stable rank does not directly provide learning guarantees.
- The experiments on MNIST do not clearly differentiate their contribution from prior work by Frei et al. 2022b.
- Several typos and unclear definitions are present throughout the paper.

### Suggestions for Improvement
We recommend that the authors improve the justification for the claim about ReLU networks' learning ability compared to leaky ReLU networks, ensuring that the argument is clearly articulated. Additionally, clarify the implications of stable rank on learning ability and provide a more distinct contribution of the MNIST experiments relative to prior work. Address the identified typos and ensure all terms are well-defined to enhance clarity and precision.