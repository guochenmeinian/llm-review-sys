ID: D4QgSWxiOb
Title: Grokking of Implicit Reasoning in Transformers: A Mechanistic Journey to the Edge of Generalization
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 8, 7, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into whether transformer models can learn implicit reasoning through the phenomenon of "grokking," focusing on two reasoning types: composition and comparison. The authors demonstrate that grokking enables models to generalize effectively, with empirical findings showing that grokking occurs in in-distribution generalization for composition reasoning but not for out-of-distribution, while both types of reasoning exhibit grokking in comparison tasks. The authors explore the internal mechanisms of grokking, correlating generalization performance with reasoning circuits. Additionally, the paper investigates two synthetic tasks: the "composition" task, which involves following a path of length 2 in a graph, and the "comparison" task, which entails looking up and comparing two items in a dataset. The findings indicate that transformers generalize these tasks only after grokking, with the composition task showing weaker generalization compared to the comparison task. Causal tracing reveals that the network accesses individual edges for the composition task at different computation stages, while the comparison task allows for all information to be stored in a single subsection. The paper also manipulates dataset characteristics to assess their impact on training and evaluates the performance of two commercially available LLMs in a zero-shot setting. The authors propose inspiring progress measures and suggest that future work could investigate similar quantitative measures and visualize them during the grokking process of the studied tasks.

### Strengths and Weaknesses
Strengths:
- The paper effectively explores the grokking phenomenon and its implications for implicit reasoning in transformer models, providing timely insights relevant to the NeurIPS community.
- It offers a comprehensive analysis of generalization performance and internal reasoning circuits, elucidating the mechanisms behind different generalization behaviors.
- The results demonstrate that well-trained transformers can outperform state-of-the-art models in complex reasoning tasks, emphasizing the importance of parametric knowledge.
- The causal tracing provides insights into how transformers solve different types of tasks, highlighting the differences in generalization potential between chained and parallel retrieval.

Weaknesses:
- The findings may have limited applicability to real LLMs due to the use of structured synthetic data and single-task training, as discussed in the appendix.
- The novelty of the insights regarding grokking may be diminished by existing literature that addresses similar mechanisms and phenomena, potentially overlapping with prior works.
- The paper overclaims its contributions, particularly in framing the work as a study of reasoning rather than a focused investigation of two rigidly structured synthetic tasks. The title may be misleading, and the framing around reasoning is seen as an overreach.
- The analysis of results is insufficient, particularly concerning dataset manipulations and the implications of findings on internal model mechanics. The suggestions for improving transformers and training lack robust support from the experiments.
- The focus on implicit reasoning may limit the applicability of the findings to broader contexts, particularly in relation to in-context learning.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the transferability of their findings to larger, more general models like LLMs, particularly regarding the implications of weight decay and parameter sharing. Additionally, we suggest providing more detailed explanations of the experimental setup, including tokenization and loss function evaluation, to enhance clarity. Addressing the limitations of using synthetic data and exploring a broader range of reasoning tasks would strengthen the paper's contributions. We also recommend improving the framing of their work to accurately reflect the investigation of two specific synthetic tasks rather than broader claims about reasoning, and adjusting the title to be less assertive. Furthermore, enhancing the analysis of results, particularly regarding the dataset manipulations and their implications, would provide clearer support for the conclusions drawn. It would be advantageous to consolidate concrete suggestions for data, training, and architecture improvements into a clear list within the paper, each linked to supporting evidence. Lastly, we encourage the authors to clarify the causal tracing methodology and its applicability to more complex reasoning scenarios, and consider conducting multiple experiments for each task to verify the consistency of the circuits identified across different settings.