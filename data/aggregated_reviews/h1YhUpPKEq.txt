ID: h1YhUpPKEq
Title: Non-Programmers Can Label Programs Indirectly via Active Examples: A Case Study with Text-to-SQL
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to annotating natural language descriptions with SQL queries using non-programmers through the APEL framework. The authors propose that non-programmers can select the correct candidate program from examples based on input/output pairs, rather than synthesizing programs from scratch. The technique involves generating seed programs and synthetic databases to maximize information gain, allowing non-programmers to evaluate candidate programs iteratively. The authors demonstrate the effectiveness of their approach through a human study and a case study on SPIDER development data, achieving comparable accuracy to expert annotations.

### Strengths and Weaknesses
Strengths:
- Timely and relevant topic, addressing the increasing reliance on automation in program synthesis.
- Well-executed evaluation showing significant improvements in labeling accuracy.
- Intuitive technique based on insights into active learning and human-in-the-loop labeling.
- Clear presentation of implementation details and extensive analyses.
- The APEL framework is well-motivated and mathematically sound.

Weaknesses:
- Small human subject pool raises concerns about the generalizability of results.
- Lack of qualitative methods in the human subject study limits insights into participant approaches.
- Potential overclaiming regarding the generalizability of the approach to non-SQL settings.
- Limited comparison with existing methods, such as self-consistency and CodeT, which could provide context for APEL's effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their findings by increasing the size of the human subject pool to enhance generalizability. Additionally, incorporating qualitative methods, such as structured interviews or post-surveys, could provide valuable insights into the annotators' decision-making processes. The authors should clarify how APEL could generalize to non-SQL settings, particularly regarding the selection of intuitive inputs and execution of candidate programs. Finally, we suggest including comparisons with existing annotation methods to contextualize APEL's performance more effectively.