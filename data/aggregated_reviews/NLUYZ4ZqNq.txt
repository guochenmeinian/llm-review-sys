ID: NLUYZ4ZqNq
Title: SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 8, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two new legal-specific large language models (LLMs), SaulLM-54B and SaulLM-141B, based on the Mixtral architecture. The authors propose significant advancements in training larger models using an extensive legal dataset and various pre-training best practices. The study emphasizes the scalability of domain adaptation and the models' instruction-following capabilities, achieving state-of-the-art performance on LegalBench-Instruct. Additionally, the paper explores domain adaptation of base models, focusing on performance comparisons with models like GPT-4, despite challenges posed by differing model sizes. The authors address the limitations of current legal benchmarks and highlight the significance of their findings in the context of legal tasks.

### Strengths and Weaknesses
Strengths:
- The paper introduces the largest domain-specific legal LLMs to date, with parameters ranging from approximately 12B to 141B, and curates the largest legal corpus for pretraining.
- Results demonstrate superiority over state-of-the-art general-purpose models, with detailed performance analyses provided, including thorough comparisons with GPT-4.
- The study is well-written, clear, and presents thorough experiments, making it a valuable contribution to legal NLP.
- The incorporation of licensing information and compliance measures reflects a commitment to best practices.

Weaknesses:
- The paper lacks comprehensive ablation studies on training methods, missing comparisons of model performance across different architectures and training strategies.
- Key details regarding the datasets, particularly for synthetic data generation and legal instruction data, are insufficiently explained.
- The evaluation does not include comparisons with existing domain-specific models, and the quality of synthetic data remains uncertain.
- The generalizability of the findings is limited by the constraints of available benchmarks, which require significant investment to develop.
- The focus on a single benchmark (LegalBench) may restrict the applicability of the results to other jurisdictions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the synthetic data generation process by detailing the prompts used and the models employed. Additionally, including examples of instruction and preference data in the appendix would enhance reader understanding. We suggest conducting a more comprehensive ablation study to illustrate the effectiveness of various training methods and comparing their models with existing legal LLMs. Furthermore, we recommend that the authors improve the clarity of the balanced accuracy label and explicitly address the limitations of their benchmarks in the paper. Finally, considering an expansion of their evaluation to other jurisdictions in future work would strengthen the applicability of their findings.