ID: HWGWeaN76q
Title: On the Convergence and Sample Complexity Analysis of Deep Q-Networks with $\epsilon$-Greedy Exploration
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the convergence and sample complexity of Deep Q-Networks (DQN) using an $\epsilon$-greedy behavior policy. The authors provide guarantees on convergence and sample complexity, demonstrating that the DQN algorithm can achieve a sample complexity of $1/\sqrt{N}$ under certain conditions. The analysis includes a novel lower bound on the Hessian of the population risk function and extends previous work by relaxing restrictive assumptions. Additionally, the authors address sample complexity in deep reinforcement learning while imposing minimal assumptions on the initial policy and no assumptions on the environment. They clarify that their initialization requirement in equation (12) pertains to the optimization analysis of the objective function, with $C_0$ reflecting the policy difference. Their findings in equation (20) can be interpreted as a representation of sample complexity, and they highlight their pioneering study on the sample complexity of DQN with an epsilon-greedy policy.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and clear, making significant theoretical contributions to the understanding of DQN with $\epsilon$-greedy exploration.  
- The derivation of sample complexity under a time-varying policy is meaningful, and the technique for bounding the Hessian matrix is novel.  
- The authors provide a clear clarification of their assumptions, emphasizing minimal requirements on the initial policy and no knowledge of the environment.  
- The paper contributes to the understanding of sample complexity in deep reinforcement learning, particularly with DQN.  
- The authors effectively engage with reviewer feedback, enhancing the rigor of their analysis.

Weaknesses:  
- The exploration strategy $\epsilon_t$ relies on prior knowledge of the optimal policy, which is impractical.  
- The assumption that mini-batches are sampled from a time-varying stationary distribution is questionable.  
- The dependence of the step-size on the number of outer loop iterations raises concerns about the robustness of the results.  
- The mathematical rigor could be improved by directly imposing equation (*) as an assumption, as suggested by one reviewer.  
- The connection between the proposed assumptions and existing literature could be more explicitly articulated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Corollary 1 by providing a more explicit method for choosing $\epsilon_t$, as the current formulation is unclear. Additionally, we suggest revising Theorem 1 to avoid referring to $W^{T,0}$ as the convergent point, as this may lead to confusion. It would also be beneficial to address the theoretical limitations highlighted in the weaknesses, particularly regarding the assumptions made about the algorithm's trajectory in Corollary 3. Furthermore, we recommend that the authors improve the mathematical rigor of the paper by imposing equation (*) directly as an assumption and adding a remark indicating that it can be relaxed to the algorithm-trajectory-dependent assumption. Lastly, we encourage the authors to provide more detailed explanations for the definitions and techniques used, such as the local convex region in equation (11) and the intuition behind Lemma 6, as well as further clarifying the connection between equation ($\star$) and (22) to enhance the overall clarity and rigor of the analysis.