ID: nbqvjkOs6S
Title: Gradient-free Decoder Inversion in Latent Diffusion Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 3, 4, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a gradient-free method for decoder inversion in latent diffusion models (LDMs), which significantly reduces computational complexity and memory usage compared to traditional gradient-based methods. The authors provide theoretical support for the method's convergence and demonstrate its effectiveness in practical applications, such as watermark classification using Stable Diffusion 2.1 and InstaFlow models. Empirical results indicate that the proposed method achieves comparable accuracy while being more efficient.

### Strengths and Weaknesses
Strengths:
1. The authors introduce a novel gradient-free method that is faster and more memory-efficient than traditional approaches.
2. The paper includes a theoretical analysis confirming the convergence of the proposed method.
3. Extensive experimental results validate the method's efficiency and effectiveness across various LDMs.
4. The writing is clear, with well-structured explanations and a focus on practical applications.

Weaknesses:
1. Comparative experiments primarily focus on memory usage and runtime, with limited evidence on accuracy compared to gradient-based methods across diverse applications.
2. The experiments validating convergence are restricted to specific hyperparameters, raising questions about generalizability.
3. The methodology description is complex, with assumptions that are not sufficiently backed by proofs.
4. The advantages of the proposed method appear limited, and there is a lack of qualitative results to support its efficacy.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology description and provide more detailed explanations of the assumptions made. Additionally, conducting an ablation study on the impact of using the Adam optimizer and cosine learning rate decay would enhance the comprehensiveness of the findings. We also suggest including qualitative results similar to those in previous works to demonstrate the effectiveness of the proposed method. Expanding the experimental scope to include diverse applications, such as image generation and editing, would provide a more robust evaluation of the method's utility. Finally, a more in-depth discussion on the accuracy trade-offs and scenarios where the proposed method may fall short would offer a balanced perspective.