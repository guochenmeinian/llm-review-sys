ID: UQpbq4v8Xi
Title: Generating Data for Symbolic Language with Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for automatically creating semantic parsing datasets by prompting large language models (LLMs), which can then be used to train smaller task-specific models. The authors demonstrate the effectiveness of their approach in generating questions and responses using the 175B Codex model, leading to performance comparable to Codex when fine-tuning T5-3B on the generated data. The method is particularly effective in low-data regimes, requiring only a few (~10) human-labeled samples. The authors conduct detailed ablations to identify essential components of the prompts that affect data quality.

### Strengths and Weaknesses
Strengths:
- The method is simple, intuitive, and highly reusable across domains.
- The experiments are comprehensive and convincing, showing large improvements in low-data settings.
- Performance is comparable to strong baselines, and the generated data can be beneficial for others.
- The relevance of the paper to the field is strong, as it reduces the need for human effort in data generation.

Weaknesses:
- The paper lacks significant novelty; the main contribution is the exploration of off-the-shelf LLM capabilities for data generation.
- There are vague details regarding the generation of natural language questions.
- Some citations are missing, and the inclusion of agreement-based verification, while interesting, is somewhat obvious.
- A deeper exploration of how SymGen might be scaled to new domains is missing.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding how natural language queries are generated, specifying whether they are solely derived from LLM prompts. Additionally, we suggest that the authors discuss the scalability of SymGen to new domains, addressing the requirements for domain-specific knowledge, execution and validation methods, and few-shot demonstrations. Furthermore, consider using 'bad' examples for contrastive training of the downstream model instead of discarding them. Lastly, we advise against relying on the Appendix for essential results, ensuring that the main text is self-contained for reviewer assessment.