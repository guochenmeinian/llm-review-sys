ID: 1IqCKEGGgw
Title: Consistency Guided Knowledge Retrieval and Denoising in LLMs for Zero-shot Document-level Relation Triplet Extraction
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GenRDK, a novel method for zero-shot document-level relation triplet extraction (ZeroDocRTE). Unlike existing approaches that focus on sentence-level extraction, GenRDK generates document-level synthetic data for unseen relations using a chain-of-retrieval (CoR) prompt to guide ChatGPT in producing relevant documents. The quality of this synthetic data is enhanced through a denoising strategy that utilizes scores from synthetic and pseudo triples. The authors demonstrate the effectiveness of GenRDK through extensive experiments, showing its superiority over competitive baselines in both ZeroRE and ZeroRTE tasks.

### Strengths and Weaknesses
Strengths:
- The innovative CoR prompts lead to improved synthetic data generation compared to vanilla and chain-of-thought prompts.
- The denoising strategy effectively addresses issues of noise in the generated data, enhancing overall quality.
- Experimental results validate the framework's effectiveness, supported by important ablation studies.

Weaknesses:
- The model's reliance on synthetic data for unseen relations raises questions about the true zero-shot nature of the approach.
- There is insufficient discussion on why existing sentence-level methods are inadequate for ZeroDocRTE.
- Uncertainty exists regarding whether baseline methods were fine-tuned on seen data, which could skew comparisons.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the zero-shot nature of their method by addressing how the training strategy aligns with typical zero-shot learning principles. Additionally, we suggest providing more detailed discussions on the limitations of existing sentence-level methods for document-level tasks, possibly including evaluation metrics that highlight their weaknesses. It is also essential to clarify whether the reported baselines were fine-tuned on seen data and to compare GenRDK with other methods that utilize generated data, such as RelationPrompt, to establish a fair benchmark. Finally, we encourage the authors to explore the sensitivity of their framework to different prompt designs and to report on the generalizability of their results across diverse datasets.