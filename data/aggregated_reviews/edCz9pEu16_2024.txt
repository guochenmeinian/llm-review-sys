ID: edCz9pEu16
Title: Can Editing LLMs Inject Harm?
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 8, 7, 6
Original Confidences: 4, 4, 3

Aggregated Review:
### Key Points
This paper presents "Editing Attacks" that utilize knowledge editing methods to inject misinformation and bias into large language models (LLMs). The authors investigate two primary risks: misinformation and bias injection, using the EDITATTACK dataset and evaluating the effectiveness of these attacks through the ROME, FT, and ICE methods. The findings indicate that editing attacks can significantly undermine the fairness and factual integrity of LLMs, posing a stealthy threat for disseminating harmful content.

### Strengths and Weaknesses
Strengths:
- The paper addresses a crucial topic in LLM security.
- It provides a clear summary of the Edit Attack technique.
- The experimental evaluation is thorough, demonstrating the efficacy of the attacks.
- The writing is well-structured and easy to understand.
- The authors provide the code and data, enhancing the credibility of their work.

Weaknesses:
- Concerns about novelty and the application scenario of the attacks.
- Lack of discussion on the new challenges posed by injecting harmful content into neural networks.
- Insufficient exploration of the implications of model providers publishing poisoned models.
- The paper does not adequately address how existing safeguard mechanisms in LLMs were bypassed.
- Questions arise regarding the handling of potentially controversial content and its classification as misinformation or bias.

### Suggestions for Improvement
We recommend that the authors improve the conciseness of the main body to allow for more detailed discussion on the preparation of the EditAttack dataset and further evaluation details. Additionally, addressing the novelty concerns by discussing new challenges in injecting harmful content into neural networks would enhance the paper's scope. The authors should also explore the implications of model providers publishing poisoned models and clarify how they bypassed existing safeguard mechanisms in LLMs. Finally, a more thorough discussion on how potentially controversial content is handled would strengthen the paper's impact.