ID: VKHWtusV6H
Title: DSI++: Updating Transformer Memory with New Documents
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DSI++, an approach that enhances Differentiable Search Indexes (DSI) for continual learning by addressing catastrophic forgetting during updates with new data. The authors analyze the issue of memorization in the FLAN-T5 model and propose techniques for implicit forgetting using Sharpness-Aware Minimization and generative memory replay. The paper is well-structured and provides experimental evidence supporting its claims.

### Strengths and Weaknesses
Strengths:  
- The motivations for the proposed designs are clear, and the paper is well-written and detailed.  
- It addresses a significant problem in continual learning, demonstrating the effectiveness of SAM and episodic replay in reducing forgetting.  
- The authors promise to release two benchmarks based on Natural Questions and MS MARCO, which could foster further research.

Weaknesses:  
- Implementation details and related work should be included in the main content rather than the appendix.  
- The experiments are limited to the FLAN-T5 architecture, lacking generalizability across different models.  
- The novelty of the approach is questioned, as SAM and episodic replay have been previously established in other NLP tasks.  
- Some experimental settings are unclear, potentially leading to misleading conclusions.

### Suggestions for Improvement
We recommend that the authors improve the paper by including implementation details and related work in the main text. Additionally, expanding experiments to include different architectures and baseline methods, such as LWF (Learning without Forgetting), would enhance the generalizability of the findings. The authors should also clarify the experimental settings to avoid ambiguity and provide a more comprehensive analysis of the dataset choices and their implications on the results. Finally, including results from larger models, like T5-XXL, would strengthen the study's conclusions.