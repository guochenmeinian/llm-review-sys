ID: gJXydPLBkt
Title: QUDeval: The Evaluation of Questions Under Discussion Discourse Parsing
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, 4

Aggregated Review:
### Key Points
This paper presents an evaluation framework for the Questions Under Discussion (QUD) task, which involves generating a question from an article that is answered by a sentence within the article. The authors propose a set of evaluation criteria inspired by error analysis of existing QUD systems and previous studies. They apply these criteria to the outputs of three QUD systems and a dev set from the DCQA dataset, with annotations provided by a trained annotator. The findings indicate that the ChatGPT system produces the most compatible questions but suffers from answer leakage, while the fine-tuned system is prone to hallucination. The paper also introduces the QUDeval dataset, containing automatically generated (implicit-question, anchor) pairs with human quality annotation, aimed at evaluating QUD parsing metrics.

### Strengths and Weaknesses
Strengths:
- The paper introduces a new and useful dataset along with well-justified evaluation criteria likely to be adopted by other researchers.
- It presents a thorough set of experiments revealing avenues for future progress in QUD tasks.
- The annotation protocol is carefully described, suggesting good quality annotations.

Weaknesses:
- The introduction lacks sufficient motivation for the task, particularly in explaining the relevance of QUD in the listed applications.
- The inter-annotator agreement is not high, raising concerns about annotation quality.
- There are minor presentation issues that should have been addressed prior to submission.

### Suggestions for Improvement
We recommend that the authors improve the introduction by clearly articulating the significance of the QUD task and its applications. Additionally, addressing the annotation quality concerns by enhancing inter-annotator agreement would strengthen the paper. Clarifications are needed in several sections, such as explaining the insufficiency of certain criteria (lines 169-171) and detailing the source of reference questions in Section 5.2. We also suggest discussing the annotator disagreement examples more thoroughly, particularly in Table 21, and ensuring that the statistical significance of performance differences in Table 1 is reported. Finally, consider revising the use of ROUGE in Table 5, as recall may not be a priority for this evaluation.