ID: DNHGKeOhLl
Title: On the Stability-Plasticity Dilemma in Continual Meta-Learning: Theory and Algorithm
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 6, 7, 7, 5, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 3, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical framework for Continual Meta-Learning (CML) that addresses the balance between stability (preserving past knowledge) and plasticity (rapid learning from new experiences) in both static and shifting task environments. The authors propose a novel algorithm, Dynamic Continual Meta-Learning (DCML), which dynamically adjusts meta-parameters and learning rates to minimize expected regret when environment changes occur. The paper derives upper bounds on average excess risk and empirically evaluates the proposed method against benchmarks.

### Strengths and Weaknesses
Strengths:
1. The paper is theoretically robust, introducing a unified framework for understanding CML and the stability-plasticity dilemma.
2. The proposed DCML algorithm shows improved performance and estimated bounds compared to baselines.
3. The experimental validation is extensive and convincing, with relevant citations and well-organized presentation.

Weaknesses:
1. The practicality and necessity of continual meta-learning remain unclear, particularly for non-experts; more real-world examples would enhance understanding.
2. The paper lacks detailed explanations on the robustness of the proposed algorithm, including parameter settings and their impacts.
3. The method introduces many hyperparameters without clear empirical verification of their effects, and the presentation could better contextualize theorems and define symbols.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by incorporating more intuitive explanations and real-world applications of continual meta-learning throughout the text. Additionally, providing a visual representation of the base model and how continual meta-learning fits into the setup would enhance reader comprehension. It would also be beneficial to include a comparison of practical scenarios such as meta-learning, continual-learning, meta-continual-learning, and continual-meta-learning in an appendix. Furthermore, we suggest that the authors clarify how the algorithm dynamically adjusts meta-parameters and learning rates, and provide more details on parameter tuning and the impact of initial settings on model behavior.