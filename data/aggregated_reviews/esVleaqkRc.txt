ID: esVleaqkRc
Title: Neur2BiLO: Neural Bilevel Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 8, 6, 4, 4, -1, -1, -1
Original Confidences: 3, 4, 3, 5, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two approximate methods for solving constrained, mixed-integer, non-linear bilevel optimization problems. The authors propose converting the bilevel problem into a single-level problem using neural networks trained offline. The upper-level approach predicts the leader's objective function based on a fixed leader decision, while the lower-level approach learns the optimal value of the follower's problem given the leader's decision. The performance trade-offs of each method are discussed, along with theoretical analysis related to approximation errors. The algorithm includes a "post-processing" step to refine approximate solutions for feasibility. Neur2BiLO is evaluated against various benchmarks, demonstrating its effectiveness compared to exact methods and other learning-based approaches.

### Strengths and Weaknesses
Strengths:  
The paper is a strong submission with high-quality writing and a clear methodology. The discussion of related works, experimental setup, and results analysis is well-presented. Neur2BiLO is comprehensively evaluated against exact and learning-based methods, showing practical effectiveness.

Weaknesses:  
A minor weakness is the lack of evaluation on problems with coupled constraints. The discussion on the smoothness of the follower's optimal value function is unclear, which is important for understanding approximation error bounds. Additionally, the paper may not be accessible to readers unfamiliar with the specialized area, and the organization makes it difficult to identify the main contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the discussion regarding the smoothness of the follower's value function and its implications for approximation errors. It would be beneficial to include a discussion on the potential for the proposed algorithm to be applied to continuous network design problems and neural architecture search, as these areas are relevant to the study. Furthermore, we suggest testing the algorithm on larger networks to better assess scalability and robustness.