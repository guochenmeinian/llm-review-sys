ID: cEb305kE1V
Title: Deep Implicit Optimization for Robust and Flexible Image Registration
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 4, 4, 4, -1, -1
Original Confidences: 4, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel image registration framework that integrates a differentiable implicit optimization layer into a neural network, aiming to merge classical and learning-based approaches. The framework employs end-to-end implicit differentiation and iterative optimization, ensuring that learned features are registration and label-aware. The authors claim that their method performs well on in-domain datasets and is robust against domain shifts, allowing for flexible transformation representations at test time without retraining.

### Strengths and Weaknesses
Strengths:
1. The paper's motivation is clear and innovative, effectively combining classical optimization techniques with neural networks through an embedded optimization layer, enhancing data consistency.
2. A significant technical achievement is the backpropagation of gradients through the optimization layer using the implicit function theorem, showcasing the paper's technical depth.
3. The analysis of loss landscapes is insightful, indicating that the flattening of the feature space by neural networks, combined with fidelity loss, improves performance.

Weaknesses:
1. The framework's registration accuracy, as measured by the Dice score, does not show clear advantages over neural-network-only methods, raising questions about the practical benefits of integrating classical optimization.
2. The absence of specific smoothness measurements, such as negative Jacobian determinants, is a critical oversight, leaving unclear whether the observed increase in Dice score is a genuine improvement.
3. Clarity regarding the reproducibility of results is lacking, with discrepancies in performance metrics compared to other methods, raising concerns about implementation fidelity.
4. The paper does not adequately compare its method to existing registration methods that combine learning with optimization, leaving questions about the novelty and effectiveness of the proposed approach.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the primary benefits of their framework compared to existing learning frameworks that incorporate optimization, such as ConvexAdam, SAMConvex, PDD-Net, and PRIVATE, by providing specific examples or quantitative comparisons. Additionally, the authors should elaborate on the significance of their capability to switch between transformation representations at test time, particularly in comparison to other methods. Clarifying the nature of anatomical landmarks incorporated into the framework and demonstrating their integration within the optimization loop would also enhance the paper. Finally, we suggest conducting more comprehensive experiments to validate the domain shift hypothesis and addressing the identified gaps in motivation and experimental comparisons to strengthen the manuscript's contributions.