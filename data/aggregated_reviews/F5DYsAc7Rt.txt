ID: F5DYsAc7Rt
Title: GRAND-SLAMIN’ Interpretable Additive Modeling with Structural Constraints
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for incorporating hierarchical structure constraints into interaction-aware Generalized Additive Models (GAMs) to enhance feature sparsity. The authors utilize soft relaxations for gradient descent and provide theoretical analyses demonstrating the statistical properties of their approach. The proposed method is framed as a combinatorial optimization problem, employing indicator variables and entropy regularization to manage pairwise interactions effectively. Additionally, the authors introduce a framework named GRAND-SLAMIN’ for end-to-end learning of sparse GAMs, leveraging soft neural decision trees and structural constraints. Furthermore, the authors integrate MLPs and soft trees for modeling interactions in data, demonstrating that their method can handle more complex models without significant increases in training time. They report that while MLPs can replace soft trees, soft trees generally outperform MLPs in terms of AUC across several datasets, including the adult dataset.

### Strengths and Weaknesses
Strengths:
- The paper offers a straightforward design for hierarchical constraints that enhances feature selection in GAMs.
- It includes comprehensive background information and related works, contributing to a well-structured presentation.
- The implementation efficiently discards unused features during training, and the theoretical analysis provides valuable insights into the proposed method's performance.
- The authors provide comprehensive experiments comparing MLPs and soft trees, showing clear performance metrics.
- The framework demonstrates scalability and adaptability to complex interactions without substantial increases in training time.
- The rebuttal addresses reviewer concerns effectively, leading to improved scores.

Weaknesses:
- The technical contribution is limited, primarily adopting existing GA2M frameworks with minor modifications. The introduction of hierarchical constraints is somewhat trivial, as similar approaches exist in the literature.
- The theoretical analysis lacks relevance to the claimed contributions, focusing on soft tree-shaped functions without addressing the hierarchical constraints.
- Presentation issues arise from unclear symbol definitions and insufficient explanations in the analysis, particularly in Section 5.
- The reliance on shallow models raises concerns about generalizability to more complex problems.
- The authors did not achieve the claimed 95% AUC on the adult dataset, which may affect the perceived efficacy of their model.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by providing specific explanations for symbols used in the analysis, such as $n$, $\bar{u}(f)$, and $\vee$. Additionally, we suggest including the number of runs and standard error or confidence intervals in the experimental results to enhance the robustness of the analysis. It would also be beneficial to explore the performance of the proposed method with various shape function classes, including those used in Neural Additive Models. Furthermore, we encourage the authors to clarify the motivation behind using soft trees over other architectures, such as MLPs, and to address the interpretability of their model by incorporating visualizations of local and global explanations. Lastly, we recommend discussing the limitations of the proposed method more thoroughly in the main text and improving the generalizability of their model by exploring deeper architectures or more complex interactions. Additionally, clarifying the performance discrepancies on the adult dataset and addressing the limitations of shallow models would strengthen the paper. Finally, including visualizations for more features in the appendix would enhance the clarity of results.