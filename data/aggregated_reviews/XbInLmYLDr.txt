ID: XbInLmYLDr
Title: DiViNeT: 3D Reconstruction from Disparate Views using Neural Template Regularization
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 4, 7, 3, 4, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DiViNet, a two-stage framework for neural 3D reconstruction from sparse and disparate views using neural templates for regularization. The first stage involves training a network to predict shape templates, while the second stage utilizes these templates to guide volumetric surface reconstruction with depth and SDF constraints. DiViNet achieves state-of-the-art performance in sparse image views, specifically targeting scenarios with minimal overlap among input images. The authors also conducted experiments with SparseNeuS but encountered convergence issues, leading them to exclude it from their comparisons. They argue that their method excels in reconstructing from disparate views while maintaining quality in dense view scenarios and justify their choice of MonoSDF as a baseline, citing its effectiveness in object-level reconstruction from sparse views, as demonstrated in the DTU dataset.

### Strengths and Weaknesses
Strengths:
- DiViNet eliminates the need for explicit cues, unlike previous methods, and effectively operates with a limited number of images.
- The proposed designs are technically sound and demonstrate strong performance in sparse view settings.
- The authors provide a clear rationale for their experimental design and the exclusion of SparseNeuS, emphasizing their focus on disparate view reconstruction.
- They effectively argue the suitability of MonoSDF for comparison, referencing its performance on the DTU dataset.

Weaknesses:
- DiViNet underperforms with dense view inputs compared to the latest methods, as evidenced by its second-best ranking in Table 2.
- The generalization of neural templates across different data distributions is unclear, raising concerns about their applicability in real-world scenarios.
- The paper lacks comparisons with closely related works on sparse view reconstruction, such as SparseNeuS and VolRecon, which diminishes the robustness of its claims.
- The authors did not present results from SparseNeuS, which raises concerns about transparency in their findings.
- The lack of experiments in settings with large overlaps may limit the perceived robustness of their method compared to existing approaches.

### Suggestions for Improvement
We recommend that the authors improve the generalization discussion of the neural templates, clarifying under what conditions they can be reused across different datasets. Additionally, including a visualization of the learned Gaussian templates alongside the corresponding sparse reconstructed point cloud would enhance understanding. We suggest conducting comparisons with SparseNeuS and VolRecon to provide a more comprehensive evaluation of DiViNet's performance. Furthermore, incorporating image-level metrics like PSNR/SSIM/LPIPS for novel view synthesis would strengthen the evaluation narrative. Lastly, we recommend improving transparency by including the results from SparseNeuS in the supplementary material, even if they are not favorable, and conducting experiments in settings with large overlaps to provide a more comprehensive comparison with previous methods.