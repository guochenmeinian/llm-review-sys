ID: q2WT19Ciad
Title: Benchmarking Generative Models on Computational Thinking Tests in Elementary Visual Programming
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 8, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for evaluating generative models' computational thinking and problem-solving capabilities, particularly through elementary visual programming tasks. The authors find that state-of-the-art models like GPT-4o and Llama3 perform comparably to average school students on these tasks. They introduce a novel synthetic data generation technique using symbolic information to enhance model performance. The study includes new synthetic datasets, comprising 758 HoC tasks and 922 ACE tasks generated with the same pipeline as the training datasets. The authors propose three variants of synthetic evaluation datasets: HoC_Hard, which includes 378 challenging code-writing tasks; HoC-Filtered, a new dataset of 758 distinct code-writing tasks; and HoC-OoD, comprising 100 out-of-distribution tasks with more complex solution structures. The updated paper will include results demonstrating the generalization ability of fine-tuned models across these varied datasets and insights into improving computational thinking in generative models.

### Strengths and Weaknesses
Strengths:
- The introduction of a novel data generation method based on symbolic techniques to improve computational thinking in generative models.
- The inclusion of new synthetic evaluation datasets enhances the robustness of the analysis.
- The differentiation of datasets (HoC_Hard, HoC-Filtered, HoC-OoD) allows for a nuanced understanding of model performance across varying task complexities.
- The paper is well-written, original, and provides extensive experiments that yield valuable analyses and interpretations.
- The authors' responsiveness to reviewer feedback and willingness to incorporate additional experiments and analyses is commendable.

Weaknesses:
- The concept of computational thinking lacks a rigorous formulation, necessitating further discussion.
- The evaluation dataset appears small, limiting the ability to make substantial claims about model performance.
- There is insufficient error analysis regarding where LLMs struggle in reasoning processes.
- The initial paper lacked sufficient detail regarding the synthetic datasets and their generation, which may have limited the clarity of the evaluation.
- The need for more thorough annotations and error analysis was identified, indicating areas for improvement in the presentation of results.

### Suggestions for Improvement
1. We recommend that the authors discuss the implications of this study for real-world applications, particularly in computational thinking education.
2. The authors should clarify their evaluation process, specifically how model outputs are parsed and evaluated.
3. We suggest conducting experiments to determine whether using Python could assist models in performing computational thinking tasks more effectively.
4. The authors should provide a justification for their data mixing strategy, including the rationale behind the chosen percentages of different sources.
5. We recommend including a comparison with existing agent frameworks (e.g., CoT, GoT) and various approaches in mathematical problem-solving (e.g., AutoGen, Data-Interpreter) to enhance the paper's depth.
6. We recommend that the authors improve the clarity of the synthetic dataset generation process and ensure that the updated paper includes comprehensive results and discussions on the generalization ability of the models.
7. Additionally, please incorporate the new experiments related to Chain of Thought and error analysis, along with more thorough annotations, to enhance the overall quality of the paper.