ID: iwC19lVBoq
Title: AVSET-10M: An Open Large-Scale Audio-Visual Dataset with High Correspondence
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 3, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AVSET-10M, a dataset comprising 10 million audio-visual samples designed to enhance research in audio-visual fields by ensuring high audio-visual correspondence through a meticulous filtering process. The dataset includes 527 unique audio categories, making it the largest publicly available dataset of its kind. The authors analyze its benefits in audio-video retrieval and vision-queried sound separation tasks, demonstrating significant contributions to the community. The dataset process is well-designed, ensuring data quality and a clear presentation of the dataset construction. The authors acknowledge privacy concerns associated with the dataset and plan to address these in the limitations section, emphasizing the importance of filtering voice-overs to maintain audio-visual consistency and providing comprehensive annotations for researchers.

### Strengths and Weaknesses
Strengths:
- The paper details a robust methodology for ensuring high audio-visual correspondence, essential for the target research areas.
- The inclusion of 527 unique audio categories greatly expands research possibilities and ensures the dataset's utility across various applications.
- With 10 million samples, AVSET-10M provides significant scale for training and evaluating large models.
- The dataset's significance and scale are well-recognized, and the dataset process is well-designed, ensuring high data quality.
- The paper is presented clearly, with substantial efforts made to address ethical issues and privacy concerns.

Weaknesses:
- The paper lacks a clear definition of "correspondence" in the dataset, which could lead to misunderstandings.
- Claims regarding the annotation process of AudioSet are inaccurate, and the filtering process for retaining instrumental performances is unclear.
- The potential bias from using the same videos in training and testing is not addressed, which could compromise the evaluation's integrity.
- Several statements are vague or unsupported, such as the claim of having the "most extensive range of audio categories."
- The limitations section is brief and does not adequately address the dataset's potential biases or the implications of its construction.
- Some reviewers noted that not all comments were fully addressed, and there are concerns regarding potential population bias due to reliance on upstream datasets without detailed analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of "correspondence" in the dataset to avoid ambiguity. Additionally, it would be beneficial to provide a more accurate description of the annotation process and clarify the filtering methods, particularly regarding voice-over filtering. We suggest conducting an ablation study to evaluate the effectiveness of each stage in the curation process. The authors should also address the potential bias introduced by using the same videos for training and testing. Furthermore, we recommend that the authors improve the discussion on privacy issues by detailing the proactive measures for regular updates to the removal list and de-identification. The authors should acknowledge the potential lack of diversity in the dataset due to reliance on a single data source and consider further analysis of population representativeness. Lastly, we encourage the authors to expand the limitations section to include a more comprehensive discussion of the dataset's biases and the implications of its construction, as well as to clarify the incremental performance boost of vision-queried sound separation in future revisions.