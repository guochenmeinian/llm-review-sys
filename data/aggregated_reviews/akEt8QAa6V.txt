ID: akEt8QAa6V
Title: GTA: A Benchmark for General Tool Agents
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 5, 7, 6, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GTA (General Tool Agents), a benchmark designed to evaluate the tool-use capabilities of large language models (LLMs) in real-world scenarios. The authors emphasize the necessity of this benchmark due to the limitations of existing evaluations that often rely on AI-generated queries, single-step tasks, and text-only inputs. The benchmark features human-written queries, real deployed tools, and multimodal inputs, assessing 16 mainstream LLMs on 229 tasks. The findings reveal significant gaps in LLM capabilities, with advanced models like GPT-4 completing fewer than 50% of the tasks, underscoring the need for improved reasoning and planning in tool use.

### Strengths and Weaknesses
Strengths:
- The benchmark utilizes real-world user queries and deployed tools, enhancing its realism and relevance.
- GTA includes a comprehensive tool set across perception, operation, logic, and creativity categories, allowing for a holistic evaluation.
- The incorporation of multimodal inputs aligns the benchmark with real-world problem-solving scenarios.
- The authors propose fine-grained evaluation metrics (InstAcc, ToolAcc, ArgAcc, SummAcc, and AnsAcc) to assess various aspects of tool use.
- The extensive evaluation of 16 LLMs provides valuable insights into their current tool-use capabilities.
- The dataset and evaluation platform are publicly available, promoting reproducibility and further research.

Weaknesses:
- The static nature of the benchmark may not adequately address dynamic tool-use scenarios.
- The human-written queries and tool chains, while ensuring quality, may limit scalability and require frequent updates as AI capabilities evolve.
- The number and diversity of tasks may not sufficiently cover all real-world scenarios, potentially biasing evaluation results.
- The paper lacks a detailed error analysis to understand specific model failures or successes.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by discussing the potential for extending it to include dynamic scenarios. Additionally, consider incorporating more AI-generated content to enhance scalability while maintaining quality. A more detailed description of the evaluation platform and the testing process for the LLMs would also be beneficial. To provide a more accurate reflection of LLM capabilities, we suggest increasing the number of questions per category and including results from enhanced LLMs that perform better under the GTA criteria. Finally, a thorough error analysis could enrich the understanding of model limitations and guide future improvements.