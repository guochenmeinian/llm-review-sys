ID: O8yHsRLwPl
Title: Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 5, 7, 6, 7, -1, -1
Original Confidences: 4, 3, 4, 2, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on distributed asynchronous SGD with heterogeneous communication and computation times for non-convex stochastic optimization problems. The authors propose a new algorithm called Shadowheart SGD, which is analyzed for its time complexity, proven to be optimal among centralized methods with compressed communication. The paper also introduces equilibrium time as a key parameter and proposes an adaptive variant, Adaptive Shadowheart SGD, which does not require prior knowledge of equilibrium time, communication, or computation times.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a general scenario in distributed/federated learning by considering asynchronous SGD, arbitrary computation and communication times, and unbiased compression.
2. The introduction of Shadowheart SGD is a significant contribution, demonstrating optimal time complexity.
3. The concept of equilibrium time is effectively introduced to characterize computations and communications in Shadowheart SGD.
4. The proposed Adaptive Shadowheart SGD enhances flexibility by not requiring knowledge of equilibrium time.

Weaknesses:
1. The writing lacks clarity, making it difficult to read; it resembles a summary of results found in the appendix, with insufficient intuitive explanations and poorly defined notations.
2. The focus on IID data distribution limits the algorithm's applicability in federated learning, rendering the contributions seem minor.
3. The experimental results are insufficient, relying primarily on logistic regression with the MNIST dataset and quadratic optimization, necessitating more extensive testing.

### Suggestions for Improvement
We recommend that the authors improve the clarity and readability of the paper by providing more intuitive explanations and better-defined notations. Additionally, consider expanding the scope beyond IID data distribution to enhance the algorithm's applicability in federated learning contexts. We suggest conducting more extensive experiments to validate the proposed algorithm, particularly in diverse scenarios beyond those currently tested. Furthermore, we encourage the authors to simplify the discussion around equilibrium time and its practical implications, and to address the impact of their method on convergence and loss in real-world training datasets.