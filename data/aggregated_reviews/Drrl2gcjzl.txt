ID: Drrl2gcjzl
Title: The Impact of Positional Encoding on Length Generalization in Transformers
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the impact of positional encodings on length generalization in decoder-only Transformer models, specifically evaluating a novel no positional encoding (noPE) approach. The authors demonstrate that noPE models can generalize better to longer sequences compared to traditional positional encoding methods, such as sinusoidal, T5, AliBi, and Rotary encodings. The evaluation encompasses various reasoning tasks, revealing that while noPE performs comparably to T5's relative positional encoding, it also exhibits unique advantages in certain scenarios. Additionally, the paper analyzes the T5 Relative Bias values in the decoder's self-attention mechanism of the pretrained HuggingFace model, identifying two patterns: Type I, where some heads mask distant tokens, and Type II, where other heads attend to distant tokens. This distinction may explain performance differences between T5 and ALiBi, supported by detailed tables of relative bias values across various buckets and heads.

### Strengths and Weaknesses
Strengths:
- The paper introduces a significant novel method (noPE) for positional encoding, contributing to the understanding of Transformers.
- The analysis of T5's attention mechanism is thorough and provides valuable insights into how different heads manage distant tokens.
- The findings provide valuable insights into the limitations of existing positional encoding methods for reasoning tasks and are considered impactful.

Weaknesses:
- The novelty of the results is somewhat overstated, as prior work has explored Transformers without positional encodings.
- The experimental setup is limited to small-scale models (~100M parameters) and synthetic tasks, raising questions about the generalizability of the findings to larger models and more complex tasks.
- There is a lack of detailed task-specific performance metrics comparing noPE with other positional encoding methods.
- The discussion of prior work could be more comprehensive, and the expectations regarding experiments on the effect of scale may be unrealistic due to resource constraints.

### Suggestions for Improvement
We recommend that the authors improve the framing of their contributions by providing a more comprehensive overview of prior work on positional encodings to clarify the novelty of their findings. Additionally, we suggest expanding the evaluation to include larger model sizes and more diverse tasks, such as summarization, to better assess the generalization capabilities of noPE. It would also be beneficial to include more task-specific performance metrics to strengthen the comparison between noPE and existing positional encoding methods. Furthermore, we encourage the authors to elaborate on the implications of their findings for future Transformer model designs and to clarify the limitations regarding the applicability of their results to larger-scale pretraining. Lastly, we recommend including the analysis of T5's attention mechanism in the Appendix for the final revision, as it is deemed essential for understanding the model's behavior.