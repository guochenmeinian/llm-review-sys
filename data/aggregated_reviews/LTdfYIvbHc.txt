ID: LTdfYIvbHc
Title: Feature-Learning Networks Are Consistent Across Widths At Realistic Scales
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical study on the convergence of key quantities in Neural Networks as width increases, focusing on feature-learning regimes. The authors investigate the dynamics of finite-width networks in the maximal update parameterization (muP) compared to their infinite-width counterparts, analyzing various metrics such as training loss, error rates, and kernel alignment across image and language tasks. The findings indicate that sufficiently wide networks exhibit similar loss and error trajectories, although this consistency diminishes for more complex tasks. Additionally, the authors demonstrate that large finite-width networks exhibit consistent performance across various metrics in online learning settings, while deviations occur in narrower networks or offline settings. The study utilizes empirical NTK and after-kernel analyses, revealing that finite width affects kernel eigenvectors but not eigenvalues. The authors also explore bias-variance decomposition through ensembling, concluding that narrow networks suffer from both bias and variance, with bias being the more detrimental factor.

### Strengths and Weaknesses
Strengths:
1. The investigation into finite-width neural networks addresses an important and open question in the field.
2. The paper features thorough experimentation across diverse architectures and tasks, contributing valuable insights into the conditions under which finite-width networks align with infinite-width behavior.
3. The exploration of boundaries for agreement and the rationale behind observed phenomena are particularly commendable.
4. The writing is clear, and the experimental results are well-motivated and effectively support the claims made.

Weaknesses:
1. Some results are not convincingly matched to the claims, particularly regarding convergence in width, as seen in the qualitative and quantitative results on ImageNet and WikiText-103.
2. The limited range of widths analyzed raises concerns about the robustness of the convergence conclusions, as only 3-5 widths are often considered.
3. The implications of establishing convergence to infinite-width feature learning limits may be modest, as these limits are complex and computationally prohibitive to understand.
4. The abstract introduces "feature-learning neural networks" without definition, potentially confusing readers.
5. Numerous typos and grammatical errors exist, though they do not hinder comprehension.
6. Figures require zooming for clarity, necessitating larger axis labels and legends. Some notation is defined but not referenced, leading to confusion. Specific figures lack clarity regarding whether they represent single test points or averages, and some descriptions in the text do not align with figure content.
7. The paper does not adequately discuss limitations, particularly regarding the use of large-width networks as proxies for infinite-width behavior.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding convergence by providing more extensive width analyses and addressing the discrepancies noted in the results. Additionally, we suggest that the authors clarify the definition of "feature-learning neural networks" in the abstract to enhance clarity. The authors should meticulously proofread the manuscript to correct typos and grammatical errors. Figures should be redesigned for better interpretability without zooming, with larger labels and legends. The authors should ensure that all defined notation is referenced appropriately and clarify the metrics used in figures, particularly distinguishing between single test points and averages. Finally, we encourage the authors to explicitly discuss the limitations of their experiments, particularly the implications of using large-width networks as proxies for infinite-width limits, and elaborate on the implications of their findings for the broader research community.