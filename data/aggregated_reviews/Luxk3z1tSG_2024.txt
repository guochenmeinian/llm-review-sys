ID: Luxk3z1tSG
Title: Can Graph Neural Networks Expose Training Data Properties? An Efficient Risk Assessment Approach
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an improved method for conducting property inference attacks on graph neural networks (GNNs), focusing on reducing the computational overhead associated with traditional approaches that rely on numerous shadow models. The authors propose a model approximation technique to generate a sufficient number of approximated models for attacks without extensive retraining and introduce a diversity-enhancing mechanism to ensure the effectiveness of these approximations. Experimental results demonstrate notable improvements in attack accuracy and efficiency compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, providing thorough theoretical explanations and innovative approaches.
- The proposed method achieves state-of-the-art attack performance while significantly reducing training time.
- The introduction of machine unlearning to enhance efficiency in training shadow models is an interesting application.

Weaknesses:
- The assumption that target and auxiliary graphs are splits of the same original graph is impractical for real-world scenarios.
- The technical novelty is limited, as the method resembles existing graph unlearning strategies without distinct improvements.
- The experimental settings are weak, lacking evaluations on larger datasets and other GNN models.
- The choice of property attributes inferred is limited, and the paper does not adequately discuss the reasons for the proposed method's superior performance over baselines.

### Suggestions for Improvement
We recommend that the authors improve the applicability of their method by using distinct network graphs for target and auxiliary graphs. Additionally, they should provide a more thorough discussion on the technical novelty of their approach compared to existing literature. Including complexity analysis of the algorithm and clarifying whether the time required to generate diverse approximated models is accounted for in efficiency comparisons would enhance transparency. Expanding the range of property attributes inferred in experiments would also strengthen the significance of the work. Finally, addressing the scalability of the attack for larger datasets and other GNN models is essential for a comprehensive evaluation.