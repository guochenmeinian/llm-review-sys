ID: JXKbf1d4ib
Title: Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel model-based algorithm, DCFP, for policy evaluation in distributional reinforcement learning (RL), achieving near-minimax-optimal performance for approximating return distributions under a generative model framework. The authors provide theoretical analysis, including a new distributional Bellman equation and a stochastic categorical CDF Bellman equation, along with empirical comparisons against quantile DP and other algorithms.

### Strengths and Weaknesses
Strengths:  
- The algorithm's approach of discretizing the distribution's support and reformulating updates as linear mappings is a significant contribution, revealing connections between categorical algorithms and linear systems.  
- The DCFP algorithm effectively leverages generative models for efficient performance and establishes minimax lower bounds for sample complexity in Wasserstein distance.  
- The paper is well-written, with clear presentation of the algorithm and theoretical results.

Weaknesses:  
- The assumption of N iid samples for each state is questioned, as more samples are typically needed for frequently visited states, raising concerns about the theoretical analysis's robustness.  
- The experimental results indicate that while DCFP has better running time, it does not outperform quantile DP in sample efficiency.  
- The practical applicability of the theoretical results is unclear, as numerical results are limited to a simple tabular environment, and the motivation for categorical approaches lacks depth.

### Suggestions for Improvement
We recommend that the authors improve the justification for the assumption of N iid samples for each state, considering the implications of states that are less frequently visited. Additionally, it would be beneficial to include results for CDP in the main paper to provide a clearer comparative analysis. We suggest discussing the stability and reliability of using sparse linear solvers as the state space scales, and addressing the scalability of the DCFP algorithm in real-world environments. Finally, we encourage the authors to clarify the motivation behind the choice of categorical approaches and to categorize the convergence rate in Proposition 2 for enhanced computational efficiency.