ID: wT2TIfHKp8
Title: Taming the Long Tail in Human Mobility Prediction
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 7, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Long-Tail Adjusted Next POI Prediction (LoTNext) framework, which addresses the long-tail issue in predicting less frequently visited points-of-interest (POIs) in human mobility data. The authors propose a Long-Tailed Graph Adjustment module and a Long-Tailed Loss Adjustment module, along with an auxiliary prediction task to enhance model generalization and accuracy. The effectiveness of LoTNext is demonstrated through experiments on two real-world datasets, Gowalla and Foursquare, where it significantly outperforms existing state-of-the-art methods.

### Strengths and Weaknesses
Strengths:
1. The code provided enhances reproducibility.
2. The paper is well-written and easy to follow.
3. The proposed method is grounded in solid motivation and introduces a unique combination of graph and loss adjustment modules.

Weaknesses:
1. The presentation quality could be improved.
2. The authors should conduct experiments on a broader range of datasets and provide more detailed analyses.
3. The paper lacks sufficient theoretical analysis to guarantee the proposed method's effectiveness.
4. The complexity of the model raises questions about its computational expense and scalability in practical applications.

### Suggestions for Improvement
We recommend that the authors improve the presentation quality of the paper. Additionally, conducting experiments on more datasets and providing a broader range of metrics would enhance the validation of the LoTNext framework. We also suggest including more detailed theoretical analysis and comparative studies with recent state-of-the-art methods, such as TPG and LLM-Move, to better demonstrate the novelty and effectiveness of the proposed approach. Furthermore, addressing hyperparameter sensitivity, particularly the settings of $\lambda_1$, $\lambda_2$, and $\lambda_3$ in Eq. 16, would strengthen the empirical results.