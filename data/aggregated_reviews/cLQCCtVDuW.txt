ID: cLQCCtVDuW
Title: HIQL: Offline Goal-Conditioned RL with Latent States as Actions
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 7, 7, 7, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HIQL, a hierarchical algorithm for offline goal-conditioned reinforcement learning (RL). HIQL employs an action-free version of IQL to learn a shared value function, from which it derives both high-level and low-level policies using AWR. The authors assert that this hierarchical structure enhances robustness to value function noise, improving performance in long-horizon goals. The effectiveness of HIQL is validated through experiments in state-based and pixel-based tasks.

### Strengths and Weaknesses
Strengths:
- The paper is well written and organized, making it easy to follow.
- HIQL integrates IQL into a hierarchical framework, offering valuable insights for offline goal-conditioned RL.
- The examples in Sections 4.1 and 4.3 aid reader comprehension.
- Thorough experiments demonstrate improved performance across various tasks.

Weaknesses:
- The hierarchical structure's robustness to value noise is primarily evidenced in discrete tasks, raising questions about its applicability in continuous tasks. Additional comparisons with smoothed value function methods are warranted.
- Reproduction details for Figure 2 are insufficient.
- The appendix's concatenation of [s,g] before feeding into $\phi$ necessitates a revision of Figure 1.
- The experiments lack additional baselines from temporal difference learning, which are essential for a comprehensive evaluation.
- The performance of the low-level policy without the high-level policy should be reported to emphasize the hierarchical structure's utility.
- An error in Eq (7) regarding the variable g and $s_{t+1}$ needs correction.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how the hierarchical structure applies to continuous tasks and provide comparisons with smoothed Q methods. More information should be included to enable the reproduction of Figure 2. Additionally, incorporating baselines that utilize temporal difference learning and goal relabeling will facilitate a more comprehensive comparison. The authors should also report the performance of the low-level policy independently to highlight the hierarchical structure's necessity. Lastly, a fix in Figure 1 and Eq (7) is needed to correct the identified error.