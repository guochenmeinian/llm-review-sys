ID: d0zla3M3LI
Title: Tree Prompting: Efficient Task Adaptation without Fine-Tuning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an innovative approach called Tree Prompting, which enhances the performance of language models (LMs) on classification tasks without fine-tuning. The authors propose a decision tree structure where each node corresponds to generated labels from prompts, serving as split features. The method demonstrates improved performance over various baselines and is competitive with fine-tuning methods. Additionally, the paper includes several extensions, such as human-written instruction prompts, dynamic prompts, and kNN prompting.

### Strengths and Weaknesses
Strengths:
- The proposed Tree Prompting method shows enhanced performance on diverse classification datasets while maintaining efficiency during inference.
- The paper conducts comprehensive experiments, demonstrating the effectiveness and robustness of the proposed method.
- The approach offers improved interpretability of models and presents a clear direction for future prompting design.
- The provided code ensures reproducibility of results.

Weaknesses:
- The experiments primarily focus on small language models, lacking demonstration of the method's impact on larger models like ChatGPT and GPT-4.
- The organization of the proposed strategies for Tree Prompting is unclear, requiring readers to frequently refer back to previous sections for relevant information.
- The paper does not provide performance results for various prompt construction methods, leaving readers uncertain about their effectiveness.
- Some technical details, such as the ensembling method in TreePrompt Ens and the stopping condition for constructing the decision tree, are not clearly explained.

### Suggestions for Improvement
We recommend that the authors improve the organization of the paper by including an overall figure that depicts the training process and the different extensions for Tree Prompting and baselines. Additionally, we suggest providing insights into the failures of baseline methods and including performance results for various prompt construction methods. It would also be beneficial to evaluate the proposed method on larger language models, such as LLama, to demonstrate its effectiveness comprehensively. Lastly, clarifying the technical details regarding the ensembling method and the stopping condition for the decision tree construction would enhance the paper's clarity.