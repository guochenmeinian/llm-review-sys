ID: f1y1tG5pAE
Title: The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained Multimodal Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Basic Language Abilities (BLA) benchmark, designed to evaluate vision-and-language models (VLMs) on linguistic constructions such as active-passive voice, coordination, and relative clauses. The authors conduct exhaustive experiments with three VLMs in zero-shot, fine-tuning, and in-context learning settings, revealing that existing models significantly underperform compared to human capabilities. The study highlights the challenges VLMs face in mastering these basic linguistic tasks, providing valuable insights into their grounding abilities.

### Strengths and Weaknesses
Strengths:
- The BLA benchmark is a timely and useful tool for assessing the linguistic capabilities of VLMs.
- The paper offers substantial contributions, including a well-motivated exploration of generative versus discriminative models, particularly with the inclusion of BLIP-2.
- The experiments are comprehensive, and the findings provide interesting insights into the limitations of current VLMs.

Weaknesses:
- The evaluation is limited to only three models and three tasks, which may restrict the generalizability of the findings.
- There is insufficient analysis of failure cases, lacking a detailed examination of the types of examples where models struggle.
- The paper does not explore a broader range of generative models, limiting the comparison necessary to substantiate claims regarding the advantages of generative over discriminative models.

### Suggestions for Improvement
We recommend that the authors improve the analysis of failure cases by providing visualizations and detailed descriptions of the examples where models fail, similar to the approach in Diwan et al. (2022). Additionally, we suggest including evaluations of other generative models, such as OpenFlamingo, MAGMA, and Fromage, to better understand the factors contributing to performance differences. Finally, it would be beneficial to investigate how larger text-only language models perform on this task, as this could provide further insights into the linguistic understanding capabilities of VLMs.