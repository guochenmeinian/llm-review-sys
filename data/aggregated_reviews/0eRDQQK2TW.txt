ID: 0eRDQQK2TW
Title: A Finite-Particle Convergence Rate for Stein Variational Gradient Descent
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 5, 8, 6, -1, -1, -1
Original Confidences: 2, 1, 2, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the convergence rate of finite-sample Stein Variational Gradient Descent (SVGD) for sub-Gaussian targets with Lipschitz scores. The authors provide convergence guarantees that hold for finite samples and rely on weaker assumptions compared to previous works. Key results include the discretization error of finite-sample SVGD relative to infinite-sample SVGD, quantified through Wasserstein (Theorem 1) and KSD (Theorem 2), leading to a finite-sample bound on KSD error (Theorem 3). The authors demonstrate that this error decays at a rate of $1 / \sqrt{\log \log n}$ (Corollary 2). 

### Strengths and Weaknesses
Strengths:  
- The paper rigorously studies the convergence of finite-sample SVGD, providing clear and well-supported results.  
- It bridges the gap between existing infinite-sample convergence guarantees and practical finite-sample implementations, offering a significant contribution to the literature.  
- The exposition is highly comprehensible, with valuable intuitive explanations accompanying the main theorems.  

Weaknesses:  
- The established convergence rate of $1 / \sqrt{\log \log n}$ is notably slow, suggesting that the bound may be loose.  
- The authors do not clearly discuss potential avenues for refining the bounds or improving the convergence rate, which would enhance the paper's insights.  
- The paper assumes a level of familiarity with mathematical concepts that may hinder accessibility for readers without a strong background in probability theory and optimization.  

### Suggestions for Improvement
We recommend that the authors improve the discussion on potential avenues for enhancing the convergence rate, including insights into which parts of the proof strategy may have contributed to the slow rate. Additionally, clarifying the suggested step size scheme in Corollary 2, particularly its dependence on sample size $n$ and dimension $d$, would be beneficial. Finally, including experimental results or comparisons with other algorithms could demonstrate the practical usefulness of the convergence rate formula and broaden the paper's relevance.