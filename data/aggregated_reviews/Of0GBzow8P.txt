ID: Of0GBzow8P
Title: The Transient Nature of Emergent In-Context Learning in Transformers
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 6, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the transience of in-context learning (ICL) in transformer networks, revealing that ICL may not persist as training progresses. The authors provide empirical evidence indicating that as models are trained, they tend to shift towards in-weights learning (IWL). The study emphasizes the significance of L2 regularization and explores various factors such as model size, dataset characteristics, and training strategies that influence ICL dynamics. The findings raise important questions about the interplay between ICL and IWL, suggesting that while ICL can emerge, it is often transient and susceptible to training conditions. Additionally, the authors report gradual improvements in IWL evaluation results over extended runs and discuss the necessity of a certain level of difficulty for ICL to emerge, noting that with 10 classes, the model fails to learn ICL and defaults to the IWL solution, aligning with findings from Chan et al. (2022). The authors are currently conducting new Zipfian experiments and have decided to remove related statements from the paper until definitive results are available.

### Strengths and Weaknesses
Strengths:
- The paper offers novel empirical findings on the transience of ICL, contributing to the understanding of transformer models.
- It systematically explores strategies to mitigate ICL transience, including L2 regularization and data distribution adjustments.
- The narrative is coherent, with a clear rationale for each experiment, enhancing the overall readability and engagement.
- The authors demonstrate a commitment to thorough investigation by performing extended runs for IWL evaluation.
- The paper effectively replicates and builds upon previous findings from Chan et al. (2022).
- The authors show responsiveness to reviewer feedback, enhancing the paper's clarity and content.

Weaknesses:
- The experiments are primarily conducted on a single small dataset, limiting the generalizability of the findings to larger, more complex datasets.
- The definitions of ICL and IWL are somewhat narrow, potentially oversimplifying the complexities of these learning mechanisms.
- There is a lack of thorough exploration of alternative explanations for the observed phenomena, which could enrich the discussion.
- The results from the new Zipfian experiments are not yet available, which may limit the paper's completeness.
- The current removal of statements regarding Zipfian experiments could leave gaps in the discussion until results are confirmed.

### Suggestions for Improvement
We recommend that the authors improve the scope of their experiments by including larger, real-world datasets to enhance the practical relevance of their findings. Additionally, exploring alternative definitions and mechanisms related to ICL and IWL could provide a more nuanced understanding of their interactions. We suggest incorporating statistical significance analyses to strengthen the conclusions drawn from the experimental results. Furthermore, addressing the clarity of notations and ensuring consistent formatting in references would enhance the paper's overall presentation. We also recommend that the authors improve the clarity of their findings by including the results from the long runs in the final version of the paper. Lastly, we suggest that the authors re-evaluate the presentation of the plots to enhance their clarity and impact once the new experimental results are available. A more detailed discussion of the implications of ICL transience on practical applications of transformer models would also be beneficial.