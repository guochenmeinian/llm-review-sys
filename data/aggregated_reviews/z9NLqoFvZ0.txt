ID: z9NLqoFvZ0
Title: State-wise Constrained Policy Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 5
Original Ratings: 4, 4, 7, 7, -1
Original Confidences: 3, 3, 3, 4, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to safe reinforcement learning (RL) by introducing the Maximum Markov Decision Process (MMDP) framework and the State-wise Constrained Policy Optimization (SCPO) algorithm. The authors address the critical issue of state-wise safety, which is prevalent in safety-critical applications. They provide mathematical guarantees for their method and demonstrate its efficacy through numerical results.

### Strengths and Weaknesses
Strengths:
- The writing quality is good, and the paper is well-structured.
- Theoretical results are sound, and the authors provide practical implementation tips to enhance reproducibility.
- The paper effectively extends the CMDP framework to incorporate state-wise safety guarantees, addressing gaps in existing methods.

Weaknesses:
- The experimental results raise concerns, particularly regarding reward performance, as baselines show similar reward values, suggesting potential issues with environment settings.
- The novelty of the approach is limited, as similar state augmentation mechanisms have been previously studied, and the algorithm appears to be a straightforward application of trust region methods.
- Theorem 1 contains a major error in the proof, specifically that equation 20 is incorrect due to the non-invertibility of \(I - P\).

### Suggestions for Improvement
We recommend that the authors improve the clarity of their theoretical contributions by addressing the major error in the proof of Theorem 1 and ensuring its correctness. Additionally, we suggest providing intuitive explanations for propositions 1 and 2 to enhance readability. It would also be beneficial to include a comparison with existing state augmentation methods and clarify the motivation behind the MMDP formulation. Lastly, we encourage the authors to check the reward settings in their experimental environments to resolve the observed anomalies in reward performance.