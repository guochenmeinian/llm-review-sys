ID: cIVj8xLVZh
Title: OW-VISCapTor: Abstractors for Open-World Video Instance Segmentation and Captioning
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new task called Open-World Video Instance Segmentation and Captioning (OW-VISCap), which involves detecting, segmenting, tracking, and describing both seen and unseen objects in videos. The authors propose two networks: an object abstractor for encoding images at the object level and an object-to-text abstractor for generating captions. The object abstractor utilizes evenly distributed points to obtain both open-world and closed-world object queries, while the object-to-text abstractor employs masked attention in the cross-attention layer. The proposed method shows superior performance in Open-World Video Instance Segmentation (OW-VIS) and Dense Video Object Captioning (DVOC) compared to existing approaches.

### Strengths and Weaknesses
Strengths:
- The authors propose a novel task and solution for video understanding.
- A thorough analysis of existing methods in OW-VIS and DVOC is provided.
- The paper is well-organized and easy to read.

Weaknesses:
- The technical contribution appears weak despite being the first method for this task.
- Evaluation is limited to one benchmark each for OW-VIS and DVOC, necessitating additional benchmarks for validation.
- There is a lack of experimental evidence supporting the claim that free-form captions improve OW-VIS performance.
- Basic VIS performance is lacking, with significant gaps compared to recent methods on the OVIS benchmark.
- The task is evaluated separately on two sub-tasks, which limits the overall assessment.
- Clarity issues regarding how video is handled and the identity retention of objects across frames.
- The paper's layout can be improved, with important model information not presented until the end of the supplementary material.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including additional benchmarks for OW-VIS and DVOC to validate the open-world capability of their method. We also suggest providing experimental evidence to demonstrate the benefits of using free-form captions. Furthermore, enhancing the basic VIS performance to align more closely with recent methods is crucial. Clarifying how video is processed, particularly regarding object identity retention, would strengthen the paper. Lastly, we encourage the authors to improve the layout for better readability and ensure that key model information is presented earlier in the text.