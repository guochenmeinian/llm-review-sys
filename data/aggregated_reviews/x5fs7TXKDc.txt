ID: x5fs7TXKDc
Title: FedNAR: Federated Optimization with Normalized Annealing Regularization
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 7, 6, 6, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the impact of weight decay on federated optimization, highlighting its critical role in enhancing generalization performance and avoiding overfitting in Federated Learning (FL). The authors propose a novel methodology, Federated optimization with Normalized Annealing Regularization (FedNAR), which modulates update magnitudes through co-clipping of gradients and weight decay. The study includes a theoretical analysis and empirical validation across various federated tasks, demonstrating improved convergence speed and performance.

### Strengths and Weaknesses
Strengths:
1. The writing and presentation are of high quality, making the concepts and methodology easy to follow.
2. The research includes robust empirical findings, examining six federated learning algorithms and providing clear insights into the effects of weight decay.
3. FedNAR is well-motivated by empirical results and is a simple yet effective enhancement adaptable to different FL algorithms.
4. The paper offers a comprehensive theoretical analysis of weight decay, with novel results that could spur further research in the federated learning community.
5. The experimental design is thorough, covering a wide range of scenarios with diverse datasets and hyperparameter configurations.

Weaknesses:
1. The theoretical bounds presented are similar to previous works without offering superior bounds, which may complicate convergence analysis.
2. The experiments are limited in scale, lacking tests on larger datasets and more challenging benchmarks, which would strengthen the findings.
3. FedNAR's performance on adaptive methods like FedAvgM and FedAdam was suboptimal, raising questions about its applicability to these algorithms.
4. The paper lacks clarity on the effects of hyperparameters such as the learning rate schedule and maximum norm on the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis by providing superior bounds for weight decay. Additionally, conducting experiments on larger and more challenging datasets, such as CIFAR100 and ImageNet, would enhance the robustness of the findings. It would also be beneficial to include an ablation study on the co-clipping threshold and clarify the impact of hyperparameters like the learning rate schedule and maximum norm on the proposed method. Finally, addressing the performance issues of FedNAR with adaptive methods and providing more details on model architectures would strengthen the paper.