ID: UixzK8evk5
Title: DistillCSE: Distilled Contrastive Learning for Sentence Embeddings
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the high variance issue in knowledge distillation when using Contrastive Learning, proposing two solutions: Group-P shuffling and averaging logits from multiple teachers. The authors demonstrate that these methods effectively reduce variance and outperform existing techniques, particularly on BERT and RoBERTa models in Semantic Textual Similarity benchmarks.

### Strengths and Weaknesses
Strengths:  
- The paper addresses an interesting problem and provides empirical evidence of variance issues in logits.  
- The proposed methods show effectiveness in benchmarks and are clearly presented.  

Weaknesses:  
- The novelty of the problem and proposed solutions is questioned, with concerns about the lack of clarity on how shuffling resolves variance issues.  
- Results appear limited, with only BERT-base showing significant improvement, and the hyper-parameter tuning may have biased results.  
- The paper does not adequately compare with other models like RoBERTa or XLNet, and the problem with the original DistillCSE is not clearly articulated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how Group-P shuffling addresses the variance problem and provide a deeper explanation of the inductive bias involved. Additionally, we suggest including comparisons with more models beyond BERT, such as RoBERTa and XLNet, to strengthen the contribution. Finally, testing the proposed regularization techniques on a broader range of knowledge distillation tasks could enhance the paper's relevance and impact.