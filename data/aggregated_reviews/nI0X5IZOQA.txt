ID: nI0X5IZOQA
Title: Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies how different aspects of cross-lingual transfer affect transfer performance by transforming English along specific axes, focusing on word-order syntactic differences, word identity alignments, and tokenizer quality. The authors propose a controlled experimental setup to analyze these factors, revealing that word identity alignments significantly impact a model's performance in different languages. The experimental results indicate that manipulating word orders can degrade performance, but this can be partially recovered through fine-tuning or further pre-training.

### Strengths and Weaknesses
Strengths:
- Clear hypothesis with sound experiments to test it.
- Good experimental setup for analyzing one variation at a time, which can be extrapolated to actual language transfer.
- The study provides sufficient support for its claims and contributes to understanding the importance of well-constructed word embeddings and tokenizer selection.

Weaknesses:
- Limited scope to English language raises concerns about the generalizability of results to other languages.
- Some experimental settings, such as reversed and randomly shuffled word orders, do not reflect practical cross-language characteristics.
- Insufficient justification for certain experimental choices, such as the rationale behind shuffling rows of the word embedding matrix.

### Suggestions for Improvement
We recommend that the authors clarify the rationale behind using only French and Japanese word orders for transforming English data. Additionally, please provide a clear explanation of how the word embedding was reinitialized and the reasoning behind shuffling rows instead of columns. It would also be beneficial to include real examples of the word-order factor in practice and to investigate the shuffling rates to account for varying levels of language similarities. To enhance the generality of the findings, we suggest including experimental results across multiple datasets and verifying the effectiveness of tokenizers from languages further removed from English, such as Japanese.