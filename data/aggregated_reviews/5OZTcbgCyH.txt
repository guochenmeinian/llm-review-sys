ID: 5OZTcbgCyH
Title: EHRCon: Dataset for Checking Consistency between Unstructured Notes and Structured Tables in Electronic Health Records
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 8, 8, 8, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents EHRCon, a dataset and framework aimed at ensuring consistency between unstructured clinical notes and structured tables in Electronic Health Records (EHRs). The EHRCon dataset comprises 3,943 manually annotated entities from 105 clinical notes, validated against database entries. The CheckEHR framework utilizes large language models (LLMs) in an eight-stage process to verify this consistency. The evaluation demonstrates the framework's performance in both few-shot and zero-shot settings, achieving a maximum recall of 61.06% with GPT-3.5.

### Strengths and Weaknesses
Strengths:
- The work addresses a critical issue in healthcare informatics with implications for patient safety.
- The EHRCon dataset is a significant contribution, providing a benchmark for future research.
- The dual schema approach (MIMIC-III and OMOP CDM) enhances the dataset's applicability.
- The CheckEHR framework is innovative, leveraging LLMs for consistency checking.
- Comprehensive evaluation using multiple LLMs offers insights into the framework's capabilities.

Weaknesses:
- The performance of the CheckEHR framework shows room for improvement, with only 61% recall.
- Lack of validation for annotator-generated SQL queries and no reporting of inter-annotator agreement raises concerns about reliability.
- The experimental design conflates model capabilities with framework effectiveness.
- The dataset's limited scope and size may not fully capture the complexity of real-world EHR inconsistencies.

### Suggestions for Improvement
We recommend that the authors improve the validation of annotator-generated SQL queries to enhance reliability. Additionally, we suggest reporting inter-annotator agreement to assess annotation reliability. The authors should evaluate LLMs directly on the task without the CheckEHR framework to clarify its added value. Incorporating long-context models could allow for processing entire clinical notes, potentially eliminating the need for note segmentation. Finally, we encourage the inclusion of confidence intervals for metrics in Tables 2 and 3 to provide a clearer understanding of the results' statistical significance.