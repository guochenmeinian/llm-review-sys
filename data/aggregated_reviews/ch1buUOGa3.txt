ID: ch1buUOGa3
Title: Expressive probabilistic sampling in recurrent neural networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 7, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 2, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on circuit algorithms for sampling-based Bayesian inference in continuous-time rate-based recurrent neural networks (RNNs). The authors argue that a linear readout from a noisy reservoir enhances expressivity compared to direct representation of the sampled distribution in the recurrent population. They demonstrate that RNNs with separate linear readouts can effectively sample from complex distributions, including multimodal and heavy-tailed distributions, which is significant for the neuroscience community.

### Strengths and Weaknesses
Strengths:
- The relevance of minimal recurrent circuit architectures for sampling complex distributions is well-articulated, and the main results are interesting.
- The empirical results show that the proposed method can sample from non-trivial non-Gaussian distributions, marking an improvement over existing literature focused primarily on Gaussian distributions.
- The theoretical framework and empirical experiments are generally well-constructed and presented.

Weaknesses:
- The novelty of Proposition 1 compared to existing work, such as Ma et al. (2015), is unclear, particularly in Section 3.1.
- The manuscript does not address the speed of relaxation to the stationary distribution, which is crucial for practical sampling-based inference in biological contexts.
- The authors' claim regarding the limitations of one-hidden-layer RSNs in approximating high-dimensional distributions lacks supporting data.
- The reliance on the ReLU activation function for key results raises concerns about the generalizability of findings, especially if alternative functions like $\tanh$ are used.
- Many figure labels and legends are too small, hindering readability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Proposition 1 by explicitly differentiating it from existing results in the literature. Additionally, addressing the speed of relaxation to the stationary distribution is essential; we suggest discussing this aspect and including experiments to evaluate convergence properties. The authors should provide data on the limitations of one-hidden-layer RSNs in high-dimensional settings and consider testing alternative activation functions to validate their findings. Increasing the font size of figure labels and legends will enhance readability. Lastly, we encourage the authors to reframe the manuscript to better connect with the broader neuroscience audience, emphasizing the biological implications of their findings.