ID: 9B57dEeP3O
Title: Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 4, 6, 3, 3, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents StoryGen, a model designed for generating a series of coherent images based on text prompts, resembling a visual storybook. The authors propose a two-pronged approach: leveraging the Stable Diffusion model for image generation and creating a diverse dataset, StorySalon, for training. The model conditions on both the text prompt and previously generated frames using a frozen CLIP encoder, with a Visual Context Model introduced for image conditioning. The dataset comprises 2,000 storybooks and 30,000 text-image pairs, filtered and re-captioned for better alignment. Qualitative results indicate impressive image generation, while quantitative results show improvements over basic baselines. Additionally, the authors integrate adaptors conditioned on both images and text into a pre-trained stable diffusion UNet, further enhancing the model's capabilities.

### Strengths and Weaknesses
Strengths:
- The focus on generating coherent images from text prompts is significant and has potential applications beyond storybook creation, such as video generation.
- The dataset of story text and images is a substantial contribution, beneficial for future visual storytelling research.
- The Visual Context Module effectively integrates image-level details into the denoising network, which could be beneficial for other tasks like image editing.
- The paper is well-framed and motivated, with strong and polished writing and presentation.
- The simplicity of the method, utilizing off-the-shelf components like LoRA and cross-attention, is a notable strength.
- The inclusion of human evaluation enhances the robustness of the findings, although the term "Human Feedback" is misleading as it refers to using ChatGPT for prompt generation rather than actual human input; "LLM" would be more accurate.
- The visual results from StoryGen are notably impressive compared to evaluated baselines.

Weaknesses:
- The ablation study shows negligible improvements from "Human Feedback," raising doubts about its contribution to the learning scheme.
- Essential evaluations, including comparisons with StoryGAN, Story-DALL-E, and AR-LDM, are missing, limiting understanding of StoryGen's improvements.
- FID is not an ideal metric for assessing style adherence, suggesting a CLIP-based metric may be more appropriate.
- Presentation issues arise as Figure 2 inaccurately suggests that StoryGen is conditioned on all past frames instead of just the most recent one.
- The model struggles to preserve style and content, as evidenced by the lack of detail in Figure 4.
- The StoryGen model without human feedback was not evaluated in the human evaluation, raising questions about the necessity of this step.
- The absence of comparative baselines limits the contextualization of StoryGen's performance; stronger baselines, such as those based on DreamBooth, could elucidate the method's effectiveness.
- Fine-tuning may affect the generality of the model; evaluating individual frame generation quality relative to the base SD model is necessary.
- The dataset's quality is questionable due to the lack of human labeling and its relatively small size compared to other datasets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of terminology by replacing "Human Feedback" with "LLM" to accurately reflect the method used. Additionally, we suggest conducting a thorough evaluation and comparison with StoryGAN, Story-DALL-E, and AR-LDM to substantiate the claimed improvements. The authors should consider using a CLIP-based metric for style assessment instead of FID. We also encourage the inclusion of more ablation studies to validate the contributions of the proposed architecture and dataset preparation methods. Furthermore, it would be beneficial to quantify the model's performance on generating stories of varying lengths to assess quality consistency. Lastly, addressing the dataset's legal and copyright considerations is crucial to ensure compliance, and we recommend improving the clarity of Figure 2 to accurately reflect the conditioning of StoryGen. Addressing the preservation of style and content in results would strengthen the paper, and including evaluations of the StoryGen model without human feedback would contextualize its importance. Incorporating stronger baselines, such as DreamBooth, would provide a clearer comparison of performance, and conducting a failure analysis to discuss limitations and potential failure cases in the main paper would be advantageous.