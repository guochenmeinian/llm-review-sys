ID: 5SUP6vUVkP
Title: Conditional Density Estimation with Histogram Trees
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a tree-based conditional density estimation model, termed "CDTree," which utilizes a Minimum Description Length (MDL) principle to optimize histogram-based partitions of the conditioning space. The authors assert that their method is non-parametric, eliminates the need for hyperparameter tuning, and provides a more accurate and robust model against irrelevant features compared to existing approaches, including kernel density estimation and neural networks. Additionally, the authors propose a method for learning decision trees that effectively estimates conditional probabilities, particularly in cases where features are irrelevant to the target variable. The experimental results demonstrate superior accuracy and compactness across various datasets from the UCI repository, showcasing the method's robustness against irrelevant features.

### Strengths and Weaknesses
Strengths:
- The experimental results are comprehensive, showcasing superior accuracy and a more compact model compared to other methods.
- The proposed method is empirically fast and does not require extensive parameter tuning.
- The algorithm is relatively novel and easy to understand.
- The authors provide a clear explanation of their method's robustness to irrelevant features, supported by experimental results.
- The proposed prior is acknowledged as a valid alternative, enhancing the paper's contribution.
- The authors effectively address reviewer concerns and clarify misunderstandings regarding their methodology and results.

Weaknesses:
- The datasets used for experiments are relatively small, limiting the empirical performance demonstration; larger datasets are recommended.
- The method assumes bounded support for the target variable.
- There are presentational issues in figures, particularly in Figure 3, which may confuse readers and require more cognitive effort to interpret compared to traditional formats.
- There is a perceived lack of sufficient evidence for the proposed method's efficacy under specific conditions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the method by formally labeling "Ours" as "CDTree." Additionally, please clarify whether all attributes in the datasets are continuous and how the method handles discrete or categorical attributes. It would be beneficial to specify the model selection criteria used for CART models and whether it aligns with Equation 4 for CDTree. We suggest moving the pseudocode into the main writeup for better accessibility. Furthermore, please address the presentational issues in Figure 3, ensuring that the meanings of symbols are clear and that the alignment of shapes is consistent. Consider providing additional context or annotations to enhance understanding of Figure 3. Lastly, we encourage the authors to strengthen the evidence supporting the method's performance under specific conditions and to provide additional empirical results on larger datasets to highlight the scalability of the proposed method.