ID: UfLH4T676K
Title: Improving Adaptivity via Over-Parameterization in Sequence Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 6, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the influence of over-parameterization on the adaptivity and generalization of sequence models, emphasizing the role of eigenfunctions in kernel regression. The authors propose an over-parameterized gradient descent method to analyze varying eigenfunction orders, demonstrating that over-parameterized models can adapt to underlying signal structures and outperform traditional methods. Theoretical results indicate that deeper over-parameterization enhances model generalization, providing insights for improving neural network performance in dynamic scenarios. Additionally, the authors explore the generalization benefits of over-parameterization in neural networks through an eigenvalue-parameterized kernel space with a fixed eigen-basis, contrasting with previous work that focuses on an adaptive kernel space. They acknowledge the significance of a referenced study on generalization capabilities and plan to include it in their revised introduction, alongside a detailed comparison.

### Strengths and Weaknesses
Strengths:
- The paper presents a novel approach leveraging over-parameterization to enhance adaptivity and generalization in sequence models, combining kernel regression techniques with gradient descent methods.
- It offers a robust theoretical foundation that transforms the understanding of neural network adaptivity beyond the NTK framework.
- The authors provide new insights into the benefits of over-parameterization, particularly through Proposition 3.4.
- The study extends the analysis to deeper layers, offering additional advantages beyond existing two-layer models.
- The clear motivation, rigorous analysis, and well-organized theoretical results contribute to its significance.

Weaknesses:
- Limited experimental validation undermines the empirical support for claims regarding the superiority of over-parameterized models in practical scenarios.
- The setting is overly constrained, assuming a fixed kernel map, which may not reflect more realistic scenarios.
- The paper lacks insightful explanations for why the proposed method improves generalization and does not sufficiently compare its performance with other algorithms.
- Initially, the paper lacked a comprehensive comparison with relevant literature, which may have limited its contextual understanding.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation by conducting a more extensive set of experiments across various datasets and model architectures to demonstrate the robustness and generalizability of the proposed methods. Additionally, we suggest including practical examples that illustrate the application of the approach in real-world settings, particularly addressing the dynamic nature of network architecture and initialization conditions. Clarifying the connection to NTK theory and elaborating on the adaptive choice of stopping time would enhance the theoretical justification. Furthermore, we recommend improving the introduction by including the referenced study and elaborating on the differences in approaches. Finally, integrating numerical experiments from the appendix into the main text would improve clarity and accessibility.