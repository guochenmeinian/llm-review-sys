ID: 3O5YCEWETq
Title: Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 8, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Tiny Time Mixers (TTM), a compact pre-trained model designed for efficient zero/few-shot multivariate time series forecasting. Built on the lightweight TSMixer architecture, TTM incorporates innovations such as adaptive patching, diverse resolution sampling, and resolution prefix tuning, enabling effective pre-training on varied dataset resolutions with minimal model capacity. The authors demonstrate TTM's superior accuracy and computational efficiency through comprehensive evaluations across multiple datasets.

### Strengths and Weaknesses
Strengths:
1. TTMs show substantial improvements in zero/few-shot forecasting capabilities, outperforming existing benchmarks by 4-40%.
2. The paper introduces novel techniques for pre-training and fine-tuning, allowing robust performance on heterogeneous datasets.
3. The architecture is lightweight, starting from just 1 million parameters, addressing the need for efficient forecasting tools.
4. Empirical studies are robust, demonstrating enhanced accuracy and lower computational demands compared to existing methods.

Weaknesses:
1. The term "adaptive patching" may be misleading, as the design appears fixed; "multiscale patching" could be more accurate.
2. The base model TSMixer is not newly proposed, and TTM requires training different models for varying context and forecast lengths, leading to performance loss when there is a mismatch.
3. Insufficient experiments exist to validate the scaling law on pre-training models, particularly regarding the tradeoff between performance and parameter size.
4. Some components of TTM lack ablation studies to sufficiently demonstrate their effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of terminology by considering "multiscale patching" instead of "adaptive patching." Additionally, including comparisons with state-of-the-art end-to-end methods for full-shot head probing would enhance the paper's relevance. To address the weaknesses, we suggest conducting further experiments to explore the scaling law and the impact of parameter size on performance. Finally, incorporating ablation studies for components like the exogenous mixer module and decoder channel-mixing would strengthen the evidence for their effectiveness.