ID: 1p6teT6F73
Title: Alternating Updates for Efficient Transformers
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 8, 7, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called Alternating Updates (AltUp) aimed at increasing the width of transformer models while minimizing computation overhead. The authors evaluate AltUp on the T5 model, demonstrating improvements on benchmarks such as GLUE, SQuAD, and TriviaQA. AltUp divides wide hidden features into blocks processed selectively, with variants like Recycled-AltUp and Sequence-AltUp introduced to enhance efficiency. The method reportedly achieves up to 87% faster inference without sacrificing accuracy.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated, addressing the important topic of efficient scaling in transformer models.
- The concept of "widening representations" is unique and effectively disentangles computation from feature dimensions.
- Comprehensive evaluations and exploration of modifications, such as Recycled-AltUp and Sequence-AltUp, are conducted.

Weaknesses:
- The effectiveness of AltUp is questioned, as improvements are not significantly pronounced in certain evaluations.
- The evaluation lacks comparisons with other efficient transformer variants, making it difficult to assess performance relative to existing methods.
- The concept of Sequence-AltUp may require further clarification regarding its relation to widening representations.

### Suggestions for Improvement
We recommend that the authors improve the evaluation section by including baselines for comparison against similar works that utilize sparsity for model acceleration. Additionally, we suggest that the authors clarify the relationship between Sequence-AltUp and widening representations. It would also be beneficial to explore the performance of AltUp on encoder-only and decoder-only architectures to strengthen the findings. Finally, addressing how AltUp compares with other efficient attention mechanisms would provide a more comprehensive understanding of its effectiveness.