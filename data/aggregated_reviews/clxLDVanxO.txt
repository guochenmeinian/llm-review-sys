ID: clxLDVanxO
Title: ReTAG: Reasoning Aware Table to Analytic Text Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ReTAG, a table-to-text model that incorporates multiple categories of reasoning for improved summary generation. The authors demonstrate that explicit control over reasoning categories enhances summary quality, achieving up to 12% improvement in human evaluations. Additionally, they plan to release approximately 32K instances of reasoning category-tagged data from the ToTTo and InfoTabs datasets.

### Strengths and Weaknesses
Strengths:
- The incorporation of multiple reasoning types addresses a significant gap in table summarization.
- Human evaluations indicate that ReTAG's outputs are more faithful and analytical compared to existing models.
- The release of a large, tagged dataset contributes valuable resources to the research community.

Weaknesses:
- The problem statement lacks clarity, particularly regarding the abstract's claims and the role of reasoning categories.
- Improvements over baseline models are minimal in some metrics, raising concerns about their significance.
- Numerous typos and grammatical issues hinder readability, and the experimental settings lack comprehensiveness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the problem statement to align the abstract with the detailed contributions in the paper. Additionally, consider addressing the significance of the reported improvements and providing a more comprehensive experimental setting, including comparisons with larger models like T5 and FLAN T5. We also suggest that the authors enhance the writing quality by correcting typos and grammatical errors, ensuring consistent terminology, and possibly formatting certain sections as tables for better clarity. Lastly, including inter-annotator agreement metrics in the evaluation would strengthen the reliability of the human assessments.