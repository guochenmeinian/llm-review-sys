ID: NVDYgEFXCy
Title: Adaptive and Optimal Second-order Optimistic Methods for Minimax Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 6, 5, 7, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents adaptive, line search-free second-order methods designed to solve convex-concave min-max problems. The authors propose algorithms that utilize an adaptive step size, requiring only one linear system to be solved per iteration. The main contributions include an adaptive second-order optimistic method achieving an optimal convergence rate of O(1/T^1.5) and a parameter-free variant that does not depend on the Lipschitz constant of the Hessian. The algorithms are empirically evaluated against existing methods, demonstrating both practical efficiency and optimal rates.

### Strengths and Weaknesses
Strengths:
1. The introduction of adaptive, line search-free second-order methods with optimal convergence rates is a significant advancement in the field.
2. The development of a parameter-free version that adapts based on local information without requiring the Lipschitz constant is particularly noteworthy.
3. The contributions are clearly articulated and supported by theoretical analysis and empirical results.

Weaknesses:
1. The parameter-free method, while innovative, may still necessitate careful tuning of initial parameters in practical applications.
2. The numerical experiments, although promising, are limited to specific problem settings and may not generalize well across diverse optimization tasks.
3. The advantages of the proposed methods over existing optimal second-order methods are not clearly demonstrated, particularly in terms of computational efficiency and convergence rates.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the limitations associated with the parameter-free method, particularly regarding initial parameter tuning. Additionally, we suggest providing experimental results that illustrate the computational costs of using line search compared to the proposed methods. It would also be beneficial to include more diverse problem settings in the numerical experiments to assess the generalizability of the algorithms. Furthermore, we encourage the authors to discuss the potential impact of deviations from the assumed Lipschitz continuity of gradients and Hessians on the robustness of their methods. Lastly, we recommend that the authors explore the connection to error feedback literature and consider extending their theoretical results to the more general problem of monotone inclusion in the appendix.