ID: Ai76ATrb2y
Title: Auditing Privacy Mechanisms via Label Inference Attacks
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 8, 3, 9, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents measures for auditing privacy-preserving mechanisms that utilize datasets with public features and private labels. The authors propose a comparison between two attackers: a weaker attacker with access to dataset features and prior knowledge of feature-label correlations, and a stronger attacker with access to privatized labels. The advantage of the stronger attacker is measured additively and multiplicatively, with upper bounds derived for randomized response (RR) and learning from label proportions (LLP). The experiments demonstrate that the proposed measures effectively evaluate privacy-utility trade-offs, revealing that RR outperforms LLP. Additionally, the authors introduce a new measure for assessing privacy in the context of label aggregation and label privacy, quantifying the risk posed to individuals in a dataset due to the output of privacy-enhancing technologies (PETs). However, concerns are raised regarding the lack of motivation for the proposed measure's superiority over existing methods, particularly entropy-based measures, and the limited scope of related work discussed.

### Strengths and Weaknesses
Strengths:
- **Originality:** The paper is the first to apply the multiplicative measure to auditing and the additive measure to non-differentially private mechanisms.
- **Significance:** The measures facilitate auditing of non-differentially private mechanisms, addressing a gap in the literature and potentially promoting the adoption of differential privacy.
- **Quality:** Thorough experiments across various datasets and parameter settings, complemented by visualizations of prior/posterior updates, enhance the results.
- **Clarity:** Generally clear presentation, with a well-motivated discussion of label inference as a relevant threat.
- The authors provide a novel approach to measuring privacy risks associated with PET outputs and attempt to address the complexities of privacy in mixed settings involving public features and sensitive labels.

Weaknesses:
- The narrative could be improved by emphasizing applications to online advertising, and the necessity of both measures is unclear, as the multiplicative measure appears superior.
- Some bounds rely on strong assumptions about data distribution, such as uncorrelated labels and features, which may not be interesting.
- Claims regarding the measures providing a "balanced and nuanced picture" and the model of adversary uncertainty need further elaboration for practitioner understanding.
- The paper does not sufficiently motivate why the proposed measure is preferable to existing measures, such as entropy-based ones.
- The discussion of related work is narrow and fails to adequately compare the new measure with established methods.
- There are inaccuracies and lack of depth in the analysis of limitations, particularly regarding composability and the assumptions made about adversaries' knowledge.
- Minor issues include unclear references, inconsistencies in notation, and lack of clarity in certain definitions.

### Suggestions for Improvement
We recommend that the authors improve the narrative by focusing more on applications to online advertising and clarify the necessity of both the additive and multiplicative measures. It would be beneficial to explain the assumptions made regarding data distribution and discuss the implications of the adversary's uncertainty model in the context of online advertising. Additionally, we suggest providing more detail on how the posterior probabilities for LLP+Geom and LLP+Laplace were calculated, as this is crucial for understanding the experimental results. We also recommend enhancing the motivation for the proposed measure by clearly articulating its advantages over existing measures, particularly in relation to entropy-based approaches. Furthermore, we suggest expanding the discussion of related work to include a broader range of privacy measures, including those based on mutual information, to provide a more comprehensive comparison. Finally, we encourage the authors to clarify the limitations of their approach, particularly regarding composability and the implications of evaluating privacy on the same dataset as the training set, while also enhancing the rigor of their technical details and ensuring that notation is consistently explained to aid reader comprehension. Addressing minor inconsistencies and typos throughout the paper will also enhance clarity and readability.