ID: fNoleQa9RX
Title: The Unmet Promise of Synthetic Training Images: Using Retrieved Real Images Performs Better
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 8, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the effectiveness of training on synthetic images generated by models like Stable Diffusion compared to real images retrieved from datasets such as LAION-2B. The authors propose a baseline that demonstrates that real images often outperform synthetic ones across various recognition tasks. Key insights include: 1) retrieved real images significantly outperform synthetic images, 2) both data sources enhance the performance of original training images, and 3) adding synthetic images can negate the benefits of real images. The authors also investigate whether synthetic training data provides useful information beyond that of its generator's training dataset, arguing that prior works, such as SynCLR, conflate targeted data sampling with synthetic data, complicating the interpretation of results. They detail how SynCLR's synthetic dataset, $D_S$, is constructed through targeted data sampling, leading to potential biases in performance comparisons against the real dataset, $D_R$. The proposed retrieval baseline disentangles the effects of data targeting and synthetic versus real data, allowing for a clearer assessment of the value added by synthetic data. The paper analyzes factors contributing to the inferiority of synthetic images, such as high-frequency artifacts and lack of fine-grained details.

### Strengths and Weaknesses
Strengths:
1. The paper is clearly written and presents a timely and interesting insight into the limitations of synthetic images, which has been overlooked in the community.
2. The analysis of synthetic image performance, particularly through an ablation study on synthetically perturbed real images, is convincing and inspiring.
3. The experimental design is robust, featuring extensive testing across various datasets and scales, and the proposed baseline is novel and impactful for future research.
4. The authors effectively identify and clarify the conflation of independent variables in existing research, providing a principled approach to experimental design.
5. The proposed retrieval baseline offers a novel method for evaluating synthetic data against targeted real data, enhancing the rigor of the analysis.

Weaknesses:
1. The technical contribution is somewhat limited, primarily focusing on nearest-neighbor retrieval without exploring more sophisticated sampling methods.
2. Some experimental settings may lead to unfair comparisons, such as the filtering of images based solely on CLIP similarity without robust quantitative indicators for image quality.
3. The paper does not sufficiently clarify the advantages of using upstream real data as a baseline compared to other datasets, nor does it compare downstream fine-tuning results of real-and-synthetic data from non-training datasets.
4. Despite the innovative approach, the findings indicate that state-of-the-art synthetic data methods still underperform compared to retrieved data, raising questions about the current efficacy of synthetic data generation techniques.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the necessity of using training data as a baseline and explore the potential advantages of synthetic images in scenarios with rare or complex concepts. Additionally, we suggest incorporating more robust quantitative indicators for evaluating synthetic image quality beyond CLIP similarity. The authors should also consider testing other state-of-the-art image models to provide a more comprehensive analysis and address the limitations of their current experimental settings. Furthermore, we recommend improving the clarity of their experimental setup and results, particularly in distinguishing between the effects of synthetic and targeted data. We suggest including more detailed discussions on the implications of their findings for future synthetic data methodologies, especially regarding the identified visual artifacts and semantic content differences that contribute to the underperformance of synthetic data. Finally, we encourage the authors to explore methods for generating unique image compositions that are absent from the generator's training set, as this could enhance the utility of synthetic data beyond existing datasets.