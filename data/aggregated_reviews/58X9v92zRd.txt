ID: 58X9v92zRd
Title: Adaptable Logical Control for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Ctrl-G, an approach to control LLM generation by enforcing logical constraints, such as maintaining specific keywords. The method comprises two components: a Hidden Markov Model (HMM) that predicts token generation based on sampled data from the LLM, and deterministic finite automata (DFAs) that check whether the generated output meets the specified constraints. The authors claim that Ctrl-G achieves high accuracy and efficiency across various tasks, including keyphrase generation and mathematical reasoning, outperforming models like GPT-3.5 and GPT-4 in certain evaluations. The paper highlights that Ctrl-G (with TULU2-7B and HMM-2B) significantly outperforms GPT-4 by 30% to 70% in overall satisfaction rates across various settings. The authors argue for the necessity of an HMM in their approach, explaining that existing methods lack probabilistic information and thus are less effective in generating coherent outputs. They emphasize that Ctrl-G generalizes beyond GeLaTo by allowing for arbitrary constraints represented as DFAs, while GeLaTo is limited to keyword-constrained generation.

### Strengths and Weaknesses
Strengths:
1. The use of an HMM to approximate LLM token generation is innovative, enhancing the exploration of the generation space.
2. Ctrl-G demonstrates superior performance compared to GPT-4 in terms of overall satisfaction.
3. The paper provides a clear rationale for the inclusion of an HMM, distinguishing it from other methods that rely solely on logical reasoning.
4. The method demonstrates competitive performance across multiple tasks and is scalable for larger models.
5. The technical contributions of Ctrl-G, particularly in deriving a polynomial-time algorithm for computing probabilities, are well-articulated.
6. The paper is well-structured and presents comprehensive experimental results, supported by human evaluations.

Weaknesses:
1. The evaluation lacks sufficient detail and comparative analysis with other related works, such as GeLaTo and simpler constrained decoding methods.
2. The training quality of the HMM is questionable, as it relies on potentially low-quality samples from a smaller model.
3. The paper does not adequately address the limitations of the proposed approach or its potential negative societal impacts.
4. The explanation of why GeLaTo inference is slower than Ctrl-G lacks depth and requires further elaboration.
5. The paper could benefit from a more detailed discussion of HMM training and its implications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental comparisons by including results from related works like GeLaTo and simpler constrained decoding methods. Additionally, the authors should provide more detailed discussions on the limitations of the HMM training process and the implications of using a smaller model for distillation. It would also be beneficial to clarify the contributions of the DFAs in the methodology section and ensure that the evaluation results distinctly demonstrate their added value. Furthermore, we suggest condensing the lengthy discussion on DFAs to enhance the overall readability of the paper. Lastly, we recommend that the authors improve the explanation of why GeLaTo inference is slower than Ctrl-G, providing a more thorough analysis beyond the observation that it "seems sequential," and add a more detailed discussion of HMM training in the methodology section to enhance clarity and understanding.