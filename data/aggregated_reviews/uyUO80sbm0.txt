ID: uyUO80sbm0
Title: Explain-then-translate: an analysis on improving program translation with self-generated explanations
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the impact of self-generated natural language explanations on code-to-code translation performance, proposing an explain-then-translate method and analyzing its effectiveness across 19 programming languages and three types of explanations. The authors introduce five heuristics for selecting better explanations to enhance the method's performance and conduct multiple ablation studies to compare the robustness of natural language explanations against programming language examples commonly used in few-shot learning.

### Strengths and Weaknesses
Strengths:
- This paper is the first detailed study on how self-generated natural language explanations can enhance code-to-code translation performance.
- It provides comprehensive examples of prompts used, aiding future reproduction of results and research in this area.
- The introduction of novel heuristic selection strategies improves the explain-then-translate method and serves as a blueprint for future research.
- Results from ablation experiments demonstrate the robustness and superior performance of natural language explanations compared to automatically generated programming examples.

Weaknesses:
- The paper lacks links to the code and dataset, making it difficult to verify the results.
- Methodology in Section 3 is vague, lacking details on the specific LLM used and the prompting process, which may hinder reproducibility.
- The study relies solely on ChatGPT without comparative evaluation against other LLMs, limiting the generalizability of findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 3 by specifying the LLM used and detailing the prompting methodology to enhance reproducibility. Additionally, we suggest including links to the code and dataset to facilitate verification of results. To strengthen the study, we encourage the authors to conduct comparative analyses with other LLMs to validate their findings and provide a more comprehensive understanding of the method's effectiveness.