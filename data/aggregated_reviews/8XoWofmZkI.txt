ID: 8XoWofmZkI
Title: Learnability of high-dimensional targets by two-parameter models and gradient flow
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 4, 8, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 2, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the learnability of a target function \( f \) in a \( d \)-dimensional Hilbert space \( \mathcal{H} \) using gradient flow on a \( W \)-parameter model with \( W < d \). The authors propose several results, including Theorem 5, which states that for \( W = 2 \), there exists a parametric map \( \Phi: \mathbb{R}^2 \rightarrow \mathcal{H} \) such that gradient flow converges to the target with high probability. However, if \( \Phi \) is constructed using "elementary functions," the set of learnable targets has Lebesgue measure 0. The paper also discusses the implications of Theorems 3 and 4, which indicate the existence of non-learnable functions under certain conditions.

### Strengths and Weaknesses
Strengths:
- The results are technically impressive and contribute significantly to the understanding of learnability in high-dimensional spaces.
- The presentation is generally clear, with well-illustrated proofs and a coherent narrative throughout the paper.

Weaknesses:
- The relevance of the problem to the NeurIPS community and machine learning is unclear, as the construction of \( \Phi \) appears pathological compared to regular parametric models used in practice.
- The proof sketch for Theorem 5 lacks clarity, particularly regarding the construction of \( \Phi \) and the definitions of key concepts like the Cantor set \( F_0 \) and boxes \( B_\alpha^{(n)} \).
- The paper does not adequately address the intuition behind the results, particularly regarding the implications of Theorem 5 in relation to infinite-dimensional target spaces.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proof sketch for Theorem 5 by providing a more detailed definition of the construction of \( \Phi \) and explaining the significance of the Cantor set \( F_0 \) and the boxes \( B_\alpha^{(n)} \). Additionally, we suggest that the authors clarify the relevance of their findings to the machine learning community, possibly by discussing connections to existing models or frameworks. It would also be beneficial to include a discussion on the implications of Theorems 3, 4, and 5 to help readers better understand the landscape of learnable functions. Finally, we encourage the authors to consider simplifying the exposition and moving some technical details to an appendix to enhance readability for a broader audience.