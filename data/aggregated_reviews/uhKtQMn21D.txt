ID: uhKtQMn21D
Title: Mechanic: A Learning Rate Tuner
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MECHANIC, a method for tuning the learning rate of any base optimizer, compatible with various learning rate schedules. The authors leverage a scheme from online convex optimization to determine a scale factor \( s_t \) based on the cumulative steps of the optimizer. They aim to simplify hyperparameter tuning, specifically focusing on the learning rate scale factor, while acknowledging that other hyperparameters may still require tuning. Experimental results demonstrate that MECHANIC can accelerate optimization in tasks like masked language modeling and image classification, although it may sometimes result in poorer generalization. Notably, MECHANIC is not a substitute for learning rate schedulers, as evidenced by the necessity for learning rate decay in CIFAR10 experiments. The authors compare their results against BERT and discuss the implications of their experimental setup, including the removal of the NSP loss and differences in training strategies compared to RoBERTa.

### Strengths and Weaknesses
Strengths:
- The authors improve upon tuned baselines, which is significant for large-scale deep learning datasets like BERT and JFT-300m, using a theoretically motivated algorithm.
- The method is parameter-free and empirically validated across various domains and architectures, addressing the bottleneck of hyper-parameter tuning in large model training.
- The authors provide a clear rationale for their experimental design and the advantages of using MECHANIC.
- The paper addresses important aspects of hyperparameter tuning, particularly the learning rate scale factor, which is a critical parameter in model training.

Weaknesses:
- The results would be more compelling if compared against more recent baselines, such as Mosaic MLâ€™s BERT codebase.
- Key experimental details are lacking, including how hyper-parameters for the base algorithm were chosen and the statistical significance of results.
- There are concerns regarding the comparison of results with RoBERTa, as it consistently outperforms the baseline models, raising questions about convergence and tuning.
- Missing ablation studies for hyper-parameters \( \lambda \) and \( \beta \), and the effects of learning rate schedulers on MECHANIC's performance are not explored.
- The performance of MECHANIC appears to be on par with well-tuned models, which may undermine its claimed benefits.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the tuning of the baseline, including a detailed explanation of the hyper-parameter selection process and the learning rate scheduler used. Additionally, providing statistical significance measures for the results would strengthen the claims. We suggest including ablation studies to analyze the effects of \( \lambda \) and \( \beta \) on performance, as well as investigating MECHANIC's efficacy with poorly tuned base algorithm hyper-parameters. Furthermore, we recommend that the authors improve the clarity of their comparisons with RoBERTa, ensuring that the differences in training setups are explicitly stated. Addressing the concerns about model convergence and the necessity of tuning other hyperparameters could enhance the overall robustness of the paper. Finally, enhancing the exposition of the theoretical background and ensuring consistent notation throughout the paper would improve readability for a broader audience.