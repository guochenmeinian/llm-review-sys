ID: MJgMMqMDu4
Title: A Single-Step, Sharpness-Aware Minimization is All You Need to Achieve Efficient and Accurate Sparse Training
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 7, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a sparse training method, $S^2$-SAM, which applies sharpness-aware minimization to address the chaotic loss landscape encountered in sparse training. The authors propose leveraging the gradient from the previous step to avoid the additional computational cost associated with traditional SAM. They theoretically demonstrate that the error from $S^2$-SAM is bounded and provide empirical evidence of its effectiveness across various sparse training methods, as well as dense training.

### Strengths and Weaknesses
Strengths:
- The method is practical, efficient, and achieves strong experimental results, including a notable clock-time comparison alongside FLOPs.
- Theoretical and extensive empirical evaluations validate the effectiveness of $S^2$-SAM for improving the generalization ability of sparse networks.

Weaknesses:
- The experimental scope is limited, as the method is tested only up to 90% sparsity on ImageNet, and a full ablation study of $S^2$-SAM's components is lacking.
- The sharpness figures are confusing, and the process for obtaining them is not clearly explained.
- The premise that sparse training is inherently more difficult than dense training is challenged by existing literature suggesting that some sparse methods can outperform dense counterparts.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by including high-sparsity results to better assess $S^2$-SAM's effectiveness in those scenarios. Additionally, conducting a full ablation study would clarify the impact of using the previous-gradient estimate compared to traditional SAM. It would also be beneficial to compare $S^2$-SAM with other regularization methods like Learning Rate Rewinding (LRR) to evaluate their relative effectiveness. Furthermore, we suggest providing a clearer explanation of the sharpness figures and detailing the computation process in an appendix. Finally, expanding the empirical validation to include other neural network architectures beyond convolutional networks would enhance the paper's breadth.