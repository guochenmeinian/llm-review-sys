ID: 82Ndsr4OS6
Title: Adversarially Trained Weighted Actor-Critic for Safe Offline Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Weighted Safe Actor-Critic (WSAC), a novel Safe Offline Reinforcement Learning algorithm that aims to outperform reference policies while ensuring safety with limited data. The authors utilize a two-player Stackelberg game for optimal convergence and safe policy improvements, demonstrating WSAC's effectiveness in continuous control environments. Additionally, the authors propose a method for accurately estimating the behavior policy in offline reinforcement learning (RL) settings, emphasizing the importance of single-policy coverage over full-policy coverage. They introduce a function approximation approach using neural networks and actor-critic methods to handle continuous state and action spaces, arguing that their method guarantees robust policy improvement in constrained Markov decision processes (CMDPs). The theoretical framework includes a pessimistic bias through a weighted average Bellman error and assures robust policy improvement without requiring all policy concentrability.

### Strengths and Weaknesses
Strengths:
- WSAC's ability to outperform behavior policies across various hyperparameters is significant for practical applications.
- The authors provide a solid theoretical foundation and empirical evidence, achieving state-of-the-art performance in safety.
- The paper effectively identifies research gaps and innovations, with a clear theoretical contribution addressing challenges in CMDPs.
- The authors effectively differentiate their approach from existing methods like ATAC, highlighting significant methodological advancements.

Weaknesses:
- Many assumptions and proofs resemble those in existing literature, particularly ATAC, with limited novel insights.
- The reliance on an explicit reference policy may hinder training in behavior-agnostic scenarios.
- The safety evaluation is based solely on cost, which may not be practical in real-world applications.
- The comparison with state-of-the-art (SOTA) algorithms is limited, primarily including older methods, and lacks comprehensive comparisons with state-of-the-art baselines.
- The discussion on the selection of cost limits and their impact on performance could be more comprehensive.

### Suggestions for Improvement
We recommend that the authors improve the comparison with more advanced state-of-the-art baselines, such as CDT and FISOR, to better contextualize WSAC's performance and demonstrate the strength of their proposed method. Additionally, the authors should consider providing empirical evidence to support claims of computational efficiency, including wall-clock time comparisons. Clarifying the term "adversarial" in the context of their approach and discussing the implications of assumptions in practical settings would enhance the paper's clarity. Expanding the discussion on the selection of cost limits and their implications for performance in various environments would ensure a fairer comparison across different settings. Finally, exploring the extension of WSAC to multi-constraint settings and addressing the capability under sparse-cost scenarios would further strengthen the contribution.