ID: SP8zIwanHD
Title: $\textbf{\emph{CLMSM}}$: A Multi-Task Learning Framework for Pre-training on Procedural Text
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a pretraining framework for entity tracking and action alignment in procedural text, specifically focusing on the recipe domain using the NPN-Cooking Dataset and the ProPara Dataset. The authors propose a model based on a Triplet Network with a BERT or RoBERTa encoder, employing a loss function that combines contrastive learning and mask-step modeling. Extensive experiments demonstrate that the proposed approach significantly outperforms state-of-the-art baselines and generalizes well to open-domain tasks.

### Strengths and Weaknesses
Strengths:
- The paper proposes a pretraining scheme that outperforms state-of-the-art baselines for procedural text tasks.
- Extensive ablation studies provide insights into the contributions of individual components of the proposed approach.
- The authors discuss individual baseline performances, enhancing understanding of the results.
- An anonymized version of the code is provided for reproducibility.

Weaknesses:
- The approach requires four training stages, introducing variability in pretraining data and parameters that may lead to brittleness.
- The novelty of the method is questioned, as it appears to combine existing approaches without clear motivation for its relevance.
- The writing is dense, with many acronyms and a lack of clear examples, making it difficult to understand for readers unfamiliar with the topic.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by including more examples to illustrate the procedural data the model is trained on. Additionally, simplifying the text and reducing the use of acronyms would enhance readability. It would also be beneficial to provide details on the statistical tests used to compute p-values and to clarify the relevance of the proposed method compared to off-the-shelf LLMs in zero-shot or few-shot settings.