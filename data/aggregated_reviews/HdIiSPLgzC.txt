ID: HdIiSPLgzC
Title: MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 8, 7, 6, 8, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents MINT-1T, the largest open-source multimodal interleaved dataset, comprising one trillion text tokens and three billion images sourced from HTML, PDFs, and ArXiv. The authors aim to address the limitations of existing datasets by providing a more diverse and larger-scale resource, which is ten times the size of OBELICS. The dataset's utility is demonstrated through experiments showing that models trained on MINT-1T can match or surpass the performance of those trained on OBELICS, particularly excelling in multi-image reasoning tasks.

### Strengths and Weaknesses
Strengths:
- MINT-1T is a diverse, large-scale open-source multimodal dataset that significantly advances the state-of-the-art for multimodal LLMs.
- The dataset's construction is well-documented, showcasing a meticulous data curation process and effective measures to remove inappropriate content.
- The experiments indicate that models trained on MINT-1T can outperform those trained on existing datasets, particularly in multi-image reasoning.

Weaknesses:
- The dataset remains closed-source until the review process concludes, which may hinder accessibility.
- There are concerns regarding the balance of document domain distribution, which could introduce biases affecting model generalization.
- The authors did not fully utilize the dataset's potential for training LLMs, raising questions about computational resource limitations.

### Suggestions for Improvement
We recommend that the authors improve the dataset's accessibility by releasing it as open-source promptly after the review process. Additionally, expanding the baseline model coverage by training models like LLaVA-NeXt-Interleave could further validate the dataset's effectiveness. Providing more quality visualizations and evaluating the dataset against modern benchmarks such as MMMU and MathVista would enhance its usability. Finally, clarifying the timeline for open-sourcing the dataset and explicitly stating the licensing terms would be beneficial.