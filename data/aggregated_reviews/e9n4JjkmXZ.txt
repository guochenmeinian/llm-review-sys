ID: e9n4JjkmXZ
Title: URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel benchmark for Uncertainty-aware Representation Learning (URL), aimed at evaluating the transferability of uncertainty estimates to zero-shot downstream tasks. The authors propose the R-AUROC metric to assess the performance of various uncertainty quantifiers trained on ImageNet and tested across eight downstream datasets. They emphasize that pretrained models should not only predict high uncertainties for out-of-distribution (OOD) data but also effectively rank uncertainties among samples. The findings reveal that achieving reliable uncertainty estimation remains a significant challenge, with all models scoring below 0.6 on R-AUROC, indicating substantial room for improvement. The authors conduct ablation studies to demonstrate the performance of their uncertainty estimators across different benchmarks and explore the relationship between task transfer difficulty and uncertainty estimator quality, finding that performance decreases with increased task shift.

### Strengths and Weaknesses
**Strengths:**
1. The paper addresses a critical problem in machine learning, contributing to the understanding of uncertainty quantification in representation learning.
2. The extensive evaluation of ten uncertainty quantifiers and the introduction of the R-AUROC metric provide valuable insights into the transferability of uncertainty estimates.
3. The manuscript includes comprehensive ablation studies and additional experiments on OOD detection, enhancing its robustness.
4. The authors provide a clear distinction between aleatoric and epistemic uncertainty, addressing potential concerns about the applicability of their uncertainty estimates across tasks.
5. The clarity and organization of the paper enhance its accessibility, supported by well-structured experiments and documentation.
6. The introduction of few-shot experiments shows promise in finetuning uncertainty estimators for downstream tasks.

**Weaknesses:**
1. The benchmark's assumptions may not adequately reflect the complexities of out-of-distribution (OOD) scenarios, as it is unclear why a good uncertainty predictor should differentiate between different OOD samples.
2. The evaluation primarily focuses on classification tasks, leaving the performance of uncertainty quantifiers across other task types ambiguous.
3. The reliance on R-AUROC may complicate comparisons between models with varying accuracies, as the metric is influenced by individual model errors and may hinder comparability between different models due to varying ground truth labels.
4. The authors acknowledge that the uncertainty estimates may not effectively capture epistemic uncertainty, which could limit their applicability in certain contexts.

### Suggestions for Improvement
1. We recommend that the authors broaden the variety of scenarios in their evaluation to include more diverse contexts, particularly OOD situations, to enhance the robustness and generalizability of their findings.
2. We suggest that the authors analyze the computational costs associated with the evaluated methods, as this would provide a more holistic view of their practicality in real-world applications.
3. We encourage the authors to address the missing related works on transferable uncertainty estimates, particularly the contributions of Ovadia et al. (2019), to better contextualize their findings within the existing literature.
4. We recommend that the authors improve the clarity of the relationship between task-specific features and the generalizability of uncertainty estimates.
5. Further exploration of the implications of model accuracy on R-AUROC values would enhance the discussion.
6. It would be beneficial to provide more insights into the results of few-shot finetuning of uncertainties on downstream tasks, as this could strengthen the practical applicability of their findings.
7. Finally, emphasizing the limitations of R-AUROC in the context of model comparisons could provide a more balanced view of the metric's utility.