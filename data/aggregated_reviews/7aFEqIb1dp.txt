ID: 7aFEqIb1dp
Title: Untrained Neural Nets for Snapshot Compressive Imaging: Theory and Algorithms
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 5, 7, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis for mask optimization and DIP-based SCI recovery methods, claiming that the proposed SCI-BDVP achieves state-of-the-art (SOTA) performance among untrained neural network (UNN) methods. The authors introduce bagged-DIP to develop SCI iterative methods and provide extensive empirical evaluations demonstrating the effectiveness of their approach in video SCI recovery, even outperforming supervised methods in noisy scenarios.

### Strengths and Weaknesses
Strengths:
1. Theoretical analysis of the proposed formulation is rigorous and rare compared to conventional works.
2. Detailed empirical evaluation supports the claims of improved performance.
3. The paper is well-structured with clear logic and convincing experimental results.

Weaknesses:
1. Formatting issues in Figure 2 hinder readability, particularly near smaller cubes.
2. The theoretical analysis cannot be generalized to all untrained networks, as it does not adequately address the conditions required for the DIP hypothesis and Lipschitz continuity.
3. The actual contribution regarding theoretical results for untrained networks is less than stated, lacking new hypotheses in snapshot compressive imaging.
4. The comparison algorithms are insufficiently defined, making it unclear how the proposed method achieves SOTA performance.
5. The paper does not analyze runtime, which is crucial for untrained methods, and lacks clarity in notation for symbols used.

### Suggestions for Improvement
We recommend that the authors improve the formatting of Figure 2 to enhance readability. Additionally, clarify how the UNNs used in this work satisfy the DIP hypothesis and Lipschitz's condition, as well as provide a more robust theoretical foundation for their claims. We suggest including recent self-supervised methods in the comparison to demonstrate the effectiveness of the algorithm more fairly. Furthermore, an analysis of runtime in comparison to existing methods should be added, along with clearer notations for symbols used throughout the paper. Lastly, consider exploring the application of the proposed method on additional datasets and under different noise models to strengthen the evaluation of its robustness and general applicability.