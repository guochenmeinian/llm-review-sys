ID: RJEC9fZ9Ma
Title: Neural Collapse To Multiple Centers For Imbalanced Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of Neural Collapse (NC) in imbalanced data, introducing the concept of Neural Collapse to Multiple Centers (NCMC). The authors establish that aligning features from minor classes with more directions enhances classification accuracy, leading to the Generalized Classification Rule (GCR). They propose an MSE-type objective function and a practical loss function to induce NCMC, achieving performance comparable to classical imbalanced learning methods. The findings are validated through both theoretical and experimental means, contributing new insights into NC applications in imbalanced learning scenarios.

### Strengths and Weaknesses
Strengths:  
- The paper is clearly written and systematically explains complex concepts, maintaining a logical flow that is accessible to a broad audience.  
- The findings significantly advance the field of deep learning and classification, offering a novel approach to imbalanced learning that matches classical methods in performance.  

Weaknesses:  
- Figure 1 should be larger for better visualization.  
- Theorems 3.3 and 3.4 demonstrate that the optimization problem $\mathbf{P}$ can induce the NCMC solution, yet the loss function in $\mathbf{P}$ is not included in the experiments; it would be beneficial to incorporate it in comparison experiments and analyze the results.  
- The authors should clarify why they opted for a Cosine Regression Loss instead of directly using the loss function in $\mathbf{P}$ if multiple centers are superior in challenging feature distributions.  
- The study referenced as [1] should be compared within this work.  
- Proposition 3.1 warrants more discussion in the main text, as it is a key motivational component of the paper.

### Suggestions for Improvement
We recommend that the authors improve the size of Figure 1 for better visualization. Additionally, we suggest including the loss function $\mathbf{P}$ in the experimental comparisons and providing a thorough analysis of the results. The authors should clarify their choice of using Cosine Regression Loss over the loss function in $\mathbf{P}$. We also encourage the authors to compare their findings with the study referenced as [1]. Finally, we believe that Proposition 3.1 deserves more space and discussion in the main text to enhance its motivational impact.