ID: vTrRq6vCQH
Title: PIXIU: A Comprehensive Benchmark, Instruction Dataset and Large Language Model for Finance
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 5, 7, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper introduces PIXIU, which encompasses an instruction tuning dataset (FIT), the first open-sourced financial large language model (FinMA) fine-tuned from LLaMA using FIT, and an evaluation benchmark (FLARE) for financial NLP and prediction tasks. The work covers five tasks across nine datasets, demonstrating FinMA's superiority over other models like BloombergGPT and GPT-4 in four out of five tasks, excluding question answering. The authors have expanded their evaluation benchmark to include one financial prediction task and three financial NLP tasks, incorporating multiple new datasets. They acknowledge the importance of risk warnings when applying financial LLMs and have addressed this in the paper's "Limitations" section. While fine-tuning LLaMA is not their primary contribution, the authors emphasize their pioneering efforts in releasing open-source instruction-tuning data and establishing a comprehensive evaluation benchmark. The manuscript includes results from non-LLM models to demonstrate the complexity of stock movement prediction and compares their model's performance with that of other LLMs and PLMs.

### Strengths and Weaknesses
**Strengths:**
- The paper provides a comprehensive framework with the first financial LLM based on instruction tuning, a multitask dataset with 136,609 samples, and an evaluation benchmark covering various financial tasks.
- Extensive evaluations show FinMA's effectiveness across multiple financial NLP tasks, contributing to future research in financial LLMs.
- The expanded benchmark with diverse tasks enhances the evaluative framework, addressing prior concerns about the suitability of stock prediction as a sole evaluation task.
- The authors have made significant efforts to clarify the limitations and risks associated with their model, which adds transparency.
- The inclusion of results from traditional models provides a more comprehensive understanding of the challenges in stock prediction.

**Weaknesses:**
- The dataset primarily assembles existing datasets, limiting the novelty of the contribution.
- The evaluation design is considered too simplistic, and the stock prediction task may not effectively assess FinLLM capabilities due to market volatility.
- Concerns remain regarding the performance gap between decoder-only LLMs and bidirectional PLMs, as well as the scientific value of fine-tuning LLaMA for specialized tasks.
- Some reviewers noted that the evaluation of larger models is still underway, which may limit the completeness of the current findings.

### Suggestions for Improvement
- The authors should include additional financial prediction tasks beyond stock movement prediction to enhance the dataset's applicability in real-world scenarios.
- The authors are encouraged to expedite the evaluation of larger models and update the results in their GitHub repository to provide a more comprehensive comparison.
- The authors should improve the clarity and depth of their discussion regarding the performance gap between decoder-only LLMs and bidirectional PLMs, ensuring that the implications of their findings are fully articulated.
- The authors should consider incorporating bidirectional LLMs, such as FinBERT and FLANG, especially for tasks like binary classification and named entity recognition, where they are expected to perform well.
- The authors need to clarify the results in Table 5, indicating which results are referenced from other works and addressing missing baseline results.
- The authors should strengthen the evaluation design to encompass more complex tasks and include risk warnings when discussing stock market predictions.
- The authors should consider including more empirical evidence or peer-reviewed references to support their claims about the effectiveness of instruction tuning across various domains.