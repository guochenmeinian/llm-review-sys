ID: TMlGQw7EbC
Title: Markov Equivalence and Consistency in Differentiable Structure Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a differentiable DAG learning method utilizing log-likelihood loss and the minimax concave penalty (MCP). The authors demonstrate that this construction allows the minimizer of the loss to identify the sparsest graph that generates the observational distribution, with equivalence to the ground truth graph under the faithfulness assumption. The method is validated through extensive experiments across various dimensions and graph structures. Additionally, new identifiability results based on maximum likelihood estimation and sparsity regularization are introduced for both Gaussian linear and general nonlinear models.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and easy to follow, with clear notation and a pedagogical approach.  
- The theoretical contributions are solid, particularly the identification of graph structures using a broad class of penalties alongside log-likelihood loss.  
- The simulations are well-designed, demonstrating the proposed method's superior performance.

Weaknesses:  
- The method appears to be a direct combination of existing works, which diminishes its novelty.  
- The log-likelihood loss used for nonlinear models may be improper due to heteroscedastic noise, necessitating a correction.  
- The paper lacks real-world dataset validation and does not provide available code.  
- Minor presentation flaws include a missing conclusion section, incorrect citation formatting, and unclear limit statements in theorems.

### Suggestions for Improvement
We recommend that the authors improve the novelty factor by addressing the omission of relevant prior work, particularly [a], which has similar contributions. Additionally, we suggest providing more comprehensive experiments that include nonlinear models to validate the proposed method further. The authors should clarify the contribution of the scale invariance section, as it currently feels weaker than the rest of the paper. It would also be beneficial to include practical examples of non-Gaussian log-likelihood scores relevant for causal discovery. Lastly, we urge the authors to correct the log-likelihood formulation for nonlinear models and ensure that all statements regarding limits are replaced with population results.