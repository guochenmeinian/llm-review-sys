ID: vA4s3kN4QE
Title: LG-VQ: Language-Guided Codebook Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel language-guided codebook learning framework, LG-VQ, addressing the limitations of existing single-modal vector quantization methods by proposing a multi-modal codebook that aligns with text for improved performance in downstream tasks. The authors introduce two semantic supervision modules and evaluate their method on reconstruction and various multi-modal tasks, demonstrating superior performance.

### Strengths and Weaknesses
Strengths:
1. The authors effectively identify the limitations of single-modal quantization methods and propose a novel multi-modal approach.
2. Two semantic supervision modules are designed to learn text-aligned codebooks effectively.
3. Comprehensive experiments validate the effectiveness of the proposed method.
4. The paper is well-written and easy to follow.

Weaknesses:
1. The details of the relationship alignment module are unclear, and Fig. 3 is difficult to understand; Section 3.2.2 should be reorganized for clarity.
2. Potential errors exist in the denominator of Eq. (5).
3. The ablation study lacks evaluation of the relationship alignment module (+L_{ras}) when used independently.
4. The necessity of extracting additional objects and relations in Figure 1 is questionable, and the lack of description for the additional detectors used raises clarity issues.
5. The novelty of the method is in question, as performance improvements may stem from the pre-trained CLIP model rather than the proposed method itself.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relationship alignment module and reorganize Section 3.2.2 to enhance understanding. Additionally, please verify the denominator of Eq. (5) for accuracy. It would be beneficial to include an evaluation of the relationship alignment module when used independently in the ablation study. Furthermore, consider addressing the necessity of extracting additional objects in Figure 1 and provide a detailed description of the detectors used. Lastly, we suggest validating the effectiveness of the method on grounding tasks and comparing it with existing image generation models to strengthen the claims of novelty and performance improvement.