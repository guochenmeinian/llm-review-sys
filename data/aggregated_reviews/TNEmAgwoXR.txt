ID: TNEmAgwoXR
Title: Confident Natural Policy Gradient for Local Planning in  $q_\pi$-realizable Constrained MDPs
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 4, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the first work achieving polynomial sample complexity for constrained Markov decision processes (CMDPs) in the more general setting of linear function approximation with $q_{\pi}$ realizability. The authors propose a primal-dual algorithm that utilizes a local access model, producing a policy that adheres to constraints while optimizing the reward function's value. The algorithm is technically sound and addresses an important problem in the field.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, summarizing main ideas and theorem statements effectively.
- It is the first to achieve polynomial sample complexity for CMDPs in the $q_{\pi}$ realizability setting.

Weaknesses:
- There is a lack of experimental results to support the theoretical claims.
- The paper appears to extend earlier work (Weisz et al., 2022) without sufficiently highlighting the technical challenges overcome in adapting it to the CMDP setting.
- The algorithm description in sections 4.2-4.3 is dense and difficult to parse, and the presentation of the learning goals is confusing.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the algorithm description in sections 4.2-4.3 to enhance readability. Additionally, the authors should elaborate on the main technical challenges in extending the previous work to the CMDP setting. It would also be beneficial to clarify the relationship between $q_{\pi}$ realizability and the linear MDP setting, and to justify the terminology "natural policy gradient." Furthermore, we suggest that the authors consider including experimental results to validate their theoretical findings and highlight the limitations of mixture policies in the context of CMDPs.