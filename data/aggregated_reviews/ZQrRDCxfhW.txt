ID: ZQrRDCxfhW
Title: Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a dynamically expanding transformer-based continual learning architecture aimed at multimodal vision-and-language tasks. The authors propose a single task attention block that utilizes task-specific tokens to learn task-specific knowledge, while also employing knowledge distillation and experience replay to mitigate catastrophic forgetting. Experimental results indicate that the proposed method outperforms several state-of-the-art approaches.

### Strengths and Weaknesses
Strengths:  
- The model achieves state-of-the-art performance in experiments.  
- The introduction of a continual learning solution for multimodal data is a notable contribution.  
- The task-specific classifier and token approach is concise and intuitive, effectively promoting performance and alleviating forgetting.  

Weaknesses:  
- The paper lacks a comprehensive comparison with recent continual learning algorithms and large-scale multimodal models like GPT-4.  
- The novelty of the dynamic expanding architecture and knowledge distillation is not clearly established against existing literature.  
- Some explanations, particularly regarding the task-specific classifier and the generation of task tokens, are confusing.  
- The performance on VQA tasks is weak compared to existing zero-shot models, necessitating validation against more recent architectures like BLIP2.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the explanation regarding the task-specific classifier, particularly in Eq. 8. Additionally, the authors should provide a more comprehensive comparison with recent continual learning algorithms and large-scale multimodal models, including showcasing results against models like GPT-4. It would also be beneficial to validate the performance of their approach on more recent base models, such as BLIP2, and to emphasize the primary contributions of their work to avoid appearing scattered. Lastly, enhancing the writing for clarity and ensuring the innovation is highlighted in the introduction would strengthen the paper.