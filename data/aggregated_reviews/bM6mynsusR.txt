ID: bM6mynsusR
Title: Function Space Bayesian Pseudocoreset for Bayesian Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 6, 7, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called Function Space Bayesian Pseudocoreset (FBPC) for constructing Bayesian pseudocoresets aimed at improving inference in Bayesian Neural Networks (BNNs). The authors propose optimizing the KL-Divergence between posteriors associated with real data and synthetic data in function space, which is typically lower in dimension than the parameter space of large networks. This method allows for better scalability and robustness against multi-modality issues, enabling the use of multiple architectures simultaneously. The experimental results indicate that FBPC outperforms existing state-of-the-art methods. Additionally, the paper introduces an algorithm that iteratively trains BPC to find the MAP solution, advancing SGD steps to obtain empirical covariance and updating the pseudocoreset based on final loss calculations. While FBPC excels in memory efficiency, it requires more time due to additional SGD steps for empirical covariance. Performance comparisons indicate that FBPC achieves results comparable to state-of-the-art dataset distillation methods while consuming less memory than MTT and handling larger datasets like Tiny-ImageNet. The authors plan to explore larger architectures and will include additional performance details in future revisions.

### Strengths and Weaknesses
Strengths:
- The method is well-motivated, addressing the challenges of variational approximations in high-dimensional models.
- The empirical estimates for function space approximations lead to significant computational speed-ups.
- The approach shows promising improvements in uncertainty quantification and robustness compared to traditional methods.
- The algorithm demonstrates significant memory efficiency compared to MTT, allowing for experimentation with larger architectures.
- Performance results indicate that FBPC is competitive with leading dataset distillation methods.
- The authors show a commitment to addressing reviewer feedback and improving the paper.

Weaknesses:
- The writing is at times ambiguous, making it difficult for readers unfamiliar with function space variational inference to follow.
- The experimental section lacks depth and could benefit from more comprehensive evaluations, including comparisons with recent distillation methods and scalability studies.
- The method's performance remains significantly below that of training on the full dataset, raising concerns about its practical applicability.
- The implications of the Jacobian and Gaussian approximations are not sufficiently discussed, which may leave readers wanting more context.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in sections 2.1 and 2.2, to provide necessary background on function-space variational inference. Additionally, we suggest including a comparison with the concept of inducing points from sparse variational approximations for Gaussian Processes, as this could enhance the discussion. 

The authors should also conduct a more detailed experimental phase, including regression results and comparisons with other models like HMC and MFVI. Furthermore, we advise including a scalability study to assess the method's performance concerning dataset size and dimensionality. Lastly, addressing the performance gap between the coreset and full dataset training would strengthen the paper's contributions. We also recommend improving the writing to reduce ambiguity throughout the paper and including a comment on the implications of the linearised Laplace approximation. Extending the discussion on the Gaussian approximation in function space and augmenting the original text with relevant content from the rebuttals, such as discussions around SVGPs, choice of divergences, and the usage of _a posteriori_ techniques, would provide a more comprehensive explanation.