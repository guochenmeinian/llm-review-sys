ID: rG3QZA7JXV
Title: CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the CRT-QA dataset, designed for complex reasoning over tabular data, which includes implicit questions, unanswerable queries, and fine-grained reasoning path annotations. The authors propose a method utilizing a language model to translate questions into code for answering, demonstrating that CRT-QA poses significant challenges even for advanced language models. The dataset's unique features and the proposed approach aim to enhance the TableQA research domain.

### Strengths and Weaknesses
Strengths:
- CRT-QA is a pioneering dataset that emphasizes multi-step reasoning, providing valuable annotations that are often missing in existing datasets.
- The paper is well-structured and clearly written, facilitating understanding.
- The proposed method shows improved performance over zero-shot and few-shot models, enhancing interpretability without relying on handcrafted examples.

Weaknesses:
- The dataset is test-only, and the authors did not evaluate table-oriented models like tableBERT, which could provide additional insights.
- There is a lack of clarity regarding the generation of ground-truth answers and the dataset's solvability, particularly since questions are partly autogenerated.
- The evaluation metrics used, such as Exact Match (EM), may not fully capture model performance, and additional metrics could enrich the analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity around the generation of ground-truth answers and provide insights into the dataset's solvability. Including evaluations from finetuned models and other architectures would offer a more comprehensive understanding of the dataset's challenges. Additionally, we suggest incorporating more diverse evaluation metrics beyond Exact Match to better assess model performance. Lastly, consider renaming the proposed model to avoid confusion with existing datasets like ARC.