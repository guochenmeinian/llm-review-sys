ID: nMH5cUaSj8
Title: GPT-ST: Generative Pre-Training of Spatio-Temporal Graph Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 4, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 1, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel spatio-temporal pre-training framework (GPST) aimed at enhancing performance in traffic management and travel planning. The framework integrates a spatio-temporal knowledge extractor, which utilizes personalized parameter learners and hierarchical hypergraph networks, along with an adaptive mask strategy to guide the learning of robust spatio-temporal representations. The authors propose that intra- and inter-class patterns reflect regularities in urban activities, derived from empirical observations. The study focuses on spatio-temporal prediction in real urban scenarios, emphasizing the generalization ability of their framework across various tasks. Extensive experiments on four real-world datasets validate the effectiveness of the proposed model, and a model ablation study comparing their approach with generative pre-training strategies demonstrates the effectiveness of contrastive pre-training methods in improving model performance.

### Strengths and Weaknesses
Strengths:
1. The paper is well-organized and presents interesting research questions regarding personalized information extraction and the modeling of semantic relationships.
2. The proposed framework can be easily integrated into existing spatio-temporal neural networks, demonstrating promising experimental results.
3. The adaptive mask strategy effectively guides the model in learning diverse relationships across regions.
4. Empirical studies indicate strong generalization capabilities across different tasks.
5. The ablation study provides valuable insights into the effectiveness of various pre-training strategies.

Weaknesses:
1. The presentation lacks clarity in several areas, including the definitions of key concepts and the implementation details of equations.
2. The paper does not adequately address related works, particularly in the context of recent baselines and SSL methods.
3. Some experimental datasets are relatively small, raising concerns about the scalability of the proposed method.
4. Some reviewers did not raise additional points for discussion, indicating a potential lack of engagement with the material.
5. The responses to specific questions could be more detailed to address all concerns thoroughly.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their presentation by providing detailed explanations of key concepts, such as the symbol $M_{r,t}$ and the definition of hyperedges. Additionally, the authors should include a more comprehensive discussion of related works, particularly recent methods like TGAT, JODIE, and TGN. It would also be beneficial to conduct an ablation study comparing the proposed mask-and-predict strategy with other popular pre-training methods designed for MPNNs to justify its effectiveness. Furthermore, we recommend that the authors improve the depth of their responses to specific questions, particularly regarding Q1 and Q5, to ensure all concerns are adequately addressed. Finally, consider expanding the dataset size to enhance the generalizability of the findings.