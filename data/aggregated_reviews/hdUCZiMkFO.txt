ID: hdUCZiMkFO
Title: Happy: A Debiased Learning Framework for Continual Generalized Category Discovery
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 4, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to Continual Generalized Category Discovery (C-GCD) through the Happy framework, which addresses the challenges of discovering new classes from unlabeled data while preventing forgetting of previously learned classes. The authors identify prediction bias and hardness bias as key issues and propose solutions including clustering-guided initialization, soft entropy regularization, and hardness-aware prototype sampling. Additionally, the authors introduce a method based on the InfoMax principle, aiming to maximize the mutual information (MI) between inputs and outputs in a self-training context for learning new classes. They provide detailed explanations of their approach, including the use of entropy regularization and self-distillation to enhance model reliability and mitigate prediction bias. Experimental results demonstrate significant performance improvements across various datasets, notably a 7.5% gain on ImageNet-100 and improvements in model stability and accuracy, particularly on datasets like CIFAR100.

### Strengths and Weaknesses
Strengths:
- Clear Contributions: The paper effectively outlines its contributions, enhancing reader comprehension of its novelty and significance.
- Theoretical Foundation: Incorporation of the InfoMax principle enhances the understanding of the method.
- Experimental Validation: Extensive experiments validate the framework, showing substantial improvements over state-of-the-art methods.
- Addressing Biases: The identification and mitigation of prediction and hardness biases are well-articulated and supported by experiments.

Weaknesses:
- Complexity of the Framework: The integration of multiple techniques adds complexity, potentially complicating implementation and tuning.
- Scalability Concerns: The paper lacks a thorough discussion on the scalability of the framework as the number of classes and stages increases.
- Evaluation Metrics: The focus on accuracy improvements overlooks other important metrics such as computational efficiency and memory usage.
- Social Impact Analysis: There is a perceived lack of a comprehensive social impact analysis.
- Hyper-parameter Stability: The method's reliance on hyper-parameters, although claimed to be near-optimal, raises concerns about its stability and generalizability across different distributions.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the scalability of the proposed framework, particularly regarding computational and memory requirements as the number of classes increases. Additionally, we suggest incorporating a broader range of evaluation metrics beyond accuracy to include computational efficiency and robustness to various data distributions. We also encourage the authors to provide explanations or experimental validations for the use of hardness-aware prototype sampling, especially in the context of utilizing old class data during the discovery phase. Furthermore, we advise addressing the potential social impacts of the Happy framework more comprehensively, including issues related to bias transmission and privacy. Lastly, we recommend that the authors clarify the theoretical foundation further to address concerns about its robustness and provide more empirical evidence regarding the stability of their hyper-parameter settings across diverse datasets to bolster confidence in their method's generalizability.