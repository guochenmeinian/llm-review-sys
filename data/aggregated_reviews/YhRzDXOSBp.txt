ID: YhRzDXOSBp
Title: Training Spiking Neural Networks via Augmented Direct Feedback Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 7, 7
Original Confidences: 3, 3

Aggregated Review:
### Key Points
This paper presents the application of augmented direct feedback alignment (aDFA) to spiking neural networks (SNNs). aDFA extends feedback alignment by replacing the derivative of the activation function \( f' \) with an arbitrary function \( g \), addressing the challenges posed by the discrete nature of \( f \) in SNNs. The authors demonstrate that aDFA outperforms backpropagation when using a differentiable surrogate function in place of \( f' \).

### Strengths and Weaknesses
Strengths:  
- The approach of using aDFA is a useful extension in the context of SNNs, potentially improving performance over traditional backpropagation methods.

Weaknesses:  
- The experiments are limited to a single hidden layer, making it difficult to draw broader conclusions about the effectiveness of aDFA in deeper networks.  
- Questions arise regarding the biological plausibility of aDFA, particularly its reliance on transmitting signed error signals, which may conflict with Dale's principle.  
- Certain statements in the manuscript are unclear or inaccurate, such as the characterization of feedback alignment as one of the earliest backprop-free algorithms.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by evaluating aDFA on deeper networks, as challenges typically arise in such architectures. Additionally, validating the approach on established SNN benchmarks, such as the spiking Heidelberg digits, would strengthen the findings. Lastly, we suggest clarifying the language in the manuscript, particularly the sentence regarding backward propagation and synaptic weight adjustments, and correcting the historical context of feedback alignment.