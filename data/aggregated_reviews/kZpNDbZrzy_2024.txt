ID: kZpNDbZrzy
Title: GTA: Generative Trajectory Augmentation with Guidance for Offline Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 6, 6, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Generative Trajectory Augmentation (GTA), a novel approach to enhance offline reinforcement learning (RL) by generating high-rewarding and dynamically plausible trajectories using a conditional diffusion model. The method involves partially adding noise to original trajectories and refining them through a denoising mechanism guided by amplified return values. The authors demonstrate that GTA improves the performance of various offline RL algorithms across multiple benchmark tasks, including both dense and sparse reward settings.

### Strengths and Weaknesses
Strengths:
- The topic of guided data augmentation for offline RL is both interesting and significant, potentially benefiting real-world applications.
- The paper is well-organized, and extensive experiments validate the proposed method's effectiveness.
- GTA integrates seamlessly with existing offline RL methods and shows improvements in data optimality and novelty.

Weaknesses:
- The lack of theoretical guarantees supporting the algorithm raises concerns about its robustness.
- The approach primarily combines existing techniques, which may limit its theoretical contribution.
- The performance of GTA is sensitive to hyperparameters, necessitating extensive tuning for different tasks.
- The method's applicability to real-world scenarios lacking reward signals is unclear.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of GTA to provide guarantees on its performance. Additionally, we suggest including ablation studies on hyperparameter selection, particularly for \(\alpha\) and \(\mu\), to clarify their impact on performance. It would be beneficial to compare GTA with diffusion-based offline RL methods, such as Decision Diffuser and AdapDiffuser, to contextualize its effectiveness. Furthermore, the authors should address the limitations regarding the applicability of GTA in environments without reward signals and provide a clearer discussion on the computational costs associated with their approach.