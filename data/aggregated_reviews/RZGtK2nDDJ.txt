ID: RZGtK2nDDJ
Title: Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 5, 7, 5, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Meta-learned Masked Auto-Encoder (MetaMAE), a novel modality-agnostic self-supervised learning (SSL) framework that enhances the Masked Auto-Encoder (MAE) through meta-learning techniques. The authors reinterpret the mask reconstruction task of MAE as a meta-learning task, integrating gradient-based meta-learning and task contrastive learning. MetaMAE is evaluated across various data modalities, demonstrating significant improvements in linear evaluation and transferability on cross-domain datasets.

### Strengths and Weaknesses
Strengths:
- The paper introduces an innovative approach to modality-agnostic SSL by leveraging meta-learning.
- It is well-structured and clearly written, facilitating comprehension.
- MetaMAE consistently outperforms previous methods in linear evaluation, indicating its effectiveness.
- The experiments are extensive, providing solid evaluations across multiple modalities.

Weaknesses:
- There are significant differences in pretraining and fine-tuning hyperparameters for various downstream tasks, as highlighted in Appendix Table 2.
- The paper lacks sufficient innovation and empirical analysis regarding the advantages of the meta-learning paradigm for modality-agnostic SSL.
- The ablation experiments show minimal impact of the proposed gradient-based and task contrast methods on most tasks.
- The experimental setup for meta-learning gradient updates requires more explanation, particularly regarding the construction of support and query sets.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the relation between the proposed method and modality-agnostic SSL, specifically addressing the advantages of the meta-learning paradigm. Additionally, we suggest including a comprehensive ablation study to validate the effectiveness of the overall framework, particularly examining the impact of using only task contrast without the gradient-based component. Furthermore, discussing the computational overhead associated with meta-learning and providing more details on hyperparameter tuning for different modalities would enhance the clarity of the paper. Lastly, we encourage the authors to double-check for typos and ensure all relevant recent works are included in the discussion.