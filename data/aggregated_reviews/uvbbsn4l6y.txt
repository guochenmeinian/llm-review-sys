ID: uvbbsn4l6y
Title: Look-back Decoding for Open-Ended Text Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new decoding algorithm called 'Look-Back', which utilizes KL divergence to manage repetitions and topic drift in open-ended text generation. The authors propose a method that compares the current vocabulary distribution with past distributions, providing a coherent and intuitive approach to text generation. The paper includes evaluations on two tasks, demonstrating the effectiveness of the method against existing techniques, particularly in terms of coherence and reduced repetitiveness.

### Strengths and Weaknesses
Strengths:
- The approach is well-justified, leveraging KL divergence to enhance semantic similarity.
- The paper is well-structured, clearly motivating the problem and detailing the proposed algorithm.
- It includes comprehensive evaluations, both automatic and human, supporting the validity of the Look-Back method.
- The experiments demonstrate that the proposed method performs well in maintaining coherence while reducing repetitions.

Weaknesses:
- There is a lack of scalability analysis for larger models (e.g., ~10B or 100B parameters).
- The use of KL divergence as a measure for semantic similarity may not be optimal, as noted in the conclusion.
- Statistical validation methods (e.g., t-tests, p-values) are absent.
- Some results in Table 2 for the Look-Back method do not appear optimal, although coherence remains high.

### Suggestions for Improvement
We recommend that the authors improve the scalability analysis to address performance on larger models. Additionally, consider exploring alternative measures for semantic similarity beyond KL divergence, and include statistical validation to strengthen the findings. We suggest revisiting the examples presented in Table 5 to ensure they provide natural continuations of the prompts. Furthermore, we recommend clarifying the computation of softmax over KL_min values, as the current explanation may be difficult to follow. Lastly, consider implementing a sliding window approach for KL-divergence calculations to enhance coherence with previously generated words.