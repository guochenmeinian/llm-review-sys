ID: Iwg4sjydHn
Title: CinePile: A Long Video Question Answering Dataset and Benchmark
Conference: thecvf
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 6, 7
Original Confidences: 4, 3, 3

Aggregated Review:
**Key Points**  
We note several critical observations from our reviews:  
1. We recommend organizing the experimental conclusions from Section 3 Human Study into corresponding figures or tables to enhance comprehension, as this section is vital for evaluating the quality of captions generated by LLM.  
2. We identify missing relevant works [1][2] that explore the effectiveness of using LLM to generate captions, suggesting that discussing and comparing these in the paper would be reasonable.  
3. We seek clarification on the meaning of the survey mentioned in "L341," as there are no relevant definitions provided.  
4. We acknowledge the paper's contribution of a large-scale video-QA dataset, comprising 8,000 video clips and around 200,000 template-based questions, focusing on temporal reasoning and video understanding.  
5. We observe that the benchmark dataset indicates room for improvement in current LLMs, with human performances being reasonable despite some ambiguities in questions and answer choices.  
6. We note that the authors did not illustrate the improvement of LLMs when fine-tuning with the proposed training dataset, which constitutes ~96% of the overall proposed dataset.  

**Strengths and Weaknesses**  
Strengths:  
- We recognize the uniqueness of the dataset, which emphasizes long video natural and temporal reasoning, increasing the challenge of the benchmark.  
- We appreciate the significant amount of data provided by the automatic pipeline, which enables downstream instruction tuning.  
- We find no apparent cons regarding the contribution of the dataset to long-video understanding.  

Weaknesses:  
- We point out the lack of organization in presenting experimental conclusions, which could hinder comprehension.  
- We highlight the absence of discussion on relevant works that could strengthen the paper's context.  
- We note the need for clarification regarding the survey and its implications.  
- We express concern over the lack of demonstration of LLM improvements when fine-tuning with the proposed dataset.  

**Suggestions for Improvement**  
We suggest the following improvements:  
1. We recommend organizing the experimental conclusions from Section 3 into figures or tables for better clarity.  
2. We advise including discussions on the missing relevant works [1][2] to provide a more comprehensive context.  
3. We propose defining the survey mentioned in "L341" to clarify its significance.  
4. We encourage the authors to illustrate the effectiveness of the training dataset in improving LLM performance, as this would enhance the paper's credibility.