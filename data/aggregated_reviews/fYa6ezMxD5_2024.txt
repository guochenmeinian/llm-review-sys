ID: fYa6ezMxD5
Title: MatFormer: Nested Transformer for Elastic Inference
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 7, 9, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, 4, 4, 4, 4, 4

Aggregated Review:
### Key Points
This paper presents Matformer, a technique for elastic inference that enables the training of a single model to extract multiple sub-models on demand. The authors apply a Matryoshka "self-stacking" of hidden states within the Feed Forward Network (FFN) blocks of transformers, training all sub-models concurrently through random sampling. Additionally, the paper explores Matryoshka Representation Learning (MRL) applied to Large Language Models (LLMs) and introduces the concept of "mix-n-match" for flexible model extraction. The authors demonstrate that models of varying sizes can be derived from shared weights, maintaining performance comparable to baseline transformer models. Experimental results validate the effectiveness of this method across both NLP and computer vision tasks, showcasing improved speculative decoding and adaptive image retrieval.

### Strengths and Weaknesses
Strengths:
- The proposed idea is original, allowing for the extraction of multiple sub-models from a single trained model.
- The approach shows effective performance in LLMs and ViTs, with notable improvements in execution speed alongside compression.
- The paper is well-written, making complex concepts accessible, with clear explanations of key concepts.
- Comprehensive evaluations across multiple datasets and domains yield surprising accuracy results with the mix-n-match approach, along with a qualitative review of training overhead and a wide range of problem statements, including ablation studies.

Weaknesses:
- The application of the Matryoshka approach to the attention block raises doubts and requires clarification.
- The training time benefits compared to training smaller models are unclear, particularly regarding FLOPs per iteration and convergence speed.
- Experiments are limited to models under one billion parameters, despite references to larger models like Llama 34B, 40B, and 70B.
- Lack of comparisons to similar methods in the literature, such as traditional dropout and nested dropout.
- Potentially costly training process due to the independent execution of each model size.
- Limited novelty beyond traditional transformer architecture and existing MRL work.
- Some figures and tables lack clarity, with specific issues noted in Table 6 and Table 7.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the application of the Matryoshka approach to the attention block, explicitly stating whether it can be applied and how it impacts optimization techniques like KV cache optimization. Additionally, we suggest providing a more thorough comparison with Flextron, as both works address similar elastic inference challenges. 

To enhance the training scheme discussion, we encourage the authors to quantify the computational costs and convergence times associated with the random sampling strategy. Furthermore, expanding the experimental scope to include models beyond 850M parameters would strengthen the findings. We also recommend improving the clarity of Table 7 by adding annotations for the metrics used and providing more framework details. Revisiting Table 6 to clarify the motivation behind the parameter reciprocal ratio and refining Figure 1 to better illustrate the interconnections of nested transformer blocks would enhance understanding. Lastly, addressing the typo "Baselime" on line 543 and ensuring that Table 10 annotations do not interfere with the table view would improve the overall presentation of the paper.