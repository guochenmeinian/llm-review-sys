ID: UNvLur0th4
Title: Finding Common Ground: Annotating and Predicting Common Ground in Spoken Conversations
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Common Ground corpus, which annotates the LDC CALLHOME American Speech corpus with events, speaker and hearer beliefs, and their shared common ground. The authors propose two main contributions: the release of this annotated dataset and initial experimental analyses on common belief classification. The paper aims to address the gap in pragmatic tasks related to common ground, which is increasingly relevant in natural language processing.

### Strengths and Weaknesses
Strengths:
- The annotation process is thorough and well-motivated, providing a valuable resource for discourse-annotated corpora.
- The paper introduces a novel approach to sentence-level belief prediction and common ground analysis based on speakers' beliefs in daily conversations.
- Comprehensive experiments and analyses are conducted on various subtasks, demonstrating the potential of the dataset.

Weaknesses:
- The dataset is limited in size, comprising only 4 dialogues and 561 utterances, which may hinder its utility and the robustness of experimental results.
- The authors' claims regarding the novelty of the corpus are questionable, as common ground has been extensively modeled in existing dialogue systems, which are not adequately referenced.
- The modeling approach lacks depth, with suggestions for using a Rational Speech Act (RSA) framework as a baseline not being addressed.

### Suggestions for Improvement
We recommend that the authors improve the dataset size by bootstrapping from existing human-annotated datasets to create a larger, silver-standard dataset. Additionally, we suggest incorporating a more detailed taxonomy of the common ground framework and addressing the potential biases in belief categories. The authors should also clarify the experimental setup, particularly regarding the input types used in their analyses. Finally, we encourage the authors to engage more thoroughly with related work in dialogue systems to strengthen their claims about the corpus's novelty and relevance.