ID: RA6rzOJ2zI
Title: Navigating Extremes: Dynamic Sparsity in Large Output Spaces
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 7, 6, -1, -1, -1
Original Confidences: 2, 2, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the application of Dynamic Sparse Training (DST) to extreme multi-label classification (XMC), addressing challenges such as large label spaces and skewed distributions. The authors propose enhancements including semi-structured sparsity with fixed fan-in connections, an intermediate layer, and an auxiliary training objective to improve gradient flow. Empirical results indicate significant reductions in GPU memory usage while maintaining competitive performance against dense models and specialized XMC methods.

### Strengths and Weaknesses
Strengths:  
- The paper is well-organized and clearly written, providing a comprehensive overview of XMC challenges and proposed DST modifications.
- It tackles the important problem of scaling DST to XMC, demonstrating its applicability beyond typical benchmarks.
- The modifications to standard DST algorithms are well-motivated and supported by empirical analysis, showing substantial memory savings with minimal loss in predictive performance.

Weaknesses:  
- Limited novelty in core techniques, as the main components are existing methods; more discussion on their interaction would be beneficial.
- The empirical nature of the paper lacks rigorous theoretical justification for the proposed modifications, with insights into underlying mechanisms being valuable.
- The training time remains a concern for time-sensitive applications, and the introduction of new hyperparameters adds complexity without thorough exploration of their sensitivity.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the novelty of their contributions, particularly clarifying how their approach differs from existing methods like [32]. Additionally, providing theoretical insights into the effectiveness of the proposed modifications would strengthen the paper. It would be beneficial to include training time estimates alongside memory usage and to explore the sensitivity of the model's performance to the new hyperparameters. Finally, we suggest adding results on more datasets with label features and comparing the proposed approach to other recent methods like DEXML and smaller-sized encoders.