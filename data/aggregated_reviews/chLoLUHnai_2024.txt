ID: chLoLUHnai
Title: Large Stepsize Gradient Descent for Non-Homogeneous Two-Layer Networks: Margin Improvement and Fast Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the dynamics of gradient descent (GD) with large learning rates for near-homogeneous logistic regressions and two-layer neural networks. The authors extend previous results on linear predictors to nonlinear predictors, demonstrating faster convergence and margin improvement with large learning rates. They characterize two phases of GD: the oscillatory Edge of Stability (EoS) phase and the stable phase, where the loss decreases monotonically. The authors provide theoretical results showing that if the empirical risk is below a certain threshold, GD enters a stable phase with accelerated convergence rates.

### Strengths and Weaknesses
Strengths:
1. The work is the first to prove margin improvement for large learning rates in a near-homogeneous model.
2. The analysis of nonlinear models is more applicable to practical scenarios compared to previous studies.
3. The paper is well-written and builds on existing literature, extending results from linear to nonlinear settings.

Weaknesses:
1. The role of large learning rates compared to small learning rates requires further illustration.
2. The main theorems are limited to linearly separable datasets, which restricts the applicability of the results.
3. The assumptions made for the analysis do not hold for widely used activation functions like ReLU, limiting the generalizability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the role and effect of large learning rates, particularly through direct comparisons with small learning rates. Clarifying the initial condition in Theorem 3.2 and explaining the dependence of the modified margin function on learning rates in the main body would enhance clarity. Additionally, addressing the limitations regarding non-differentiable activation functions and emphasizing the novelty of the paper in light of these constraints would strengthen the manuscript. Lastly, considering a broader class of activation functions beyond the current assumptions could improve the relevance of the findings.