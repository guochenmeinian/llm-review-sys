ID: NGuGVT7ar2
Title: Enhancing LLM Reasoning via Vision-Augmented Prompting
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 4, 8, 5, 5, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel prompting technique called visual-augmented prompting (VAP) aimed at enhancing the reasoning capabilities of large language models (LLMs). The authors propose that VAP mimics human cognitive processes by integrating visual and textual information through an iterative image generation process. The effectiveness of VAP is validated across four tasks: Geometry Intersection Counting, Sudoku Puzzles, Time Series Prediction, and the Traveling Salesman Problem, demonstrating significant performance improvements over traditional chain-of-thought (CoT) frameworks.

### Strengths and Weaknesses
Strengths:  
1. The VAP method is simple yet effective, aligning with human cognitive processes and integrating both intermediate textual and visual results to enhance reasoning accuracy and interpretability.  
2. Results indicate that VAP significantly outperforms all baselines across the four tasks evaluated.  
3. The manuscript is well-written and easy to follow.  

Weaknesses:  
1. The authors do not address the time consumption of the iterative reasoning process and the trade-off between the number of images drawn and model performance.  
2. A broader range of LLM models (e.g., LLaMA 3, GPT-4) should be incorporated for a more comprehensive comparison.  
3. The impact of different graphic rendering tools on reasoning results remains unexplored.  
4. Scalability concerns arise, as VAP shows a significant accuracy drop when handling complex geometry problems with more than three shapes.  
5. Despite outperforming baselines, VAP's overall performance remains inferior to traditional methods.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the efficiency of the iterative reasoning process in the limitations section, particularly if it is time-consuming. Additionally, we suggest incorporating a wider range of LLM models for comparison to ensure fairness, as traditional LLMs are expected to outperform MLLMs in text-only tasks. It would also be beneficial to explore how different graphic rendering tools affect the final reasoning results and to provide more details on the planning and drawing processes to mitigate potential errors in image generation.