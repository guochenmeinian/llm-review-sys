ID: MXze4H7opg
Title: SLTrain: a sparse plus low rank approach for parameter and memory efficient pretraining
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SLTrain, a novel method for pre-training large language models (LLMs) that integrates sparse and low-rank matrix structures to improve parameter and memory efficiency. The low-rank component is learned through matrix factorization, while the sparse component is created by randomly selecting sparsity support and learning only the non-zero entries. The authors demonstrate that SLTrain significantly reduces memory requirements for LLM pre-training while achieving competitive performance compared to state-of-the-art methods like ReLoRA and GaLore.

### Strengths and Weaknesses
Strengths:
1. SLTrain is the first method to pre-train LLMs using sparse and low-rank matrices.
2. The authors provide extensive experimental results across various model sizes, showcasing SLTrain's advantages in memory reduction and parameter efficiency.
3. The paper includes a solid theoretical motivation for the combination of sparse and low-rank components, supported by empirical analysis.

Weaknesses:
1. As model size increases, the perplexity score gap between SLTrain and Full-Rank models widens, indicating potential scalability challenges.
2. Memory efficiency does not translate into faster training speeds due to the inclusion of sparse components, and the efficiency during inference remains unclear.
3. The lack of strong theoretical justification for the sparse method raises questions about the impact of sparsity on performance and the potential benefits of using structural sparse patterns.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the sparse method and clarify how the degree of sparsity affects performance. Additionally, the authors should consider evaluating the efficiency of SLTrain during inference, specifically comparing memory and time requirements for both sparse and low-rank matrices versus larger dense weight matrices. Exploring the use of structural sparse patterns, such as butterfly matrices, could enhance training performance and efficiency. Finally, we suggest conducting multiple runs to assess the impact of randomness in sparse factors on results and providing a clearer scope regarding the applicability of SLTrain to other LLMs beyond the Llama model family.