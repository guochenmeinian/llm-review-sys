ID: doQwq4kwFc
Title: A Quadratic Synchronization Rule  for  Distributed Deep Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4

Aggregated Review:
### Key Points
This paper presents a novel approach to distributed deep learning by introducing the Quadratic Synchronization Rule (QSR), which dynamically adjusts the synchronization period (H) in proportion to the learning rate decay. This method allows local gradient methods, such as Local SGD, to reduce communication overhead while improving test accuracy, as demonstrated through experiments on ImageNet with ResNet and ViT models. The authors argue that selecting the appropriate H can enhance generalization, although this selection process is complex.

### Strengths and Weaknesses
Strengths:
- The authors provide robust theoretical and experimental support for their proposed method.
- The adaptive adjustment of H based on the square of the learning rate decay significantly improves communication efficiency compared to existing methods.

Weaknesses:
- The paper's novelty is somewhat limited, as it builds on established concepts rather than introducing entirely new techniques.
- Some experimental results show minimal differences among approaches, suggesting that including standard deviations would be beneficial.

### Suggestions for Improvement
We recommend that the authors clarify their choice of baselines for designing H, considering including "faster training lofi" and "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models" as potential comparisons. Additionally, the authors should investigate whether similar speedups can be achieved across different hardware setups, as their current results are based on a single hardware configuration. In Section 4.1, the authors should address other factors affecting generalization, such as high learning rates and weight decay. Finally, we suggest exploring the performance of their method in training without repetition settings, which is particularly relevant for large language models (LLMs).