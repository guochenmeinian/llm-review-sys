ID: rUldfB4SPT
Title: PRED: Pre-training via Semantic Rendering on LiDAR Point Clouds
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 8, 5, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel pre-training framework, PRED, for outdoor LiDAR point clouds that leverages image semantics through neural rendering to address challenges like incompleteness and occlusion. The authors propose a semantic rendering module for decoding semantics from BEV feature maps and a point-wise masking mechanism to enhance performance. Extensive experiments demonstrate significant improvements over various baselines and state-of-the-art methods in 3D object detection and BEV map segmentation tasks.

### Strengths and Weaknesses
Strengths:
1. The integration of image semantics via neural rendering is a creative approach to tackling issues in point cloud pre-training.
2. The paper is well-written, with clear descriptions and aesthetically pleasing figures that effectively illustrate core problems.
3. Extensive experiments validate the proposed method's effectiveness across multiple benchmarks.

Weaknesses:
1. The technical contribution is limited, with some strategies, like the BEV conditioned semantic neural rendering, previously explored in the literature.
2. The paper lacks clarity on the impact of the image segmentation model choice on pre-training performance and does not conduct ablation studies to justify the use of DeepLabv3.
3. The rendering process may introduce noise and semantic ambiguity, as it assigns multiple labels to points, and the optimization may include irrelevant points.
4. There is insufficient comparison between the proposed neural rendering and point-to-pixel projection methods, raising questions about the claimed superiority.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their presentation by adding a column in Tables 1 and 2 to indicate whether additional image signals are used and their types. Additionally, it would be beneficial to include a paragraph discussing the overlap between pixel semantics and downstream task labels in Section 4.1. To strengthen the technical contribution, we suggest conducting ablation studies on the choice of image segmentation model and its effects on performance. Finally, addressing the computational cost and time complexity of the pre-training framework would enhance the paper's rigor.