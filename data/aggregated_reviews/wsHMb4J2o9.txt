ID: wsHMb4J2o9
Title: The Feature Speed Formula: a flexible approach to scale hyper-parameters of deep neural networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 8, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Backward-Feature Angle (BFA) as a novel metric for predicting and controlling feature learning in deep neural networks (DNNs). It introduces the backward-to-feature kernel (BFK) to analyze the evolution of intermediate layer features and relates feature movement to an angle $\theta_\ell$. The authors recover known hyperparameter (HP) scalings and propose a new scaling for large depth ReLU MLPs. However, the paper lacks experimental validation of its proposed parameterizations.

### Strengths and Weaknesses
Strengths:
1. The BFA and BFK are intriguing concepts that provide valuable insights into feature learning.
2. The main results, particularly Theorems 2.1 and 3.2, are clearly articulated, and their proofs are straightforward.
3. The contributions are well-defined and contextualized within existing literature.
4. The paper is well-written and accessible, making complex topics easier to understand.

Weaknesses:
1. The novelty of the HP scaling extension is limited, primarily focusing on known results like muP.
2. Some results are applicable only under restrictive conditions, such as a single input.
3. The experimental findings are insufficiently detailed, lacking clarity on data assumptions and reproducibility.
4. The paper could benefit from being less technical; delegating some proofs to the Appendix and incorporating visualizations would enhance readability.

### Suggestions for Improvement
We recommend that the authors improve the experimental validation of their proposed parameterizations by conducting tests in real learning scenarios to demonstrate their utility. Clarifying the assumptions about the data used in experiments and ensuring reproducibility would strengthen the findings. Additionally, we suggest that the authors streamline the mathematical exposition and consider moving some proofs to the Appendix to enhance clarity. Lastly, exploring larger batch sizes and addressing the limitations of their proposed scaling would provide a more comprehensive understanding of their contributions.