ID: c3Pakdyi3t
Title: $\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Trans-LoRA, a method for transferring pre-tuned Low-Rank Adapter (LoRA) components across different models without requiring access to original task data. The authors propose generating synthetic data from the source model and using a discriminator to filter this data for the target model, ensuring that the transferred LoRA parameters maintain or improve performance. The effectiveness of Trans-LoRA is validated through experiments on various benchmarks and models, demonstrating its utility across multiple tasks.

### Strengths and Weaknesses
Strengths:
- The motivation for addressing compatibility issues between base models and PEFT components is clear and engaging.
- The writing is well-crafted, making the paper easy to understand.
- The experimental design is robust, covering compatibility across different base models, various PEFT methods, and a wide range of tasks.
- The combination of synthetic data generation and discriminator filtering is innovative and well-suited for Parameter-Efficient Fine-Tuning (PEFT).

Weaknesses:
- The application scope appears limited due to the requirement of training PEFT components with an additional discriminator, which incurs extra costs and may not be feasible for all users.
- Further discussion on scalability is needed, as updating multiple PEFT components can be time-consuming.
- The diversity of the synthetic data generated should be considered, as the focus on in-distribution generation may overlook important aspects.
- More baselines of data-free knowledge distillation are needed to strengthen the evaluation.
- The method may not be generic enough for encoder-only models, and its reliance on the generative ability of the source model could limit its applicability.

### Suggestions for Improvement
We recommend that the authors improve the discussion on scalability and the potential computational overhead associated with the discriminator. Additionally, the authors should explore the integration of other synthetic data generation methods, such as ProGen and GOLD, to enhance data quality. A more detailed examination of the properties of the synthetic data, including its diversity and potential biases, would strengthen the paper. Furthermore, we suggest that the authors clarify the method's applicability to encoder-only models and consider addressing the limitations related to user behavior assumptions in the context of training the discriminator.