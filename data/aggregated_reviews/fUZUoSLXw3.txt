ID: fUZUoSLXw3
Title: Two Sides of One Coin: the Limits of Untuned SGD and the Power of Adaptive Methods
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 6, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the behavior of untuned Stochastic Gradient Descent (SGD) in smooth nonconvex settings, demonstrating that it suffers from an exponential dependence on the smoothness constant when the learning rate is not properly tuned. The authors argue that this exponential dependence is unavoidable, supported by a constructed class of 1-dimensional nonconvex functions. They also analyze adaptive methods like Normalized SGD, AMSGrad, and AdaGrad, showing that these methods can avoid such exponential dependence. Additionally, the paper provides a novel analysis of AMSGrad that removes the bounded gradients assumption.

### Strengths and Weaknesses
Strengths:
1. The new result on AMSGrad is a valuable addition to the literature.
2. The paper effectively emphasizes the benefits of normalization in deterministic settings, a point often overlooked in the community.
3. The theoretical perspective on the explosive gradient problem is interesting and well-justified, complemented by numerical illustrations.

Weaknesses:
1. The result regarding exponential dependence on the smoothness constant is a known consequence from existing literature, which diminishes its novelty.
2. The numerical experiments are preliminary, limited to a single dataset (MNIST) and a small network, warranting a more comprehensive investigation.
3. The paper's complexity and the variety of setups discussed may confuse readers, as the true contributions are not clearly delineated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by explicitly distinguishing between known results and their novel findings. Additionally, expanding the numerical experiments to include larger practical networks would strengthen the empirical validation of their claims. We also suggest addressing the questions raised regarding the constructed functions and the implications of using constant versus diminishing step sizes in practice, particularly in relation to generalization in overparameterized networks.