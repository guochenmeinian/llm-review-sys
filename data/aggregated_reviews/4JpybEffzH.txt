ID: 4JpybEffzH
Title: Non-Autoregressive Document-Level Machine Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an innovative exploration of non-autoregressive translation (NAT) models in document-level machine translation (MT), introducing a sentence alignment method that allows the MT decoder to operate at the sentence level while the encoder functions at the document level. The authors evaluate the effectiveness of their approach against one benchmark using three datasets, revealing that while NAT models improve decoding speed, they currently underperform compared to autoregressive models. The study also highlights challenges faced by NAT models in managing discourse phenomena and utilizing document context.

### Strengths and Weaknesses
Strengths:  
- The paper establishes a benchmark for NAT in document-level MT, contributing valuable insights to an underexplored area.  
- It introduces a simple yet effective sentence alignment strategy that enhances performance over existing benchmarks.  
- The comprehensive evaluation of various NAT architectures provides a solid foundation for future research.

Weaknesses:  
- Experiments are limited to one language pair and small datasets, which undermines the generalizability of the findings.  
- The absence of statistical significance scores limits the validation of the results.  
- The paper does not adequately address recent advances in efficient methods for handling long inputs or realistic scenarios in measurement speed.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their findings by conducting experiments with additional language pairs and larger datasets. Including statistical significance scores would enhance the credibility of the results. Furthermore, we suggest incorporating recent advancements in efficient methods for long inputs and addressing practical considerations such as batch sizes and evaluation metrics. Additionally, clarifying the distinctions between G-Transformer (AT) and G-Transformer (NAT) would improve the paper's clarity.