ID: XLXCWNNWvL
Title: Training Simultaneous Speech Translation with Robust and Random Wait-k-Tokens Strategy
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to Simultaneous Speech Translation (SimulST) by addressing the modality gap between audio and text through a two-stage training method. The authors propose using the Montreal Forced Aligner (MFA) for token-level semantic alignment pre-training and introduce a robust and random wait-k tokens strategy for simultaneous decoding. Results from MuST-C v1.0 demonstrate that the proposed method outperforms existing SimulST techniques across various latency conditions and is competitive in offline speech translation tasks. The paper includes comprehensive ablation studies that highlight the contributions of each model component.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and provides significant contributions to the field of SimulST, particularly through its effective pre-training strategy and robust wait-k tokens policy.
- The proposed method achieves a favorable balance between translation quality and latency, as evidenced by strong performance across multiple language pairs.
- The thorough analysis and ablation studies enhance understanding of the model's components and their effectiveness.

Weaknesses:
- The reliance on external alignment tools and pretrained models like wav2vec2.0 may limit effectiveness in low-resource language scenarios.
- The training process is complex, involving multiple stages and losses, which could hinder reproducibility.
- The related work section lacks references to relevant studies that could better position the paper within the existing literature.

### Suggestions for Improvement
We recommend that the authors clarify the version of wav2vec 2.0 used in their experiments and consider conducting ablation studies on the impact of mixup (Fang et al. 2022) during pre-training. Additionally, the authors should address the implications of using external alignment tools in relation to ASR data usage. We suggest including missing references to relevant works in the related work section and providing a more detailed analysis of the contributions of individual losses in the fine-tuning stage. Finally, we encourage the authors to enhance the clarity of figures and ensure that important visualizations are included in the main content.