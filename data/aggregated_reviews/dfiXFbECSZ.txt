ID: dfiXFbECSZ
Title: LoFiT: Localized Fine-tuning on LLM Representations
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 7, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LoFiT, a procedure for localized fine-tuning of LLMs that selects a task-specific subset of attention heads by tuning head-wise learnable scales and subsequently tuning the biases for these heads. The authors evaluate LoFiT on three benchmarks using LLMs from the Llama-2 and Gemma families, comparing it against alternative head selection methods and several PEFT algorithms. The results indicate that LoFiT outperforms other inference-time intervention methods and is competitive with PEFT methods while being more parameter-efficient.

### Strengths and Weaknesses
Strengths:
- The authors propose a simple and effective algorithm for head selection that is easy to modify or reuse.
- The paper includes comprehensive ablations and sub-analyses, demonstrating well-structured experiments.
- It is well-written, organized, and easy to read, with strong experimental results showing LoFiT's effectiveness.

Weaknesses:
- The main evaluations are limited to three tasks, raising questions about the general applicability of LoFiT compared to PEFT methods. A broader range of tasks would strengthen the paper.
- The methodology lacks thorough comparisons with ITI, and the reasons for LoFiT's superior performance over ITI remain unexplored.
- There is insufficient detail regarding inference-time intervention baselines, particularly in dataset construction and hyperparameter selection.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of LoFiT by evaluating it on a wider array of tasks, including those from PEFT literature, to clarify its applicability. Additionally, exploring the learned bias vectors in comparison to ITI could provide insights into performance differences. It would also be beneficial to include more detailed descriptions of the inference-time intervention baselines and the selection process for hyperparameters. Lastly, we suggest incorporating an analysis of the effects of the number of heads \( K \) on performance, as well as a study on data efficiency to highlight LoFiT's strengths further.