ID: Cth1PyCwZt
Title: Beyond accuracy: understanding the performance of LLMs on exams designed for humans
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 7, 6, 8, -1, -1, -1
Original Confidences: 4, 2, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation of LLMs using psychometric modeling techniques, specifically Item Response Theory (IRT), on a Brazilian college-entrance exam (ENEM). The authors demonstrate that IRT provides a more informative assessment of LLMs compared to traditional accuracy metrics, highlighting its ability to distinguish between human-like and non-human-like response patterns and to evaluate the reliability of exams in measuring LLM capabilities.

### Strengths and Weaknesses
Strengths:  
1. The paper offers a comprehensive evaluation method for LLM performance, arguing that accuracy metrics often fail to capture the full extent of LLM abilities.  
2. The results section is methodological, assessing IRT scores and their reliability through various metrics, enhancing the robustness of the evaluations.  
3. The experimental design is well-structured, with clear comparisons to human performance and detailed discussions of results.

Weaknesses:  
1. The results analysis lacks depth; some high-level observations do not provide sufficient insight into LLM behaviors, and more detailed analyses of specific question subsets are needed.  
2. Evaluations are limited to variations of the ENEM dataset, raising concerns about the generalizability of the methods to other datasets.  
3. Some conclusions appear unsubstantiated or confusing, requiring clearer interpretations and more rigorous scientific backing.

### Suggestions for Improvement
We recommend that the authors improve the clarity and depth of the results analysis, particularly by providing more detailed insights into LLM behaviors and the specific questions contributing to scores. Additionally, demonstrating the applicability of IRT on other datasets would strengthen the argument for its broader use. We also suggest addressing the "what now?" message in the discussion, outlining how to encourage the ML/AI community to adopt IRT more extensively and discussing its limitations compared to accuracy-based metrics. Furthermore, clarifying the methodology behind question translations and providing more details on the evaluation of "poor model fit" in the IRT model would enhance the paper's rigor.