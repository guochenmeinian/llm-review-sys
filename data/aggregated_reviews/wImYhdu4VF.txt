ID: wImYhdu4VF
Title: Learning a 1-layer conditional generative model in total variation
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 7, 3, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the sample complexity of learning conditional generative models without assumptions on the input distribution, applying the Maximum Likelihood Estimator (MLE) to linear regression and 1-layer networks with ReLU activation. The authors demonstrate that MLE achieves small total variation error with sample complexities of \(O(k/\epsilon^2 \log(1/\epsilon))\) for linear regression and \(O((kd + d^2) / \epsilon^2 \log(kd\kappa/\epsilon))\) for 1-layer networks. The extension to multilayer networks is discussed, contingent on access to internal activations. The results indicate that MLE is a promising approach for learning feed-forward generative models from limited samples, although the computational aspects of the optimization problem are not thoroughly analyzed.

### Strengths and Weaknesses
Strengths:
- The paper provides a solid theoretical foundation for understanding the sample complexity of learning multi-layer ReLU networks using the MLE method, achieving polynomial sample complexity without distributional assumptions.
- The clarity and conciseness of the writing enhance the presentation of concepts and results.
- The work introduces a novel perspective on distribution learning, avoiding identifiability issues and requiring no specific distributional assumptions aside from a tame bound on the condition number of the noise.

Weaknesses:
- The requirement for access to intermediate activations to extend the theory to multilayer networks is impractical.
- The assumption that the learner understands the model architecture may not hold in practice.
- The computational aspects of MLE, particularly regarding neural networks, are not thoroughly examined, and the results may be limited by the Gaussian distribution assumption for the data.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the computational aspects of the optimization problem to enhance the practical applicability of their results. Additionally, addressing the requirement for accessing intermediate activations in multilayer networks could strengthen the theoretical framework. Clarifying the implications of the derived sample complexity bounds on the feasibility and scalability of learning generative models in real-world applications would also be beneficial. Finally, providing explicit comparisons with classical results and addressing the concerns regarding the concavity of the MLE objective would enhance the paper's contribution.