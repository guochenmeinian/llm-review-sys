ID: soUXmwL5aK
Title: Interpretable Generalized Additive Models for Datasets with Missing Values
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 6, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents M-GAM, a novel extension of Generalized Additive Models (GAMs) that incorporates missingness indicators while ensuring model sparsity through l0 regularization. The authors demonstrate that M-GAM achieves comparable or superior accuracy to traditional imputation methods, particularly on datasets with missingness at random, while also improving runtime efficiency. The approach aims to maintain interpretability and avoid the complexities associated with naive inclusion of missingness indicators.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and structured.
- The proposed method is novel and effectively addresses missing data in GAMs while maintaining sparsity.
- Extensive experiments robustly support the claims made.

Weaknesses:
- The reliance on l0 regularization may constrain the proposed method's performance.
- The empirical evaluation lacks strong improvement in performance on real datasets compared to existing methods.
- The analysis does not sufficiently compare M-GAM with decision trees or other models that handle missing values, limiting the discussion on applicability.
- The title does not clearly indicate the study's focus on GAMs, and the runtime comparison lacks depth.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by conducting distinct experiments with synthetic missingness where the ground truth is known, as well as a mix of MCAR and MAR scenarios. Additionally, we suggest including a more detailed discussion on the interpretability of M-GAM in the text, particularly in comparison to existing methods. The authors should also consider addressing the limitations of their approach regarding the performance on real-world datasets and explore the potential for incorporating their mechanism into recent neural GAM approaches for better scalability.