ID: LDwsvLQTLx
Title: Open-vocabulary vs. Closed-set: Best Practice for Few-shot Object Detection Considering Text Describability
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 5, 7, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the extension of open vocabulary object detector (OVD) benefits to few-shot detection (FSOD) for object classes that are challenging to describe textually. It utilizes the ODinW dataset, comprising 35 datasets categorized based on "text-describability," defined by CLIP-classification accuracy. The authors empirically compare closed-set object detection (COD) and OVD models, revealing that while OVD outperforms COD in high text-describability scenarios, its advantages diminish in low text-describability contexts. Additionally, the paper explores the effectiveness of OVD methods in FSOD scenarios, focusing on the types of FSOD problems where vision and language connections are most beneficial. However, the fine-tuning of all models using different few-shot techniques raises questions about the validity of the comparisons made between OVD and COD methods.

### Strengths and Weaknesses
Strengths:
1. The problem addressed is meaningful and practical, focusing on the challenges of few-shot detection for hard-to-describe classes.
2. The paper is well-organized and presents a comprehensive evaluation of various models under the few-shot setting.
3. The authors provide a detailed exploration of OVD methods and their application in FSOD.
4. The response to reviewer feedback indicates a willingness to engage with critiques and clarify their methodology.

Weaknesses:
1. The use of CLIP-classification accuracy as a measure of "text-describability" is flawed, as it does not directly reflect the complexity of verbalizing visual concepts.
2. The dataset is split at the "dataset-level," which may overlook the nuances of category-level differences in text-describability.
3. The experiments lack sufficient explanation, particularly regarding the unexpected performance of OVD compared to COD in low text-describability scenarios.
4. The organization of the abstract and introduction is confusing, hindering understanding of the problem and motivation.
5. The writing contains informal expressions and errors that detract from the academic tone.
6. The fine-tuning of models contradicts the stated intention of comparing OVD and closed-set methods, leading to potential misinterpretation of results.
7. The lack of comparison between visual prompts and fine-tuning methods leaves an open problem unaddressed.

### Suggestions for Improvement
We recommend that the authors improve the definition of "text-describability" beyond relying solely on CLIP classification accuracy, potentially incorporating human annotators for more nuanced insights. Additionally, we suggest conducting splits at the category level rather than the dataset level to better account for variations in text-describability among categories. It is also critical to provide a thorough analysis of the finding that OVD performs worse than COD in low text-describability scenarios, as this is a significant and counterintuitive result. Furthermore, we recommend improving the organization of the abstract and introduction to clearly outline the problem and motivation. Revising the writing to eliminate informal expressions and errors would enhance clarity and professionalism. To strengthen the validity of their comparisons, we advise the authors to clarify their methodology regarding fine-tuning and consider including a comparison of visual prompts with fine-tuning methods in future research. Lastly, addressing the limitations of the work and including discussions on failure cases would enhance the paper's depth and clarity.