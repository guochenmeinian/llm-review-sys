ID: 3x3XhZ9AqX
Title: TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TELEClass, a minimally supervised hierarchical text classification method that addresses challenges in understanding large structured label spaces and distinguishing similar class semantics using only class names for supervision. TELEClass integrates large language models (LLMs) and task-specific features from an unlabeled corpus to enrich the taxonomy with class-indicative features, employing novel LLM-based data annotation and generation techniques. Experiments demonstrate that TELEClass significantly outperforms previous baselines and achieves comparable performance to zero-shot prompting of LLMs with considerably reduced inference costs.

### Strengths and Weaknesses
Strengths:
1. TELEClass effectively addresses weak supervision in hierarchical text classification, providing innovative solutions that enhance text classification technology.
2. The experimental design is comprehensive, evaluating the method across two public datasets and comparing it with various strong baseline models.
3. The paper is well-structured and presents ideas clearly, facilitating reader comprehension.

Weaknesses:
1. The methodology section lacks clarity, making it difficult for readers to grasp the specific approaches used, particularly in Figures 1 and 2.
2. There is an overreliance on background knowledge from related works, such as TaxoClass, which may hinder understanding for readers unfamiliar with these concepts.
3. The experimental comparisons are limited, relying primarily on outdated baselines and a single LLM model (GPT-3.5), which raises questions about the robustness and generalizability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology section to enhance reader understanding, particularly in the presentation of Figures 1 and 2. Additionally, the authors should provide a more detailed explanation of the entire pipeline of the method and clarify the relationships between Initial Core Classes, Enriched Key Terms, and Refined Core Classes. To strengthen the experimental results, we suggest incorporating more recent baseline models and additional LLMs, such as Llama 3, to evaluate performance comprehensively. Furthermore, the authors should discuss the implications of hyperparameter choices and consider a more sophisticated prompting strategy that incorporates taxonomy enrichment to potentially enhance zero-shot LLM performance.