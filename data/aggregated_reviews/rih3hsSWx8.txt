ID: rih3hsSWx8
Title: Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 7, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 1, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a thorough investigation of the generalization behavior of tensor neural networks (t-NNs) for the first time, bridging the gap between their practical success and theoretical analysis. The authors derive upper bounds for the generalization gaps and propose that compressing t-NNs with a transformed low-rank structure enhances adversarial training efficiency and tightens bounds. They demonstrate that adversarial training in highly over-parameterized settings leads to t-NNs with approximately transformed low-rank weights and derive sharp adversarial generalization bounds in this context.

### Strengths and Weaknesses
Strengths:
- The paper provides a comprehensive analysis of t-NNs' generalization behavior, supported by firm theoretical proofs.
- The exploration of generalization bounds with a transformed low-rank structure is well-motivated and theoretically sound.
- The authors present their findings with precise notation and an extensive appendix, enhancing clarity.

Weaknesses:
- The work lacks empirical evaluations to support the theoretical findings, particularly regarding approximately transformed low-rank weights.
- The writing assumes familiarity with t-product layers without adequately introducing their significance, which may hinder understanding.

### Suggestions for Improvement
We recommend that the authors improve the paper by incorporating empirical evaluations to validate the theoretical results and demonstrate the tightness of the upper bounds. Additionally, providing a brief explanation of the intuition behind t-NN layers and clarifying how the results differ from those of standard fully connected networks would enhance comprehension. Addressing the potential for extra regularizations, such as LoRA, during training could also be beneficial.