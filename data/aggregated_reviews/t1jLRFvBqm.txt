ID: t1jLRFvBqm
Title: Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 6, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VideoSAUR, a method for unsupervised video object segmentation that utilizes a temporal feature similarity loss alongside a feature reconstruction loss. The authors aim to improve object-centric learning by integrating a slot attention-based framework with self-supervised ViT encoders (DINO). The experimental results indicate that VideoSAUR achieves state-of-the-art performance on various benchmarks, including MOVi-C and YouTube-VIS, although comparisons with state-of-the-art methods are limited.

### Strengths and Weaknesses
Strengths:
- The proposed temporal feature matching loss is sound and effective, contributing to strong experimental results.
- The paper is well-organized, clear, and easy to follow, with a comprehensive ablation study that provides valuable insights.
- State-of-the-art performance is reported on several datasets, demonstrating the method's effectiveness.

Weaknesses:
- The approach primarily combines existing techniques without significant novelty, and the authors do not adequately clarify the relationship between their feature similarity loss and optical flow prediction from SAVi.
- There is a lack of comparison with state-of-the-art methods in the main paper, particularly on YouTube-VIS, where the proposed method is expected to perform well.
- The results on YouTube-VIS are low across all methods, suggesting that the proposed method may not be as effective in real-world scenarios.
- Presentation issues include inconsistent notation and insufficient figure references, which hinder clarity.

### Suggestions for Improvement
We recommend that the authors improve the transparency of their contributions by clearly explaining how their feature similarity loss relates to existing methods, particularly SAVi. Additionally, we suggest including comparisons with state-of-the-art methods in the main paper, especially on YouTube-VIS, to better contextualize the results. A more thorough ablation study should be conducted to elucidate the reasons behind the performance on MOVi-E, including evaluating the impact of using actual optical flow in the objective. Finally, we encourage the authors to address the presentation issues by ensuring consistent notation and providing adequate references to figures throughout the text.