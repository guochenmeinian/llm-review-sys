ID: e8i7OaPj0q
Title: Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 5, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel gradient clipping technique for differential privacy (DP) training algorithms, termed "automatic clipping," which aims to simplify the tuning of hyperparameters while maintaining privacy and computational efficiency. The authors provide a convergence analysis under a symmetric gradient noise assumption and validate their method through comprehensive experiments across various vision and language tasks, demonstrating that it matches or surpasses existing state-of-the-art techniques.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clear, making it accessible to readers.
- The proposed method simplifies DP training by eliminating the need for tuning DP-specific hyperparameters.
- The experiments are thorough, showcasing strong empirical performance and practical utility.

Weaknesses:
- The convergence analysis relies on a symmetric gradient noise assumption, limiting comparability with prior works.
- The justification for the effectiveness of automatic clipping is sometimes misleading, particularly regarding the implications of the dot product analysis in Figure 2.
- The paper lacks a critical comparison with related works, such as Yang et al. (2022), which could enhance its significance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding the advantages of automatic clipping over the rescaled DP-SGD method. Specifically, providing tangible proof that their method is significantly less expensive to tune would strengthen their contribution. Additionally, we suggest including a comparison of the sensitivity of the AUTO-S method to the $\gamma$ factor against the sensitivity of the rescaled DP-SGD method to the $R$ factor, as this could validate the proposed method's advantages. Lastly, addressing the misleading aspects of the dot product analysis in Figure 2 and incorporating relevant literature would enhance the paper's rigor and impact.