ID: MN7d0S2i1d
Title: Sample and Computationally Efficient Robust Learning of Gaussian Single-Index Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 7, 4, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies efficient parameter estimation from generalized linear models with adversarial noise, assuming a known link function. The authors propose an algorithm that achieves a sample complexity of $d^{\lceil k^*/2 \rceil}$, where $k^*$ is the information exponent. The algorithm employs projected gradient descent initialized with PCA for tensor unfolding, aligning with a known lower bound for even $k^*$ in the realizable setting. The work also explores the agnostic learning of single-index models, aiming for performance close to the optimal. The authors argue that existing works provide necessary conditions to lower the information exponent to $2$ only under the assumption that the distribution of $y$ is known to the learner. They assert that their algorithms do not succeed in the corruption model described and clarify that there exist function/distribution pairs where the information exponent equals the generating exponent and is greater than $2$, indicating that cited works do not generally achieve improved sample complexity in their setting.

### Strengths and Weaknesses
Strengths:
- The topic is engaging, particularly the insights provided by the adversarial setting.
- The paper covers a wide class of link functions without assumptions on data distribution, aside from Gaussian marginals.
- The technical contributions enhance understanding of sample complexity under adversarial noise.
- The paper provides a clear theoretical framework for understanding the limitations of existing algorithms in the agnostic model.
- It effectively identifies scenarios where the information and generating exponents exceed $2$, contributing to the discourse on sample complexity.

Weaknesses:
- The paper's length makes it challenging to verify the correctness of dense proofs within the review timeframe.
- The central ideas appear similar to existing works, raising questions about the novelty of contributions.
- The justification for using the CSQ class over SQ is insufficient, especially in light of recent findings on data reuse and label transformation.
- The reliance on specific assumptions may limit the applicability of the results.
- The potential for more efficient results under strengthened assumptions remains unproven, leaving some questions unresolved.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions, particularly for the vector $w^\star$, and emphasize its connection to Problem 1.1. Additionally, the authors should clarify the meaning of $c_{k^*} = \Omega(1)$ in Assumption 1 and ensure that the relationship between the performance and the recursion in Theorem 3.5 is more explicit. It would also be beneficial to discuss the implications of label transformations and data reuse in the context of adversarial noise. Furthermore, we suggest that the authors address the potential for preprocessing to reduce sample complexity and provide a more detailed comparison with recent works on SQ complexity. Lastly, we recommend that the authors improve the discussion on the implications of their theoretical results by providing more concrete examples of scenarios where their algorithms outperform existing methods and explore the conditions under which their assumptions can be relaxed to enhance the applicability of their findings.