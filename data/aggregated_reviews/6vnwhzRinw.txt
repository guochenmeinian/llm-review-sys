ID: 6vnwhzRinw
Title: Efficient Uncertainty Quantification and Reduction for Over-Parameterized Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an epistemic uncertainty assessment framework that guarantees statistical coverage with low computational costs for over-parameterized neural networks. The authors propose a Procedural-Noise-Correcting (PNC) predictor, utilizing an auxiliary network trained on zero labels to mimic training variability, supported by asymptotic results from Neural Tangent Kernel (NTK) theory. The framework includes methods for constructing confidence intervals using a limited number of retrained networks across various problem settings.

### Strengths and Weaknesses
Strengths:  
- The introduction of the PNC predictor is innovative and effectively addresses uncertainty quantification.  
- The paper is well-written, and the supplementary materials enhance its completeness.  
- The approach to disentangle procedural uncertainty is novel and could lead to valuable future research.  

Weaknesses:  
- The empirical evaluation is limited to low-dimensional toy problems, raising concerns about scalability.  
- Coverage studies are based on only 40 samples, leading to significant binomial errors that are not adequately discussed.  
- The paper lacks clarity in certain sections, particularly regarding missing notations and the experimental protocol, which could benefit from comparisons to established methods and datasets.  
- There is insufficient discussion on limitations, including the size requirements for neural networks and the societal impact of the research.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including more realistic problems and datasets, such as MNIST, CIFAR-10, and CIFAR-100, to assess the PNC's performance in both regression and classification tasks. Additionally, increasing the number of samples in coverage studies and reporting results that account for finite sample uncertainty would enhance the reliability of findings. Clarifying the narrative in Section 3 and reorganizing complex propositions could improve readability. Finally, addressing the limitations of the proposed method and providing code for reproducibility would strengthen the paper's overall impact.