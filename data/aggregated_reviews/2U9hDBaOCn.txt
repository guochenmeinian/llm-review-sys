ID: 2U9hDBaOCn
Title: Specialist or Generalist? Instruction Tuning for Specific NLP Tasks
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into instruction tuning by analyzing the effects of "generalist" and "specialist" datasets on model performance. The authors propose tuning three models based on Llama-7b with various combinations of generalist data (GPT4-Instruct, LIMA) and specialist data, assessing the amount of specialist data required for effective specialization across multiple tasks. The findings indicate that while more specialized training data improves performance, generalist data also contributes positively, albeit insufficiently on its own.

### Strengths and Weaknesses
Strengths:
- The analysis is insightful, covering a wide array of tasks and experiments, with robust evidence supporting the findings.
- The paper is well-structured and easy to read, making it accessible to the NLP community.
- It addresses a significant question in transfer learning, relevant given the rise of generalist instruction tuning datasets.

Weaknesses:
- Insights regarding the implications of generalist data sizes are insufficient, particularly concerning performance on target tasks.
- The findings may appear trivial, primarily suggesting that more data (both generalist and specialist) leads to better performance without exploring limitations such as convergence.
- The paper lacks a systematic guide as claimed, with insufficient details on the analysis and methodology.

### Suggestions for Improvement
We recommend that the authors improve the analysis of generalist data sizes and their implications on performance, potentially expanding on insights presented in Figure 3. Additionally, we suggest that the authors provide a more systematic guide as indicated in their contributions, clarifying the overlap between generalist datasets and justifying the choice of 50k specialist training examples. It would also be beneficial to soften claims regarding dataset performance differences, particularly in Experiment II, and to include a broader range of tasks beyond classification and sentiment analysis. Finally, we encourage the authors to integrate a longer description of generalist datasets in an appendix for clarity.