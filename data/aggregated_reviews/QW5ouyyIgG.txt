ID: QW5ouyyIgG
Title: CoDA: Collaborative Novel Box Discovery and Cross-modal Alignment for Open-vocabulary 3D Object Detection
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for open-vocabulary 3D object detection (OV-3DDet), addressing the challenges of localizing and classifying novel objects in point clouds. The framework comprises two modules: the 3D Novel Object Discovery (3D-NOD) module, which generates pseudo box labels for novel objects using both 3D geometry and 2D semantic priors, and the Discovery-Driven Cross-Modal Alignment (DCMA) module, which aligns feature spaces between 3D point clouds and image/text modalities. The proposed method is evaluated primarily on the SUN RGB-D dataset, demonstrating its effectiveness. Additionally, the paper analyzes the impact of CLIP on 3D object detection performance across different settings, revealing that while CLIP does not degrade performance when novel categories are not considered, the performance of '3D-CLIP' is lower than that of 3DETR in open-vocabulary detection. However, when CLIP is integrated during training, the authors' method significantly outperforms '3D-CLIP' in both novel and base categories, although the base category performance is slightly lower than 3DETR.

### Strengths and Weaknesses
Strengths:
1. The relevance of open-vocabulary 3D object detection is underscored, particularly for applications in autonomous driving and robotics, where predefined object classes are impractical.
2. The technical soundness of the proposed method is evident, particularly in the joint training of the localization and classification modules.
3. The paper provides a comprehensive evaluation of CLIP's role in 3D object detection, demonstrating clear performance metrics across various configurations.
4. Significant improvements in both $AP_{Novel}$ and $AP_{Base}$ are reported when integrating CLIP during training, highlighting the method's effectiveness.

Weaknesses:
1. The experimental results lack convincing evidence, with several areas for improvement identified, including the need for comparisons with existing methods and clearer settings for the compared methods.
2. The writing style is overly verbose, leading to redundancy that detracts from clarity.
3. The paper does not adequately address the computational complexity of the proposed framework or analyze failure cases.
4. The base category performance of the proposed method is slightly lower than that of 3DETR, which may raise concerns regarding its robustness in certain scenarios.
5. The reliance on cross-modal alignment may limit the applicability of the findings to other datasets or settings not covered in the study.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including comparisons with existing methods such as [14] and [R1], and clarify the settings in Table 1 regarding super category annotations and training epochs. Additionally, expanding the evaluation to include the ScanNetV2 dataset would provide a more comprehensive assessment of the framework's performance. We also suggest streamlining the writing to enhance readability and clarity. Furthermore, addressing the sensitivity of hyperparameters in Table 3 with more insights would strengthen the analysis. Lastly, we recommend that the authors improve the discussion surrounding the slightly lower $AP_{Base}$ compared to 3DETR, providing further insights into potential causes and implications, and exploring the robustness of their method across different datasets to validate its generalizability beyond the tested categories.