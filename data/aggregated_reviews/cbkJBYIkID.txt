ID: cbkJBYIkID
Title: Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called Proactive Defensive Backdoor (PDB) aimed at countering backdoor attacks in deep neural networks by proactively injecting a defensive backdoor during training. PDB embeds a trigger in the model's inputs during inference to neutralize malicious triggers while maintaining model utility. The authors conduct extensive experiments to evaluate PDB's effectiveness against various backdoor attacks and compare it with five state-of-the-art defense methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and clearly written.
- It introduces a novel approach for mitigating backdoor attacks.
- Extensive experiments demonstrate the effectiveness of the proposed method.

Weaknesses:
- The novelty of the contribution is questionable, as it builds on existing proactive attack-based works.
- The paper lacks sufficient explanations for experimental results, particularly regarding discrepancies in performance metrics.
- Sensitivity to the defensive poisoned dataset needs further evaluation.
- The accuracy on benign samples is inferior compared to other methods, raising concerns about potential degradation of performance with lower poisoning ratios.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the novelty of their approach by contrasting it with existing methods, particularly BackdoorIndicator. Additionally, the authors should clarify the experimental settings and provide a more detailed analysis of results, especially for Table 2, where the ASR results for PDB are significantly different from baselines. Conducting experiments on the trigger pattern position and evaluating the sensitivity of the defensive poisoned dataset would strengthen the paper. Finally, the authors should clarify the running time metrics in Table 4 and ensure that comparisons are made with inference time rather than training time.