ID: 3xRaWBD2YB
Title: Polynomial Width is Sufficient for Set Representation with High-dimensional Features
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a proof that for symmetric neural networks, specifically DeepSets, exact representations for symmetric functions can be achieved with a symmetric embedding layer width that is polynomial in the set size and input dimension. The authors propose that this approach significantly reduces the width requirements compared to previous exponential bounds, particularly in high-dimensional scenarios, and extends the understanding of permutation-invariant and permutation-equivariant set functions. Additionally, the authors study continuous permutation-invariant functions and their representation through specific architectures, proposing two architectures: the *linear layer + power mapping (LP)* and the *linear layer + exponential activation (LE)*, detailing their conditions and implications for continuous mappings. They assert the validity of their main claims and provide a link to a revised version of their work that includes corrected proofs and results.

### Strengths and Weaknesses
Strengths:  
- The proof technique is clever, demonstrating the ability to continuously invert a specific symmetric embedding, which is a novel insight into DeepSets architecture.  
- The manuscript is clearly written, and the results extend previous studies by tightening the upper bound from exponential to polynomial, making it significant for high-dimensional applications.  
- The authors have made significant revisions to address previous proof-related issues, enhancing the clarity and correctness of their claims.  
- The proposed architectures are well-defined, with clear mathematical formulations that contribute to the understanding of continuous mappings.

Weaknesses:  
- The paper lacks a robust discussion on the trade-offs of the proposed parameterization, particularly regarding the implications of non-smoothness and the potential need for exponentially large widths elsewhere in the network.  
- There is an insufficient exploration of practical implications, particularly how the results can enhance existing models in fields like particle physics and computer vision.  
- The proof relies on Lemma 4.9, which is unproven and potentially incorrect, raising concerns about the validity of the second part of Theorem 3.1.  
- The manuscript does not provide experimental verification to support the theoretical claims, which would strengthen the argument.  
- The revised results and proofs have not been made accessible to all reviewers, potentially hindering a comprehensive evaluation of the work.  
- There are still concerns regarding the completeness of the examination of the LE architecture within the rebuttal phase.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the trade-offs associated with the parameterization, particularly addressing the implications of non-smoothness and the potential for large widths in other parts of the network. Additionally, it would be beneficial to include a section on the practical implications of the results, specifically how they can inform improvements in typical network models such as GNN and PointNet. We also suggest providing experimental verification of the results to bolster the theoretical claims. Furthermore, the authors should clarify the proof of Lemma 4.9 and ensure that the presentation of Theorem 3.1 distinctly separates the upper and lower bounds for better clarity. Lastly, we recommend improving the accessibility of their revised results and proofs to all reviewers to facilitate a thorough assessment, and providing further clarification on the implications of their findings, particularly regarding the LE architecture, to strengthen their argumentation.