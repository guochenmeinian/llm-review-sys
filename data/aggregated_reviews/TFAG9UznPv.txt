ID: TFAG9UznPv
Title: On the Scalability of Certified Adversarial Robustness with Generated Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical study on the impact of synthetic data on the certified adversarial robustness of deep learning models. The authors explore how synthetic data can enhance both robustness accuracy and clean accuracy, particularly focusing on $l_2$ and $l_\infty$ robustness across CIFAR-10 and CIFAR-100 datasets. The findings indicate that while synthetic data improves certified robustness, its effects differ from those on empirical robustness. The paper includes ablation studies and offers guidance on hyperparameters such as dropout rate, epochs, model sizes, learning rate schedulers, and the ratios of original to synthetic data.

### Strengths and Weaknesses
Strengths:
1. The paper conducts experiments with multiple models and datasets, demonstrating the generalizability of the findings.
2. The ablation studies consider various hyperparameters, providing a comprehensive analysis.
3. The guidance on comparing certified accuracy among different approaches is valuable for the research field.

Weaknesses:
1. The paper lacks a thorough review of related works, particularly in section 2, where it overlooks deterministic approaches like IBP, SABR, and TAPS, which limits generalizability.
2. The correlation between the generalization gap and certified accuracy is discussed, but the potential negative impact of dropout and synthetic data on certified accuracy is not adequately addressed.
3. The achieved clean and robustness accuracies are notably low compared to state-of-the-art architectures, raising concerns about practical applicability.

### Suggestions for Improvement
We recommend that the authors improve the literature review in section 2 by including a discussion of deterministic approaches such as IBP, SABR, and TAPS to enhance the generalizability of their findings. Additionally, we suggest that the authors provide the generalization gap of the model trained with $\rho=0.85$ and synthetic data to clarify the observed negative impact on certified accuracy. Furthermore, we encourage the authors to conduct experiments comparing their results against state-of-the-art convex-bound-propagation approaches to better contextualize the effectiveness of synthetic data. Lastly, we advise the authors to perform multiple runs with different seeds for key configurations to assess the meaningfulness of the observed improvements.