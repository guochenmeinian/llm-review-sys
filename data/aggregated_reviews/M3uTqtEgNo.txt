ID: M3uTqtEgNo
Title: Rethinking and Improving Multi-task Learning for End-to-end Speech Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates methods to enhance multi-task learning for end-to-end speech translation (ST). The authors propose using cosine similarity between gradient vectors from different tasks to measure task consistency, leading to several findings: 

a) High consistency between the ASR and ST tasks at the acoustic encoder, but low consistency at the text encoder and decoder, suggesting the use of only the ASR task for training the acoustic encoder.

b) Low consistency between MT and ST, particularly at the text encoder, attributed to length discrepancies and representation gaps. The authors propose a modified shrinking method and a local-to-global convolution approach to address these issues.

c) Variability in consistency between ST and auxiliary tasks over training, leading to a task weighting strategy that adjusts the importance of auxiliary tasks based on their consistency and training gradient impact, eliminating the need for separate fine-tuning.

Experiments on MuST-C show a 0.9 BLEU improvement over the ConST baseline across eight language pairs.

### Strengths and Weaknesses
Strengths:
- The investigation of task consistency provides valuable insights into the utility of auxiliary tasks.
- Proposed methods, including the improved shrinking method and local-to-global training, are reasonable extensions that enhance performance.
- Overall performance on MuST-C is commendable.

Weaknesses:
- Writing lacks clarity, making it difficult to understand and replicate the proposed approaches.
- The impact of proposed methods on consistency is not clearly demonstrated, and some claims lack sufficient support.
- Key details regarding model architecture and experimental setups are missing, hindering comparisons with previous work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing to ensure that the proposed approaches are easily understood and replicable. Specifically, they should clarify how the L2G extractor integrates with the Transformer and define terms such as "u" and "s" in equation 9. Additionally, we suggest providing results that explicitly demonstrate how the proposed methods improve consistency. The authors should also ensure that their approach considers both task impact and consistency in the weighting strategy, as the current equations appear to focus solely on task impact. Finally, we encourage the inclusion of a comprehensive model diagram to aid reader understanding of the algorithmic process.