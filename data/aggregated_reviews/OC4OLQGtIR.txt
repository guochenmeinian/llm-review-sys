ID: OC4OLQGtIR
Title: Reducing Sequence Length by Predicting Edit Spans with Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for instruction-tuning large language models (LLMs) to generate edit spans for localized sequence transduction tasks, such as grammatical error correction and paraphrasing. The authors demonstrate that fine-tuning LLMs on edit representations improves performance on these tasks while maintaining performance on open-ended tasks that utilize plain text representations. The approach achieves state-of-the-art results across several tasks and proposes a new output-oriented method that could serve as a baseline in the NLP community.

### Strengths and Weaknesses
Strengths:
* Proposes a straightforward method for fine-tuning LLMs that enhances performance on localized sequence transduction tasks without significantly degrading performance on open-ended tasks.
* Demonstrates substantial reductions in output length when using edit representations.
* Achieves significant performance improvements on task-specific data compared to existing baselines.

Weaknesses:
* The concept of using edit representations in sequence-to-sequence models lacks novelty, as evidenced by prior works like Stahlberg and Kumar's Seq2Edits and Mallinson et al.'s EdiT5.
* Some statements in the paper are unclear, leading to potential confusion for readers.
* No performance improvement was observed in 13 of the 32 cases discussed, with insufficient explanation provided.

### Suggestions for Improvement
We recommend that the authors improve clarity in several areas, including:
* Clarifying the statement regarding token indices in L186 to avoid confusion.
* Providing examples from open-ended tasks as mentioned in L254.
* Revising the phrase "fine-tuned data" in L300 for better understanding.
* Labeling the columns in Table 1 with appropriate metrics.
* Clarifying the computational cost savings during inference in L337, specifying that it applies only to localized tasks.
* Correcting the performance comparison in Table 3 and its caption to accurately reflect the metrics.
* Including a discussion on the computing costs associated with the proposed method, as previously agreed upon in the authors' rebuttal.