ID: KgeQqLI7OD
Title: Towards Visual Text Design Transfer Across Languages
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 7, 7, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the MuST-Bench dataset, which contains same-style glyphs across multiple languages, annotated from translated film posters. The authors propose the SIGIL framework, which generates glyphs from text prompts using examples of glyphs in the same style. The contributions include the dataset's utility for future research and the strong performance of SIGIL in experiments against robust baselines.

### Strengths and Weaknesses
Strengths:
1. The new dataset is quite useful, providing ground truth glyphs in similar styles across languages, facilitating supervised training for style-guided glyph generation.
2. The proposed SIGIL method demonstrates good performance, producing high-quality generated images and outperforming baselines quantitatively.
3. The experiments are well-designed, incorporating a comprehensive suite of evaluation metrics, including CLIP score, OCR, MLLM evaluation, and user studies.

Weaknesses:
1. The methodology description of SIGIL is unclear, lacking critical details about inputs during training and inference, and the training losses are confusing.
2. There are no quantitative results on ablation studies, and existing studies focus only on hyperparameters and text prompt design, missing evaluations on the necessity of objectives like $L_{glyph}$ and reinforcement learning components.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the SIGIL methodology by explicitly detailing the inputs used during training and inference, as well as clarifying the training losses. Additionally, we suggest including quantitative results from ablation studies to demonstrate the necessity of each component, particularly regarding objectives like $L_{glyph}$ and the reinforcement learning aspect. Furthermore, we encourage the authors to scale up the dataset size to enhance its appeal to the research community and to provide a more robust justification for the choice of CLIP for evaluating image similarity. Lastly, we recommend adding more baselines for comparison to strengthen the evaluation of their method.