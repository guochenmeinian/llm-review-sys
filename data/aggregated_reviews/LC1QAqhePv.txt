ID: LC1QAqhePv
Title: SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 7, 6, 7, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents SciInstruct, a self-reflective instruction annotation framework aimed at addressing data scarcity in the science domain. The authors construct a comprehensive dataset that includes diverse scientific topics such as physics, chemistry, and mathematics, and propose an annotation framework to enhance step-by-step reasoning for scientific questions. Extensive experiments demonstrate the high quality of the dataset and its effectiveness in improving the performance of various large language models (LLMs).

### Strengths and Weaknesses
Strengths:
1. The paper provides a broad and diverse dataset that addresses the scarcity of high-quality scientific data for training LLMs.
2. The self-reflective annotation framework enables LLMs to autonomously generate and refine reasoning.
3. Thorough analysis and benchmarking showcase the effectiveness of SciInstruct across multiple scientific domains.
4. The dataset and code are publicly available, promoting transparency and further research.

Weaknesses:
1. The scope of the paper is over-claimed, particularly regarding the dataset's application for training scientific language models; clarification is needed in the title and introduction.
2. The novelty of the self-reflective annotation framework is limited, as related concepts have been discussed in prior work. The authors should emphasize their key contributions.
3. Concerns arise regarding the classifier's ability to address the proposed challenge of incorrect intermediate reasoning, necessitating further experimental analysis.
4. The safety of the dataset is questionable, particularly regarding sensitive information in chemistry; an analysis of this aspect is required.
5. The performance of closed-source models like GPT-4 and Claude is not reported, complicating the evaluation of SciGLM's performance relative to these models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the scope of their work in the title and introduction. Additionally, the authors should discuss related works to highlight the novelty of their self-reflective annotation framework. It would be beneficial to rethink the proposed challenge regarding the classifier's training data and conduct more experimental analysis. We encourage the authors to conduct a safety analysis of the dataset, particularly in relation to sensitive information in chemistry. Furthermore, the authors should report the performance of closed-source models in their evaluations to provide a clearer comparison. Lastly, we suggest enhancing the writing style for better readability and reorganizing sections for improved flow.