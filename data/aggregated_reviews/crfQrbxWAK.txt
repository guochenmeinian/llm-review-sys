ID: crfQrbxWAK
Title: Schema-adaptable Knowledge Graph Construction
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the task of "schema-adaptable knowledge graph (KG) construction," aimed at extracting <typed head entity, relation, typed tail entity> KG triplets from natural language text while accommodating evolving schemas without retraining the extraction model. The authors propose a methodology for dataset derivation and introduce the ADAptive Knowledge Graph Construction (ADAKGC) model, which utilizes a pre-trained T5 language model with a schema-conditioned decoding strategy. The evaluation against state-of-the-art approaches shows marginal improvements over UIE in various settings.

### Strengths and Weaknesses
Strengths:
- The task is interesting and relevant, potentially driving further research.
- Good reproducibility with code and data provided.
- Sufficient experimental validation, including multiple KG construction sub-tasks and ablation tests.

Weaknesses:
- Improvements of ADAKGC over UIE are marginal, raising concerns about practical feasibility.
- The architecture of ADAKGC is similar to UIE, lacking significant novelty.
- Definitions of schema expansions are unclear, and the feasibility of encoding large schemas is questionable.

### Suggestions for Improvement
We recommend that the authors improve the clarity of definitions and examples related to horizontal and vertical schema expansions, particularly regarding the introduction of sub-relations and the conditions for class levels. Additionally, we suggest that the authors discuss the limitations of schema size that can be processed by ADAKGC in the corresponding section. It would also be beneficial to provide a statistical analysis of the performance differences between ADAKGC and the baselines to substantiate claims of superiority. Lastly, we encourage the authors to enhance the writing quality, particularly in the experimental settings and model details, to reduce confusion.