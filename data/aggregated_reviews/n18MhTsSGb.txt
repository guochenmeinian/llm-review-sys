ID: n18MhTsSGb
Title: 2Direction: Theoretically Faster Distributed Training with Bidirectional Communication Compression
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 4, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents 2Direction, a new GD-based distributed training algorithm for convex optimization problems that incorporates bi-directional compression. The authors claim that 2Direction achieves state-of-the-art theoretical communication complexity in communication-efficient distributed training, arguing that their analogy between gradient descent (GD) and stochastic gradient descent (SGD) is nearly perfect. They emphasize that compressed gradient methods, like 2Direction, require more iterations but have lower per-iteration communication costs, making the method suitable for communication-bound scenarios where communication is more expensive than computation. The theoretical analysis demonstrates that 2Direction improves communication complexity in certain scenarios without being worse in others, and the authors believe their theoretical contributions stand independently of practical implementations.

### Strengths and Weaknesses
Strengths:
1. The scope and main problem addressed in the paper are well-defined and clear.
2. The consideration of bi-directional compression is a novel contribution, as existing literature primarily focuses on uplink communication.
3. The theoretical analysis and results are mostly clear and robust, with comparisons to several state-of-the-art alternatives.
4. The paper introduces a theoretically significant method that achieves new communication complexity bounds and provides a robust theoretical framework addressing complex issues in distributed optimization.

Weaknesses:
1. The communication complexity analysis lacks clarity, particularly regarding the dependency between round cost and the number of communication rounds.
2. 2Direction demands more memory and computational resources compared to previous methods, such as DG and AGD, and this overhead is not adequately discussed.
3. The experimental section is weak, featuring only a single logistic regression problem with two datasets in supplementary material, lacking empirical evidence demonstrating the practical benefits of 2Direction, particularly regarding convergence time and applicability in real-world scenarios.
4. The assumptions made, such as zero computation cost, are criticized for lacking justification and clarity, which may obscure the method's practical relevance.
5. The presentation requires improvement, as the paper is not self-contained and relies heavily on supplementary material for key ideas.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the communication complexity analysis, particularly regarding the relationship between round cost and communication rounds. Additionally, the authors should address the memory and computational overhead of 2Direction, especially in the context of federated learning. We suggest including more comprehensive experimental results in the main body of the paper to strengthen the evaluation of 2Direction and provide evidence of its practical benefits, particularly in terms of convergence time. Furthermore, we recommend that the authors clearly articulate the assumptions made in the paper and discuss their implications on the practical applicability of the method. Enhancing the presentation to ensure that the paper is self-contained and does not rely excessively on supplementary material will also be beneficial.