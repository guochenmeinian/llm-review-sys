ID: HTLJptF7qM
Title: Noisy Label Learning with Instance-Dependent Outliers: Identifiability via Crowd Wisdom
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 5, 7, 6, -1
Original Confidences: 3, 5, 4, 4, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to learning with noisy labels by modeling instance-dependent confusion matrices as outliers, which is a departure from traditional methods that assume a consistent confusion matrix. The authors propose a crowdsourcing strategy with a column sparsity constraint and an end-to-end, one-stage differentiable loss function to effectively identify outliers and improve classifier performance. The contributions are supported by theoretical results and experimental validation demonstrating enhanced testing accuracy.

### Strengths and Weaknesses
Strengths:
- The problem setting is realistic, addressing instance-dependent noisy samples as outliers.
- The proposed crowdsourcing strategy is theoretically grounded and offers generalization guarantees.
- The presentation is clear, and the method is supported by thorough theoretical results and empirical validation on real-world datasets.

Weaknesses:
- The lack of large-scale datasets in experiments limits the applicability of the findings.
- Theorem 3.6's requirement for $S \rightarrow \infty$ may trivialize results when sufficient annotators are present.
- The organization of the proposed method could be improved for clarity.
- The assumption that most samples share a common nominal transition matrix may not hold in all scenarios.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by incorporating large-scale datasets, such as those mentioned in the NeurIPS 2022 paper and the dopanim dataset. Additionally, we suggest conducting experiments to verify the impact of annotator sparsity on identifiability and performance. Clarifying the definitions of key terms in the abstract, such as "outliers" and "the model of interest," would enhance reader comprehension. Finally, we encourage the authors to explore the implications of their findings in scenarios with highly imbalanced noise and to consider the use of existing estimators for initializing the variable $\mathbf A_m$ in the proposed loss function.