ID: MICrZCQzoN
Title: Generalization bounds for mixing processes via delayed online-to-PAC conversions
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 5, 4, 7, 7, -1, -1, -1
Original Confidences: 3, 3, 5, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for establishing generalization error bounds for non-i.i.d data, extending the online-to-PAC conversion technique to scenarios with delayed feedback. The authors illustrate methods for non-i.i.d online-to-PAC conversion, converting online algorithms to delayed counterparts, and deriving generalization bounds that incorporate dynamic hypothesis learning. The framework is shown to be applicable to various online learning algorithms and is designed to translate low regret into low generalization error.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and effectively motivates the study of generalization error in dynamic environments.
- The proposed framework is elegant, general, and applicable to many real-world data generation processes.
- The paper is easy to follow, with a helpful introductory section that contextualizes the reduction in the i.i.d case.

Weaknesses:
- There is a notable logical gap: Lemma 3 provides a regret bound independent of $P^*$, while Corollaries 3 and 4 require a $P^*$-dependent regret bound, which needs addressing.
- The technical novelty appears limited, as the framework seems to be a straightforward extension of existing methods in the i.i.d setting.
- The instantiation of the framework remains high-level and abstract, lacking specific examples that demonstrate comparability to existing results.

### Suggestions for Improvement
We recommend that the authors improve the discussion on related work, particularly regarding previously unknown results that could be proved with the proposed method. Additionally, we suggest providing specific instantiations of Theorem 4 using EWA and FTRL, as well as addressing the significance of the delayed learning setup compared to the standard online setup of Lugosi-Neu (2003). Furthermore, we encourage the authors to clarify the conditions under which their results hold for a broader class of bounded loss functions and to elaborate on potential future directions and limitations of their framework.