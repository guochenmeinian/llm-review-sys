ID: ZULq9QV8rH
Title: Self-Supervised Learning with Lie Symmetries for Partial Differential Equations
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 8, 3, 7, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach for self-supervised learning (SSL) of partial differential equations (PDEs) utilizing symmetry transformations for data augmentation. The authors propose a joint embedding framework that learns general-purpose representations from heterogeneous data, demonstrating improved performance in tasks such as regressing PDE coefficients and enhancing neural solvers' time-stepping capabilities.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a novel application of Lie symmetry groups for SSL in PDEs.
- It provides a comprehensive overview of symmetry groups relevant to various PDEs, which is beneficial for the community.
- The results indicate a significant improvement over traditional supervised learning methods.

Weaknesses:
- The novelty of the approach is marginal, as the SSL framework is not specifically tailored for PDEs and resembles existing literature.
- The evaluation lacks comparisons with important baseline models, limiting the demonstration of the learned representation's effectiveness.
- The simplicity of certain regression tasks raises questions about the complexity of the approach, and the authors do not adequately address boundary conditions during augmentation.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contribution by customizing the SSL framework specifically for PDEs rather than relying on established methods. Additionally, the authors should include comparisons with key baseline models such as MPPDE, FNO, and DeepONets to substantiate the effectiveness of their learned representations. Clarifying the complexity of regression tasks and addressing the preservation of boundary conditions during augmentation would enhance the theoretical foundation of the work. Finally, we suggest that the authors tone down claims regarding foundation models and provide more detailed experimental results involving various neural operator architectures.