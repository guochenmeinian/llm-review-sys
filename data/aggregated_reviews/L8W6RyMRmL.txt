ID: L8W6RyMRmL
Title: Reduce Human Labor On Evaluating Conversational Information Retrieval System: A Human-Machine Collaboration Approach
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel human-machine collaborative approach called HumCoE aimed at reducing human labor in evaluating conversational information retrieval (CIR) systems. The authors propose utilizing active testing to strategically select representative data for human annotation, marking the first application of active testing in CIR evaluation. HumCoE selects data based on the probability of inconsistency between the CIR model's predictions and pseudo labels from a surrogate model. Empirical results indicate that HumCoE achieves evaluation results comparable to full human evaluation while requiring minimal human labor (<1%) across various CIR tasks, such as conversational QA and clarifying questions.

### Strengths and Weaknesses
Strengths:
- Addresses the significant challenge of labor-efficient evaluation methods for conversational systems, with potential practical impact.
- Introduces an innovative approach by integrating active learning and human-AI collaboration, demonstrating effectiveness through comprehensive empirical analysis across multiple datasets and tasks.
- The paper is well-structured, with clear implementation details and sufficient analysis supporting its conclusions.

Weaknesses:
- The need for a differentiable evaluation metric to estimate sample hardness is a limitation, as relevance scores are addressed via ChatGPT, which showed low agreement with human scores.
- The methods section lacks clarity, particularly regarding the surrogate model's usage and the evaluation metrics (stability and consistency), which require further explanation.
- The choice of datasets and models used in the study raises questions, particularly regarding their alignment with CIR tasks and the rationale for selecting only certain models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methods section, particularly by providing more detailed explanations of the surrogate model and the evaluation metrics used. Including running examples could enhance understanding. Additionally, the authors should discuss the sensitivity of HumCoE scores to different surrogate models and conduct experiments to illustrate this. We suggest that the authors clarify the rationale behind the chosen datasets and models, and consider expanding their analysis to include scenarios where HumCoE may not approximate full human evaluation effectively. Finally, addressing the low agreement of ChatGPT with human scores would strengthen the paper's claims regarding automatic scoring reliability.