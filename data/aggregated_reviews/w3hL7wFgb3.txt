ID: w3hL7wFgb3
Title: We're Afraid Language Models Aren't Modeling Ambiguity
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new dataset, AMBIENT, which is a linguistically annotated benchmark of 1,645 examples featuring various types of ambiguity. The authors argue for the significance of ambiguity in communication and its implications for language models (LMs). They conduct tests on pretrained LMs to generate, recognize, and disentangle ambiguity, including a case study on political claims that highlights the benefits of disambiguation. The paper also investigates how LLMs like ChatGPT and GPT-4 handle ambiguity, revealing ongoing challenges despite fine-tuning on relevant datasets.

### Strengths and Weaknesses
Strengths:
- The paper addresses a vital issue in language, providing a well-constructed and reliable corpus that is beneficial for both the NLI community and linguists.
- The dataset is robust, with a careful construction process and sound experimental evidence supporting the findings.
- The motivation for the research is well-articulated, particularly through the case study on political claims.

Weaknesses:
- The authors do not explicitly state the language of the dataset, which is a minor but important oversight.
- The results could be more insightful if grouped by data collection methods and types of ambiguity.
- There are concerns regarding the prompting strategies used in experiments, which may not fully leverage modern techniques that could enhance model performance.
- The evaluation methodology for human performance versus model performance lacks clarity and consistency.

### Suggestions for Improvement
We recommend that the authors improve clarity by explicitly noting the language considered in the dataset. Additionally, we suggest that the authors group results based on data collection methods and types of ambiguity to enhance interpretability. It would be beneficial to explore and document alternate prompting strategies, such as chain-of-thought or self-critique, to assess their impact on model performance. Finally, we encourage the authors to clarify the evaluation setups for human and model performance to ensure consistency and transparency in their methodology.