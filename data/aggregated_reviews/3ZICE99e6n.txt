ID: 3ZICE99e6n
Title: ReTR: Modeling Rendering Via Transformer for Generalizable Neural Surface Reconstruction
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 7, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a learning-based framework for generalizable neural surface reconstruction that utilizes transformers to model the rendering process, addressing limitations of traditional volume rendering. The authors propose a new architecture, ReTR, which replaces the volume rendering equation with a render transformer and introduces an occlusion transformer for finer feature extraction. Extensive experiments on multiple datasets demonstrate superior performance compared to state-of-the-art methods.

### Strengths and Weaknesses
Strengths:
1. The framework effectively addresses the oversimplification of traditional volume rendering, providing a novel approach to modeling photon-particle interactions.
2. The paper is well-structured and presents a clear logical flow, making it easy to follow.
3. The proposed method achieves state-of-the-art performance across various datasets, supported by comprehensive experiments and ablation studies.

Weaknesses:
1. The concept of using 3D feature volumes with hybrid resolution is not novel, as it has been previously proposed in NeuralRecon. The authors should conduct separate ablation studies to validate the impact of their variations.
2. The ablation study for the occlusion transformer could be improved by ensuring a fair comparison through self-attention computations.
3. Visual comparisons with other methods, particularly regarding robustness against variations in rendering, are limited and should be expanded.
4. The main diagram lacks clarity and could be presented more elegantly, with clearer definitions of components.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the main diagram and provide explicit definitions for all components, particularly in the reconstruction transformer section. Additionally, the authors should include more visual comparisons with other methods, such as SparseNeus and VolRecon, to substantiate claims of robustness. It is also crucial to present timing evaluations for both training and evaluation scenarios to address concerns regarding efficiency. Finally, we suggest conducting separate ablation studies to validate the contributions of the proposed hybrid feature extraction and occlusion transformer.