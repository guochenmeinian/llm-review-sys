ID: ALISPmDPCq
Title: ConStat: Performance-Based Contamination Detection in Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 5, 8, 6, 5, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new performance-based definition of contamination in large language models (LLMs), focusing on its effects rather than causes. The authors propose ConStat, a statistical method that detects and quantifies contamination by comparing performance on primary and reference benchmarks using a set of reference models. The effectiveness of ConStat is demonstrated through extensive evaluations across diverse model architectures, benchmarks, and contamination scenarios, revealing significant contamination in several popular models.

### Strengths and Weaknesses
Strengths:
- The motivation for the new definition of contamination is clear and easy to understand.
- The experiments are extensive, demonstrating that ConStat can outperform previous methods in contamination detection and quantification.
- The paper is well-written and structured, effectively communicating ideas.

Weaknesses:
- The reliance on synthetic reference datasets raises concerns about their quality, as the authors did not use a sufficiently rigorous method to ensure high-quality generation.
- The method identifies contamination based solely on performance changes, which can lead to false positives and does not guarantee actual contamination detection.
- The applicability of the method is limited to models similar to the reference models, which restricts its generality.

### Suggestions for Improvement
We recommend that the authors improve the elaboration on the limitations of using synthetic datasets, including a manual review of a subset for quality assurance. Additionally, we suggest that the authors clarify how benchmark-specific contamination is assessed and provide more details on the methodology to enhance understanding. It would also be beneficial to address the potential for false positives in performance-based contamination detection and discuss the implications of using reference models that may themselves be contaminated.