ID: 4jEjq5nhg1
Title: Operator Learning with Neural Fields: Tackling PDEs on General Geometries
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 7, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for solving partial differential equations (PDEs) using Implicit Neural Representations (INR) within an operator learning framework. The authors propose a two-stage pipeline that encodes input conditions into latent vectors, processes these vectors, and decodes them to produce output functions, effectively bypassing discretization. The method demonstrates significant improvements in initial value problems and shows competitive results across various PDE tasks.

### Strengths and Weaknesses
Strengths:
- The proposed method is highly innovative and leverages the advantages of neural fields for discretization-free PDE solving.
- The clarity and quality of writing are excellent, making the paper easy to follow.
- Empirical results show that the method outperforms existing baselines in several tasks.

Weaknesses:
- The paper lacks sufficient discussion of related work, particularly transformer-based methods that also handle irregular geometries.
- There is insufficient exploration of the method's stability and the training process, particularly regarding the bi-level optimization and error bars.
- The generalization capabilities of the method across different geometries and boundary conditions are not adequately justified or explained.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including relevant transformer-based methods and discussing their implications. Additionally, the authors should explore the stability of the bi-level optimization process and report error bars for selected cases. Clarifying the inductive biases of the encoding process and how boundary conditions are incorporated would enhance understanding. Finally, we encourage the authors to provide more detailed empirical evaluations, including comparisons with discrete neural solvers and discussions on the training time and hyperparameter sensitivity.