ID: OH5xp2W43y
Title: PopAlign: Population-Level Alignment for Fair Text-to-Image Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 8, 7
Original Confidences: 4, 4

Aggregated Review:
### Key Points
This paper presents a population-level preference optimization (PopAlign) method aimed at addressing biases in text-to-image generation that sample-level reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) cannot resolve. The authors propose that while sample-level optimization is effective for large language models (LLMs), it does not translate effectively to text-to-image (T2I) generation. The experiments are well-designed, demonstrating improvements with the new approach.

### Strengths and Weaknesses
Strengths:  
- The method is computationally efficient while maintaining high-quality image generation.  
- The paper contributes to the development of fair text-to-image generation models, showcasing originality and clarity.  
- The experimental design effectively highlights the improvements achieved by the proposed method.  

Weaknesses:  
- Some unresolved issues remain, such as the transferability of "Fairness-Alignment" across different contexts (e.g., generating equal numbers of male and female professionals).  
- Certain abbreviations and relationships among models (e.g., “SDXL-SFT,” “SDXL-PopAlign”) are vague and require clarification.  
- The training data assumptions are unclear, particularly regarding prompt generation, and examples are limited to single individuals related to professions.  
- There are typographical errors and broken links noted in the text (lines 277 and 289).  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relationships between the various model abbreviations and provide additional details in the appendix regarding the datasets required for SFT and DPO. It would be beneficial to include comparisons between SDXL-DPO and SDXL-PopAlign, as PopAlign is inspired by DPO. Additionally, clarifying the training data assumptions and expanding the examples beyond single individuals would enhance understanding. Finally, addressing the typographical errors and broken links is essential for the paper's overall quality.