ID: X90tyXDe8z
Title: JaxMARL: Multi-Agent RL Environments and Algorithms in JAX
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 6, 6, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents JaxMARL, a JAX-based library that implements multi-agent reinforcement learning (MARL) environments and algorithms, showcasing significant speed improvements in training. The authors introduce environments such as SMAX and STORM, alongside popular MARL algorithms like QMIX and PPO, achieving speedups of 14x to 12500x over traditional methods. While the evaluation demonstrates the library's efficiency, it primarily validates correctness rather than offering insights for developing new algorithms or environments. The novelty of the introduced environments is debated, as they are not seen as unique contributions compared to existing benchmarks like Hanabi.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- JaxMARL offers substantial speed improvements, allowing for faster iteration and exploration of new research avenues.
- The implementation of complex environments in JAX is substantial, with impressive speedups demonstrated.
- The framework includes high-quality implementations of various algorithms across multiple environments, addressing existing challenges in MARL.
- The environment is diverse and categorized effectively for MARL research, providing a solid foundation for experimentation.

Weaknesses:
- The paper lacks insights into developing new MARL algorithms or designing environments, limiting its contribution to the field.
- The environments SMAX and STORM lack unique features that would classify them as novel contributions.
- Flexibility in the JAX environments is restricted, particularly regarding varying the number of agents and dynamic policy selection.
- There is no practical guide for customizing environments, which may hinder researchers from adapting the library to their needs.
- The justification for not evaluating Q-learning in certain environments is insufficient, and the support for multi-GPU training is unclear.
- There is a lack of comprehensive benchmarks to validate the correctness and equivalence of the proposed environments against their original counterparts.
- The speedup comparisons may be misleading if not conducted under identical conditions.

### Suggestions for Improvement
We recommend that the authors improve the discussion on whether SMAX inherits key features from SMAC, quantitatively assessing the effectiveness of methods across both environments. Additionally, we suggest providing a practical guide for developing customized environments to enhance usability. Clarifying the evaluation methods for speed comparisons and addressing the limitations regarding agent variability and observation sizes would strengthen the paper. We also recommend improving the benchmarking section by testing all applicable environments against their original counterparts using the same baseline algorithm in JaxMARL, including MABrax, and presenting the training curves in an appendix. Furthermore, a detailed discussion on how the proposed baselines handle massive rollout data, particularly focusing on the trade-offs between training updates and sampled timesteps/episodes, as well as how to tune hyperparameters like NUM_ENVS and BATCH_SIZE for effective learning in off-policy Q-learning settings, would enhance the clarity and utility of the framework for the community. Finally, justifying the absence of Q-learning evaluations in specific environments and exploring multi-GPU training capabilities would enhance the library's robustness.