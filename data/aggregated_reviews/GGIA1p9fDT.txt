ID: GGIA1p9fDT
Title: CORNN: Convex optimization of recurrent neural networks for rapid inference of neural dynamics
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 7, 8, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to replace FORCE/RLS fitting of biologically-flavored recurrent neural networks (RNNs) with a faster and more robust training scheme, specifically focusing on the CORNN framework. The authors propose altering the training objective to a cross-entropy loss, using an approximate pre-computable Hessian for Newton steps, implementing a suitable fixed point initialization, employing teacher forcing, and utilizing ADMM to enforce constraints. They argue that the convexity of the loss function is established through a linear + sigmoid structure under cross-entropy, and that the choice of \(c_t\) aligns Hessians across subproblems, enhancing training speed. The experiments demonstrate improvements in metrics such as accuracy of recovered weights, fits to observed activity, and runtime, while emphasizing the need for real-time analysis. However, the applicability of their method to real neuroscientific datasets remains a concern, as the authors highlight the necessity for interventional experiments to validate their model's performance.

### Strengths and Weaknesses
Strengths:
- Good algorithm performance assessed across multiple dimensions (speed, inference accuracy, reconstruction accuracy).
- Clear mathematical foundation for the convexity of the loss function and its implications for training speed.
- Thoughtful comparisons to baseline models and well-articulated discussion on the separability of the loss function, highlighting independent neuron processing.
- Emphasis on real-time and streaming settings, with experiments addressing model mismatch.
- ADMM approach allows for various potential constraints.

Weaknesses:
- Some key sources of potential model error are not addressed (e.g., correlated noise, wrong nonlinearity).
- Lack of empirical results on real datasets, limiting assessment of reconstruction accuracy and raising questions about the model's practical applicability.
- Concerns regarding identifiability and the recovery of synaptic weights are not sufficiently addressed, potentially misleading readers about the method's capabilities.
- The methods section becomes less clear towards the end, with many undefined variables in equations.
- The authors' argument against using public datasets for validation lacks consideration of alternative approaches that could demonstrate the model's effectiveness.
- The paper does not sufficiently clarify the implications of using a non-standard equation form for the networks.

### Suggestions for Improvement
We recommend that the authors improve clarity in the methods section by defining all variables in the main text. Additionally, we suggest including proof of concept results on real neuroscientific datasets to demonstrate the applicability of their method. A clearer discussion on identifiability and the limitations of the CORNN framework in recovering synaptic weights should be added to avoid misconceptions. We also encourage the authors to refine their argument regarding the use of public datasets, considering the potential benefits of testing their model on continuous time series data. Furthermore, it would be beneficial to conduct experiments on real data to compare neural dynamics computed via this method with other methods. Lastly, please ensure that all instances of "miss-match" are corrected to "mismatch" throughout the manuscript, and discuss the implications of using a non-standard equation form while providing a clearer delineation of which technical choices significantly impact the results.