ID: sGrYJQZMQo
Title: Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework called Active Instruction Tuning, which aims to enhance the generalization capacity of instruction tuning models by actively selecting tasks based on a new metric termed "Prompt Uncertainty." The authors demonstrate that their method outperforms baseline strategies on two instruction tuning datasets, NIV2 and Self-Instruct, and introduce a Task Map tool that categorizes tasks based on prompt uncertainty and prediction probability, providing insights into task characteristics.

### Strengths and Weaknesses
Strengths:
- The proposed method of using prompt uncertainty for task selection is innovative and addresses the challenge of efficient task selection in large-scale instruction tuning.
- Comprehensive experiments validate the effectiveness of the proposed method, showing superior performance over existing baseline strategies.
- The Task Map tool offers significant insights into task quality and characteristics, suggesting directions for future research.

Weaknesses:
- There is a gap between the performance of the proposed schema and fully trained instruction tuning, raising concerns about the practicality of training on a limited number of tasks.
- The training process involving an updated task pool lacks clarity regarding whether it employs continual learning or requires retraining, which could be time-consuming.
- The experiments are limited to specific datasets, raising questions about the generalizability of the findings to other types of datasets or real-world scenarios.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the training process, specifically whether the model is retrained with an updated task pool or if it employs a continual learning approach. Additionally, we suggest including comparisons with well-known active learning methods to strengthen the validation of the proposed approach. Addressing the scalability and generalizability of the method across diverse datasets would enhance the robustness of the findings. Furthermore, we encourage the authors to clarify the distinction between task-level and instance-level selection to improve understanding of the selection process. Lastly, incorporating continual learning into the experiments could provide valuable insights into the method's applicability in dynamic task environments.