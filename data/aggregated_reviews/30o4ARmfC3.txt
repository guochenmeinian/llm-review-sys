ID: 30o4ARmfC3
Title: Evolving Connectivity for Recurrent Spiking Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 6, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an application of the evolving connectivity (EC) framework for training recurrent spiking neural networks (RSNNs) using Natural Evolution Strategies (NES). The authors reformulate weight-tuning as a search into parameterized connection probability distributions, achieving performance comparable to deep neural networks while outperforming gradient-trained RSNNs on standard robotic locomotion tasks. However, concerns arise regarding reproducibility, citation of related works, and the empirical nature of the results.

### Strengths and Weaknesses
Strengths:
- The motivation for developing RSNNs focusing on connectivity probability rather than weight magnitudes is compelling, enabling implicit fitting of many samples without overfitting.
- The EC algorithm is clearly explained, demonstrating superior performance and accuracy compared to existing algorithms for targeted robotics benchmarks.
- The approach alleviates gradient estimation issues, making it more suitable for neuromorphic hardware.

Weaknesses:
- The lack of code availability hinders verification of the authors' claims.
- The omission of relevant prior work, particularly Stockl et al. (2021), raises ethical concerns regarding proper citation and contribution perception.
- The study lacks theoretical contributions and does not analyze the impact of varying sample sizes on results, leaving questions about robustness and efficiency unanswered.
- The novelty of the weight-based parameterization and NES methods is unclear, and the energy consumption comparison with other models is not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the manuscript by providing code availability to enhance reproducibility. Additionally, clarify the omission of Stockl et al. (2021) and address the theoretical contributions of the work. It would be beneficial to include an analysis of how varying sample sizes affect results and to perform thorough hyperparameter tuning. We also suggest that the authors compute energy consumption to demonstrate the efficiency of the proposed framework and conduct experiments comparing 1-bit RSNNs utilizing ES/SG algorithms to further validate their claims. Lastly, we encourage minor edits for clarity, particularly regarding the RSNN architecture and hyperparameters.