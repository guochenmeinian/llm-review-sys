ID: 9utMGIbHBt
Title: UDPM: Upsampling Diffusion Probabilistic Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 6, 4, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Upsampling Diffusion Probabilistic Model (UPDM), a novel generative model aimed at reducing the number of diffusion steps required to generate high-quality images, thereby enhancing computational efficiency compared to previous methods. The authors propose a new training and sampling scheme that incorporates downsampling in the forward process and enables simultaneous denoising and upsampling in the Markov process.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, making it easy to follow.
- The motivation for the model is clear, and the proposed method is computationally efficient compared to traditional diffusion models.
- The mathematical derivations support the reliability of the proposed process.

Weaknesses:
- Some symbols are not fully explained upon first use, and the datasets may not fully validate the effectiveness of the method.
- The comparison metrics are limited to FID, and the compared methods are relatively outdated.
- The authors do not clarify how to determine the number of upsampling stages or the resolutions needed for balancing performance and computational cost.
- There is a lack of ablation studies on the loss function, and the experiments are limited to a single resolution of 64x64, leaving scalability unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of symbol definitions and ensure that all datasets adequately validate their method's effectiveness. Additionally, please provide more comprehensive comparisons with up-to-date methods and expand the comparison metrics beyond FID. We suggest including heuristics or empirical studies to guide the selection of upsampling stages and resolutions. Furthermore, we encourage the authors to conduct ablation studies on the loss function and to clarify the computational logic behind steps less than 1 in Table 1. Finally, please provide more details about the network's structure and its inference latency compared to baselines.