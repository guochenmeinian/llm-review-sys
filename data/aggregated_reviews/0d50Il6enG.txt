ID: 0d50Il6enG
Title: Non-parametric classification via expand-and-sparsify representation
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on non-parametric classification using expand-and-sparsify (EaS) techniques. The authors propose two algorithms for sparsification: the first selects the $k$ largest values from a higher-dimensional mapping of the input data, proving its minimax-optimal convergence rate. The second algorithm employs empirical $k$-thresholding under the assumption of a multivariate Gaussian mapping, also demonstrating minimax optimality. The paper further explores binary classification on the unit sphere and provides a method that achieves convergence to the Bayes error at rates of $\mathcal{O}(n^{-\frac{1}{d}})$ and $\mathcal{O}(n^{-\frac{1}{d_0}})$ for data on a $d_0$-dimensional manifold.

### Strengths and Weaknesses
Strengths:
1. The integration of expand-and-sparsify techniques into non-parametric classification is novel.
2. Both proposed algorithms are shown to be minimax-optimal, with rigorous proofs and clear writing.

Weaknesses:
1. The motivation for using expand-and-sparsify in non-parametric classification is insufficiently addressed, lacking clarity on its necessity and applicability.
2. The datasets used in experiments are not adequately explained, and there is no evaluation of the second algorithm or varying values of $k$.
3. The contribution relative to previous work, particularly Dasgupta and Tosh [2020], is marginal, with many ideas derived from that source.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the use of expand-and-sparsify techniques in non-parametric classification, detailing specific scenarios where this approach is beneficial. Additionally, the authors should provide a thorough explanation of the datasets used in the experiments and include evaluations for the second algorithm and different values of $k$. It would also be beneficial to compare the two algorithms directly and specify their performance under various conditions. Lastly, addressing the minor typographical errors and inconsistencies in notation would enhance the overall presentation of the paper.