ID: v30HbVOxJR
Title: You Shall not Pass: the Zero-Gradient Problem in Predict and Optimize for Convex Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 3, 6, 6, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 1, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the zero-gradient problem within the Predict-Then-Optimize (P&O) framework for convex optimization, asserting that the number of active constraints defines the size of the null space of the Jacobian $\nabla_{\hat{w}}x^\ast(\hat{w})$, with normals to the active constraints forming the basis of this null space. The authors propose a surrogate optimization method that mitigates this issue, utilizing a Quadratic Programming (QP) approximation and an r-smoothing technique. They argue that if the loss function gradient lies within the span of these normals, the total gradient becomes zero, extending the known zero-gradient phenomenon beyond linear cases. The effectiveness of the proposed method is evaluated through numerical experiments across two domains, including comparisons with existing methods.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant problem in optimization, specifically the zero-gradient issue in P&O, and introduces a notable theoretical contribution to understanding this problem.
- The proposed method is technically sound, with clear theoretical backing and effective experimental results demonstrating its potential.
- The authors provide a detailed rebuttal to critiques, clarifying the relationship between their work and prior literature, and the organization and clarity of the paper are commendable.

Weaknesses:
- The claim of novelty regarding the zero-gradient theorem is questionable, as similar issues have been acknowledged in prior literature, and the authors' interpretation of existing methods, particularly those by Wilder et al. (2019), may not fully account for alternative approaches that mitigate zero-gradient issues.
- The experiments lack comprehensive comparisons with existing methods, making it difficult to assess the proposed method's relative performance, and there is a need for more comprehensive empirical validation using established test problems.
- The motivation behind the QP approximation lacks clarity, leading to questions about its novelty and effectiveness compared to true objective functions, and the writing quality is inconsistent, with insufficient detail in the abstract and introduction regarding the contributions and engagement with past work.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related work to acknowledge existing solutions to the zero-gradient issue, such as those proposed by Elmachtoub and Grigas (2017) and Wilder et al. (2019). Additionally, the authors should include baseline comparisons in their experiments to demonstrate the effectiveness of their surrogate method against established approaches. Clarifying the motivation behind the QP approximation and the r-smoothing technique, as well as providing insights into the parameter tuning process, would enhance the paper's rigor. We also suggest that the authors conduct further experiments using established test problems from the literature, such as those in the PyEPO library, to validate their method's effectiveness. Finally, addressing the clarity of notation and definitions throughout the paper would improve overall readability.