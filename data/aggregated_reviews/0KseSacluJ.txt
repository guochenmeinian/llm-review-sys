ID: 0KseSacluJ
Title: CoFie: Learning Compact Neural Surface Representations with Coordinate Fields
Conference: NeurIPS
Year: 2024
Number of Reviews: 28
Original Ratings: 3, 7, 6, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CoFie, a novel neural surface representation method that optimizes the representation of local shapes through a coordinate field, significantly reducing shape errors and improving parameter efficiency. The authors propose a hierarchical representation utilizing MLP with quadratic layers to enhance expressiveness, and they utilize a sparse irregular latent cloud/grid of voxel cells decoded via a quadratic MLP decoder. The authors argue that their method is distinct from existing approaches such as DeepSDF and DeepLS, emphasizing its computational efficiency and ability to handle complex shapes. Experimental results demonstrate CoFie's strong generalization capabilities in shape reconstruction tasks, outperforming previous methods in accuracy and efficiency, particularly on small datasets. However, concerns are raised regarding its performance on larger datasets and comparisons with established baselines, as the proposed method does not convincingly outperform them, especially when evaluated on Thingi10K and ShapeNet datasets.

### Strengths and Weaknesses
Strengths:
- CoFie effectively uses a coordinate field to align local shapes, reducing spatial complexity and enhancing MLP-based implicit representations.
- The incorporation of quadratic layers improves the model's ability to capture local shape geometry.
- The authors provide a strong theoretical foundation for the use of quadratic MLPs and demonstrate computational efficiency, achieving comparable performance with fewer training iterations.
- The paper provides extensive theoretical grounding and clear methodological explanations, supported by convincing experimental results.
- The authors show a commitment to addressing reviewer concerns through detailed rebuttals and additional metrics.

Weaknesses:
- The evaluation of CoFie's performance is limited, particularly regarding comparisons to state-of-the-art methods, as it omits recent approaches and relies on a small dataset of only 1000 shapes.
- The choice of baselines for comparison is criticized as outdated, lacking recent methods that utilize latent grids/clouds.
- The claims regarding compactness and parameter efficiency lack thorough evaluation across different dimensions, such as convergence speed and computational footprint.
- The evaluation is limited to Chamfer Distance, with a recommendation for additional metrics to strengthen the assessment of the proposed method.
- Reviewers question the relevance of the performance results, suggesting that strong results on small datasets do not guarantee similar performance on larger scales.
- The paper lacks clarity in distinguishing between generalizable auto-encoding (GAE) and generalizable auto-decoding (GAD) methods, leading to confusion in comparisons.
- Several citations are missing, and the writing lacks clarity in certain areas, making it difficult to understand specific methodological choices.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including comparisons with more recent state-of-the-art methods, such as Neural Kernel Surface Reconstruction and 3D2VS, and by testing on larger datasets like CARLA or ShapeNet-13 to assess scalability and generalization. Additionally, the authors should clarify the theoretical derivations, particularly regarding equation 14, and ensure all relevant citations are included. We also suggest that the authors explore the impact of grid size on model performance and consider using Fourier embeddings as an alternative to quadratic layers. Furthermore, incorporating at least one additional evaluation metric beyond Chamfer Distance would provide a more comprehensive assessment of the method's performance. Finally, we advise the authors to clearly delineate between GAE and GAD methods in their comparisons to avoid confusion and ensure fair evaluations. Enhancing the clarity of the writing, especially regarding the input data and training processes, would significantly benefit the paper's comprehensibility.