ID: a75F45dBHK
Title: Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Orchid, a novel deep learning architecture that addresses the quadratic complexity of traditional attention mechanisms while capturing long-range dependencies and enabling in-context learning. The key innovation is a data-dependent global convolution layer that dynamically adapts its kernel based on the input sequence using a dedicated conditioning neural network. Two simple conditioning networks are designed to maintain shift equivariance in the data-dependent convolution operation. Orchid achieves high expressivity with quasilinear scalability for long sequences, outperforming attention-based models like BERT and Vision Transformers with smaller model sizes. The empirical results demonstrate Orchid's capabilities across language modeling and image classification tasks, particularly in associative recall tasks and on the CIFAR-10 dataset. The conditioning networks utilize $Conv1d()$ operations in both spatial and spectral domains, allowing for effective information capture from local tokens and spectral components. The authors also discuss the implications of using block-diagonal weights in MLP blocks and the differences between Orchid and M2 architectures.

### Strengths and Weaknesses
Strengths:
- The paper introduces an innovative method for increasing the expressivity of subquadratic models for long-range dependencies.
- The Orchid architecture is well-written, with clear descriptions and comprehensive empirical evaluations.
- Empirical results show significant improvements over established models, including BERT and recent architectures like Hyena Hierarchy and Monarch Mixer.
- The design of conditioning networks effectively balances computational efficiency and performance.
- The empirical results and runtime benchmarks highlight Orchid's efficiency compared to Attention and FlashAttention.

Weaknesses:
1. Citation issue: There is a problem with the citation on page 3 that needs clarification and correction.
2. Computational complexity: The authors should clarify how the computational complexity scales with the number of layers, given that the weights are derived from a neural network.
3. Conditioning networks: More details about the two conditioning networks introduced in the manuscript are needed for better understanding, particularly regarding their local versus global roles.
4. Experimental setup: Clarification on which conditioning network is used in the experiments would enhance reader comprehension.
5. Block-diagonal matrices: The absence of an ablation study on the impact of block-diagonal matrices in MLP layers limits understanding of their role in the architecture.
6. Interpretability and explainability: The paper lacks a detailed analysis of the model's interpretability and explainability; techniques to visualize and interpret learned representations should be developed.
7. Experimental comparisons: The choice to compare against CKConv in only one experiment raises concerns about consistency in experimental comparisons.
8. Applicability: The inability to apply the proposed technique to causal models limits its applicability in practical scenarios.
9. Clarity of presentation: Some reviewers expressed dissatisfaction with the clarity of the conditioning network's explanation and the presentation of key figures.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the citation issue on page 3. Additionally, the authors should provide a detailed explanation of how computational complexity scales with the number of layers, considering the data-driven nature of the weights. To enhance understanding, we suggest including more information about the two conditioning networks and specifying which conditioning network was utilized in the experiments. We also encourage the authors to conduct an ablation study on the use of block-diagonal matrices in the MLP layers to justify their design choice. Furthermore, developing techniques to visualize and interpret the learned representations would improve the model's transparency and trustworthiness. We recommend improving the clarity of the conditioning network's role by explicitly stating that while the input is mixed across the entire sequence, the conditioning itself is local. Additionally, we encourage the authors to remove references to unused components, such as Type II conditioning and Cross-Attention alternatives, to avoid hindering future research. Enhancing Figure 2.1 to align with conventional network block illustrations would also aid clarity. Finally, including a section in the appendix to explain the connection between the conditioning network and self-attention could further assist readers in understanding the architecture.