ID: DPhTTeoyjC
Title: LM vs LM: Detecting Factual Errors via Cross Examination
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the LMvLM framework, a method for assessing factual errors in open-domain, closed-book QA answers from LLMs through a cross-examination approach. An "Examiner" questions the "Examinee" about their claims, ultimately determining the accuracy of the original answer. The authors conduct experiments across multiple generative QA datasets and three LLM-pair settings, demonstrating that this conversational method outperforms traditional baselines. The paper provides a thorough analysis of the data annotation process, suggesting that the benchmark could be valuable for future research.

### Strengths and Weaknesses
Strengths:
- The approach to factuality verification is innovative and interpretable.
- Extensive experiments across four datasets confirm the method's generalizability.
- The paper offers a comprehensive analysis of prior work, contextualizing its contributions effectively.

Weaknesses:
- The limited number of LLM-pair experiments restricts exploration of the method's applicability.
- The closed-book, open-domain QA setup may not reflect real-world scenarios where retrieval capabilities are prevalent.
- Some experiments, such as those involving false synthetic claims, appear trivial and do not add significant value.

### Suggestions for Improvement
We recommend that the authors improve the diversity of LLM-pair experiments to explore various configurations and their impact on performance. Additionally, a discussion section addressing the implications of retrieval capabilities in the experimental setup would enhance the paper's relevance. The authors should clarify the necessity of the false synthetic claims experiment and consider whether the number of questions asked correlates with successful outcomes. Finally, we suggest that the authors explore the application of their framework to other tasks involving factuality checking, such as summarization and retrieval-enabled QA.