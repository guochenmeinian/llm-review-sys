ID: 5PrShrKxoX
Title: Transfer Q-star : Principled Decoding for LLM Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents 'Transfer Q*', a decoding strategy designed to align language models with target reward models without fine-tuning. The authors propose a method for estimating the optimal Q* function, which is essential for approximating the RL optimal policy, using Direct Preference Optimization (DPO). The paper details both direct and indirect transfer decoding methods, highlighting their theoretical characteristics and demonstrating state-of-the-art performance on target reward models through extensive experiments. Additionally, the authors introduce a novel approach to estimating the value of Controlled Decoding (CD) based on the SFT policy, emphasizing its capability for decoding-time alignment using next token probabilities without parameter editing. The improvement in Q* estimation through DPO is acknowledged as valuable, although concerns arise regarding the distributional shift from DPO and its implications for model learning.

### Strengths and Weaknesses
Strengths:  
- The approach of aligning to specific values through decoding algorithms without fine-tuning is highly valuable, particularly in estimating the token-level Q* function, which is challenging. Using DPO-trained models as baselines for this estimation is innovative and contributes significantly to the field.  
- The rigorous theoretical analysis characterizing the sub-optimality gap and KL divergence enhances the credibility of the proposed methods.  
- The comprehensive experimental evaluation across various datasets and model architectures showcases the robustness of the method.  
- The authors demonstrate a wide range of experiments and efforts to address review comments, contributing to the paper's development.  
- The theoretical upper bound and Pareto-optimality are effectively shown through experiments.  

Weaknesses:  
- The introduction of Equation 5 is insufficiently detailed; its derivation should be included in the Appendix or referenced appropriately.  
- The explanation of Controlled Decoding (CD) is inaccurate, as it misrepresents the use of $Q^{\pi_{sft}}$. Additionally, Figure 1 requires revision to accurately reflect the contributions of the proposed method.  
- Equation 12 is believed to be incorrect, as it should reference the model $\pi_{ref}$ before exploration rather than the aligned model $\pi_{BL}$.  
- A crucial experiment analyzing the trade-off between KL divergence and the reward model is missing, which is essential for understanding the optimization dynamics in RL.  
- The paper lacks a thorough exploration of hyperparameter sensitivity, particularly regarding the decoding alignment parameter Î±.  
- There is significant divergence from the original distribution, necessitating a thorough analysis of hyperparameter adjustments and trade-offs.  
- The potential for CD to learn a DPO model raises fairness concerns, which require experimental validation to address the mismatch with the base model.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Equation 5 by providing its derivation in the Appendix or citing relevant references. Additionally, please correct the inaccuracies regarding Controlled Decoding and revise Figure 1 accordingly. It is crucial to reassess Equation 12 to ensure it accurately reflects the theoretical foundations. We also suggest conducting experiments to analyze the trade-off between KL divergence and the reward model, as this is vital for understanding the implications of using DPO-trained models. Furthermore, we recommend improving the analysis of hyperparameters $\beta$ to assess the trade-off with divergence comprehensively. Lastly, we suggest experimentally demonstrating and discussing the mismatch that occurs when applying a Q-function learned from DPO to the base model, and a major revision of the overall tone of the current draft and rebuttal is necessary to enhance clarity and depth.