ID: bxltAqTJe2
Title: $\textit{From Chaos to Clarity}$: Claim Normalization to Empower Fact-Checking
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents claim normalization, a task aimed at simplifying complex social media posts into normalized claims. The authors propose CACN, an LLM-based approach that demonstrates strong performance on the curated dataset CLAN, which includes fact-checked posts paired with their normalized claims. The methodology is well-justified, and the evaluation of CACN is thorough, utilizing multiple metrics and models.

### Strengths and Weaknesses
Strengths:  
- The proposed CACN method is innovative and builds upon prior research.  
- The dataset CLAN is clearly developed and replicable, offering significant potential for future research.  
- The paper is well-written, with clarity and cohesion throughout.  
- The evaluation of CACN is robust, with comprehensive performance metrics and a strong error analysis in Section 7.

Weaknesses:  
- Concerns exist regarding the potential contamination of the GPT model's outputs, which may introduce biases in claim normalization.  
- The novelty of the CACN method appears limited, as it combines existing techniques without significant innovation.  
- The description of the baseline methods lacks clarity, particularly regarding finetuning specifics.  
- The human evaluation sample size is too small to substantiate claims of qualitative superiority.  
- Issues related to data leakage and the accuracy of the GFC API output need to be addressed.

### Suggestions for Improvement
We recommend that the authors validate the quality of the GFC API output to ensure the accuracy of the ground truth. Additionally, it is crucial to discuss the data leakage problem and its implications. We suggest clarifying the baseline description, particularly whether finetuning refers to full-parameter or parameter-efficient prompt tuning. To ensure fairness in comparisons, the same method should be applied across different models. We also recommend including results for the proposed CACN method in Table 4 to provide a complete evaluation. Furthermore, the authors should examine the few-shot learning exemplars to explain the performance degradation observed compared to 0-shot learning. Lastly, increasing the sample size for human evaluation would strengthen the claims regarding qualitative superiority.