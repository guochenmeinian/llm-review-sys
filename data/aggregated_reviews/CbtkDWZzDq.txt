ID: CbtkDWZzDq
Title: Ex Uno Pluria: Insights on Ensembling in Low Precision Number Systems
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 4, 5, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel low precision ensembling method, LPE-BSR, which generates an ensemble of models from a single pre-trained model using Bernoulli stochastic rounding. The authors claim that this approach enhances ensemble diversity and improves performance on image classification and MMLU tasks while addressing scalability challenges. Extensive empirical analyses validate the effectiveness of the proposed method, showing that it can outperform traditional models.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and clearly written, making it easy to follow.
- The proposed idea of leveraging quantization noise for ensemble diversity is innovative and has potential.
- Empirical results indicate improved performance for large model ensembles, demonstrating good ensemble diversity.

Weaknesses:
- The paper lacks novelty, primarily presenting an empirical study rather than introducing a fundamentally new method for ensembling or quantization.
- Practical utility is limited due to high inference costs associated with large models, and the performance improvement over full precision models is marginal.
- The related work section is insufficiently detailed, lacking comparisons with existing ensembling techniques such as Deep ensembling, Bayesian methods, and weight averaging.
- Some experimental results are misleading, and additional clarity is needed in figures and tables, particularly regarding error estimates and the differences among subfigures.

### Suggestions for Improvement
We recommend that the authors improve the related work section by providing a more comprehensive literature review and making direct comparisons with existing ensembling methods, including Deep ensembling and Bayesian approaches. Additionally, the authors should analyze the performance of their method in conjunction with weight averaging and include comparisons with Gaussian noise-based ensembles. Clarifying the results in figures and tables, particularly by adding captions and error estimates, would enhance interpretability. Finally, addressing the inference latency concerns and providing detailed compute savings analysis would strengthen the paper's contributions.