ID: BrvLTxEx08
Title: Learning Equilibria in Adversarial Team Markov Games: A Nonconvex-Hidden-Concave Min-Max Optimization Problem
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 4, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 7
Original Confidences: 4, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 4

Aggregated Review:
### Key Points
This paper presents a study on policy gradient methods for computing a Nash equilibrium in adversarial team Markov games (ATMGs), utilizing an occupancy measure-based regularization to address the non-convex minimax optimization challenge. The authors develop an efficient algorithm that enables independent policy gradient steps for team members while the adversary employs regularized steps, proving that this approach can achieve an epsilon-Nash equilibrium with polynomial iteration/sample complexities under mild conditions. The main result, Theorem 3.3, demonstrates that the algorithm requires a polynomial number of samples based on various parameters, and the authors also show the feasibility of computing a near saddle point for certain structured non-convex minimax optimization problems.

### Strengths and Weaknesses
Strengths:
- The authors propose a novel policy gradient method for finding a near Nash equilibrium in ATMGs, achieving polynomial iteration/sample complexities.
- A new regularization technique for the adversarial player is introduced, circumventing the need for linear programming present in existing methods.
- The paper advances the understanding of computing Nash equilibria in ATMGs without known transitions and rewards, addressing limitations of existing methods.
- It introduces innovative concepts, such as regularizing the adversary's value function using the squared l_2 norm of their state visitation distribution, enhancing the convexity of the adversary's value function.
- The paper is well-organized and includes a comprehensive literature review, along with a new convergence result for gradient descent on nonconvex Holder-continuous functions.

Weaknesses:
- The paper suffers from multiple writing issues, including an overly verbose technical overview and a cumbersome preliminary section with confusing notation.
- The motivation for using the policy gradient method and the occupancy measure-based regularization is inadequately explained, making it difficult to assess the broader impact of the work.
- The polynomial iteration/sample complexities are deemed highly sub-optimal, and the method is limited to small state/action spaces, raising scalability concerns.
- Theorem 3.3's conditions are not sufficiently clarified, and its proof lacks readability.
- The algorithm's "inner loop" requires the adversary to perform multiple steps for each team step, contrasting with stronger independent learning methods.
- There is insufficient explanation regarding the updates to r(x) in Algorithm 2, particularly the interpretation of subtracting nu * \hat lambda.
- The paper lacks an example demonstrating where Phi^nu(x) is not weakly convex, which would support the claim that Moreau envelope techniques are inapplicable.
- The claim of solving "the main open question from [65]" is misleading, as no explicit open question is referenced in that paper.
- There are no experimental validations of the proposed method, and algorithm descriptions are confusing, lacking clarity on key functions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in the introduction and technical overview, to better align with NeurIPS standards. The motivation for the policy gradient method and the occupancy measure-based regularization should be explicitly articulated to enhance the paper's impact. Additionally, we suggest providing clearer explanations of the conditions for Theorem 3.3 and presenting its proof in a more accessible manner. The authors should consider including numerical examples to validate their method and clarify the algorithm descriptions, particularly regarding the functions VIS-REG-PG and REINFORCE. Furthermore, we recommend improving the clarity of Algorithm 2 by providing a more detailed explanation of why it functions as intended, particularly regarding the updates to r(x). Including an example that illustrates the non-weak convexity of Phi^nu(x) would substantiate the claim about Moreau envelope techniques. Lastly, addressing the scalability issues and exploring variance-reduction techniques could significantly enhance the practical applicability of their findings, and the authors should clarify or rephrase the assertion regarding the open question from [65] to avoid misleading readers.