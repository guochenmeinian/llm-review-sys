ID: KRlG7NJUCD
Title: DAW: Exploring the Better Weighting Function for Semi-supervised Semantic Segmentation
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 7, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a systematic analysis of the trade-off in semi-supervised semantic segmentation regarding the selection of pseudo-labels. The authors propose a novel method called distribution-aware weighting (DAW), which models the confidence distributions of pseudo-labels and derives an optimal weighting function as a hard step function. The method aims to minimize the negative impacts of inaccurate yet utilized pseudo-labels versus correct yet discarded ones. Experimental results across various benchmarks demonstrate the effectiveness of DAW compared to state-of-the-art methods.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow, providing sufficient details in the supplementary material.
2. The theoretical analysis of the trade-off is sophisticated and offers valuable insights for future research.
3. The proposed method surpasses state-of-the-art alternatives, particularly under limited labeled data conditions.

Weaknesses:
1. The operation of the proposed Distribution Alignment is somewhat confusing, and further clarification on Eq. (12) is needed.
2. The paper lacks a discussion on limitations and broader impacts, and the notation inconsistencies in the equations could lead to confusion.
3. The performance of DAW on larger datasets and its comparison with other methods, such as FlexMatch and FreeMatch, are not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the operation of Distribution Alignment, particularly by providing a more detailed explanation of Eq. (12). Additionally, the authors should explore the potential benefits of re-computing $(\mu^{+}, \sigma^{+})$ and $(\mu^{-}, \sigma^{-})$ over the entire labeled set after each iteration. It would also be beneficial to investigate DAW's performance on larger datasets like COCO-Stuff and ADE20K. Finally, we suggest that the authors include a discussion on the limitations of their work and clarify the normalization factor (\alpha) used in their confidence distribution modeling.