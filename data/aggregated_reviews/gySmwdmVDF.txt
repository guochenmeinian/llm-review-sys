ID: gySmwdmVDF
Title: Query-based Temporal Fusion with Explicit Motion for 3D Object Detection
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 4, 6, 4, 5, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 5, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Query-based Temporal Fusion Network (QTNet) that enhances object queries from previous frames to improve current object queries through a Motion-guided Temporal Modeling (MTM) module. The proposed method achieves notable performance improvements over BEV-based and proposal-based techniques on the nuScenes dataset while maintaining negligible computational costs.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel temporal fusion paradigm leveraging query-based features.
2. The framework significantly enhances the query-based baseline with minimal runtime cost.
3. The clarity of presentation and figures aids in understanding the proposed concepts.

Weaknesses:
1. The novelty is limited as the approach builds on existing DETR-based detectors, sharing similarities with methods like CenterFormer.
2. The experiments lack comprehensiveness, particularly in comparing against other state-of-the-art methods and datasets.
3. The performance improvements are modest, and the analysis of scenarios where the method excels is insufficient.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including missing references such as SpatialDETR and DETR3D. Additionally, we suggest conducting experiments to validate the hypothesis regarding the necessity of background propagation in BEV-based methods. Clarifying Section 3.2 and addressing the questions raised about the transformer architecture, cost matrix, and query alignment would enhance understanding. Finally, providing a more thorough analysis of performance across different scenarios and datasets, as well as including ablation studies to substantiate the benefits of the MTM design, would strengthen the paper.