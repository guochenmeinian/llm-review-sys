ID: 3csuL7TVpV
Title: Decoding-Time Language Model Alignment with Multiple Objectives
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 7, 5, 8, 7
Original Confidences: 3, 2, 4, 3

Aggregated Review:
### Key Points
This paper presents Multi-Objective Decoding (MOD), a decoding-time alignment method for language models (LMs) that aligns them with multiple objectives simultaneously. MOD combines models aligned for individual rewards and allows for any weightings, including negative ones. It utilizes a common form among f-divergence regularized alignment approaches to derive an efficient decoding strategy that selects the next token from an algebraic combination of predicted probabilities. The authors theoretically demonstrate the sub-optimality of existing methods and establish optimality guarantees for MOD. Empirical results indicate a 12.8% overall reward improvement over a parameter-merging baseline when optimizing for three objectives.

### Strengths and Weaknesses
Strengths:
* **Flexibility in Objectives Alignment**: MOD effectively aligns LMs with multiple objectives without retraining, accommodating various user needs.
* **Efficiency and Simplicity**: The algorithm operates efficiently at decoding time, requiring only the manipulation of probabilities.
* **Theoretical Robustness and Empirical Validation**: The method is supported by a strong theoretical foundation and empirical evidence demonstrating its effectiveness across tasks.

Weaknesses:
* The requirement to load multiple models concurrently can be computationally intensive, potentially limiting scalability with more objectives or larger models.
* The baselines used for comparison are weak, particularly as some do not outperform the best individual model across experiments.
* The presentation could be simplified, particularly regarding the Legendre transformation and its implications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the reformulation using Legendre transformation in Section 4.2, perhaps by providing a more accessible derivation for the case of KL regularization. Additionally, including more robust baselines, such as weighted averaging or voting, would strengthen the comparative analysis. The authors should also address the computational overhead of running multiple policies at decoding time and discuss potential negative impacts or failure modes, especially in scenarios with conflicting objectives. Finally, providing guidelines for setting and adjusting preference weightings would enhance the practical applicability of MOD.