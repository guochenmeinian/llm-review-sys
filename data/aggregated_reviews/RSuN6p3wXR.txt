ID: RSuN6p3wXR
Title: APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an attention prompt tuning method for parameter-efficient tuning of pre-trained language models (PLMs). The authors propose that existing prompt tuning can be viewed as a special case of attention prompt tuning, which incorporates learnable query, key, and value prompts into the attention layer of the Transformer. Extensive experiments on the SuperGLUE benchmark demonstrate the method's effectiveness across various tasks and model scales.

### Strengths and Weaknesses
Strengths:
1. Novel method that offers a unified form of prompt tuning.
2. Insightful analysis connecting the proposed tuning with previous methods.
3. Comprehensive experiments validating the proposed approach.
4. Clear writing and thorough discussion.

Weaknesses:
1. Lack of detailed comparisons with previous methods and important baselines.
2. Insufficient explanation regarding the calculation of Q in the context of updated features.
3. Missing training time comparisons for other baselines.

### Suggestions for Improvement
We recommend that the authors improve the paper by including a detailed comparison among prompt tuning methods, specifically addressing their trainable parameters and training times. Additionally, a comparison with LoRA and Adaptor, as widely used parameter-efficient tuning methods, should be included. Furthermore, clarifying the reasoning behind the calculation of Q and reporting the training times of other baselines in Table 8 would enhance the paper's rigor. Lastly, we suggest correcting the notation in Figure 3 (c) from "Matmul" to "MatMul."