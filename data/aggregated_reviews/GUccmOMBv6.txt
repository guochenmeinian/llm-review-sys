ID: GUccmOMBv6
Title: CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FRHOzen (Frozen Reducible Hold Out Loss), a novel data selection method for targeted pre-training of language models, utilizing an empirical Bayes-inspired approach to derive a computationally efficient selection criterion based on the relative loss values of two auxiliary models. The authors evaluate FRHOzen on language modeling tasks, including domain adaptation from C4 to Books and multiple-choice question answering, demonstrating that it consistently outperforms training on eight times more randomly selected data and scales effectively across model sizes.

### Strengths and Weaknesses
Strengths:
- The method is intuitive and well-motivated, leveraging a Bayesian perspective while remaining practical.
- The authors provide a thorough analysis of computational costs and demonstrate the method's effectiveness in improving downstream task performance.
- Empirical results suggest favorable scaling properties, with smaller models yielding improvements when used to train larger models.

Weaknesses:
- The evaluation primarily focuses on a decoder-only LM (olmo), raising concerns about the applicability of results to real-world scenarios and other model architectures.
- The assumption of a pre-set budget n in Algorithm 1 is questioned, as predicting the optimal n during pre-training is challenging.
- The method's reliance on prior knowledge of downstream tasks limits its versatility, and the lack of diversity in selected data may lead to overfitting.

### Suggestions for Improvement
We recommend that the authors improve the rigor of the derivation in Section 2.1, Bayesian Data Selection, to provide a more formal understanding of Bayesian optimization. Additionally, we suggest broadening the domain transfer experiments to include multiple downstream target distributions to validate the method's effectiveness across varied scenarios. Testing the method on larger models beyond 1.2B parameters would also enhance its scalability demonstration. Finally, incorporating a diversity regularizer term could address concerns regarding data diversity and potential overfitting.