ID: c5dRV9tA3K
Title: EMMA-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual Representation Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 7, 7, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper proposes EMMA-X, an EM-like multilingual pre-training algorithm that utilizes both supervised parallel data and unsupervised monolingual data to train an encoder model capable of producing language-agnostic representations. The method employs a GMM classifier, pre-trained with parallel data, to generate similarity rankings between input sentences, which are then used to train the encoder model. The authors validate their approach through experiments on 12 cross-lingual tasks, achieving higher scores compared to unsupervised models like XLM-R.

### Strengths and Weaknesses
Strengths:
- The methodology appears original, particularly the rank-based classifier.
- The paper is well-structured, with clear presentation and method description.
- Extensive experiments demonstrate the effectiveness of the proposed method, supported by a theoretical analysis.

Weaknesses:
- The evaluation and comparison methodology is flawed, as the paper compares a semi-supervised method with unsupervised models, invalidating the results in Table 1.
- The choice of GMM classifier lacks justification, and the selection of four semantic ranks appears arbitrary without experimental support.
- Notable baselines like LASER and CRISS are not mentioned or compared, despite their relevance to language-agnostic representation.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by comparing their method with relevant baselines such as LASER and CRISS to provide a more comprehensive analysis. Additionally, the authors should justify the choice of the GMM classifier and the number of semantic ranks, possibly through experiments that explore alternative classifiers and configurations. It would also be beneficial to move model details from the appendix to the main text for better clarity. Lastly, addressing the limitations of the model regarding the dependency on parallel data for initialization would strengthen the paper.