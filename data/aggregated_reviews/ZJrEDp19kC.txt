ID: ZJrEDp19kC
Title: Visually Grounded Continual Language Learning with Selective Specialization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on module specialization for continual learning in vision-and-language contrastive learning, introducing the Lifelong Language Compositions (LILAC) benchmark with two synthetic datasets. The authors propose an alternating learning scheme of adaptation and consolidation phases, testing various module selection strategies across two architectures (FiLM and cross-modal transformer). Experimental results indicate that gradient-based importance scores effectively select modules for specialization, outperforming standard baselines like EWC and ER.

### Strengths and Weaknesses
Strengths:
- The study addresses selective specialization in vision-and-language models, contributing to continual learning.
- It introduces two datasets with compositional structures.
- The experiments explore multiple module selection strategies, demonstrating that the best strategy surpasses common baselines.
- The paper is generally well-written, with findings connected to existing literature.

Weaknesses:
- The novelty is limited; the motivation for the datasets is unclear, and the paper lacks a compelling takeaway.
- Some important baselines, such as EWC and ER, are not the best representatives of continual learning methods, and comparisons with more recent methods are missing.
- The writing, particularly in the results section, is hard to follow, with unclear notation and insufficient explanations connecting results to conclusions.
- The synthetic nature of the datasets raises concerns about the generalizability of the results to real-world scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in the results section, to ensure that figures and their interpretations are straightforward. Additionally, the authors should provide a clearer motivation for the creation of the two datasets and discuss their relevance to continual learning. It would be beneficial to include comparisons with more recent continual learning methods to strengthen the argument for specialization. Furthermore, we suggest incorporating metrics that reflect learning over time to align with the continual learning framework. Lastly, clarifying the definitions and roles of terms used in the paper, such as "module," would enhance reader comprehension.