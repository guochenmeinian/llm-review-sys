ID: rxsCTtkqA9
Title: Matrix Compression via Randomized Low Rank and Low Precision Factorization
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 7, 5, 5, 6, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 2, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to low-rank matrix factorization under a bit budget constraint, focusing on quantization and sketching techniques. The authors propose an algorithm that utilizes Gaussian random projections to generate low-rank factors, L and R, such that the product LR approximates a given matrix A in the Frobenius norm. The incorporation of sketching into the quantization process is shown to yield improved results compared to naive methods like rounding SVD factors. Additionally, the authors introduce an optimization framework called LPLR, which aims to enhance data approximation through low-rank representations and can outperform naive quantization under certain conditions, particularly when a balance between target rank and bit budget is achieved. The paper acknowledges the need for empirical comparisons with QRP and discusses the implications of using the entire data matrix for optimization in an online setting. Theoretical guarantees and empirical evaluations support the effectiveness of the proposed methods, while the authors clarify the error dependency on $d/m$.

### Strengths and Weaknesses
Strengths:
- The problem addressed is significant, as low-rank approximations and low-precision techniques are increasingly relevant for compressing large neural networks.
- The method leverages the "flattening" property of Gaussian sketches, leading to improved quantization results.
- The authors effectively address reviewer concerns and clarify the relationship between LPLR and existing methods.
- The paper demonstrates a clear understanding of the trade-offs involved in low-rank approximation and quantization.
- The commitment to cite relevant QRP papers enhances the contextual framework of the research.
- The paper is well-organized, with clear communication of ideas and rigorous theoretical analysis.
- Empirical results convincingly demonstrate the advantages of the proposed algorithm across various tasks.

Weaknesses:
- The contribution, while valuable, lacks novelty as it primarily combines existing techniques without introducing fundamentally new concepts.
- The reliance on the entire data matrix for optimization in the online procedure raises questions about practical applicability.
- The paper lacks empirical results comparing LPLR with QRP, which could strengthen its claims.
- The experimental settings and comparisons could be clearer, particularly regarding the performance of the proposed method versus other algorithms.
- The explanation of the error dependency on $d/m$ could be more rigorously detailed.
- Some references in the Related Works section are deemed inappropriate, and the paper lacks a thorough discussion of limitations and potential societal impacts.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental settings and provide a more detailed comparison with a broader range of existing sketching and matrix factorization algorithms, including empirical comparisons with QRP to substantiate the advantages of LPLR. Additionally, the authors should address the following points:
- Include lower bounds on the trade-off between approximation accuracy and bit budget.
- Clarify the running time of experiments and discuss whether the bit bounds imply actual savings in memory usage.
- Revise the abstract to include the approximation relationship A â‰ˆ LR.
- Define terms like "saturated" and "unsaturated" for clarity.
- Improve the discussion surrounding the online optimization setting to clarify its best application scenarios, particularly regarding streaming data.
- Add a proof and revise the description of the error dependency on $d/m$ to enhance clarity and rigor.
- Consider adding discussions on quantization-aware training and its potential benefits.
- Ensure that all technical details, such as the choice of sketch size m, are clearly articulated in both the main text and the experimental section.