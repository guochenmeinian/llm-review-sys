ID: G7NZljVOol
Title: L-TTA: Lightweight Test-Time Adaptation Using a Versatile Stem Layer
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 5, 5, 8, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents L-TTA, a novel lightweight approach to test-time adaptation (TTA) that focuses on minimizing uncertainty in the stem layer of deep neural networks. The method employs a Domain Embedding Layer (DEL) utilizing discrete wavelet transforms and a Gaussian Channel Attention Layer (GCAL) to achieve this goal, significantly reducing memory and computational requirements compared to existing TTA methods. The authors clarify that their method enhances model performance under domain shifts by fine-tuning the first convolutional layer, termed the stem layer, which is crucial for leveraging the performance of subsequent frozen layers. They demonstrate competitive performance on standard TTA benchmarks, including CIFAR-10-C and CIFAR-100-C, while using considerably less memory. The authors provide extensive experimental results, including memory usage and accuracy comparisons with state-of-the-art methods, and compare their results with existing methodologies such as T3A and LAME.

### Strengths and Weaknesses
Strengths:
- The paper proposes a fundamentally new approach to TTA, emphasizing stem-layer adaptation and uncertainty minimization, which enhances its applicability in real-world scenarios.
- L-TTA achieves competitive results on benchmark datasets with significantly reduced memory overhead.
- Comprehensive experimental results are provided, showcasing competitive performance against existing methodologies.
- The organization of the manuscript and clarity in explanations enhance the understanding of the proposed approach.

Weaknesses:
- The method relies on frequent resets (every 10 iterations), raising concerns about its stability and effectiveness for continual adaptation, particularly for complex datasets like ImageNet-C.
- The paper lacks statistical significance, as it does not report error bars and experiments are run on a single seed.
- The experimental section could benefit from clearer comparisons with other TTA methods, particularly regarding memory usage, and some tables require additional context to fully convey the significance of the results.
- Experimental details are incomplete and inconsistent, including the omission of CIFAR100-C results and limited comparisons with relevant baselines like EcoTTA.
- The method section is confusing, particularly regarding the GCAL and DEL components, which lack clarity in their descriptions and operational details.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the method section by explicitly detailing the loss functions used during pre-training and TTA, and clarifying the roles of GCAL and DEL. Additionally, we suggest conducting ablation experiments to analyze the impact of the warm-up strategy on performance and exploring alternative stabilization techniques to enhance continual adaptation capabilities. It would also be beneficial to include statistical significance in the experimental results and provide a comprehensive comparison with other lightweight TTA methods, such as MECTA. Furthermore, we recommend improving the clarity of the experimental section by integrating results for EcoTTA where applicable and ensuring that comparisons with other methodologies are straightforward. Finally, we encourage the authors to discuss the broader applicability of L-TTA beyond vision applications and to ensure that all experimental results are consistently presented across datasets.