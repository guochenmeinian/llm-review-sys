ID: ubzNoJjOKj
Title: HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution
Conference: NeurIPS
Year: 2023
Number of Reviews: 29
Original Ratings: 7, 7, 7, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HyenaDNA, a genomic foundational model that leverages the Hyena architecture to pretrain on the human reference genome. The model can handle ultra-long sequences (up to 450k) at single nucleotide resolution with significantly fewer parameters than previous models. The authors introduce a warm-up technique to stabilize training and employ a soft prompting method for downstream tasks, demonstrating the framework's efficacy through various experiments. Additionally, the paper analyzes the finetuning of the AttnDNA model on Nucleotide Transformer datasets, revealing that AttnDNA significantly underperformed compared to HyenaDNA, suggesting the Hyena operator contributes to performance gains. The authors acknowledge the shallow Transformer architecture of AttnDNA (2 layers) as a potential reason for its inferior performance, especially on small datasets. The paper also discusses the implications of k-mer tokenization, the computational efficiency of fine-tuning, and the challenges in reproducing results across different models and datasets. Furthermore, it provides a comprehensive analysis of the HyenaDNA model, highlighting its performance in genomic applications and addressing discrepancies with previous works, particularly the Nucleotide Transformer (NT). The authors propose a clear methodology for pretraining and finetuning, providing code and instructions for reproducibility, and emphasize the model's efficiency, achieving competitive results despite its smaller size compared to larger models.

### Strengths and Weaknesses
Strengths:
1. The work addresses the critical issue of developing foundational models for DNA sequences.
2. HyenaDNA's ability to train on ultra-long sequences allows for capturing long-range dependencies, a significant advancement over prior models limited to shorter sequences.
3. The paper provides a thorough analysis of the performance differences between AttnDNA and HyenaDNA, highlighting the impact of model architecture.
4. Experimental results appear promising, showcasing the model's capabilities.
5. The manuscript is well-written and clear, effectively addressing reviewer concerns and providing detailed responses.
6. The ability to reproduce results using the provided code demonstrates the robustness of the model.

Weaknesses:
1. Certain sections, particularly 3.1, may be challenging for readers unfamiliar with the previous Hyena manuscript, with undefined terms and unclear references.
2. The evaluation of short sequences may not align with the paper's focus on long-range modeling, and comparisons with models like Enformer are lacking.
3. The paper does not provide sufficient insights into the model's performance compared to baselines, particularly regarding efficiency and fine-tuning processes.
4. There are inconsistencies in reported performance metrics, particularly regarding the efficiency of fine-tuning and the number of parameters updated.
5. The reliance on short-range benchmarks may not adequately demonstrate the model's capabilities in long-range tasks.
6. There is a noted lack of long-sequence benchmarks, which raises questions about the model's performance in that context.
7. The model's effectiveness relative to its size remains somewhat unclear, as it contradicts general trends in deep learning.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 3.1 by defining all relevant terms and ensuring all references are clear. Additionally, including a comparison with the Enformer model would provide a more comprehensive evaluation of HyenaDNA's performance. We suggest that the authors elaborate on the fine-tuning process in Section 4.2 and provide more technical details about the baseline CNN model used in the GenomicsBenchmark. Furthermore, it would be beneficial to clarify the efficiency of fine-tuning HyenaDNA compared to Nucleotide Transformer and to address the missing dataset in the comparison with Nucleotide Transformer. We also recommend improving the clarity of the computational efficiency claims, particularly regarding the differences in parameter updates between models. Providing more detailed information on the pre-training process and the specific configurations used in the Docker image would enhance reproducibility. Including a comparison of training times between scratch training and pre-trained models in the manuscript would be beneficial. Finally, we encourage the authors to conduct ablation studies to explore the fundamental differences between HyenaDNA and existing models, and to address potential concerns about data leakage and the stability of pre-training to strengthen the paper's credibility.