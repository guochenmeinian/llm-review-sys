ID: tWkL7k1u5v
Title: Improving Equivariant Model Training via Constraint Relaxation
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 7, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to relax the equivariance constraints in equivariant networks by incorporating a non-equivariant residual in the network weights. The proposed framework can be applied to various equivariant architectures, such as vector neurons and SE(3)-equivariant GNNs, and can also extend to approximately equivariant networks by modulating the strength of the unconstrained weights. Experimental results indicate moderate performance improvements across several tasks and architectures.

### Strengths and Weaknesses
Strengths:  
The paper addresses significant optimization challenges in equivariant neural networks, providing a clear and motivated methodology. The framework's general applicability to various equivariant architectures is well-demonstrated through multiple examples.

Weaknesses:  
* Formatting issues hinder readability, particularly with citation parentheses.  
* Theoretical contributions are lacking; the paper does not adequately explain the optimization challenges faced by equivariant networks or how the proposed framework addresses these issues. Previous works have established error bounds for approximately equivariant networks, yet this paper lacks sufficient analysis of equivariance error or learning results.  
* Experimental results show only minor performance gains, and some models appear not to have converged after 200 epochs, raising concerns about the significance of the findings. The small differences in performance metrics, such as in Table 1, lack error bar analysis for verification.

### Suggestions for Improvement
We recommend that the authors improve the theoretical motivation for their approach, specifically addressing the optimization challenges associated with equivariant networks. It would be beneficial to include a detailed analysis of the equivariance error and how to interpret learning results in the context of imperfect data symmetry. Additionally, we suggest that the authors conduct further experiments to ensure models converge and provide full results, including error bars for performance comparisons. Clarifying the scheduling choice of $\theta$ and exploring its sensitivity, as well as the regularization coefficient $\lambda_\text{reg}$, would enhance the paper's rigor. Finally, consider discussing the potential for different weighing coefficients for each network layer in the Lie derivative regularization to account for error accumulation.