ID: Re2NHYoZ5l
Title: Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for environment-aware affordance prediction in articulated object manipulation, focusing on both object-centric affordance and environmental constraints. The authors propose a contrastive learning method and a robot-target conditioned occlusion field to enhance data efficiency and generalization to complex occlusion scenarios. The evaluation demonstrates improvements over previous methods in simulated environments, with real robot demonstrations validating the utility of the learned affordances.

### Strengths and Weaknesses
Strengths:
- The work addresses significant limitations in prior affordance learning, allowing for per-point affordance prediction in complex environments.
- Adequate quantitative and qualitative experiments support the effectiveness of the proposed method, with ablation studies justifying key components like the occlusion field and contrastive learning.
- The novel task design that incorporates environmental constraints is insightful and potentially beneficial for downstream robotic tasks.

Weaknesses:
- The paper lacks a clear definition of "per-point affordance" and related terms, making the motivation difficult to grasp without prior knowledge of related works.
- Key aspects of the problem formulation, such as the scene point cloud and its segmentation, are inadequately explained.
- The description of the robot-target conditioned occlusion field is confusing, particularly regarding the construction of the open and connected set $S$ and its relationship to the scene point cloud.
- The action space is limited to pushing and pulling, and the test scenarios may not be sufficiently challenging.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions for "affordance" and "interactable" in the introduction or section 3, ideally with concrete examples. Additionally, the authors should provide a detailed explanation of the scene point cloud and its segmentation process in both simulated and real-world contexts. The description of the occlusion field in section 4.3.1 needs to be reworked for clarity, particularly regarding the construction of $S$ and the norms used. Furthermore, expanding the action space beyond pushing and pulling and providing more diverse test scenarios would enhance the robustness of the evaluation. Finally, addressing the questions raised regarding data preprocessing, robot positioning, and inference time in real-world applications would strengthen the paper's contributions.