ID: XqcXf7ix5q
Title: Locality-Aware Generalizable Implicit Neural Representation
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 6, 7, 3, 5, 5, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 2, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for generalizable implicit neural representations (INRs) that enhances locality awareness by combining a transformer encoder with a locality-aware INR decoder. The authors propose that their method utilizes localized latent vectors to improve image reconstruction and novel view synthesis, allowing for the synthesis of unseen views of 3D objects based on limited observations. The method employs selective token aggregation and multi-band feature modulation to capture local information and improve the expressive power of modulation. The shared decoder is updated through backpropagation, while localized latents are specific to each data instance. Experiments demonstrate the effectiveness of the proposed approach in image reconstruction, few-shot novel view synthesis, and conditional image synthesis, achieving state-of-the-art performance.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important problem in implicit neural representation with a clear motivation.
- The proposed method is well-structured and easy to follow.
- Extensive experiments validate the effectiveness of the method across multiple benchmarks, demonstrating potential improvements in high-resolution image reconstruction and novel view synthesis tasks.
- The authors provide detailed explanations of the architecture and parameterization of their model, clarifying the roles of shared and localized components.
- The writing is generally clear, and the results are well-presented.

Weaknesses:
- The concept of "locality" is not formally defined, lacking intuitive examples or formulations.
- There is confusion regarding the definition of generalizable INRs, with some reviewers arguing that the method does not align with established definitions of INRs.
- The focus on pure MLPs neglects recent hybrid neural representations; a discussion on this would be beneficial.
- The calculation of model size and compression ratios has been criticized, as it may not accurately reflect the architecture's complexity.
- Some technical details, such as the impact of hyperparameters and model size, are inadequately addressed.
- The evaluation metrics primarily rely on PSNR, with limited exploration of local detail metrics.
- The paper's contributions are perceived by some as overlapping with existing autoencoder frameworks, raising questions about its novelty.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of "locality" by providing formal formulations and intuitive examples. Additionally, we suggest enhancing the clarity of their definitions and terminology regarding INRs to address the misunderstandings noted by reviewers. A discussion on the implications of using pure MLPs versus hybrid representations should be included. We recommend revising the calculations of model size and compression ratios to accurately reflect the architecture's complexity. It would be beneficial to provide clearer distinctions between their framework and traditional autoencoders, particularly in the context of novel view synthesis. We suggest enhancing the analysis of model efficiency and runtime, as well as including more diverse qualitative results to illustrate the model's limitations. Finally, we encourage the authors to clarify the role of hyperparameters and provide more detailed evaluations beyond PSNR to emphasize local details in their results.