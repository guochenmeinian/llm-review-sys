ID: 1kmIDTfQ4N
Title: BERT Has More to Offer: BERT Layers Combination Yields Better Sentence Embeddings
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new method called BERT-LC (BERT Layers Combination) for obtaining sentence embeddings by combining specific layers of BERT-based models in an unsupervised manner. The authors argue that different layers of BERT capture various features, and their method outperforms baseline BERT and other models across seven semantic textual similarity (STS) datasets and eight transfer tasks. They also introduce an algorithm that accelerates the identification of optimal layer combinations and integrate this method with the CLS pooling head.

### Strengths and Weaknesses
Strengths:
- The proposed method is concise and effective compared to selected baselines.
- The authors conduct extensive experiments on multiple pre-trained language models, demonstrating the method's effectiveness.
- The approach is simple to understand and shows consistent improvements across various datasets and tasks.

Weaknesses:
- The writing lacks clarity, particularly in the abstract and certain mathematical sections, making it difficult to follow.
- The proposed method is based on a relatively simple modification of the BERT model, raising questions about its novelty.
- The evaluation is limited to a few standard tasks, which may not sufficiently demonstrate effectiveness in real-world scenarios.
- The computational complexity is higher due to the need for retraining classifiers for each layer combination.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in the abstract and complex mathematical sections, to enhance readability. Additionally, we suggest providing a detailed comparison with fine-tuned models, especially on datasets like SST, to substantiate claims of state-of-the-art performance. The authors should also consider including a thorough analysis of interpretability and a comprehensive time comparison of different methods in Appendix A. Finally, we advise careful formatting of Algorithms 1 and 2 to improve their readability.