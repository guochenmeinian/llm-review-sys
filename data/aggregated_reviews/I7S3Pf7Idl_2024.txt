ID: I7S3Pf7Idl
Title: Semantic Membership Inference Attack against Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 7
Original Confidences: 4, 4

Aggregated Review:
### Key Points
This paper presents the Semantic Membership Inference Attack (SMIA), an advanced version of traditional Membership Inference Attacks (MIAs) aimed at large language models (LLMs). SMIA utilizes the semantic content of inputs and their perturbations to enhance inference accuracy, effectively determining whether specific data points were part of the modelâ€™s training set. The authors propose a neural network-based method to evaluate model behavior with semantically perturbed inputs, demonstrating improved performance over existing MIA methods. Empirical evaluations on models like Pythia and GPT-Neo indicate that SMIA achieves superior detection of training data through semantic understanding, evidenced by high AUC-ROC performance and TPR at low FPR settings.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and logically organized.
- Extensive evaluations of various MIA approaches applied to LLMs are conducted.
- The SMIA method for distinguishing between members and non-members is relatively lightweight.

Weaknesses:
- SMIA's assumption of access to loss values or log probabilities of the target model limits its practical applicability, as adversaries may not have such access in real-world scenarios.
- The method's reliance on generating and evaluating numerous perturbed neighbors can be computationally expensive, particularly for larger datasets or models, affecting scalability.
- The evaluation is constrained to a 130-150 word limit, which may not adequately capture performance variations based on word length.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the precautions taken against overlap between members and non-members datasets, particularly regarding the deduplication techniques and the Wikipedia cutoff dataset. Additionally, we suggest that the authors compute metrics such as the MAUVE score to assess overlap more effectively. Furthermore, we encourage the authors to expand their evaluation to include varying word lengths and to consider testing their work on the WikiMIA dataset, which serves as a benchmark for MIA attacks.