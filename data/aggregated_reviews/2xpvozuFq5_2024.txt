ID: 2xpvozuFq5
Title: OneSAM: Modality-agnostic for segment anything model in medical images
Conference: thecvf
Year: 2024
Number of Reviews: 3
Original Ratings: 5, 6, 6
Original Confidences: 4, 3, 5

Aggregated Review:
### Key Points
We note that the manuscript titled **"OneSAM: One model for segment anything model in medical images on Laptop"** introduces a novel model, OneSAM, which integrates LiteMedSAM with image encoder adapters and the SAM-HQ mask decoder. The authors claim that OneSAM is trained on the challenge dataset and evaluated against baseline models. However, we find the manuscript lacking in details necessary for reproducibility, particularly in the abstract, introduction, methodology, and results sections.

### Strengths and Weaknesses
Strengths of the manuscript include a step-by-step description of OneSAM and the techniques employed, such as the SAM-HQ mask decoder and data augmentation. However, weaknesses are evident in the insufficient elaboration on the ablation study results, unclear performance metrics, and a lack of detailed explanations regarding the training of the adapter and the SAM-HQ mask decoder. Additionally, we observe that the speed of the model is a significant drawback, as it is slower than the baseline, and sections 4.2, 4.3, and 4.4 lack depth in analysis.

### Suggestions for Improvement
We suggest that the authors enhance the **abstract** by including a summary of their proposed method and results. In the **introduction**, they should clarify the differences between MedSAM and LiteMedSAM and improve the transition between paragraphs. The **methodology** section requires more details on input data dimensions, a brief description of LiteMedSAM, and clarification of acronyms in equations. In the **experiments** section, we recommend specifying the tasks referred to, providing clearer training protocol details, and including validation scores. Lastly, we advise that the authors elaborate on the ablation study results and the reasons for varying running times between 2D and 3D objects, as well as improve the analysis in sections 4.2, 4.3, and 4.4.