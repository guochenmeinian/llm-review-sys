ID: ohixFcMzEr
Title: Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models
Conference: AAAI
Year: 2023
Number of Reviews: 2
Original Ratings: 6, 3
Original Confidences: 4, 5

Aggregated Review:
### Key Points
This paper presents research on narrow AI for answering questions using domain-specific Knowledge Graphs (KGs) to reduce hallucinations in large language model (LLM) responses. The authors propose a methodology that leverages logical programming to enhance the factuality of LLMs. The experimental results are reported on the standard MetaQA dataset, which includes 134,741 facts and 9 relations.

### Strengths and Weaknesses
Strengths:  
- The experimental results utilize the MetaQA dataset, demonstrating the ability to reason over a substantial knowledge graph.  
- The focus on reducing hallucinations in LLM responses and improving factuality through KGQA is a relevant contribution.  

Weaknesses:  
1. The manuscript lacks novelty; justification for the originality of the work is needed.  
2. An architecture diagram of the proposed methodology is missing, which should highlight the authors' contributions.  
3. The performance validation could be enhanced by including the MFAQ dataset alongside MetaQA, despite its challenges.  
4. The manuscript does not sufficiently differentiate itself from existing works, particularly from Mihindukulasooriya et al. (2023).  
5. The mathematical representation is weak; the authors should refine equations and provide more logical representations.  
6. Evaluation measures such as BLEU, METEOR, ROUGE, Exact Match, accuracy (A), hallucination rate (H), and missing rate (M) need further elaboration.  
7. The writing lacks clarity, cohesion, and connectivity, necessitating improvements for better reader comprehension.  
8. The related work section is underdeveloped and should be updated with recent research.

### Suggestions for Improvement
We recommend that the authors improve the novelty justification of their work and include an architecture diagram to clarify their contributions. To validate performance, we suggest incorporating the MFAQ dataset and addressing the challenges it presents. The authors should refine the mathematical representations and enhance the evaluation metrics by including BLEU, METEOR, ROUGE, Exact Match, accuracy (A), hallucination rate (H), and missing rate (M). Additionally, we advise improving the clarity and cohesion of the writing to facilitate reader understanding. Lastly, we recommend updating the related work section with recent publications to strengthen the manuscript's context.