ID: jsgYYXaSiS
Title: Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel test-time adaptation method for vision-language models (VLMs) called Dual Prototype Evolving (DPE). The authors propose a mechanism that evolves two sets of prototypes—textual and visual—during test time to accumulate task-specific knowledge and enhance multi-modal representations. The method optimizes learnable residuals based on alignment and self-entropy losses, demonstrating effectiveness across 15 benchmark datasets, outperforming previous state-of-the-art methods in both performance and computational efficiency.

### Strengths and Weaknesses
Strengths:
1. The writing is clear, and the motivation for the method is well-articulated.
2. The experimental results are comprehensive and validate the proposed approach effectively.
3. The introduction of dual prototypes for evolving task-specific knowledge is a novel concept that enhances the generalization capabilities of VLMs.

Weaknesses:
1. The core idea lacks novelty, as it combines existing techniques that have been explored in prior works.
2. The performance gains appear marginal, raising concerns about the method's efficacy being dependent on extensive hyperparameter tuning.
3. The complexity of the model, particularly with learnable residuals and dual prototype evolution, may pose implementation challenges.
4. The efficiency comparison with backpropagation-free methods is not comprehensive, and the computational overhead introduced by the dual prototype evolution mechanism needs further clarification.

### Suggestions for Improvement
We recommend that the authors improve the novelty aspect by providing a detailed comparison with concurrent works such as DMN-ZS and TPS, including core idea design and performance metrics. Additionally, a thorough analysis of hyperparameters, particularly the impact of the (top-)M parameter, should be included to guide robust performance in test-time scenarios. The authors should also discuss the implications of alignment loss on performance, particularly the trade-off between alignment and potential performance drops. Finally, a more comprehensive efficiency comparison with backpropagation-free methods should be provided to better contextualize the computational costs associated with DPE.