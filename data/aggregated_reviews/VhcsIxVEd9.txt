ID: VhcsIxVEd9
Title: DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 6, 7, 5, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel self-supervised pretext task for Vision Transformers (ViTs) called DropPos, aimed at enhancing their spatial reasoning and location awareness. By dropping a large random subset of positional embeddings and predicting the actual positions based solely on visual appearance, DropPos addresses the insensitivity of ViTs to input token order. The authors identify challenges such as discrepancies between pre-training and fine-tuning, trivial solutions, and similar visual content among patches. They propose position smoothing and attentive reconstruction strategies to mitigate these issues. Experimental results demonstrate that DropPos outperforms supervised pre-training and achieves competitive results against state-of-the-art self-supervised methods across various benchmarks.

### Strengths and Weaknesses
Strengths:
1. The main claim is compelling, suggesting that ViTs can learn effective representations through a simple patch prediction task, yielding competitive results against mainstream pre-training tasks.
2. Extensive ablation studies are provided to validate the design choices and offer insights.
3. The introduction of position smoothing and attentive reconstruction effectively addresses challenges related to similar visual appearances among patches.

Weaknesses:
1. The paper requires additional experiments and deeper analysis to substantiate key assertions, particularly regarding the insensitivity of ViTs to input token order and its implications.
2. The lack of comparison with recent works, such as Jigsaw-ViT, limits the contextual understanding of DropPos's effectiveness.
3. Insufficient exploration of the initialization of positional encoding and its impact on performance, as well as a lack of in-depth analysis of the learned representations and their positional awareness.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the insensitivity of ViTs to input token order, including potential benefits and implications of this characteristic. Additionally, a comparison with Jigsaw-ViT should be included to contextualize the DropPos approach. We suggest conducting further experiments to analyze the initialization strategies for positional encoding and their effects on performance. Moreover, enhancing the clarity of the DropPos method's implementation details and providing a more thorough exploration of its scaling properties on larger architectures would strengthen the paper. Lastly, we encourage the authors to investigate transfer learning and robustness evaluations on datasets like iNaturalists and Places to validate the effectiveness of DropPos across various scenarios.