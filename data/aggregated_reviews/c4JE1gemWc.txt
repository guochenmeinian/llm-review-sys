ID: c4JE1gemWc
Title: SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 7, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DivSafe, a multi-dimensional benchmark for evaluating the safety performance of large language models (LLMs) across various tasks and prompt types. It includes four test sets for open-end text generation and safety content discrimination, and examines the effects of prompt engineering techniques. The authors evaluate 11 LLMs and find that most exhibit lower safety performance on discrimination tasks compared to open-end generation, indicating poor generalization of safety training.

### Strengths and Weaknesses
Strengths:
- The motivation for the benchmark is clearly articulated, addressing the need for a comprehensive safety evaluation of LLMs.
- The evaluation includes a diverse range of proprietary and open-source LLMs, allowing for robust comparisons.
- The findings regarding the impact of prompt engineering techniques on safety performance are insightful.

Weaknesses:
- The paper lacks consideration of additional security scenarios, particularly those related to violence, racial discrimination, and adult content.
- The evaluation of jailbreak attacks does not include more advanced techniques, which may affect the accuracy of results.
- The validity of the evaluation index, particularly the use of LlamaGuard-7B for toxicity assessment, requires further discussion.
- The scalability and transferability of the benchmark are not adequately clarified.
- Details regarding prompt engineering are insufficient for reproducibility.

### Suggestions for Improvement
We recommend that the authors consider including more security scenarios, especially those that contradict mainstream social values, such as violence and racial discrimination. Additionally, the authors should explore more advanced jailbreak attacks like GCG, PAIR, and AutoDAN to enhance evaluation robustness. We suggest discussing the validity of the evaluation index in greater depth, potentially by employing multiple detection models and using ensemble methods for toxicity assessment. Furthermore, the authors should clarify the scalability and transferability of the benchmark to other tasks and LLMs, and provide more detailed explanations of prompt engineering techniques to facilitate reproducibility. Lastly, a dedicated section discussing the limitations of the study would strengthen the paper.