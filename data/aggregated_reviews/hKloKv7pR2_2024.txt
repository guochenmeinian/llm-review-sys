ID: hKloKv7pR2
Title: Rethinking Optimal Transport in Offline Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel perspective on offline reinforcement learning (RL) by formulating it as a partial optimal transport (OT) problem. The authors propose viewing the policy $\pi$ as a transport map from the state distribution $d^\beta$ to the behavior policy $\beta(\cdot\mid s)$, demonstrating that the dual form of this problem can be expressed as a maximin optimization problem. The effectiveness of the proposed algorithm is validated through experiments on various offline RL benchmarks, particularly in maze environments.

### Strengths and Weaknesses
Strengths:
1. The formulation of offline RL as a partial OT problem is innovative and promising.
2. The maximin formulation allows for easier optimization compared to traditional OT problems, which often require Lipschitz constraints.
3. Experimental results show significant performance improvements over existing baselines, especially in antmaze tasks.

Weaknesses:
1. The dependence of the policy $\pi$ on the choice of $s$ in $\beta(\cdot\mid s)$ raises questions about the clarity of the transport map.
2. The explanation of the toy example environment is unclear, particularly regarding the necessity for the agent to seek the shortest path.
3. The workings of PPL, PPL-CQL, and PPL-R are not well articulated; pseudocode for each variant would enhance understanding.
4. Formatting issues, such as excessive underlining and misaligned elements, detract from the paper's readability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the transport map's dependence on $s$ and provide a more detailed explanation of the advantages of OT compared to existing regularization methods like behavior cloning (BC) or KL regularization. Additionally, we suggest including pseudocode for PPL, PPL-CQL, and PPL-R to aid comprehension. Addressing the formatting issues will also enhance the overall readability of the paper.