ID: v8u3EFAyW9
Title: Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 4, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a distributional reinforcement learning algorithm, Perturbed Quantile Regression (PQR), which utilizes a Perturbed Distributional Bellman Operator (PDBOO) to mitigate biased exploration caused by one-sided risk tendencies in action selection. The authors argue that this method allows for diverse action selection while maintaining risk neutrality. Theoretical analysis demonstrates that the perturbation strength should be scheduled to converge to the unique fixed point of the Bellman optimality equation. The paper critiques the optimism in the face of uncertainty (OFU) paradigm for causing biased exploration and sub-optimal value distributions. Empirical evaluations on a 4-state chain MDP and 55 Atari games indicate competitive performance compared to existing methods like IQN and Rainbow, showing clear empirical performance benefits.

### Strengths and Weaknesses
Strengths:
- The introduction of PDBOO is a novel approach that enhances exploration while ensuring risk neutrality.
- The theoretical framework provides valuable insights into the scheduling of perturbation strength.
- The juxtaposition of distributional reinforcement learning's advantages in handling aleatoric uncertainty while addressing epistemic uncertainty through exploration is compelling.
- The methods proposed to mitigate the one-sided risk tendency are reasonable and align with the paper's contributions.
- The background provided in Section 2 is thorough, aiding comprehension of the methods.
- The technical formulation of the PDBOO is clear, with well-defined concepts that support further method development.
- Section 4 effectively outlines the empirical study objectives, framing the performance of the PQR algorithm.
- Experimental results show promising performance improvements, particularly in QR-DQN.

Weaknesses:
- The novelty of the algorithm may be overstated, as similar outcomes could arise from QR-DQN with a dynamic distortion risk measure.
- The theoretical results are limited, focusing only on the convergence of first moments of return distributions, with unclear implications from Theorem 3.3.
- The influence of the schedule $(\Delta_t)_{t\geq 0}$ and Dirichlet parameter $\beta$ is not adequately explored, suggesting potential for more optimal data-based schedules.
- The paper lacks a thorough discussion on the limitations of PDBOO and PQR, particularly regarding scenarios where the method may not be effective.
- The phrase “without losing the risk-neutral objective” in the abstract is unclear.
- Strong statements in lines 50-53 require additional justification to enhance persuasiveness.
- Claims regarding PQR's performance on n-chain and Atari are speculative and require rigorous analysis linking theory to empirical results.
- The empirical results discussion lacks depth, and ablation studies on PQR would clarify the impact of different components.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis by establishing convergence for more than just the first moments of return distributions. Clarifying the implications of Theorem 3.3 and providing a more interpretable upper bound would strengthen the theoretical contributions. Additionally, a detailed investigation into the effects of the schedule $(\Delta_t)_{t\geq 0}$ and Dirichlet parameter $\beta$ should be included. The authors should enhance the discussion of limitations, particularly addressing when PDBOO may not be suitable and comparing it with other exploration strategies like p-DLTV. We also recommend clarifying the meaning of “without losing the risk-neutral objective” in the abstract and reiterating the two types of uncertainty in line 43 to strengthen the connection to earlier concepts. Avoiding the interchangeable use of “deep” RL and “distributional” RL would improve clarity. Justifications for strong statements in lines 50-53 should be provided, possibly through examples or citations. Expanding the discussion on DLTV's limitations in Section 2.2 would enhance understanding of the contributions. Lastly, we suggest a more rigorous analysis connecting theoretical insights in Sections 3.2 and 3.3 to empirical performance, and including ablation studies on PQR to elucidate the effects of different algorithm components on performance.