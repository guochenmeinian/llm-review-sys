ID: 2NkGfA66Ne
Title: Segment Anything in 3D with NeRFs
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for 3D segmentation by leveraging the Segment-Anything Model (SAM) and NeRF, termed SA3D. The authors propose a two-step approach that lifts 2D segmentation results from SAM to 3D using a well-trained NeRF. Initially, a rendered reference view is processed by SAM to obtain a segmentation, which is then optimized through a mask inverse rendering and cross-view self-prompting strategy. The experiments demonstrate the effectiveness of this pipeline across various datasets.

### Strengths and Weaknesses
Strengths:
1. The proposed method is simple yet effective, providing a general interactive 3D object segmentation paradigm without heavy pre-training.
2. The method is efficient, completing 3D segmentation in minutes with a pre-trained NeRF and SAM.
3. Comprehensive experiments cover various datasets and comparisons to state-of-the-art methods, yielding persuasive results.
4. The paper is well-written and easy to follow.

Weaknesses:
1. The method can only segment one object at a time, which may not align with the expectation of segmenting "anything" in 3D.
2. The reliance on prompts for 2D segmentation is a long-studied area, and the authors should evaluate alternative methods or clarify SAM's irreplaceability.
3. The method's robustness is questionable if the target object is not fully observed in any view, and the impact of the reference view choice on segmentation results needs clarification.
4. The computational efficiency of the proposed algorithm is not discussed, leaving its importance unclear.

### Suggestions for Improvement
We recommend that the authors improve the method to allow for the segmentation of multiple objects simultaneously to align with the expectations set by the title. Additionally, we suggest evaluating alternative 2D segmentation methods alongside SAM to justify its use. The authors should also address the robustness of the method in scenarios where the target object is not fully visible and clarify the strategies for optimizing performance based on reference view selection. Furthermore, including a discussion on computational efficiency and runtime comparisons with baseline methods would strengthen the paper. Lastly, we encourage the authors to demonstrate the framework's applicability to other foundation models and provide evaluations on standard 3D segmentation benchmarks for a more comprehensive comparison.