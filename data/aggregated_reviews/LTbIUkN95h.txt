ID: LTbIUkN95h
Title: Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for Markov Decision Processes (MDPs) characterized by mixed systems, where states are divided into stochastic and pseudo-stochastic components. The authors propose an augmented sample generator (ASG) to enhance sample efficiency by generating new pseudo-stochastic states while retaining stochastic states. They establish theoretical guarantees for the convergence of their approach, demonstrating its applicability in real-world scenarios such as queueing systems and wireless networks. The paper includes simulations that validate the effectiveness of ASG in improving performance in reinforcement learning tasks.

### Strengths and Weaknesses
Strengths:
- The introduction of a mixed systems setting is relevant and reflects real-world applications.
- The ASG algorithm is well-suited for the proposed framework, supported by solid theoretical analysis and empirical results.
- The paper is well-written, with a focused contribution that effectively addresses sample efficiency in reinforcement learning.

Weaknesses:
- The distinction between mixed systems and existing MDPs with exogenous inputs is unclear, suggesting that more examples of unique mixed systems are needed.
- The assumption of access to the reward function and pseudo-stochastic state transition function may limit the algorithm's applicability.
- Some simulation results lack clarity, and certain plots appear unreasonable, necessitating further explanation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the distinction between mixed systems and MDPs with exogenous inputs, potentially by providing more unique real-world examples. Additionally, it would be beneficial to justify the assumption regarding access to the reward function and transition function more thoroughly. We suggest enhancing the simulation section by providing detailed explanations for the reward function determination, state sampling distributions, and the definition of episodes. Addressing the concerns regarding the validity of augmented pseudo-stochastic states in policy learning, particularly in the wireless downlink network example, would also strengthen the paper. Lastly, correcting the identified typos and ensuring the completeness of sentences will enhance the overall presentation.