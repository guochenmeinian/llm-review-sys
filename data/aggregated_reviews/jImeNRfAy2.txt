ID: jImeNRfAy2
Title: Self-Detoxifying Language Models via Toxification Reversal
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel prompt-based method for language detoxification that involves two forward passes during inference: the first calculates a toxification direction using negative and positive prompts, while the second steers the output away from this direction. The authors claim significant improvements over prompt-based and some finetuning-based baselines. The methodology is mathematically sound and well-evaluated, contributing to the understanding of detoxification in language models.

### Strengths and Weaknesses
Strengths:
- The method is simple yet effective.
- The experimental design is thorough, incorporating both automatic and manual evaluations.
- The paper is well-written and easy to follow, with clear contributions.

Weaknesses:
- The approach introduces significant computational costs.
- Experiments are limited to GPT-2, lacking comparisons with other detoxification baselines.
- The clarity of the objectives and task definition regarding LLM detoxification is insufficient.
- Human evaluation suffers from low annotator agreement and a limited number of evaluators.

### Suggestions for Improvement
We recommend that the authors improve the discussion on computational costs and consider conducting experiments on additional base models. Clarifying the task definition—whether the goal is to generate the same output non-toxically or simply produce non-toxic content—is essential. Additionally, we suggest including more evaluations on content similarity and relevance, as well as addressing the low agreement among annotators in human evaluations. Finally, providing examples of the outputs presented to evaluators and clarifying the metrics used in evaluations would enhance the paper's comprehensibility.