ID: 6QFe3vPbYZ
Title: miniCodeProps: a Minimal Benchmark for Proving Code Properties
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 7, 6
Original Confidences: 4, 3

Aggregated Review:
### Key Points
This paper presents miniCodeProps, a benchmark of 201 program specifications compatible with the Lean proof-assistant, aimed at assessing LLM capabilities in generating formal proofs for code verification. The authors manually translate programs from Tons of Inductive Programs (TIP) in Haskell to Lean 4, designing lightweight, self-contained properties for the dataset, including unique properties not found in standard benchmarks. They evaluate the benchmark in two modes: full-proof generation and tactic-by-tactic generation, using gpt-4o and a compatible ntp-ctx model, respectively. The results indicate that current state-of-the-art (SOTA) LLM technology is insufficient for non-trivial tasks, while an iterative refinement approach shows promise in improving results on challenging problems.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant problem relevant to formal proof verification and program synthesis, appealing to the safegenai community.
- It is well-written, with clear presentation of ideas and extensive details on the constructed dataset for miniCodeProps.

Weaknesses:
- The methodology for performing the miniCodeProps benchmark lacks detailed explanation beyond a brief discussion of baseline calculations.
- There is limited analysis of benchmark results, leaving the strengths and weaknesses of each baseline approach unclear.
- Concerns arise regarding the fairness of comparing the accuracy of the three discussed baselines, particularly in how scores are derived for full proof generation versus tactic-by-tactic generation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology by providing a more detailed explanation of the miniCodeProps benchmark execution. Additionally, we suggest enhancing the analysis of benchmark results to better elucidate the strengths and weaknesses of each baseline approach. Finally, we encourage the authors to address the fairness of the comparisons made between the different baseline scores to strengthen the evaluation's validity.