ID: kRwQCAIA7z
Title: Dimension-free Private Mean Estimation for Anisotropic Distributions
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 6, 7, 7, 7, -1, -1, -1
Original Confidences: 4, 3, 4, 1, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents new differentially private (DP) algorithms for mean estimation in high-dimensional distributions characterized by anisotropy, where variances differ significantly across dimensions. The authors address the "curse of dimensionality" by proposing two main contributions: (1) a DP algorithm for known covariance that achieves sample complexity independent of the dimension \(d\), relying instead on \(tr(\Sigma^{1/2})\), and (2) an algorithm for unknown covariance that reduces the dimension dependence from \(d^{1/2}\) to \(d^{1/4}\). The analysis is supported by proof sketches and contextualizes improvements over prior work.

### Strengths and Weaknesses
Strengths:
- The paper is clear and well-written, despite its mathematical complexity, with sound technical analysis.
- The motivations for improved rates based on covariance structure are intuitive and well-articulated.
- The study addresses a fundamental problem in differential privacy, achieving optimal results for known covariance and making significant improvements for unknown covariance.

Weaknesses:
- Results for known covariance are relatively straightforward, primarily relying on established techniques from prior works.
- The lack of empirical validation raises concerns about the practical applicability of the theoretical findings.
- The presentation could be improved by reorganizing sections for better flow, particularly introducing key concepts earlier.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the unknown covariance case, particularly regarding assumptions about the covariance matrix's structure. Additionally, including empirical tests to validate theoretical results would enhance the paper's impact. Addressing the gap between upper and lower bounds for unknown covariance could also strengthen the contributions. Finally, consider presenting the preliminaries and key algorithms earlier in the paper to aid reader comprehension.