ID: T0e4Nw09XX
Title: Universal Rates for Active Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 8, 7, 7, -1, -1
Original Confidences: 3, 3, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a characterization of distribution-dependent learning rates for realizable active learning, revealing that any hypothesis class is universally learnable with rates that vary based on complexity measures. The authors propose a star number based VCL-tree variant, which, alongside original VCL-trees, delineates the four possible rates of universal active learning. The work contributes to the understanding of universal rates in active learning, complementing existing literature on interactive learning.

### Strengths and Weaknesses
Strengths:  
- The paper makes a significant contribution to the field of universal active learning, providing deep insights and characterizations of optimal learning rates, which address an open question by Balcan et al. (2010).  
- The introduction of a new complexity measure effectively distinguishes hypothesis classes, highlighting scenarios where active strategies can outperform passive learning.  
- The star number based modification of VCL trees is a natural and effective approach.

Weaknesses:  
- Pages 8-9 require polishing for clarity.  
- The content of Appendix A.4, which compares results to passive and interactive learning, should be integrated into the main body for better visibility.  
- The analysis heavily relies on non-adaptive active learning algorithms; concrete examples of such algorithms and methods for converting existing algorithms to non-adaptive counterparts are needed.  
- There are questions regarding the apparent contradiction in learning rates and the relevance of high-probability bounds in this context.

### Suggestions for Improvement
We recommend that the authors improve the clarity of pages 8-9 and consider integrating Appendix A.4 into the main body of the paper. Additionally, please provide concrete examples of non-adaptive active learning algorithms and methods for converting existing algorithms to non-adaptive forms without performance degradation. Clarification on the $e^{-n}$ learning of intervals versus the $\Omega(1/\varepsilon)$ rate under the uniform distribution is also necessary, as well as an analysis of the computational aspects of the studied algorithms.