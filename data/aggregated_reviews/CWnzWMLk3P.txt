ID: CWnzWMLk3P
Title: FAVAS: Federated AVeraging with ASynchronous clients
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 4, 7, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel Asynchronous Federated Learning algorithm, FAVAS, aimed at addressing the heterogeneities in client computation speeds. The authors provide a theoretical convergence analysis and experimental results demonstrating that FAVAS outperforms existing baselines, particularly the QuAFL algorithm. Key contributions include: 1) FAVAS weights updates based on the number of local iterates from each client, improving the asymptotic bound on the total number of clients; 2) The incorporation of asynchronous client training and client-sampling strategies enhances efficiency; 3) The authors clarify the convergence guarantees of their model and emphasize practical implications of their findings. Experimental results indicate superior performance across various datasets.

### Strengths and Weaknesses
Strengths:
1. The algorithm efficiently utilizes client computation by allowing continuous updates until interrupted, differing from traditional methods.
2. The authors effectively redesign local updates and weighting to achieve unbiased aggregation, clearly distinguishing their approach from QuAFL.
3. The mathematical convergence analysis is robust, relying on reasonable assumptions, with clear explanations provided in the main text and detailed proofs in the appendix.
4. The paper offers a comprehensive comparison with existing literature, detailing the experimental setup.
5. The authors have effectively addressed most reviewer concerns, leading to improved clarity and understanding of the proposed methods.
6. The incorporation of client-sampling is justified, particularly in realistic federated learning scenarios with a large number of clients.

Weaknesses:
1. The relationship between the number of local iterates and training quality is underexplored; additional statistics on local iterates could enhance the study's depth.
2. The novelty of the work is questioned, as it heavily draws from QuAFL, raising concerns about its theoretical contributions.
3. Several technical terms and concepts, such as "unbiased aggregation" and the reweighting scheme, require further clarification to avoid confusion.
4. There remains some ambiguity regarding the convergence guarantees of the final model, particularly in relation to the average model $\gamma_{T-1}$.
5. The proposed asynchronization mechanism may not fully eliminate waiting times caused by slow clients, raising questions about its effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the relationship between local iterates and training quality, potentially including mean and standard deviation statistics for a fair comparison with baselines. Additionally, we suggest enhancing the novelty of the paper by clearly articulating the theoretical advancements beyond QuAFL. Clarifying the reweighting parameter $\alpha^i$ and the definition of $E_{t+1}^i$ is essential for reader comprehension. Furthermore, we encourage the authors to provide a detailed comparison of convergence rates between FAVAS and QuAFL in Table 1, and to address the concerns regarding the assumptions on client delays and the implications of sampling a large subset of devices. We also recommend that the authors improve clarity around the convergence guarantees by adding a sentence to explain that in practice, the final version of the trained model is considered. Lastly, we suggest that the authors clarify the necessity of the asynchrony mechanism, particularly how it mitigates waiting times for slow clients, and address the potential variance issues related to the number of local SGD steps in the context of FedAvg.