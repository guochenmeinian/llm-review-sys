ID: IGTbT9P1ti
Title: Connecting Multi-modal Contrastive Representations
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 7, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for aligning multi-modal contrastive representations (C-MCR) by leveraging overlapping modalities to connect distinct embedding spaces without requiring paired data. The authors argue that their approach outperforms existing methods, such as AudioCLIP and ULIP 2, which utilize large datasets like AudioSet and Objaverse. They propose a framework that utilizes inter- and intra-MCR alignment objectives and a semantic enhancement strategy to achieve robust alignment. The effectiveness of the method is demonstrated through experiments on various audio-visual tasks, achieving state-of-the-art performance in a zero-shot manner. The authors provide statistical metrics and T-SNE visualizations to demonstrate the effectiveness of their aligned embedding space, showing significant improvements in modality alignment compared to original CLIP and CLAP spaces.

### Strengths and Weaknesses
Strengths:
- The motivation for the C-MCR framework is clear, addressing the challenge of aligning representations without expensive paired data.
- The proposed approach is straightforward and easy to understand, adding minimal parameters while requiring limited additional training.
- The paper achieves impressive results, outperforming existing methods on multiple audio-visual tasks.
- Comprehensive ablation studies justify the design choices made in the methodology.
- The authors provide a robust theoretical framework for addressing the challenges of obtaining high-quality data pairs in audio-visual tasks.
- The inclusion of statistical metrics and T-SNE visualizations effectively illustrates the improvements in modality alignment.

Weaknesses:
- The necessity of the proposed innovative ideas could be better supported with stronger evidence, particularly regarding the significance of aligning shared embedding spaces.
- The experiments primarily focus on audio-visual tasks, lacking evaluations on unimodal tasks, which limits the generalizability of the findings.
- The comparison with baseline models is not entirely fair, as they were trained on different datasets, making it difficult to convincingly demonstrate the proposed method's superiority.
- The paper lacks a thorough analysis of the proposed method's limitations and potential applications to other modality settings.
- The paper's contribution may appear less substantiated without a deeper exploration of the aligned embedding space.
- The reliance on large datasets for comparison could overshadow the unique aspects of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the necessity for their innovative ideas by providing stronger evidence and discussing the advantages of their alignment method over traditional dataset labeling. Additionally, the authors should include more detailed descriptions of the evaluation tasks and datasets in the experiments. To establish the significance of the proposed method, it is crucial to include appropriate baselines trained on the same datasets. Furthermore, we suggest exploring the applicability of the proposed method to additional modality settings and providing a more comprehensive discussion of its limitations and future work. Lastly, we recommend that the authors improve the depth of analysis regarding the aligned embedding space to further substantiate their contributions and clarify how their method can be integrated with large-scale datasets to enhance the paper's impact.