ID: NIcIdhyfQX
Title: Q-Distribution guided Q-learning for offline reinforcement learning: Uncertainty penalized Q-value via consistency model
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the QDQ method, addressing the challenge of Out-of-Distribution (OOD) actions in offline reinforcement learning by penalizing Q-values in uncertain regions. The authors propose a truncated Q-value dataset for the behavior policy and utilize a consistency model to estimate uncertainty for state-action pairs via one-step sampling. This approach allows for pessimistic value estimation, mitigating Q-value overestimation for OOD actions. The paper includes theoretical results supporting the QDQ method and demonstrates its advantages through experiments on the D4RL benchmark.

### Strengths and Weaknesses
Strengths:  
1. The paper introduces an innovative uncertainty estimation approach using a consistency model, avoiding the computational overhead of maintaining multiple Q-networks and ensuring higher fidelity without iterative processes.  
2. It is grounded in a solid theoretical foundation, demonstrating algorithm feasibility and convergence.  
3. The writing is clear, with a logical structure and flow.

Weaknesses:  
1. The assumption that a consistency model can accurately learn Q-value distributions lacks justification; the necessity of consistency for Q-values is not argued.  
2. There is a potential mathematical issue regarding the use of $V_{\epsilon}(s,a)$ as $Var(Q^{\pi})$ instead of $Var(Q^{\pi_{\beta}})$, which requires clarification.  
3. The paper lacks comparisons with related work, particularly methods using ensembles for uncertainty estimation and those making pessimistic estimates for OOD actions.  
4. The presentation is somewhat confusing, particularly regarding the introduction of the consistency model and its implications for the method's theoretical results.

### Suggestions for Improvement
We recommend that the authors improve the justification for the necessity of consistency in Q-value distributions. Additionally, please clarify the mathematical distinction between $Var(Q^{\pi})$ and $Var(Q^{\pi_{\beta}})$ and provide a rationale if the latter is indeed used. We also suggest including comparisons with methods like EDAC and PBRL in the main evaluation to strengthen the paper's contributions. Furthermore, we encourage the authors to enhance the clarity of the presentation, particularly in introducing the consistency model and discussing its implications for the method's theoretical framework. Lastly, a discussion on the tuning of hyperparameters and their impact on performance would be beneficial.