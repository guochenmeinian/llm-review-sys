ID: 3ZjaXTPWiE
Title: NanoBaseLib: A Multi-Task Benchmark Dataset for Nanopore Sequencing
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 4, 9, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 4, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents NanoBaseLib, a novel benchmark dataset designed for applying machine learning to nanopore sequencing. NanoBaseLib integrates 16 datasets across four major tasks: Base Calling (BC), Modification Detection (MD), PolyA Detection (PD), and Segmentation and Event Alignment (SA). The authors emphasize the accessibility of the dataset and provide a comprehensive framework for benchmarking various tools, which should facilitate ML researchers in developing models for nanopore sequencing data analysis. They argue that while Hidden Markov Models (HMMs) are commonly used for BC and SA, more advanced deep learning models are necessary to enhance accuracy, particularly for BC, which currently achieves only ∼90% accuracy against a target of ∼99.9%. The authors highlight the importance of unsupervised or semi-supervised learning for the SA task due to the lack of high-quality labels and assert that all tasks are significant for various biological applications, providing detailed information about each model on their website.

### Strengths and Weaknesses
Strengths:
- The authors propose a solid and comprehensive dataset construction pipeline.
- The writing is clear and well-organized, making the content accessible to non-experts.
- Detailed documentation and a well-designed dataset website enhance usability.
- The authors effectively address the necessity of advanced machine learning techniques for improving task performance.
- The newly launched website provides comprehensive resources, including detailed dataset documentation and user-friendly access to raw data.

Weaknesses:
- There are contradictions regarding the dataset's benchmarking capabilities, raising concerns about its practical application.
- Critical details, such as batch effect handling and comprehensive dataset statistics, are insufficiently addressed.
- The writing could be improved for clarity and conciseness.
- The authors did not rank the tasks based on priority or potential benefits, which may leave researchers uncertain about where to focus their efforts.
- The dataset documentation could still be enhanced to ensure clarity regarding the locations of raw data acquisition.

### Suggestions for Improvement
We recommend that the authors benchmark the performance of at least one deep learning baseline method for each dataset in NanoBaseLib. Additionally, please clarify the quality of the assembled data and its classification as "ground truth." It would be beneficial to provide further information on the necessity of advanced machine learning for the tasks commonly solved by HMMs and to prioritize tasks based on potential benefits. We suggest improving the ranking of the tasks in terms of priority or likely benefits to guide researchers in selecting the most promising areas for exploration. Ensure that raw signal data is accessible for all tasks and provide explicit links to all raw data repositories. We also suggest improving the documentation of tools used in the study, including a new supplemental table summarizing tool limitations. Lastly, please address the clarity of the writing, particularly in the introduction, and ensure that critical details about dataset statistics are included.