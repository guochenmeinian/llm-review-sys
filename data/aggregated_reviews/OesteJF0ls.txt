ID: OesteJF0ls
Title: Decomposable Transformer Point Processes
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 5, 5, -1, -1, -1
Original Confidences: 3, 4, 2, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel transformer-based approach for modeling marked temporal point processes (MTPPs) through a Decomposable Transformer Point Process (DTPP). The authors propose a decomposition of the log-likelihood into a conditional probability mass function (CPMF) for event types and a conditional probability density function (CPDF) for inter-event times, utilizing a transformer architecture. The experimental results indicate that the approach achieves state-of-the-art performance in next-event prediction and long-horizon forecasting, outperforming traditional thinning-based methods.

### Strengths and Weaknesses
Strengths:
- The writing and presentation are clear and precise, with a solid technical introduction.
- The experiments convincingly demonstrate the model's performance, with transparency in reporting variance.
- The innovative framework effectively utilizes a transformer architecture, marking a significant advancement in the field.

Weaknesses:
- The primary contribution is the application of transformer architecture, lacking significant theoretical advancements or novel methodologies.
- The motivation for decomposing the log-likelihood is unclear, and the writing could be challenging for readers unfamiliar with the field.
- The paper could benefit from more detailed ablation studies and additional visualizations to enhance clarity and understanding.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation for the decomposition of the log-likelihood, possibly by providing an analytical argument for its benefits over the intensity function. Additionally, including a high-level recap of the thinning algorithm would aid reader comprehension. We suggest conducting more detailed ablation studies to isolate the contributions of different components and integrating more visualizations of learned intensity functions and attention mechanisms to enhance the paper's clarity and impact.