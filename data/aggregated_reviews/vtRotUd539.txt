ID: vtRotUd539
Title: Average gradient outer product as a mechanism for deep neural collapse
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into Deep Neural Collapse (DNC) using the average gradient outer product (AGOP) and introduces the Deep Recursive Feature Machines (Deep RFM) model. The authors provide empirical evidence that DNC occurs in deep RFMs and theoretical analyses in high-dimensional and kernel learning settings. They demonstrate that the mechanisms leading to DNC in RFMs and traditional deep neural networks (DNNs) are similar, emphasizing the role of AGOP in this phenomenon.

### Strengths and Weaknesses
Strengths:
- The paper employs a novel data-based approach using AGOP to explain DNC, supported by both theoretical analysis and empirical evidence.
- The extensive experiments across various architectures and datasets enhance the credibility of the findings.
- The writing is generally clear, with a thorough literature review that contextualizes the results.

Weaknesses:
- The paper suffers from readability issues, particularly in sections 4.2 and 4.3, where complex ideas are poorly articulated.
- Notations and terminology are inconsistent, leading to confusion, especially regarding the roles of different kernels and assumptions.
- The implications of the results for well-known models and algorithms are not clearly articulated, raising questions about their broader relevance.

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of sections 4.2 and 4.3 by separating empirical evidence from theoretical results and providing clearer introductory statements. Additionally, we suggest revising the notations for consistency and clarity, particularly regarding the definitions of kernels and feature maps. The authors should explicitly state the implications of their findings for more conventional models and clarify the role of non-linearities in achieving neural collapse. Finally, addressing the readability issues throughout the paper will enhance its overall impact and accessibility.