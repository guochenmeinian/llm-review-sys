ID: llv2GnH5bD
Title: Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel retrofitting method to enhance emotional aspects in pre-trained language models (PLMs) using supervised contrastive learning. The authors conduct multiple experiments demonstrating the effectiveness of their approach, which includes a dual-objective retrofitting loss. The results indicate a modest improvement in F1-score compared to baseline methods.

### Strengths and Weaknesses
Strengths:  
- The experiments are extensive and well-executed, validating the proposed method's effectiveness.  
- The paper is clearly organized and well-written, making the methodology easy to understand.  
- The approach is computationally less expensive and can retro-fit openly available PLMs to improve affect-aware representations.

Weaknesses:  
- The method lacks significant innovation, as it builds on existing fine-tuning strategies and loss functions.  
- There is no comparison with the latest state-of-the-art models, such as GPTs, which limits the contextual relevance of the findings.  
- The performance improvement is minimal, and the corpus used for retrofitting is relatively small compared to larger datasets.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by exploring alternative loss functions beyond contrastive learning and justifying their choice of loss functions more thoroughly. Additionally, we suggest that the authors include comparisons with newer generative models to contextualize their findings better. It would also be beneficial to reduce the space dedicated to existing work and focus more on the analysis of experimental results. Lastly, addressing the absence of recent references in the related work section would strengthen the paper's relevance.