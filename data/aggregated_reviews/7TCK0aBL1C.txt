ID: 7TCK0aBL1C
Title: IaC-Eval: A Code Generation Benchmark for Cloud Infrastructure-as-Code Programs
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 8, 6, 3, -1, -1
Original Confidences: 4, 5, 2, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents IaC-Eval, an advanced benchmark for evaluating large language models' (LLMs) capabilities in generating Infrastructure-as-Code (IaC). The dataset comprises 458 human-curated scenarios that cover a variety of AWS services, each including a natural language problem description and an infrastructure intent specification. The authors find that contemporary LLMs, including GPT-4, perform poorly on this benchmark, indicating a significant need for improvements in IaC code generation.

### Strengths and Weaknesses
Strengths:
1. The authors tackle the challenging problem of evaluating code generation models in the open domain of IaC.
2. The dataset is comprehensive, covering a wide range of AWS services and providing a robust basis for evaluation.
3. The evaluation framework is well-constructed, considering various metrics and model types.

Weaknesses:
1. The benchmark lacks true execution verifiability, relying on handcrafted rules rather than functional testing of generated IaC.
2. The focus is primarily on AWS services, limiting the benchmark's applicability to other cloud environments.
3. The paper suffers from dense and convoluted explanations, making it difficult to follow the methodology and results.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by incorporating deployable artifacts that can be functionally tested, such as requiring the deployment of a lambda function and invoking it with input. Additionally, consider expanding the benchmark to include other cloud providers like Azure and Google Cloud to enhance its relevance. We suggest providing clearer and more concise explanations of the methodology, particularly regarding the intent specification and evaluation process. Furthermore, adding evaluations for security risks in IaC could provide valuable insights. Lastly, discussing the process of obtaining prompts and the quality of human annotations would strengthen the paper.