ID: MhU0zxuZ5K
Title: On the Dimensionality of Sentence Embeddings
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the dimensionality of sentence embeddings, proposing a method to achieve lower-dimensional embeddings with minimal performance loss on downstream tasks. The authors explore the impact of varying dimensions in BERT-like models, conducting experiments that reveal interesting observations about the relationship between embedding size and task performance. They introduce a two-step training strategy to optimize the encoder and pooler independently, aiming to mitigate performance degradation associated with reduced dimensions.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and easy to follow.
- It provides interesting findings regarding the benefits of lower-dimensional embeddings for specific tasks.
- The two-step training method is simple to implement and demonstrates robust empirical outcomes across multiple tasks.

Weaknesses:
- The idea lacks novelty, as dimension adjustment is a well-known concept, and the proposed method resembles training tricks without solid theoretical backing.
- The evaluation lacks strong baseline comparisons and is limited to a narrow range of tasks, primarily STS and text classification.
- The paper does not adequately explain the mechanisms behind observed performance improvements, particularly regarding the significant accuracy gains noted in some experiments.

### Suggestions for Improvement
We recommend that the authors improve the theoretical grounding of their observations by providing a solid explanation for the performance changes associated with different dimensionalities. Additionally, expanding the range of tasks evaluated would enhance the study's impact and applicability. It would also be beneficial to include stronger baseline comparisons in the experiments to better contextualize the results. Finally, addressing the question of why performance can improve with lower dimensions, particularly in cases like the d=32 pooler, would add depth to the findings.