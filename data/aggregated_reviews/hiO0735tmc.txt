ID: hiO0735tmc
Title: Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 4, 8, 9, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Event Stream GPT (ESGPT), an open-source library designed to extend generative pre-trained transformers (GPTs) to continuous-time sequences of complex events, particularly in electronic healthcare records (EHRs). ESGPT facilitates dataset construction, provides a modeling API for GPTs with intra-event causal dependencies, and standardizes evaluation processes for fine-tuning tasks. The framework supports efficient pre-processing of datasets, including MIMIC IV, and demonstrates that the proposed pipeline achieves fine-tuning performance that surpasses networks trained from scratch, with improvements in AUROC for readmission risk prediction and in-hospital mortality prediction. The authors emphasize the importance of ESGPT for enhancing reproducibility in modeling by offering a standardized data extraction and pre-processing pipeline, while also addressing concerns regarding empirical comparisons, modeling performance, and the framework's extensibility to different data modalities.

### Strengths and Weaknesses
**Strengths:**
- ESGPT expands the utility of GPT models to non-text applications, particularly in healthcare, enhancing their versatility.
- The library streamlines the process of building GPTs for event sequences, simplifying dataset construction and evaluation.
- Comprehensive documentation and a well-structured API facilitate user engagement and collaboration.
- The authors have expanded the discussion on limitations and proposed mitigation strategies, enhancing the manuscript's robustness.
- Additional empirical comparisons demonstrate ESGPT's performance against existing pipelines, particularly in terms of computational efficiency.
- The clarity of problem framing has been improved, with specific examples of tasks that the framework can address.

**Weaknesses:**
- The framework's focus on continuous-time event sequences limits its applicability to other data types or domains.
- The paper lacks comprehensive empirical comparisons across multiple datasets and algorithms, which could strengthen its claims.
- Transparency regarding data handling and pre-processing steps is insufficient, leading to potential misconceptions about data loss and storage efficiency.
- The novelty of the Nested Attention model is not clearly articulated, which may confuse readers regarding its originality.
- The paper does not sufficiently address the ethical implications of using pre-trained models on sensitive medical datasets.

### Suggestions for Improvement
We recommend that the authors improve the paper by discussing potential extensions of ESGPT to other contexts beyond continuous-time event sequences. Additionally, we suggest enhancing empirical comparisons by including a broader range of datasets and algorithms to substantiate the performance claims of ESGPT. Improving transparency in the data pipeline regarding when and why data is discarded would clarify the implications of pre-processing choices. To better highlight the novelty of the Nested Attention model, we recommend providing a more detailed explanation of its architecture and its distinctions from existing models. Furthermore, addressing the ethical concerns related to the use of pre-trained models on medical datasets is essential. Finally, providing clearer documentation on the Nested Attention model and improving the installation process to reduce dependency issues would benefit users.