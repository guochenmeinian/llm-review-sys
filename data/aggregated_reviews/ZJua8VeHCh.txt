ID: ZJua8VeHCh
Title: OssCSE: Overcoming Surface Structure Bias in Contrastive Learning for Unsupervised Sentence Embedding
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the surface structure bias problem in SimCSE, an unsupervised sentence representation learning method. The authors propose a solution involving data augmentation to generate "Paraphrase" and "Negation" forms of sentences, employing two triplet losses to maintain a gap between the similarity of original sentences and their paraphrase or negation forms. Additionally, a regression loss is introduced to mitigate catastrophic forgetting during training.

### Strengths and Weaknesses
Strengths:  
- The paper effectively identifies and addresses the surface structure bias problem, providing valuable insights into unsupervised sentence embedding methods.  
- The methodology is well-structured, employing a max margin loss and recall loss to enhance performance and minimize forgetting.  
- Comprehensive empirical studies demonstrate the effectiveness of the proposed method across various architectures and tasks.

Weaknesses:  
- The experimental design has notable flaws, including insufficient ablation studies and a lack of clarity regarding the number of new sentences used in training.  
- The reliance on the prompt technique raises questions about its relevance to the core issue of structure bias, and comparisons with baselines lacking prompts are inconsistent.  
- The paper lacks detailed exploration of hyperparameters and does not provide open-source code, complicating reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including ablation studies with at least three configurations: "w Max," "w Recall," and "w Recall & Max," to directly illustrate the effectiveness of the proposed method. Additionally, we suggest reporting average results from multiple random seeds to enhance the credibility of the findings. Clarifying the number of new sentences introduced during training and providing a thorough exploration of hyperparameters would strengthen the paper. Finally, committing to open-source code for the constructed probe dataset would facilitate reproducibility and validation of results.