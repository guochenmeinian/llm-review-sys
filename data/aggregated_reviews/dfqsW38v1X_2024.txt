ID: dfqsW38v1X
Title: QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 8, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents QuaRot, a novel quantization approach for Large Language Models (LLMs) that utilizes randomized Hadamard transformations to address outlier features in activations, weights, and KV caches. The authors propose that this method allows the entire model to be quantized to 4 bits with minimal performance loss, achieving significant computational efficiency and memory savings. The paper demonstrates that applying the Hadamard transformation improves quantization performance, particularly in compute-bound scenarios, while maintaining up to 99% of zero-shot performance on the LLAMA2-70B model.

### Strengths and Weaknesses
Strengths:
- The method achieves commendable accuracy in aggressive quantization regimes, enabling substantial compute and memory savings.
- The paper is well-structured, with a clear background section and thorough quantization ablation studies, contributing valuable insights into the effects of various quantization choices.
- The application of randomized Hadamard transformations is innovative, effectively managing outlier features and enhancing quantization.

Weaknesses:
- The motivation for using Hadamard matrices as orthogonal matrices is not sufficiently established, and the distinction between the proposed transformations and existing methods is unclear.
- Comparisons with other relevant works lack detail, particularly in ensuring coherent quantization granularity across benchmarks.
- The experimental results, while promising, show significant performance drops compared to FP16 and INT8 baselines, raising questions about the overall effectiveness of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the justification for using Hadamard matrices by clearly articulating their advantages over other orthogonal matrices. Additionally, we suggest providing more detailed comparisons with existing methods to enhance transparency and contextualize the results better. It would be beneficial to include the same benchmark methods across all tables for consistency. Furthermore, addressing the performance discrepancies observed in the experimental results, particularly regarding the INT4 quantization, would strengthen the paper's contributions. Lastly, we encourage the authors to explore alternative rounding algorithms that may yield better performance for activations.