ID: etd0ebzGOG
Title: VPP: Efficient Conditional 3D Generation via Voxel-Point Progressive Representation
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 4, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a voxel-point progressive representation for efficient 3D generation, utilizing a voxel semantic generator and a point upsampler to handle multi-category objects. The authors demonstrate the method's effectiveness through extensive experiments, achieving state-of-the-art results. The proposed approach supports various applications, including generation, editing, and completion of 3D shapes.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and organized, with clear presentation of results.
2. Extensive experiments yield impressive results, showcasing the method's capabilities across multiple tasks.
3. The proposed method effectively addresses limitations of existing techniques and applies to a broader range of downstream tasks.

Weaknesses:
1. The method primarily generates shapes from the ShapeNet dataset, lacking results from unseen categories.
2. The distinction between the proposed Voxel Semantic Generator and existing methods like CLIP-Sculptor is unclear.
3. The paper does not adequately address the novelty of its components, many of which are derived from prior works.
4. The inference efficiency metrics lack specificity, only providing hours or seconds without exact times.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing ablation studies on network architectures to assess the impact of specific components on performance. Additionally, clarifying the differences between the Voxel Semantic Generator and similar existing methods would enhance the paper's contribution. It would also be beneficial to include exact inference times in Table 1 and to address the potential for zero-shot generation capabilities more thoroughly. Lastly, we suggest including a discussion on limitations and future work in the main paper to better inform the community.