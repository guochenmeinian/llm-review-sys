ID: r6xGZ0XL2g
Title: Meta-Learning Adversarial Bandit Algorithms
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 7, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 2, 2, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a meta-learning framework for adversarial bandits, focusing on optimizing the initialization and hyperparameters of inner learners across multiple tasks. The authors propose a meta-algorithm that combines outer learners to enhance performance in multi-armed bandits (MAB) and bandit linear optimization (BLO). The framework includes theoretical analyses that yield task-average regret guarantees, particularly utilizing the Tsallis regularizer for MAB and self-concordant barriers for BLO.

### Strengths and Weaknesses
Strengths:
1. The problem addressed is well-motivated, as the adversarial variant of meta-learning has not been previously solved.
2. The algorithmic framework is clearly illustrated, making it easy to understand.
3. The paper is the first to consider meta-learning under adversarial bandit feedback, providing strong theoretical guarantees.

Weaknesses:
1. The design of the meta-learner lacks novelty, as the analyses of both meta-learners and base-learners are not sufficiently innovative.
2. There is uncertainty regarding the generalizability of the task similarity measure, particularly in cases with outlier tasks.
3. The paper does not include empirical results, which raises questions about the practical applicability of the proposed methods.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the meta-learner design and provide a more thorough discussion on the implications of task similarity, especially in the presence of outliers. Additionally, including empirical results would strengthen the paper's contributions and validate the theoretical claims. We also suggest addressing the computational complexity associated with the Multiplicative Weights method and providing a clearer explanation of the task-similarity measure in relation to existing methods.