ID: pGeAcYhnN5
Title: Speculative Decoding with CTC-based Draft Model for LLM Inference Acceleration
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel architecture and training technique for LLM speculative decoding, termed CTC-drafter, which aims to enhance the reliability and acceptance rate of generation candidates. The authors propose replacing the draft module with a Transformer and utilizing CTC-based loss instead of CE loss, employing pseudo-labels from the base model during training. The CTC beam search process is used during inference to produce final candidates. Experimental results indicate that CTC-drafter achieves higher speedup due to increased acceptance rates and a greater number of accepted tokens.

### Strengths and Weaknesses
Strengths:
- Combining CTC loss with sequence prediction in text-only LLMs is an innovative approach that could inspire further research.
- The method demonstrates significant improvements in token acceptance rates and impressive speedups.

Weaknesses:
- CTC's conditional independence problem raises questions about its effectiveness in addressing low acceptance rates, necessitating a thorough justification of its use.
- The relationship between CTC output decoding and known methods like CTC prefix beam search needs clarification.
- The absence of an ablation study on (Transformer Layer + CE loss) or (Linear layer + CTC loss) leaves unclear which changes contribute most to performance improvements.
- The speedup calculations lack clarity, as they are based on token acceptance rates rather than real system performance.
- The architecture of the Attention Draft Model is difficult to understand, particularly regarding input representation and positional encoding.

### Suggestions for Improvement
We recommend that the authors improve the justification for using CTC loss, particularly in light of its conditional independence issue. Clarifying whether the CTC output decoding aligns with prefix search decoding is essential. Conducting an ablation study on the contributions of different components to performance would strengthen the findings. Additionally, providing clearer details on speedup calculations, intermediate representation layers, and training overhead is necessary. We also suggest including a comparison with "Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding" to enhance the analysis. Lastly, improving the explanation of the Attention Draft Model's architecture and its input processing would aid reader comprehension.