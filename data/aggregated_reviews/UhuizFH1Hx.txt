ID: UhuizFH1Hx
Title: GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a newly constructed dataset, GSAP-NER, for named entity recognition (NER) focused on machine learning-related entities extracted from scientific literature. The dataset includes 54,598 mentions across 100 publications, featuring unique entity types such as Method, MLModel, and DatasetGeneric. The authors propose baseline models based on BERT variants, including Roberta and SciBERT, to facilitate future research. The dataset aims to support various analyses, although its motivation and applicability are questioned. The authors also benchmark various pretrained models, revealing performance gaps among entity types and exploring model performance under low-data regimes.

### Strengths and Weaknesses
Strengths:
- The dataset offers a focused contribution with a well-thought-out annotation scheme and a multi-annotator experiment demonstrating good inter-annotator agreement.
- The proposed dataset, GSAP-NER, provides a challenging benchmark for the scholarly information extraction community.
- The paper's methodology includes full-text annotation and a thoughtful paper selection policy, enhancing the dataset's relevance for future research.
- The authors evaluate a range of general and science-specific BERT-based models, providing insights into performance under limited data conditions.

Weaknesses:
- The motivation for the dataset's uniqueness is only partially convincing, particularly regarding its applicability compared to existing datasets like SciREX.
- The explanation of the NER models is insufficient, lacking detail on how BERT representations lead to span annotations.
- Definitions of new entity types, particularly MLModel versus ModelArchitecture, are unclear, contributing to low inter-annotator agreement.
- The scope of the ML method category is ambiguous, leading to low agreement and questioning the necessity of separate categories for model-related spans.
- The selection process for annotated papers may introduce bias, as it favors modeling papers over those discussing dataset creation or evaluation.
- The paper does not adequately address the limitations of the dataset or the baseline models, nor does it provide a complete "Data Card" for the dataset.
- The paper lacks a discussion of ambiguous cases and phenomena that could inform future work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation for the dataset by addressing why existing methods, such as those used in SciREX, are insufficient. Additionally, please provide a detailed explanation of the NER model architecture, including how BERT representations are utilized for span annotations. It is crucial to offer clear definitions distinguishing MLModel from ModelArchitecture to assist annotators. We also suggest reconsidering the scope of the ML method category and exploring whether it could serve as a parent category for model-related spans. Furthermore, address the potential bias in the selection process by considering citation counts instead of download metrics. Including a comprehensive "Data Card" that outlines the dataset's intended use cases, limitations, and demographic considerations regarding authorship is essential. Lastly, including a discussion on ambiguous cases and challenging spans for annotators would enhance the paper's depth, and we encourage the authors to clarify the motivation behind annotating reference links and URLs, as well as to explore the inclusion of metrics as entities in their dataset.