ID: ajnThDhuq6
Title: Improving Robustness with Adaptive Weight Decay
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 5, 4, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for adaptive weight decay (AWD) that dynamically tunes the weight decay hyperparameter during adversarial training to enhance model robustness. The core idea is to maintain a constant ratio between the updates of the weight decay and the cross-entropy loss. The authors evaluate their method against fixed weight decay hyperparameters, demonstrating improved performance in image classification and robustness against label noise. Furthermore, the authors compare their AWD method against state-of-the-art (SOTA) models, particularly focusing on robustness and clean accuracy metrics. They clarify that their training settings align with common practices in the literature, differing from those in previous works, which utilize tailored learning rate schedules. The authors emphasize that their WRN32-10 model achieves superior robustness and clean accuracy compared to the WRN34-10 model from prior research, despite using conventional training settings without additional optimizations.

### Strengths and Weaknesses
Strengths:
1. The experimental results clearly illustrate the advantages of AWD over traditional methods, as shown in concise figures (Figures 2 & 3).
2. The paper is well-written, with clearly formulated methods and strong empirical evidence supporting the proposed approach.
3. The authors provide a clear comparison of their method against SOTA models, highlighting the effectiveness of AWD in preventing robust overfitting.
4. The clarification of training settings and metrics strengthens the validity of their results.

Weaknesses:
1. The writing lacks clarity, particularly regarding the research gap and motivation for the study.
2. The benefits of AWD for pruning, as mentioned in the abstract, are inadequately supported.
3. The determination of the constant value for the decay over the gradient (DoG) is not clearly articulated.
4. The paper does not compare AWD with state-of-the-art methods in a comprehensive manner, which raises concerns about its relative effectiveness.
5. The hyperparameter DoG significantly influences results, questioning the method's adaptability.
6. The authors acknowledge that the training settings used in prior works differ significantly, which may limit the direct comparability of results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in articulating the research gap and motivation. Additionally, we suggest providing a more robust theoretical foundation for AWD and including comparisons with recent state-of-the-art methods to validate its effectiveness. The authors should also clarify the process for selecting the DoG hyperparameter, including the search intervals and steps, to enhance the method's perceived adaptability. Furthermore, addressing the editorial inconsistencies in terminology and notation will improve the overall reading experience. Lastly, we recommend that the authors include a detailed comparison of their training settings with the learning rate schedule used in prior works and consider redoing Table 2 using those settings to provide a more comprehensive analysis of AWD's performance under different conditions.