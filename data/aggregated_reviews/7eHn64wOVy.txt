ID: 7eHn64wOVy
Title: Random-Access Infinite Context Length for Transformers
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 5, 5, 5, 8, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel architecture for long-range decoder-transformers by introducing "landmark tokens" that represent blocks of input tokens. The attention mechanism utilizes these landmark tokens as multiplicative gates to manage attention across both local and distant tokens. During inference, the model selects relevant blocks based on landmark tokens, allowing for a hierarchical attention structure. This approach enables the extension of LLaMa 7B's input length from 2k to 10k tokens. The authors also propose a "Grouped Softmax" operation to facilitate attention calculations and a new method for positional encoding to address potential confusion from retrieved contexts. The authors claim that their method allows for "infinite context" by demonstrating capabilities with 10k to 32k tokens in synthetic datasets, asserting improved interpretability and retrieval abilities, emphasizing that each token can attend to any other token across blocks. However, the empirical support for these claims, particularly in realistic tasks, remains limited.

### Strengths and Weaknesses
Strengths:
- The method significantly extends the input length of models like LLaMA, enhancing their practical applicability.
- The proposed method demonstrates flexibility in training without requiring a fixed target sequence length.
- The exploration of positional encodings provides valuable insights and contributes to the broader language modeling community.
- The authors clarify the implementation details, which enhances understanding of their approach.

Weaknesses:
- The claim of "infinite context length" is unsubstantiated, as the model's ability to leverage attention over the entire past has not been demonstrated.
- The claims regarding advantages over Memorizing Transformers and Transformer-XL lack empirical validation, with only Transformer-XL being compared, which performs better in some contexts.
- The paper lacks sufficient evaluation against relevant baselines, and the "infinite context" assertion is questioned, as existing models have demonstrated much larger contexts.
- The explanation of positional encoding and the Grouped Softmax mechanism is unclear, leading to potential confusion.
- The experiments primarily focus on synthetic tasks, lacking qualitative or quantitative results to support claims of interpretability and efficiency.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the explanations regarding positional encoding and Grouped Softmax to enhance reader comprehension. Additionally, the authors should provide empirical evidence demonstrating the model's efficiency gains, such as memory usage and decoding-time speedup, particularly in long-context scenarios. It is crucial to include comparisons with other relevant baselines, such as Memorizing Transformers, TRIME-long, and sparse transformers, to validate the proposed method's effectiveness. Furthermore, conducting ablation studies on the impact of positional encodings and block sizes would strengthen the evaluation. Lastly, addressing the computational limitations and providing a clearer discussion on hyperparameter choices would enhance the paper's overall robustness. The authors should also conduct meaningful experiments in realistic tasks, such as book summarization or language modeling, to substantiate their assertions regarding interpretability and retrieval capabilities.