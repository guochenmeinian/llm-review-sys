ID: 5g0Z6PdogJ
Title: Testably Learning Polynomial Threshold Functions
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 7, 6, 5, 6, -1, -1, -1
Original Confidences: 2, 4, 2, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on testably learning polynomial threshold functions (PTFs) under the Gaussian distribution, demonstrating that degree-d PTFs can be testably learned up to an error of $\epsilon$ with a runtime of $n^{poly(1/\epsilon)}$. The authors link testable learning with distribution fooling and show that the techniques from previous works cannot be directly applied to PTFs. The paper also provides a new low-degree polynomial construction for approximating PTFs, addressing technical challenges in adapting fooling techniques to testable learning.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant gap in learning theory by extending testable learning to PTFs, providing the first sample and computational complexity results for this class.
- The runtime of $n^{poly(1/\epsilon)}$ aligns with the best known for agnostic learning of PTFs, marking a notable achievement.
- The writing is clear and the paper is easy to follow.

Weaknesses:
- The dependence of the algorithm's runtime on the degree d of the PTF is sub-optimal, with the order of $1/\epsilon$ being significantly worse than in agnostic learning.
- The practical applicability of the results is questioned due to the poor dependence on $\epsilon$ and degree, which may render the results less useful.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the significance of their contributions, particularly the techniques they emphasize, such as the use of Taylor expansion or the reduction of PTFs to multilinear PTFs. Additionally, we suggest exploring ways to reduce the runtime dependence on the degree d, potentially providing better bounds for smaller degrees like d=2 or d=3. Lastly, we encourage the authors to discuss the broader implications of their results and techniques for other learning theory problems.