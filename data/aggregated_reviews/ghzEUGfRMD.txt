ID: ghzEUGfRMD
Title: Scaling Laws for Hyperparameter Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 7, 7, 6, 7, -1, -1, -1
Original Confidences: 1, 3, 3, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a hyperparameter tuning scheme that utilizes an ensemble of power-law models as a surrogate for Bayesian optimization (BO) to estimate the future performance of hyperparameter configurations. The authors claim significant improvements over baseline methods in hyperparameter optimization benchmarks, demonstrating the effectiveness of their approach through rigorous experimentation across various datasets and tasks.

### Strengths and Weaknesses
Strengths:
- The writing is generally clear, and the method is well described, with substantial empirical support for the claims made.
- The integration of power law patterns into the surrogate function for BO is innovative and enhances the prediction of learning curves.
- The paper is well-structured, with a clear presentation of hypotheses and results that advance the state-of-the-art in hyperparameter optimization.

Weaknesses:
- The clarity of the paper could be improved, particularly in the abstract and the presentation of the algorithm, which could be moved from the appendix to the main text.
- There is a lack of figures explicitly showing power laws, which makes it difficult to assess the claims regarding their applicability.
- The originality of the work is somewhat diminished by insufficient citations of related literature, particularly concerning the use of power laws in hyperparameter optimization.
- The final analysis section regarding the use of DPL for large language models (LLMs) is less convincing and lacks comprehensive discussion on the computational costs involved.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the abstract to better convey the paper's direction and concepts. Additionally, moving the algorithm box to the main text would enhance understanding. It would be beneficial to include more figures that explicitly demonstrate power laws to substantiate the claims made. We also suggest that the authors differentiate their work more clearly from existing literature on power laws in hyperparameter optimization by including relevant citations. Furthermore, a more thorough discussion of the practical limitations of applying their method to large models, including computational costs, would strengthen the paper. Lastly, addressing the mathematical formulation issues noted in the reviews would enhance the overall quality and clarity of the work.