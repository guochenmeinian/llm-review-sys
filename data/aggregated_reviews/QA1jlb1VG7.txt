ID: QA1jlb1VG7
Title: CITB: A Benchmark for Continual Instruction Tuning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for the Continual Instruction Tuning (CIT) problem, consisting of two task streams derived from the SuperNI dataset. The authors propose an evaluation protocol to assess performance across old, upcoming, and unseen tasks. The findings indicate that existing continual learning (CL) methods do not effectively leverage rich natural language instructions, and surprisingly, simple fine-tuning does not lead to significant forgetting, attributed to the semantic richness of task instructions.

### Strengths and Weaknesses
Strengths:
- The establishment of a CIT benchmark is beneficial for the community and includes comprehensive metrics for evaluation.
- The paper is well-written, with extensive experiments that reveal the unexpected effectiveness of simple fine-tuning over other CL methods.
- The thoughtful evaluation protocol assesses performance across various task phases.

Weaknesses:
- The paper lacks an in-depth analysis of task characteristics, such as instruction style and task similarity, which could provide insights into the observed effectiveness of fine-tuning.
- There is no mention of code availability or supplementary materials, which diminishes the contribution of the benchmark.
- The evaluation is limited to a small model, raising questions about the applicability of the findings to larger models.

### Suggestions for Improvement
We recommend that the authors improve the analysis of task characteristics, including instruction style and task similarity, to better understand the dynamics of knowledge transfer and forgetting in CIT. Additionally, we suggest including generative replay methods in the benchmark and providing code or supplementary materials to enhance reproducibility. Lastly, consider evaluating larger models to validate the observations made with the current smaller model.