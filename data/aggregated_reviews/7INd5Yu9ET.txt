ID: 7INd5Yu9ET
Title: Long-Term Fairness with Unknown Dynamics
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to ensuring long-term fairness in reinforcement learning (RL) within dynamically responsive populations, framing it as a constrained Markov Decision Process (MDP). The authors introduce two algorithms, L-UCBFair and R-TD3, where L-UCBFair operates under a simplified linear MDP assumption, while R-TD3 is designed for a broader setting. The numerical experiments demonstrate that these methods consistently outperform baseline approaches, highlighting the ability of RL to balance short-term utility with long-term fairness objectives. Additionally, the paper includes a comparison of regret and distortion bounds to existing literature, proposing to include a more comprehensive explanation of these bounds in the appendix.

### Strengths and Weaknesses
Strengths:
- **Originality**: The authors uniquely address the fairness problem in machine learning by considering a dynamic state distribution influenced by the agent's actions, extending the static fairness discussions in existing literature.
- **Quality and Clarity**: The introduction effectively presents the motivation, and the related works section comprehensively discusses fairness in non-stationary settings and safe RL. The numerical experiments are clearly articulated.
- **Significance**: The proposed methods exhibit superior performance compared to baselines, and the theoretical analysis for L-UCBFair under the linear MDP assumption is thorough. The introduction and comparison of regret and distortion bounds enhance the paper's contribution to the field.

Weaknesses:
- **Weakness 1**: The clarity of Algorithm 1 could be improved, as several terms are not adequately defined, complicating comprehension.
- **Weakness 2**: The advantages of R-TD3 over L-UCBFair are not sufficiently elaborated, leaving its enhancements unclear.
- **Weakness 3**: The experiments utilize simple synthetic datasets; the authors should evaluate their methods in more complex environments, such as those used in existing literature on long-term fairness.
- **Weakness 4**: The results in Figure 1 appear predictable, as long-term planning methods should naturally outperform myopic approaches. A comparison with more robust baseline methods that consider underlying dynamics is warranted.
- **Weakness 5**: The current explanation of the regret and distortion bounds may lack depth, necessitating further elaboration.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Algorithm 1 by elaborating on the defined terms in Section 3.1.1. Additionally, the authors should provide a more detailed discussion on the advantages of R-TD3 compared to L-UCBFair. We suggest assessing the proposed methods in more complex simulation environments, such as those involving lending or college admissions, to enhance the robustness of the findings. Furthermore, a comparison with more sophisticated baseline methods that account for underlying dynamics would strengthen the results presented in Figure 1. Lastly, we recommend that the authors improve the explanation of the regret and distortion bounds in the appendix to provide greater clarity and depth.