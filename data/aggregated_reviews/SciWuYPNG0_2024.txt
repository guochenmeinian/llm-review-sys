ID: SciWuYPNG0
Title: Information Re-Organization Improves Reasoning in Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 5, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called Information Re-organization (InfoRE) designed to enhance the reasoning capabilities of large language models (LLMs). Unlike existing methods that focus on refining reasoning processes, InfoRE emphasizes the re-organization of contextual information to identify logical relationships before reasoning. The method involves extracting logical relationships and pruning redundant content to minimize noise, demonstrating an average improvement of 4% across various reasoning tasks. Extensive experiments validate the method's effectiveness across multiple models and datasets.

### Strengths and Weaknesses
Strengths:
- The method is intuitive and introduces a novel approach to restructuring information prior to reasoning, potentially laying a foundation for future advancements in LLM reasoning.
- The paper shows consistent performance improvements across tasks, supported by a comprehensive ablation study.
- The methodology is easy to follow and well-structured.

Weaknesses:
- The connection to existing prompt engineering works raises concerns about the paper's technical depth, which may be insufficient to justify its complexity.
- The baseline performance is relatively weak, necessitating the inclusion of additional baselines for comparison.
- The limited ablation study raises doubts about the necessity of the proposed complex design, particularly regarding the pruning component.

### Suggestions for Improvement
We recommend that the authors improve the technical depth of the paper by incorporating more robust baselines for comparison. Additionally, consider addressing the intricacies of the extraction and pruning processes to enhance practicality for real-time applications. We suggest exploring the use of LLMs for the information pruning part instead of a pre-trained BERT, and investigating the potential benefits of using an end-to-end model for simultaneous extraction and pruning. Furthermore, conducting experiments on datasets with injected noise or web-crawled data would provide insights into the method's effectiveness in minimizing noise. Lastly, clarifying the rationale behind the method's naming and providing more details about the extraction process and its evaluation would strengthen the paper.