ID: Z2he2Y0MoH
Title: Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the infinite-width limit of deep equilibrium networks (DEQs), demonstrating that the output converges to a Gaussian process. The authors prove that the limits of infinite width and infinite depth commute for DEQs, a result that contrasts with findings for traditional neural networks. The analysis is supported by numerical experiments that validate the theoretical claims.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, with clear explanations of the main technical points.
- The theoretical analysis is meticulous, and the numerical results effectively support the findings.
- The existence of a non-degenerate infinite depth limit is a notable contribution, suggesting stability in very deep networks.

Weaknesses:
- The practical implications of the results are unclear, and the authors should better articulate the significance of their findings in the context of existing literature.
- The focus on DEQs at initialization without addressing learning dynamics limits the scope of the work.
- Some figures are difficult to interpret, particularly Figures 3 and 4, which could benefit from additional detail.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the potential implications of their results, particularly how the commutation of limits might explain DEQs' competitive performance compared to state-of-the-art models. Additionally, including a paragraph on limitations in the conclusion section would enhance the paper's clarity. We also suggest providing more intuition regarding the differences between the DEQ and standard feedforward networks in the wide width limit, as well as addressing the training kernel (NTK) analysis, which could provide deeper insights into the model's behavior. Lastly, clarifying the figures, especially Figures 3 and 4, would aid in reader comprehension.