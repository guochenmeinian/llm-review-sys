ID: 5uZQ6spv9u
Title: BRAINTEASER: Lateral Thinking Puzzles for Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel multiple-choice question answering task focused on lateral thinking puzzles, aiming to assess creative reasoning rather than traditional logical reasoning. The authors propose a dataset enriched with human-validated distractors and two adversarial versions of puzzles to enhance robustness against memorization. The study evaluates both instruction-based and commonsense models, revealing significant performance gaps between human and model responses.

### Strengths and Weaknesses
Strengths:  
- The dataset is challenging and of high quality, validated through human efforts.  
- The paper is well-structured, clearly written, and presents a compelling argument for the relevance of lateral thinking in enhancing LLM capabilities.  
- The experiments are detailed and the data collection procedure is sound.  

Weaknesses:  
- The evaluation metrics, particularly accuracy, are deemed unreliable, with a recommendation for additional metrics like F1 to provide a clearer performance picture.  
- Concerns arise regarding the task's difficulty, as it may exceed the capabilities of both humans and models, raising questions about the appropriateness of the task for LLMs.  
- The use of smaller models in zero-shot settings may not yield convincing results, and the impact of few-shot examples on model performance is unclear.

### Suggestions for Improvement
We recommend that the authors improve the evaluation metrics by incorporating additional measures such as F1 to better assess model performance. Additionally, we suggest providing further clarification on how the models follow reasoning paths and addressing the potential issue of memorization by testing models with known training data. It would also be beneficial to explore classifying the dataset into more nuanced question types to enhance the effectiveness of few-shot examples. Lastly, we encourage the authors to elaborate on the selection process for human respondents to ensure that the task's difficulty is appropriately calibrated.