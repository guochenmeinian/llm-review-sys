ID: AYDBFxNon4
Title: Linking In-context Learning in Transformers to Human Episodic Memory
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 4, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study that establishes a correspondence between induction heads in transformers, which contribute to in-context learning (ICL), and the Contextual Maintenance and Retrieval (CMR) model of human episodic memory. The authors demonstrate mechanistic and behavioral similarities, showing that CMR-like attention patterns emerge in intermediate layers of large language models (LLMs) during training. The work provides a novel perspective on understanding ICL mechanisms through cognitive science models.

### Strengths and Weaknesses
Strengths:
1. The paper presents an original connection between mechanistic interpretability of LLMs and cognitive models of human memory, offering insights into both fields.
2. The authors provide a thorough analysis comparing induction heads and CMR, including detailed mapping and empirical support from experiments on pre-trained LLMs.
3. The overall presentation is clear and logically structured, making it accessible to readers from both machine learning and cognitive science backgrounds.

Weaknesses:
1. The writing lacks clarity, making it difficult for readers to follow, particularly those without a strong background in mechanistic interpretability, cognitive modeling, and neuroscience.
2. The experiments focus on a specific prompt design that may not fully capture the complexities of natural language processing, limiting the generalizability of the findings.
3. The paper does not establish a causal link between CMR-like behaviors and ICL, leaving it unclear whether these behaviors are necessary or merely a byproduct of training.
4. There is limited exploration of the biological plausibility of the proposed mechanisms and a lack of quantitative comparisons to human behavioral data.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in Section 4.1, to ensure it is accessible to the machine learning community. Additionally, including comparisons to human data would illustrate how temporal contiguity and forward asymmetry biases appear in humans. We suggest that the authors explore the implications of hyperparameter fitting and provide baselines to strengthen their claims. A deeper discussion on the biological plausibility of the proposed mechanisms and a quantitative comparison to human behavioral data would enhance the paper's impact. Finally, we encourage the authors to consider testing their findings on a broader range of Transformer architectures to assess generalizability.