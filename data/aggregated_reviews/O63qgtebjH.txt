ID: O63qgtebjH
Title: Scalable Primal-Dual Actor-Critic Method for Safe Multi-Agent RL with General Utilities
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 7, 6, 7, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extension of constrained single-agent general utilities reinforcement learning (RL) to a constrained multi-agent setting, where the global objective is additive across agents. The authors propose a k-hop communication method to address the challenges of agent communication in multi-agent environments. The paper provides theoretical analyses of convergence and sampling complexity, alongside experimental results demonstrating the algorithm's effectiveness in various tasks.

### Strengths and Weaknesses
Strengths:
- The extension to a multi-agent constrained setting is significant, particularly for general utility types.
- The k-hop policies are an interesting approach, although their novelty is uncertain.
- The convergence analysis is thorough for both exact and stochastic settings.
- The paper offers a clear mathematical formulation for safe multi-agent RL, encouraging further exploration in the intersection of Safe RL and MARL.
- Comprehensive experimental results validate the proposed method across different tasks.

Weaknesses:
- The additive global objective function does not account for inter-agent dependencies, which may limit exploration effectiveness.
- The clarity of contributions is questionable, especially if prior work has already addressed similar multi-agent extensions.
- The lack of baseline comparisons in the main text diminishes the demonstration of the proposed method's strengths.
- The communication protocol among unconnected neighboring agents is unclear, raising concerns about communication costs and data latency.
- The assumption regarding agent independence is overly strong, potentially undermining the algorithm's applicability in practical settings.
- The scalability of the algorithm is not adequately analyzed, particularly in dense agent scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by explicitly distinguishing their work from prior research. It would be beneficial to include baseline comparisons in the main body of the paper to strengthen the argument for the proposed method's effectiveness. Additionally, we suggest that the authors clarify the communication protocol for neighboring agents and analyze the communication complexity of their algorithm. Addressing the assumption of agent independence and discussing its implications in practical scenarios would enhance the robustness of the paper. Finally, we encourage the authors to explore the scalability of their approach in dense agent environments and consider integrating discussions on potential connections to graph neural networks.