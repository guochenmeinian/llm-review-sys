ID: jKLyKeZfzv
Title: MOTE-NAS: Multi-Objective Training-based Estimate for Efficient Neural Architecture Search
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Multi-Objective Training-based Estimate (MOTE) for efficient Neural Architecture Search (NAS), utilizing landscape view and convergence speed to estimate neural architecture performance. The authors propose two reduction strategies to expedite MOTE generation, achieving superior search performance on NasBench201 compared to other training-free NAS methods. MOTE-NAS employs a proxy that combines macro-level loss landscape smoothness and micro-level convergence speed, demonstrating state-of-the-art accuracy on benchmarks like CIFAR-10, CIFAR-100, and ImageNet-16-120 while significantly reducing computational costs.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a clear formulation of the problem.
- MOTE's theoretical analysis is sound, and experimental results are promising, particularly in reduced search spaces.
- The proposed proxy shows significant correlation with final accuracy, enhancing the efficiency of NAS.

Weaknesses:
- Code availability is lacking, which limits reproducibility.
- Experimental results on ImageNet are subpar, raising concerns about the adequacy of the training time reported.
- The title suggests a broader scope than the paper addresses, as it only considers accuracy without other critical metrics like parameters, FLOPs, and latency.
- The reduction strategies lack novelty, as similar low-fidelity estimation methods have been previously proposed.

### Suggestions for Improvement
We recommend that the authors improve the paper by making the code publicly available to enhance reproducibility. Additionally, the authors should provide a detailed analysis of the experimental results on ImageNet, clarifying the training time and its sufficiency for convergence. It would be beneficial to include a broader range of performance metrics beyond accuracy in the evaluation. We also suggest that the authors emphasize the impact of non-convexity and non-linearity in DNNs on MOTE's performance and conduct an ablation study on the effects of skip-connections and non-linear operations. Furthermore, the RD transformation method should be elaborated, including the rationale behind the choice of the VGG model and its applicability to other datasets. Lastly, we encourage the authors to evaluate MOTE on a wider variety of search spaces, including supernets and Transformer-based models, to assess its generalizability.