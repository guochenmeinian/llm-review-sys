ID: hpUNou0UaJ
Title: impact of sample selection on in-context learning for entity extraction from scientific writing
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of in-context learning (ICL) sample selection methods for entity extraction from scientific documents, utilizing KATE, Perplexity, BM25, and Influence methods. The authors find that while a finetuned RoBERTa model achieves superior results on most datasets, ICL methods demonstrate robustness in low-data scenarios and their effectiveness varies by domain and entity type. The paper provides a comprehensive evaluation across five datasets and discusses the implications for future NLP tasks.

### Strengths and Weaknesses
Strengths:
- The paper offers a thorough analysis of ICL sample selection methods, contributing valuable insights to a rapidly evolving field.
- It compares ICL methods with finetuned models, highlighting the strengths of ICL in low-resource settings.
- The findings are insightful and could inform future research in entity extraction and related NLP tasks.

Weaknesses:
- The results do not significantly surpass the established baseline, raising concerns about the overall impact.
- The evaluation lacks consistency, particularly regarding the stability of GPT outputs across multiple runs.
- The paper does not sufficiently explore the reasons behind the varying performance of different selection methods.
- It relies solely on GPT-3.5 without considering other models or the impact of prompt design.

### Suggestions for Improvement
We recommend that the authors improve the analysis by including comparisons with other few-shot entity extraction approaches and exploring additional NLP tasks, such as relation extraction and classification. It would also strengthen the paper to incorporate a broader range of LLMs, including instruction-based models like FLanT5 and Galactica, and to provide a detailed description of prompt templates. Additionally, we suggest conducting significance tests to validate the reported results and including an anonymous source code repository to enhance reproducibility. Finally, considering advanced scientific pre-trained models like SPECTER and SciNCL could provide a more robust baseline for comparison.