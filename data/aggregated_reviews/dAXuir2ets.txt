ID: dAXuir2ets
Title: SpaFL: Communication-Efficient Federated Learning With Sparse Models And Low Computational Overhead
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SpaFL, a federated learning framework designed to enhance communication efficiency and reduce computational overhead by optimizing sparse model structures. The authors propose using trainable thresholds to achieve structured sparsity, allowing only the exchange of these thresholds between clients and the server, which significantly minimizes communication costs. Empirical results demonstrate that SpaFL performs well on popular benchmarks, achieving lower computational overhead while maintaining competitive accuracy. Additionally, the authors aim to improve convergence rates while addressing communication costs and privacy guarantees, proposing a theoretical comparison with existing methods, particularly focusing on convergence analysis relative to models with time-varying pruning.

### Strengths and Weaknesses
Strengths:  
1. The paper is generally well-written and easy to follow.  
2. The novel approach of using trainable thresholds is effective for improving communication efficiency and reducing computational costs.  
3. Valid experiments show that SpaFL outperforms baseline methods on image datasets.  
4. The authors provide a solid theoretical foundation for their algorithm, demonstrating its convergence properties and privacy guarantees.  
5. The response to reviewer comments indicates a willingness to enhance the paper by including comparisons with recent methods and clarifying communication cost calculations.  

Weaknesses:  
1. The main figure is confusing and lacks clarity on how thresholds are selected and combined across clients.  
2. Theoretical analysis, particularly regarding the generalization bound, convergence rate, and communication complexity, is insufficiently detailed.  
3. Comparisons with more recent baseline methods are lacking, as the current baselines are outdated.  
4. The assumption of a constant bounded gradient is criticized as potentially unrealistic in the context of federated learning.  
5. The initial version lacks a thorough theoretical comparison with existing methods and clarity in communication cost calculations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the main figure by providing a high-level explanation of the threshold selection process and how these thresholds are combined. Additionally, we suggest including a more thorough analysis of the generalization bound, convergence rate, and communication complexity to enhance the theoretical foundation of the work. It is crucial to compare SpaFL with more recent methods to demonstrate its relevance and effectiveness in the current landscape of federated learning. We also recommend that the authors improve the discussion on the feasibility of the assumption regarding constant bounded gradients. Finally, we encourage the authors to clarify the computation of uplink and downlink communication costs, potentially by following established equations from relevant literature, and to provide results that align with the methodology in existing studies.