ID: fem6BIJkdv
Title: Representation Learning via Consistent Assignment of Views over Random Partitions
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 5, 3, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for self-supervised learning that addresses the collapsing problem in clustering-based contrastive learning by introducing random partitioning of prototypes. The authors propose a divide-and-conquer approach that stabilizes training and enhances performance without adding extra hyperparameters. They emphasize the necessity of utilizing a higher number of prototypes to effectively leverage local information for unsupervised local view agreement, arguing that while fewer prototypes require each to encapsulate broader variations, increasing the prototype count allows for more specific representations that capture local changes within the data manifold. The method, CARP, is empirically validated across various datasets, showing improvements in kNN and retrieval tasks, with robustness to prototype count variations. However, the contributions are questioned, particularly regarding the comparison with existing methods like CARL and CoKe, as well as the overall effectiveness of the proposed method.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, clearly distinguishing novel contributions from prior work.
- The random partitioning strategy effectively mitigates stability issues associated with a large number of prototypes.
- The authors provide a clear rationale for the necessity of a higher number of prototypes, linking it to the ability to capture local data variations effectively.
- Extensive experiments demonstrate the method's robustness across diverse downstream tasks, with empirical evidence supporting the adaptability of CARP across different prototype counts.

Weaknesses:
- The contributions are not clearly articulated, especially in relation to CARL, which reportedly performs well with a large number of prototypes.
- The evaluation primarily focuses on retrieval tasks, neglecting other potential applications such as full fine-tuning and dense prediction tasks.
- The proposed method does not consistently outperform all baselines, raising questions about its overall effectiveness.
- The simplicity of the approach may limit its applicability to a narrow range of scenarios, particularly unsupervised clustering.
- The introduction of a new hyperparameter (partition size) raises concerns about the claimed reduction in tuning efforts.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions, particularly in comparison to CARL, by addressing the experimental setup and tuning parameters. Additionally, including evaluations on Vision Transformers (ViTs) could strengthen the paper's contributions. We suggest expanding the discussion on the necessity of using a large number of prototypes and exploring the implications of multi-crop augmentation on performance. Furthermore, we encourage the authors to conduct further comparisons with related baselines, particularly in clustering and transfer learning evaluations, to provide a more comprehensive understanding of their method's performance. Lastly, we suggest that the authors clarify the trade-offs involved in prototype selection and explore automated methods for optimizing prototype counts in future work.