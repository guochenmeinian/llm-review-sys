ID: hYjRmGqq5e
Title: A2PO: Towards Effective Offline Reinforcement Learning from an Advantage-aware Perspective
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 6, 7, 6, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents A2PO, an offline reinforcement learning method designed to address constraint conflicts in mixed-quality datasets collected from various behavior policies. A2PO utilizes a Conditional Variational Auto-Encoder (CVAE) to disentangle action distributions and optimize policies towards high advantage values. The authors propose an additional actor π to learn the latent variable z~∗, enhancing action selection precision compared to a random vector input. The experimental results demonstrate A2PO's effectiveness through extensive experiments on the D4RL benchmark, showing superior performance compared to existing offline RL methods and the CVAE policy, particularly in mixed-quality datasets.

### Strengths and Weaknesses
Strengths:
1. The introduction of CVAE for disentangling behavior policies in mixed-quality datasets is novel.
2. The paper is well-structured, clearly written, and easy to follow.
3. Extensive experimental analysis and ablation studies substantiate the method's advantages.
4. The A2PO policy demonstrates a significant performance advantage over the CVAE policy in various scenarios.

Weaknesses:
1. The comparison with existing techniques lacks depth and does not sufficiently analyze the algorithm's effectiveness or conditions for failure.
2. The paper overlooks recent work on policy constraints relevant to A2PO, such as PRDC, which could strengthen the argument for its advantages.
3. Implementation details regarding the selection of high-quality samples and the accuracy of advantage information require clarification.
4. Some reviewers noted the need for additional baseline comparisons and an ablation study to further validate the findings.

### Suggestions for Improvement
We recommend that the authors improve the depth of their comparisons with existing methods, particularly by including analyses of why A2PO works effectively. Additionally, discussing the conditions under which A2PO may fail would enhance the paper's robustness. We suggest incorporating comparisons with recent policy constraint methods like PRDC, as well as return-conditioned methods DT and %BC, and data rebalance methods ReD to demonstrate A2PO's relative advantages. Furthermore, we advise including an ablation study on the A2PO regularization term in the actor loss. Clarifying how to ensure a sufficient number of high-quality samples when selecting ξ = 1 and guaranteeing the accuracy of the introduced advantage information would strengthen the implementation details. Lastly, we recommend updating the description of the policy evaluation paragraph and correcting the score reports related to baselines in the gym task, as well as updating the score reports in the antmaze v0 version environment.