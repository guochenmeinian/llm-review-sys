ID: 6rabAZhCRS
Title: Explaining Predictive Uncertainty with Information Theoretic Shapley Values
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 4, 6, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for explaining the uncertainty in predictions made by deep neural networks (DNNs) by adapting the Shapley value framework. The authors propose a modified Shapley value that connects to the entropy of the predictive distribution, aiming to decompose uncertainty into aleatoric and epistemic sources. The paper includes theoretical bounds to support the coverage of the explanations and demonstrates the method through various experiments.

### Strengths and Weaknesses
Strengths:
- The integration of KL divergence and entropy with Shapley values is novel and potentially impactful for quantifying feature importance.
- The paper is well-organized, with a thorough literature review and clear theoretical derivations.
- Extensive experiments validate the proposed method and its relevance to uncertainty quantification in machine learning.

Weaknesses:
- The manuscript lacks sufficient context regarding existing works that have explored Shapley values for explaining conditional variance, such as Williamson and Feng (2020).
- The motivation for focusing on uncertainty rather than model output is not convincingly articulated, and the proposed method does not clearly demonstrate superiority over traditional Shapley values.
- There are concerns regarding the clarity of experimental details, including the ensemble methods used and the hyperparameters involved.
- The proposed Shapley value's interpretation when using specific value functions ($v_{KL}$ and $v_{CE}$) is unclear, particularly regarding its output being zero.

### Suggestions for Improvement
We recommend that the authors improve the contextualization of their work by discussing existing literature on Shapley values and conditional variance, particularly addressing the contributions of Williamson and Feng (2020). Additionally, the authors should clarify the motivation for their focus on uncertainty and provide a more compelling argument for the advantages of their approach over traditional methods. It is essential to enhance the experimental section by including detailed descriptions of the methods and hyperparameters used. Finally, we suggest that the authors clarify the interpretation of the Shapley values when employing the value functions $v_{KL}$ and $v_{CE}$, ensuring that the implications of a zero output are well understood.