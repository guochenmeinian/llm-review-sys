ID: t4CnKu6yXn
Title: Investigating the role of modality and training objective on representational alignment between transformers and the brain
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 8, 8, 4, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the brain-model alignment of various large language models (LLMs) and vision-language models (VLMs), revealing differing degrees of alignment based on input modality and training objectives. The authors find that models perform variably across different brain regions, with some models excelling in higher-level cognitive functions not directly tied to specific modalities. The study employs a unique dataset of videos and utilizes searchlight RSA techniques, contributing novel insights into the field.

### Strengths and Weaknesses
Strengths:
- The selection of a diverse range of models, particularly those of similar sizes, enhances the clarity of the impact of input modality and training objectives on brain-model alignment.
- The use of video input for brain-model alignment analysis is innovative, as prior studies have primarily focused on text and image modalities.
- The application of searchlight RSA techniques provides corroborative evidence for the findings.

Weaknesses:
- The discussion lacks depth and critical analysis, moving too quickly to conclusions without sufficient context.
- The choice of models raises questions, particularly the inclusion of CodeLlama without clear justification and the absence of key models like CLIP.
- The paper overlooks significant literature on brain-language alignment, limiting the contextual interpretation of results.
- Methodological issues arise from comparing models that vary in multiple respects, complicating the understanding of underlying inductive biases.

### Suggestions for Improvement
We recommend that the authors improve the depth of their discussion and provide a more critical analysis of their findings. Clarifying the rationale for model selection, particularly regarding CodeLlama, and including key models like CLIP would strengthen the paper. Additionally, situating the results within the existing literature on brain-language alignment is essential. We suggest that the authors consider audio models given the multimodal nature of the stimuli. To enhance reproducibility, explicitly state which layers and parts of the layers were analyzed, and consider a cross-validation approach to identify the most predictive layers for each brain region. Finally, revising Figures 2 and 3 for clarity and summarizing key results more concisely would improve the overall presentation.