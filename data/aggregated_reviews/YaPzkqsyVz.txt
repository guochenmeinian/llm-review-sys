ID: YaPzkqsyVz
Title: Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel Multimodal Query Suggestion (MMQS) task aimed at generating query suggestions based on both user textual inputs and query images to enhance search result intentionality and diversity. The authors propose the RL4Sugg framework, which utilizes Large Language Models (LLMs) and Multi-Agent Reinforcement Learning from Human Feedback to optimize the suggestion generation process. Extensive experiments demonstrate the effectiveness of RL4Sugg, showing an 18% improvement over existing methods and successful implementation in real-world search engine products.

### Strengths and Weaknesses
Strengths:
- The paper introduces the MMQS task and provides a clear, well-organized structure.
- The proposed RL4Sugg framework effectively leverages LLMs and multi-agent reinforcement learning.
- Comprehensive experiments validate the framework's effectiveness on real-world datasets.

Weaknesses:
- Technical justifications for certain design choices, such as the selection of Image-Suggestion Alignment (ISA), Image-Suggestion Generation (ISG), and Image-Suggestion Matching (ISM) for the RewardNet, are lacking.
- The necessity and design of Agent-D are not well justified, and comparisons with alternative methods like rule-based filtering are missing.
- Experiments are conducted solely on non-public datasets, raising concerns about generalizability.

### Suggestions for Improvement
We recommend that the authors improve the justification for the selection of ISA, ISG, and ISM in the RewardNet, possibly including ablation study results. Additionally, clarify the role of Agent-D and consider whether Agent-I could be modified to handle both generation and diversity tasks. We also suggest including comparisons with rule-based filtering methods and providing more context on the cold-start issue, including examples. Furthermore, the authors should address the prompt format for data generation and clarify whether evaluations are based on one or multiple suggestions generated by each model. Lastly, significance testing should be added to validate performance improvements over the OPT structure.