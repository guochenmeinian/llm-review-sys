ID: RMdnTnffou
Title: Coarse-to-Fine Concept Bottleneck Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel label-free Concept Bottleneck Model (CBM) that enhances ante-hoc interpretability through a hierarchical concept representation. The model incorporates a two-level hierarchy, where high-level concepts represent scenes/objects and lower-level concepts denote specific attributes at a patch level. The authors demonstrate improved accuracy and concept prediction using a Jaccard index-based metric for evaluating concept prediction quality.

### Strengths and Weaknesses
Strengths:
1. The hierarchical representation of concepts is intuitive and novel for interpretability in CBMs.
2. Comprehensive experiments are conducted across diverse large-scale datasets and baselines.
3. The presentation is strong, with clear motivations.

Weaknesses:
1. Concerns exist regarding the interpretability and soundness of label-free CBMs that depend on CLIP embedding similarity for concept prediction.
2. The authors should include standard CBMs as a baseline for concept prediction accuracy to assess performance gaps.
3. The model's behavior with concept intervention, akin to original CBMs, should be qualitatively explored.

### Suggestions for Improvement
We recommend that the authors improve the interpretability of the model by investigating how concept detection is grounded, possibly using qualitative tools such as saliency maps or activation maximization. Additionally, we suggest including standard CBMs as a baseline in the experiments to evaluate the performance gap in concept prediction accuracy. Lastly, exploring the system's behavior with concept interventions would provide valuable insights into its interpretability.