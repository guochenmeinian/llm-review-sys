ID: NnoAj91HZX
Title: No-Regret M${}^{\natural}$-Concave Function Maximization: Stochastic Bandit Algorithms and NP-Hardness of Adversarial Full-Information Setting
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of online learning variants of ${\rm M}^\natural$-concave function maximization, which generalizes maximum flows on graphs and has applications in resource allocation. The authors propose a learning strategy achieving $O(K N^{1/3} T^{2/3})$-regret in stochastic settings by demonstrating that the greedy algorithm is "robust to local errors." Additionally, they establish NP-hardness for the adversarial setting, indicating that sub-linear regret is unattainable in polynomial time unless P=NP.

### Strengths and Weaknesses
Strengths:
- The paper addresses a broad class of functions, enhancing the potential applicability of the results.
- The "robust to local errors" theorem for ${\rm M}^\natural$-concave functions may hold independent significance.
- The proofs are well-structured and comprehensible, which is commendable given the technical complexity.
- The NP-hardness result highlights a crucial distinction between stochastic and adversarial settings.

Weaknesses:
- The techniques for the upper bound are relatively straightforward post-Theorem 3.1, raising questions about potential improvements, such as achieving sublinear regret in $K$ or enhancing the bound for $T$ to $O(\sqrt{T})$.
- The motivation for the stochastic online setting could be strengthened with more concrete examples to illustrate broader applicability.
- The absence of experimental evaluations limits the demonstration of theoretical contributions in practical scenarios.

### Suggestions for Improvement
We recommend that the authors improve the motivation for the stochastic online setting by providing more concrete examples to clarify the applicability of their results. Additionally, we suggest exploring potential enhancements to the upper bound, such as achieving sublinear regret in $K$ or refining the bound for $T$ to $O(\sqrt{T})$. Finally, we encourage the authors to include experimental evaluations to validate the theoretical contributions in real-world contexts.