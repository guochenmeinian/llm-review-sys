ID: 4XTvXMSZPO
Title: DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 7, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DigiRL, a novel autonomous reinforcement learning (RL) approach designed for in-the-wild device control through graphical user interfaces (GUIs). The authors propose a two-stage training process: an offline RL phase utilizing static demonstrations followed by an online RL phase where the agent interacts with real environments. DigiRL leverages pre-trained visual language models (VLMs) and introduces a scalable Android learning environment with a robust reward model. The evaluation demonstrates a significant improvement in task success rates over existing state-of-the-art agents, establishing DigiRL as a benchmark in device control.

### Strengths and Weaknesses
Strengths:
- The application of RL for developing a digital agent for device control is compelling.
- The implementation of a scalable Android learning environment is commendable, and open-sourcing this would benefit the research community.
- The clarity and structure of the paper enhance its readability.
- The experimental results, particularly the ablation studies, are thorough and informative.

Weaknesses:
- The definition of the problem as a POMDP could be improved by considering a contextual POMDP.
- The paper lacks a pseudo-code or illustrative diagram to clarify the method.
- There is ambiguity regarding how the policy and value network are conditioned on the context $c$.
- The novelty of the contributions is limited, as many techniques used are well-established in the field without significant customization.
- The focus on a specific domain may restrict the broader applicability of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the policy and value network conditioning on context $c$. Additionally, including a pseudo-code or diagram would facilitate understanding of the method. To enhance the novelty, consider providing theoretical insights into the design choices made for offline and online RL. We also suggest expanding the comparison with other online RL methods beyond Filtered BC to better justify DigiRL's advantages. Lastly, addressing the balance between the two estimators in section 4.2 and ensuring the offline and online data usage aligns with the intended methodology would strengthen the paper.