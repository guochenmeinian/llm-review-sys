ID: 7arAADUK6D
Title: Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 6, 6, -1, -1
Original Confidences: 3, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents DEEPEN, a novel training-free ensemble framework that leverages the complementary strengths of various large language models (LLMs) by fusing informative probability distributions at each decoding step. The authors propose transforming the probability distributions from each model's vocabulary space into a shared "relative representation" space, addressing vocabulary discrepancies. The method aggregates these representations and maps them back to the vocabulary space of a primary LLM to determine the generated token.

### Strengths and Weaknesses
Strengths:
1. DEEPEN effectively addresses the vocabulary mismatch problem, enabling collaboration among heterogeneous LLMs without additional training.
2. Extensive experiments across six benchmarks demonstrate consistent performance improvements and stability compared to baseline methods.
3. The framework is well-structured, mathematically motivated, and shows good empirical performance with valuable insights from ablation studies.

Weaknesses:
1. The method's performance is sensitive to hyperparameter choices, such as the relative ensemble learning rate, which may complicate tuning.
2. The paper lacks a detailed analysis of DEEPEN's generalization to unseen data and different task types beyond the evaluated benchmarks.
3. The performance heavily relies on anchor word selection, yet the impact of different strategies for anchor word selection is not thoroughly explored.
4. Some sections could be condensed for clarity, and the evaluation primarily compares against baselines in limited settings.

### Suggestions for Improvement
We recommend that the authors improve the analysis of DEEPEN's generalization capabilities across various tasks and unseen data distributions. Additionally, a more thorough exploration of anchor word selection strategies and their impact on performance would enhance the paper. We also suggest condensing repetitive sections to improve reading flow and including comparisons with baseline methods in all experiments for a more comprehensive evaluation.