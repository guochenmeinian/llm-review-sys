ID: u6FuiKzT1K
Title: Leveraging Contrastive Learning for Enhanced Node Representations in Tokenized Graph Transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GCFormer, a novel graph transformer (GT) architecture aimed at enhancing node classification on both homophilic and heterophilic graphs. The authors propose a method that samples a fixed number of positive and negative tokens for each node, restricting attention computation to these tokens. The sampling relies on similarity scores derived from node attributes and topological information. Additionally, a contrastive loss is introduced alongside the supervised cross-entropy loss to improve node representation by encouraging proximity to positive samples and distance from negative samples. GCFormer is evaluated across various node classification tasks with differing homophily levels.

### Strengths and Weaknesses
Strengths:
- The experimental study is comprehensive, utilizing eight datasets with diverse homophily levels.
- All baselines are tuned within the same hyper-parameter ranges, ensuring fair comparisons.
- An ablation study demonstrates the effectiveness of the proposed approach, particularly the strategy of learning representations close to positive tokens and distant from negative tokens.

Weaknesses:
- The paper lacks clear disambiguation from prior work, weakening the motivation for the proposed model.
- The authors primarily compare GCFormer to only two positive-sampling GTs, while omitting other relevant models that perform attention across all node pairs.
- Claims regarding the limitations of other models, such as GraphGPS, are not sufficiently supported by evidence or comparisons.

### Suggestions for Improvement
We recommend that the authors improve the disambiguation of their work from prior models, particularly by clearly defining the class of architectures they refer to as "tokenized graph Transformers." Additionally, the authors should include more representative baselines, such as VCR-Graphormer, to strengthen their experimental results. It is essential to provide detailed explanations of how negative token sequences are constructed and how virtual nodes are utilized in learning representations. Furthermore, we encourage the authors to present empirical evidence supporting their claims about the shortcomings of models like GraphGPS, potentially by including it as a baseline in their experiments.