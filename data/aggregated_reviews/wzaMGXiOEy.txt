ID: wzaMGXiOEy
Title: Intermediate Fine-Tuning Improves Mathematical Reasoning in Smaller Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 5, 8, 7, 6
Original Confidences: 4, 3, 4, 4

Aggregated Review:
### Key Points
This paper presents an effective method for enhancing mathematical reasoning in smaller models through intermediate fine-tuning (IFT) on a synthetically generated arithmetic dataset. The authors report significant performance improvements on reasoning tasks, particularly on the GSM8K dataset. However, the focus on arithmetic deficiencies raises concerns about novelty, and the paper lacks exploration of alternative tasks and broader applications.

### Strengths and Weaknesses
Strengths:
- The proposed method is straightforward, and extensive experiments validate its effectiveness on GSM8K math reasoning tasks.
- IFT significantly improves reasoning task performance while preserving prior model knowledge, demonstrating robustness across diverse datasets.
- The authors successfully programmatically generate a dataset with arithmetic tasks, including fractions and percentages, addressing the challenges of dataset creation.
- Relevant works are thoroughly discussed and cited, establishing clear relationships to the submission.

Weaknesses:
- The lack of novelty is evident as previous works have addressed LLMs' arithmetic performance using various methods.
- The absence of a clear methodology for optimal checkpoint selection during IFT limits the approach's consistency.
- The method's heavy reliance on the GSM8K dataset raises concerns about its applicability to other reasoning tasks without significant modifications.
- The study does not explore whether the advantages of IFT are confined to specific model architectures like FlanT5.
- The research does not identify a methodology to determine the optimal stopping point for IFT to ensure maximum accuracy.
- Potential biases in the generated data for IFT are not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by exploring alternative tasks and broader applications beyond arithmetic. Additionally, clarifying the methodology for generating IFT data and distinguishing it from the "GOAT" method would enhance the paper's rigor. We suggest including a fair comparison regarding FLOPs by training the baseline for more epochs to match the FLOPs used in IFT. Furthermore, investigating the necessity of using an arithmetic dataset and analyzing the model's performance on GSM8K validation/test sets would provide deeper insights into the effectiveness of IFT. Lastly, addressing potential biases in the generated data would make the research more engaging and robust.