ID: XW4t7P2hpN
Title: Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the efficacy of pretraining large autoregressive language models (LMs) with retrieval mechanisms. The authors reproduce the RETRO model and conduct a thorough comparison with GPT across various tasks, revealing that RETRO outperforms GPT in text generation with reduced repetition, higher factual accuracy, and lower toxicity. Additionally, RETRO excels in knowledge-intensive tasks on the LM Evaluation Harness benchmark but performs similarly to GPT on other tasks. The authors also introduce RETRO++, which enhances performance in open-domain question answering.

### Strengths and Weaknesses
Strengths:  
- The paper provides a detailed reproduction of RETRO and builds upon it, demonstrating the advantages of incorporating retrieval in pretraining.  
- Comprehensive experimental analysis supports the claims, showcasing significant performance improvements.  
- Clear writing and structured flow enhance the presentation of findings.

Weaknesses:  
- The novelty of the method is questioned, as it closely follows existing work on RETRO.  
- Limited baseline comparisons, particularly with models that utilize instruction tuning or RLHF, weaken the conclusions.  
- The computational costs and statistical significance of results are not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by including additional baseline models, such as GPT + retrieval at inference time, to provide a more comprehensive comparison. Additionally, clarifying the computational costs associated with training RETRO and RETRO++ would strengthen the paper. It would also be beneficial to specify the number of runs per experiment and whether the metrics presented are averages. Finally, incorporating the additional experiments and insights mentioned in the author responses would enhance the overall strength of the paper.