ID: DSVGACQ3sO
Title: Demystifying amortized causal discovery with transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 3, 6, 4, 4, -1, -1, -1, -1
Original Confidences: 5, 2, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical study of the generalization capabilities of supervised causal discovery methods, particularly focusing on the CSIvA model. The authors explore how training on diverse structural causal models (SCMs) can enhance generalization to various causal models, arguing that constraints on training data implicitly define priors for test observations. They provide a theoretical basis for this approach, suggesting that training on mixtures of identifiable SCMs improves performance.

### Strengths and Weaknesses
Strengths:
- The paper addresses the behavior of amortized causal discovery methods, an area that has not been extensively studied.
- Empirical insights validate intuitions regarding identifiability and generalization, contributing interesting findings to the field.

Weaknesses:
- The analysis is limited to the bivariate case, which undermines claims about generalizing to larger systems and misrepresents the contributions relative to existing literature.
- The paper lacks a thorough comparison and deeper analysis of the prediction performance on diverse SCMs, and some sections, such as 3.2, do not contribute novel insights.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by explicitly stating the limitations of their analysis to the bivariate case and recalibrating claims about the generalization of transformers in causal discovery. Additionally, we suggest that the authors provide a more comprehensive comparison of their results with existing literature and consider a more detailed exploration of the prediction performance on various SCMs. It would also be beneficial to clarify the implications of their findings regarding the identifiability of graphs and to reconsider the terminology used, such as adopting "supervised causal learning" instead of "amortized causal discovery."