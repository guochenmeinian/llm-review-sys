ID: CFez7MFUFd
Title: Cross-Scale Self-Supervised Blind Image Deblurring via Implicit Neural Representation
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 5, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a self-supervised method for blind image deconvolution (BID) that utilizes implicit neural representations (INRs) to represent both the latent image and the blur kernel. The authors propose a cross-scale consistency loss that enhances model efficacy by leveraging relationships among blurred images, latent images, and blur kernels across different scales. Experimental results indicate that the proposed method outperforms existing self-supervised methods on various datasets.

### Strengths and Weaknesses
Strengths:
1. The introduction of INRs for cross-scale representation is innovative and beneficial.
2. The paper includes extensive theoretical and experimental analysis, demonstrating the method's superiority through various datasets and ablation studies.
3. The presentation is generally clear and well-organized, with substantial quantitative and visualization materials.

Weaknesses:
1. The novelty of the proposed method is limited, as it closely resembles existing methods, particularly one referenced in Ref1, without sufficient discussion of differences.
2. The design choices regarding network architectures for generating images and kernels lack clarity, particularly in balancing their complexities.
3. The paper does not adequately compare its method with Ref1 or address the feasibility of generated kernels.
4. Some experimental settings and method principles are vague, and the justification for loss functions used is insufficient.

### Suggestions for Improvement
We recommend that the authors improve the novelty discussion by clearly articulating the differences between their method and that in Ref1. Additionally, clarify the rationale behind the chosen network architectures for image and kernel generation, and consider addressing how to balance their complexities. It would be beneficial to include comparisons with Ref1 and provide visualizations of the generated kernels to assess their feasibility. Furthermore, we suggest enhancing the clarity of experimental settings and justifying the choice of loss functions, potentially exploring alternatives like L2 and L1 loss.