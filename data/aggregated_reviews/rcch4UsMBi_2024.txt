ID: rcch4UsMBi
Title: Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 5, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents generalized instruction tuning (GLAN), a scalable method for generating synthetic instruction data using a taxonomy of human knowledge and capabilities. GLAN constructs this taxonomy by decomposing knowledge into fields and disciplines, allowing for the generation of diverse instruction datasets without seed examples. Extensive experiments demonstrate that GLAN outperforms or matches state-of-the-art models across various benchmarks, particularly in mathematical reasoning and coding tasks.

### Strengths and Weaknesses
Strengths:
1. Comprehensive Coverage of Evaluation: The paper includes extensive experiments showing GLAN's effectiveness across multiple dimensions, outperforming popular instruction-tuned LLMs.
2. Minimization of Human Involvement: The approach significantly reduces human involvement, requiring verification only at the taxonomy construction stage, enhancing scalability.
3. Customizability and Extensibility: The taxonomy-based method allows for easy incorporation of new fields or skills by adding nodes to the taxonomy.
4. Strong Performance: GLAN shows promising scaling properties and does not overfit to training data, ensuring generalizability across domains.

Weaknesses:
1. Limited Novelty: The method's novelty is constrained as similar top-down designs have been used in previous works, and main experimental results appear mediocre compared to other methods.
2. Lack of Human Curriculum Utilization: The approach does not leverage existing human curriculum structures, potentially missing valuable insights and introducing bias.
3. Computational Cost Comparison: The paper lacks a comparison of computational costs with similar methods, which is crucial for evaluating efficiency.
4. High Variability in Results: There is significant variability in GLAN's performance across categories, particularly weaker results in humanities and social sciences.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the diversity of generated synthetic data to prevent redundancy. Additionally, we suggest incorporating existing human curriculum structures to enhance the taxonomy and mitigate bias. A detailed comparison of computational costs with similar methods should be included to illustrate efficiency. Finally, addressing the high variability in performance across different categories, particularly in humanities and social sciences, would strengthen the paper's findings.