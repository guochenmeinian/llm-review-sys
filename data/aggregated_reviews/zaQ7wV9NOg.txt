ID: zaQ7wV9NOg
Title: Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework  for Online RL
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 8, 7, 7, 7, 7, -1, -1, -1
Original Confidences: 4, 3, 4, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm, $\texttt{OPTIMISTIC NPG}$, which combines natural policy gradient methods with optimistic policy evaluation for online reinforcement learning (RL). The algorithm is computationally efficient and achieves optimal sample complexity in linear Markov Decision Processes (MDPs), outperforming existing methods. It also extends to general function approximation, making it the first algorithm to demonstrate polynomial sample complexity for learning near-optimal policies in this broader context. The authors provide rigorous theoretical guarantees and practical examples to validate their claims.

### Strengths and Weaknesses
Strengths:
1. The algorithm is simple, efficient, and easy to understand.
2. The theoretical framework is sound, with clear proofs and arguments.
3. It achieves optimal dependence on dimension \(d\) in linear cases and provides sample complexity results for both tabular and general function approximations.
4. The paper is well-written and organized, with a clear literature comparison.

Weaknesses:
1. The technical novelty for general function classes is unclear, requiring further explanation on constructing confidence sets.
2. The algorithm's performance in tabular scenarios is inferior to existing results.
3. The dependency on \(\epsilon\) under linear MDPs is not optimal.
4. There is a lack of experimental validation to support claims of computational efficiency.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the construction of confidence sets for general function classes. Additionally, addressing the computational efficiency challenges in general function approximation would strengthen the paper. We suggest providing experimental results to validate the theoretical claims of computational efficiency. Furthermore, clarifying the algorithm's performance in tabular scenarios and optimizing the dependency on \(\epsilon\) would enhance the contribution. Lastly, correcting the typo in Line 284 and addressing the concerns raised about sample complexity comparisons with existing literature would be beneficial.