ID: DvRTU1whxF
Title: Retinotopy Inspired Brain Encoding Model and the All-for-One Training Recipe
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 4, 4, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 3, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an architecture for a brain encoding model that includes a frozen DINO-trained ViT-B backbone with learnable convolutional layers, a differentiable spatial sampling layer, and a softmax-based layer selector. The authors propose an "All-for-One" training recipe that incorporates "dark knowledge distillation" and introduces a new parcellation method called "veROIs." The method is validated through visualizations of learned spatial sampling grids, an ablation study, and image retrieval tasks.

### Strengths and Weaknesses
Strengths:
- The paper combines techniques in a novel way, utilizing online-learned sampling grids and soft backbone layer assignment, which is reasonably biologically motivated.
- The clustering-based ROI assignment is innovative within the context of decoding.
- Definitions and dimensions for most variables are provided, and the figures are illustrative.

Weaknesses:
- The all-for-one training scheme lacks sufficient detail, making it difficult to evaluate its effectiveness.
- Clarity issues exist regarding the use of veROIs, the training subjects, and the model's generalizability across different datasets.
- The justification for repeated distillation is weak, and the paper does not adequately compare against simpler models or state-of-the-art techniques.
- Notation and clarity in figures and tables are confusing, and there are minor linguistic and typographical errors.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the all-for-one training scheme and provide more details on the use of veROIs, including how they relate to network distillation and image retrieval. Additionally, the authors should clarify the training subjects used in different stages and how the model handles varying voxel counts across datasets. We suggest strengthening the justification for repeated distillation and including comparisons against linear-regression models and other state-of-the-art techniques. Furthermore, we advise simplifying the notation and improving the clarity of figures and tables. Lastly, addressing minor linguistic issues and providing a summary or pseudocode of the proposed method would enhance the paper's presentation.