ID: 4bJufOS6No
Title: On Learning Multi-Modal Forgery Representation for Diffusion Generated Video Detection
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for detecting fake videos generated by diffusion models through a Multi-Modal Forgery Representation (MMFR). The authors propose a video-level detection algorithm, MM-Det, which utilizes In-and-Across Frame Attention (IAFA) to balance frame-level forgery traces and integrates a Dynamic Fusion Strategy. They also establish a high-quality dataset of diffusion-generated videos, demonstrating the effectiveness of their method across various benchmarks.

### Strengths and Weaknesses
Strengths:
1. The proposed multimodal representation fusion, IAFA, and Dynamic Fusion Strategy are valid and reasonable.
2. The creation of a new dataset for video forensics is a significant contribution that can benefit the research community.
3. The motivation for using LMMs in video forensics is clear and potentially pioneering.

Weaknesses:
1. The paper lacks clarity on how VQVAE amplifies diffusion features, and the relationship between the training and test sets and the DVF dataset is not adequately described.
2. The contribution of the in-and-across frame attention mechanism appears limited, and comparisons with existing methods like DE-FAKE are missing.
3. The experimental content is insufficient, with only two tables, making it difficult to fully validate the proposed method's effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how VQVAE is utilized in the method and provide a detailed explanation of the relationship between the training/test sets and the DVF dataset. Additionally, we suggest including comparisons with other video-level ViTs and discussing the limitations of the in-and-across frame attention mechanism. Expanding the experimental content to include more diverse tests would strengthen the validation of the proposed method. Finally, a discussion on the computational demands and generalization to non-diffusion generated videos would enhance the paper's robustness.