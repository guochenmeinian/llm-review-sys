ID: RI6HFZFu3B
Title: Deep Graph Neural Networks via Flexible Subgraph Aggregation
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 7, 5, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel perspective on the expressive power of Graph Neural Networks (GNNs) through subgraph aggregation, addressing performance degradation in traditional deep GNNs due to overlapping aggregated subgraphs. The authors propose a sampling-based generalized residual module, SNR, which theoretically enhances GNNs' ability to utilize information from multiple k-hop subgraphs. Extensive experiments validate the effectiveness of the SNR module.

### Strengths and Weaknesses
Strengths:
1. The novel approach of rethinking GNN expressive power via subgraph aggregation is intriguing.
2. The paper is well-structured, clearly articulating the motivation and construction of a flexible residual module to enhance GNN performance while mitigating overfitting.
3. Experimental results convincingly demonstrate the proposed method's effectiveness.

Weaknesses:
1. The manuscript lacks a comprehensive review of related works addressing oversmoothing and overfitting in deep GNNs.
2. Further analysis of experimental results is necessary, particularly regarding the SNR module's performance on the Citeseer dataset compared to InitialRes.
3. There are typographical errors, such as referring to "four data sets" instead of "six datasets."
4. Some baseline performances reported are questionable, particularly regarding GCNII's performance on benchmark datasets.
5. The experiments predominantly focus on small-scale datasets, and testing on larger datasets would provide more robust validation.
6. Detailed analysis of the learned mean and variance of the normal distribution is lacking, and performance plots as the number of layers increases would better illustrate robustness to oversmoothing.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including more related works on oversmoothing and overfitting in deep GNNs. Additionally, the authors should provide further analysis of the experimental results, particularly addressing the performance discrepancies noted in Table 4. We suggest correcting typographical errors and enhancing the clarity of the presentation by simplifying complex sentences. Furthermore, conducting experiments on larger datasets and providing detailed analyses of the learned parameters would strengthen the paper. Lastly, we encourage the authors to include a plot illustrating performance changes with increasing layers to evaluate robustness against oversmoothing.