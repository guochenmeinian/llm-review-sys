ID: PRBsEz8rnV
Title: No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen Representations
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 8, 7, 4, 6, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FUNGI (Features from Unsupervised GradIents), an unsupervised method to enhance representations of pretrained models, particularly Vision Transformers (ViTs). The authors propose combining gradients from various self-supervised objectives with embeddings from pretrained models, followed by dimensionality reduction using PCA. The effectiveness of FUNGI is demonstrated through extensive experiments across image classification, semantic segmentation, and text classification tasks.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and structured, with clear background characterization and motivation for the algorithm.  
- The simplicity of the proposed method facilitates understanding and implementation, supported by provided PyTorch pseudocode.  
- Extensive experimental evaluation across various datasets and pretrained models showcases the effectiveness of FUNGI.  

Weaknesses:  
- Evaluation is limited to nearest neighbor retrieval; testing on (few-shot) linear probing or K-Means clustering would be beneficial.  
- Lack of analysis on random projection properties, particularly regarding the choice of projection matrix sampled from {-1, 1} versus random Gaussian matrices.  
- Some relevant works on using gradients as features for downstream tasks are not discussed, which could provide additional context for the proposed method.  
- The computational cost of extracting gradients for multiple objectives may not justify the performance gains, and the reasoning behind performance enhancements from self-supervised gradients is insufficiently addressed.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of FUNGI by including tests on linear probing and clustering tasks to better assess its feature engineering capabilities. Additionally, we suggest providing a thorough analysis of the random projection method used, including a comparison with Gaussian matrices. The authors should also incorporate discussions of related works that utilize gradients as features and clarify the theoretical underpinnings of their approach. Furthermore, addressing the scalability of FUNGI with larger ViT models and exploring its applicability to CNNs and hybrid architectures would enhance the paper's impact. Lastly, clarifying the dependency of the method on PCA for dimensionality reduction and the mean calculation of reported results would strengthen the manuscript.