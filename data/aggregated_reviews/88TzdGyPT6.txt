ID: 88TzdGyPT6
Title: Benign overfitting in leaky ReLU networks with moderate input dimension
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 5, 8, 4, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies benign overfitting in two-layer leaky ReLU networks trained with hinge loss for binary classification, relaxing the dimension requirement from $d = \Omega(n^2 \log n)$ to $d = \Omega(n)$. The authors establish conditions for benign and non-benign overfitting based on the signal-to-noise ratio, demonstrating that the network converges to a max-margin linear classifier under certain conditions. The results are supported by theoretical proofs, including matching upper and lower bounds for misclassification probability in specific parameter regimes.

### Strengths and Weaknesses
Strengths:
- The paper effectively reduces the dimensionality requirement for benign overfitting from $n^2$ to $n$.
- It provides clear conditions for benign overfitting and demonstrates the implicit bias towards max-margin classifiers.
- The technical overview is well-written and elucidates the main proof ideas.

Weaknesses:
- The reliance on linear separability limits the applicability of the results to more complex models.
- There is a lack of empirical experiments, making the theoretical conclusions less convincing.
- The paper does not adequately compare its findings with related work, particularly [1], in terms of problem setting and proof techniques.
- Some theorems lack clarity, particularly regarding their dependence on parameters such as $n$, $d$, and $k$.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the implications of linear separability and consider generalizing the data generation process. Including empirical experiments would strengthen the paper's conclusions. Additionally, we suggest providing clearer explanations for theorems and assumptions to enhance reader comprehension. A more detailed comparison with [1] should be included to highlight the differences in techniques and findings. Finally, clarifying the points of confusion regarding Theorems 3.2 and 3.3, particularly their applicability across various parameter regimes, would be beneficial.