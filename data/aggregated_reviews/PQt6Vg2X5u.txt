ID: PQt6Vg2X5u
Title: Recursive PAC-Bayes: A Frequentist Approach to Sequential Prior Updates with No Information Loss
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 7, 6, 8, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a PAC-Bayesian framework that enables sequential prior updates without dividing the training dataset. The authors propose a novel decomposition of the expected loss of randomized classifiers, allowing for a recursive bounding of the posterior loss. Additionally, they generalize ternary split-kl PAC-Bayes bounds to encompass general discrete random variables. The empirical evaluations demonstrate that their method outperforms existing state-of-the-art techniques.

### Strengths and Weaknesses
Strengths:
- Theorem 5 represents a significant advancement in the study of data-dependent priors, with numerous potential applications.
- Experimental results indicate a notable improvement over prior methods.

Weaknesses:
- Sections 3 and 4 are overly dense in notation, making them challenging to read.
- The paper does not adequately discuss the implications of Recursive PAC-Bayes for streaming or online learning scenarios.
- The experiments utilize half of the data for prior construction, yet the proportion used is critical and warrants exploration of different data-splitting ratios.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Sections 3 and 4 by reducing the density of notation and providing more detailed explanations of key concepts. Additionally, a discussion on the applicability of Recursive PAC-Bayes to streaming learning scenarios would enhance the paper. We suggest conducting experiments with varying data-splitting proportions to better understand the impact on prior construction. Furthermore, addressing the limitations of the derived bounds, particularly regarding their applicability beyond binary classification, would strengthen the manuscript. Lastly, we encourage the authors to clarify the relationship between their approach and existing methods that optimize priors without data splitting, as this would provide a more comprehensive understanding of their contributions.