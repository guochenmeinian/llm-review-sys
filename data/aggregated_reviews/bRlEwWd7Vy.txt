ID: bRlEwWd7Vy
Title: Distributionally Robust Bayesian Optimization with $\varphi$-divergences
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 3, 8, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extension of distributionally robust Bayesian optimization (DRBO) to the case of $\phi$-divergences, which include Kullback-Leibler divergence, total variation, and $\chi^2$-divergence. The authors propose a computationally tractable algorithm for maximizing a reward function that depends on a context parameter drawn from a distribution, aiming for distributional robustness. The main result, Theorem 1, reformulates the distributionally robust objective (DRO) as a standard stochastic optimization problem adjusted by a variance term for total variation and $\chi^2$-divergence cases. The authors derive a robust regret bound and conduct numerical experiments on standard reference functions. Additionally, the paper presents a method for reducing an infinite dimensional optimization problem to a finite dimensional variable, specifically retaining only $\lambda$ and $b$ in the reduced problem. This reduction is significant, especially when $p_t$ is continuous or finitely supported, as it decreases the number of optimization variables from potentially large sizes to just two. The motivation behind the comparisons of different DRBO methods in Figures 2 and 3 is clarified, indicating that the robust regret values are plotted to identify the efficiency of various algorithms over iterations, a common practice in the Bayesian optimization community.

### Strengths and Weaknesses
Strengths:  
The authors provide tractable characterizations of the distributionally robust objective, extending prior work by Kirschner et al. The reduction of optimization variables is a notable computational improvement. The paper is well-written, clearly illustrating key concepts, and presents novel results that significantly enhance the applicability of distributionally robust optimization. The clarification of the motivation for comparisons in Figures 2 and 3 enhances the understanding of the methodology.

Weaknesses:  
A primary weakness is the implicit assumption of finiteness in the context set, which contradicts claims of extending beyond this assumption. The numerical experiments focus on low-dimensional functions without adequately demonstrating the benefits of continuous support over discrete support. The rationale for comparing robust regret values across different settings remains unclear. Additionally, the paper's presentation suffers from clarity issues, grammatical errors, and unexplained notation. The absence of figures limits the presentation of results, although intermediate results are provided.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding the finiteness assumption in the context set and provide a thorough comparison to bounds obtained in other works, such as [23]. The experimental section should better illustrate the advantages of the continuous support reformulation. We also suggest reworking the manuscript to address grammatical issues, clarify underlying assumptions, and define all relevant quantities early in the text. Furthermore, please elaborate on the significance of the regret bound and clarify the motivation behind the comparisons in Figures 2 and 3, explicitly addressing why robust regret values are relevant across different settings. Additionally, we suggest including a table with intermediate results over different iterations, even if the results are negative, to facilitate discussion on the performance of each method under varying conditions. Lastly, consider including a more comprehensive literature review to strengthen the connection between DRO and BO.