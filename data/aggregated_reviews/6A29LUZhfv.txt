ID: 6A29LUZhfv
Title: MixEval: Deriving Wisdom of the Crowd from LLM Benchmark Mixtures
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 5, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MixEval, a new benchmarking framework designed to evaluate large language models (LLMs) by matching web-mined user queries with existing benchmarks. MixEval aims to reflect real-world human preferences and offers a two-stage benchmark reconstruction pipeline that includes wild query detection and grounding existing benchmarks in mined queries. The authors introduce MixEval-Hard to enhance the benchmark's ability to differentiate strong models. The framework demonstrates a high correlation with human preference leaderboards like Chatbot Arena while being more efficient in terms of time and cost.

### Strengths and Weaknesses
Strengths:
1. The authors effectively align web queries with mainstream benchmarks, providing a novel approach to simulating real-world user preferences.
2. The experimental results validate that MixEval and MixEval-Hard outperform singular benchmarks and align well with Arena Elo.
3. The paper offers extensive analysis and comparisons with existing benchmarks, yielding valuable insights into their strengths and weaknesses.

Weaknesses:
1. The methodology lacks clarity on how new queries are integrated into the testing process, particularly regarding adaptive rewriting to match ground truth answers.
2. The experimental setup in Figure 5 is inadequately described, leaving unclear whether 0-shot or 5-shot learning is employed.
3. The potential for contamination in the benchmark pool remains unaddressed, as many existing benchmarks may not effectively represent human preferences.

### Suggestions for Improvement
We recommend that the authors improve the explanation of how new queries are utilized in the testing process, specifically addressing adaptive rewriting to align with ground truth answers. Additionally, we suggest providing a clearer description of the experimental setup in Figure 5, including the formatting of inputs for "Mixed" and "Original." To strengthen the validity of MixEval, we encourage the authors to conduct ablation studies demonstrating the effectiveness of the "User query" and "Ground truth-benchmark" processes in constructing benchmarks. Furthermore, addressing the contamination issue by exploring methods to ensure the integrity of the benchmark pool would enhance the robustness of the proposed framework.