ID: FAimEpR9Fh
Title: What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the task of defeasible moral reasoning, which associates actions with their moral implications based on context and provides rationales for these judgments. The authors introduce the δ-RULES-OF-THUMB (δ-ROT) dataset, comprising 1.2 million entries that support this task. They employ an iterative self-distillation method to generate high-quality training data and validate it through human evaluation. The paper aims to enhance understanding of how context influences moral judgments in natural language processing (NLP).

### Strengths and Weaknesses
Strengths:
- The novel task of defeasible moral reasoning is well-defined and relevant for practical applications.
- The δ-ROT dataset is extensive and provides valuable resources for future research.
- The iterative self-distillation methodology effectively improves model quality, transitioning from costly API calls to smaller models.
- The paper is detailed and logically structured, making it easy to follow.

Weaknesses:
- There is an over-reliance on the iterative self-distillation method, raising concerns about potential biases from the initial seed knowledge.
- The dataset's size may compromise quality, and its similarity to the ProsocialDialogue dataset is not adequately addressed.
- Generalizability of the proposed methods to other NLP tasks and cultural contexts remains unclear.
- The paper lacks strong baseline comparisons and does not sufficiently engage with existing literature on morality and large language models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing and consider restructuring the related work section to follow the introduction for better reader comprehension. Additionally, we suggest including a preliminary study on the dataset to demonstrate its utility. It is crucial to address the potential biases in the iterative self-distillation process and provide a discussion on the implications of teacher-misleading behavior. Furthermore, we encourage the authors to include strong baseline models for comparison and to cite relevant literature, particularly regarding the similarities with the ProsocialDialogue dataset. Lastly, we advise reducing the use of bold formatting in the results section in favor of italics for better readability.