ID: DxYDP3B31K
Title: Enhancing Generative Retrieval with Reinforcement Learning from Relevance Feedback
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a reinforcement learning-based approach to generative retrieval, addressing two main issues: the discrepancy between token-level optimization and document-level relevance estimation, and the overemphasis on top-1 results. The authors propose a method, GenRRL, which utilizes a relevance reward model distilled from various relevance models to train a differentiable search index (DSI) through reinforcement learning. Experiments on datasets like MS MARCO Document Ranking and Natural Questions demonstrate that GenRRL outperforms existing generative retrieval baselines.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated and clearly written, effectively highlighting the need for improved optimization in generative retrieval.
- The proposed reinforcement-based method is novel and shows promising experimental results, indicating its potential interest to the research community.
- The experiments illustrate the effectiveness of the proposed approach compared to other methods.

Weaknesses:
- The justification for training the reward model is unclear, as direct use of combined scores from BM25, DPR, or LLAMA could suffice.
- There are concerns regarding the reported performance of DPR and ANCE, which appear significantly lower than previously documented results.
- The model's complexity, requiring the training of a dense model (DPR), may pose challenges.
- Some steps in the reward model training lack clarity and could benefit from further explanation.

### Suggestions for Improvement
We recommend that the authors improve the justification for the reward model's training by explaining why direct score usage is insufficient. Additionally, please clarify the discrepancies in DPR and ANCE performance compared to existing literature and ensure proper citations for baseline results. We suggest conducting experiments to explore the use of cross-attention rankers like monoT5 or RankT5 in the reward model development. Furthermore, providing additional details on the advantages of generative retrieval over traditional methods, particularly regarding differentiable search indices, would strengthen the paper. Lastly, we encourage the authors to address the potential arbitrariness in the reward model training steps and consider experimenting with different loss functions in reward model training.