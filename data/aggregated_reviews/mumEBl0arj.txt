ID: mumEBl0arj
Title: Thinker: Learning to Plan and Act
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 8, 6, 7, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents *Thinker*, a model-based reinforcement learning (RL) algorithm that enhances a Markov Decision Process (MDP) by enabling an agent to generate plans in a learned world model prior to executing real-world actions. The method integrates planning directly into the MDP, allowing the agent to learn planning strategies through reinforcement learning. The authors demonstrate that Thinker achieves state-of-the-art performance on Sokoban and competitive results on Atari 2600 benchmarks, showing significant performance improvements over Dreamer-v3 and MuZero on Sokoban when using environment frames as a reference. The authors clarify that Thinker is tightly coupled with its model-based RL algorithm but suggest that other RL algorithms could also be integrated with the Thinker-augmented MDP. Additionally, the paper discusses the computational costs associated with planning using K steps at every environment step.

### Strengths and Weaknesses
Strengths:
- The method's conceptual simplicity and broad applicability to existing RL algorithms are notable.
- Experimental evaluation is thorough, with challenging benchmarks like Sokoban and 57 Atari games, reducing the likelihood of cherry-picked results.
- The additional experiments and clarifications effectively address previous concerns regarding the methodology.
- The updated network names enhance clarity, and the results indicate that Thinker outperforms existing algorithms.
- The inclusion of a new figure elucidates the Thinker planning process, and the paper is well-structured with clear writing.

Weaknesses:
- The empirical benefits of Thinker are unclear; it is uncertain how much they derive from actual planning versus an enhanced feature space. The plateauing of performance after K=10-20 planning steps raises questions about the algorithm's effectiveness and efficiency.
- The paper lacks comparisons to other model-based RL baselines and exploration of how Thinker interacts with other RL algorithms, limiting the contextual understanding of Thinker's performance and providing valuable insights.
- The claim of generality in expressing various planning algorithms may be overstated, as the Gym-like interface complicates the implementation of certain algorithms.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the planning phase by including qualitative assessments of Sokoban rollouts and additional ablations, such as random action selection during planning. Furthermore, we suggest applying the Thinker augmentation to other model-free RL algorithms to validate its effectiveness beyond Impala. Clarifying the expressiveness of Thinker regarding different planning algorithms would strengthen the paper's claims. We also recommend including a figure showing example trees with varying K and L to aid reader comprehension and exploring the potential for variable length planning to mitigate the costs associated with K steps at every environment step. Lastly, it would be beneficial to compare Thinker with other RL algorithms in terms of wall clock time and discuss the implications of these findings in the limitations section, posing it as a question for future work. Enhancing the writing and presentation, particularly in explaining complex ideas succinctly, would make the paper more accessible to a broader audience.