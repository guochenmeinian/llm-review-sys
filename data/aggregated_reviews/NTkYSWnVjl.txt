ID: NTkYSWnVjl
Title: Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image Classification and Object Detection
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a few-pixel querying blackbox attack for image classification and object detection models, termed Remember and Forget Pixel Attack using Reinforcement Learning (RFPAR). The authors propose a novel pixel-based black-box attack that employs a dual-stage reinforcement learning process, optimizing pixel edits to effectively alter classification targets or suppress objects in detection models. The attack demonstrates high attack success rates (ASR) with minimal pixel modifications across various models and datasets, including ImageNet and MS-COCO. Experimental results indicate that RFPAR achieves higher success rates while maintaining lower values for $L_0$ and query counts across different models, including transformer-based models like ViT-L, Swin-V2-T, and Deit-B, as well as object detection models such as ATSS and Deformable DETR.

### Strengths and Weaknesses
Strengths:  
- The authors present an effective attack achieving high ASR with few pixel edits, demonstrating its applicability to both image classification and object detection.  
- The approach is original, utilizing a dual-stage reinforcement learning process, and the experiments are thorough, confirming RFPAR's effectiveness across diverse models.  
- The paper provides comprehensive tables that clearly summarize experimental results, facilitating easy reference.  
- RFPAR demonstrates superior performance metrics compared to other attacks, indicating its effectiveness.

Weaknesses:  
- The presentation of the "Forget Process" appears overly simplistic, as it mainly involves resetting the RL agent without substantial contribution compared to the "Remember Process."  
- The visibility of pixel edits, particularly against contrasting backgrounds, raises concerns about the attack's subtlety.  
- The evaluation lacks a dedicated section on related works and does not sufficiently discuss limitations or defense testing against adversarial defenses.  
- The initial difficulty in locating results within the PDF may hinder accessibility for reviewers.  
- The presentation of results could benefit from clearer contextual explanations regarding the significance of the metrics.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the ablation details in Section 3.5, specifically addressing how optimization changes with different ablations. Additionally, we suggest including a dedicated section discussing related works to better contextualize the contributions. It would also be beneficial to conduct and present defense testing against adversarially trained models to assess the robustness of RFPAR. Furthermore, we encourage the authors to elaborate on the potential risks and implications of their attack, particularly in real-world applications, and propose mitigation strategies to enhance the robustness of neural networks against such attacks. Finally, we recommend improving the clarity of the results section to enhance accessibility for reviewers and providing more contextual explanations for the metrics used in the tables to aid in understanding the implications of the findings.