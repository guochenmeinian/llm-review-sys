ID: EfpZNpkrm2
Title: QuanTA: Efficient High-Rank Fine-Tuning of LLMs with Quantum-Informed Tensor Adaptation
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 8, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel PEFT method, QuanTA, inspired by quantum circuits, which theoretically leverages the universality theorem and rank representation theorem for efficient high-rank adaptations across various downstream tasks. The authors demonstrate that QuanTA outperforms existing methods like LoRA and DoRA, particularly in complex reasoning tasks, through extensive experiments on multiple datasets.

### Strengths and Weaknesses
Strengths:
1. The paper is well-motivated, providing sufficient theoretical proof and justifications for the proposed method.
2. The experimental results are robust, showing superior performance compared to other baselines with significantly fewer activated parameters.
3. The integration of quantum concepts into fine-tuning is original and demonstrates promising results.

Weaknesses:
1. The performance of QuanTA on textual understanding tasks, particularly with BERT or RoBERTa, remains unexplored, despite these models being the initial focus of PEFT methods.
2. There is a lack of discussion regarding the ordering of qubit pairs and whether selecting pairs only once is sufficient for the universality theorem.
3. The paper does not compare QuanTA with high-rank methods like MoRA or KronA, limiting the scope of its evaluation.

### Suggestions for Improvement
We recommend that the authors improve the exploration of QuanTA's performance on textual understanding tasks, particularly with BERT or RoBERTa. Additionally, it would be beneficial to include experiments that combine QuanTA with other PEFT methods to investigate potential performance boosts. Clarifying the implications of qubit pair selection in the universality theorem and providing comparisons with high-rank adaptation methods would strengthen the paper. Lastly, we suggest including evaluations of training time and GPU memory usage relative to other baselines to provide a more comprehensive assessment of the method's efficiency.