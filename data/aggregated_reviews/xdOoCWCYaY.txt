ID: xdOoCWCYaY
Title: Towards Data-Agnostic Pruning At Initialization: What Makes a Good Sparse Mask?
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 6, 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a large-scale empirical analysis of models from the NAS-Bench-Macro benchmark, leading to the development of the node-path balancing principle and the Node-Path Balancing Pruner (NPB), a data-agnostic pruning-at-initialization (PAI) scheme. The node-path balancing principle posits that networks with a good balance between effective nodes and paths exhibit higher performance, particularly in high sparsity regimes (>99%). The NPB algorithm addresses a constrained optimization problem to achieve this balance at a specified sparsity level, aiming to enhance model efficiency and classification accuracy compared to existing PAI methods. The authors provide empirical evidence from shuffling experiments that demonstrate how reshuffling affects performance by altering the number of effective paths and nodes, while also discussing the complexities involved in choosing the parameters $\alpha$ and $\beta$, which are essential for achieving optimal performance.

### Strengths and Weaknesses
Strengths:
- The paper provides a compelling analysis of the NAS-Bench-Macro benchmark, establishing a connection between model performance and the balance of effective nodes and paths.
- The NPB pruning-at-initialization algorithm demonstrates competitive results against state-of-the-art methods like PHEW while requiring fewer FLOPs during inference.
- The authors provide a thorough explanation of the NPB principle and its relevance to sparse neural networks, supported by empirical results from shuffling experiments.
- The rationale for the proposed metrics is clearly articulated, and the authors acknowledge the need for further clarification and improvements in the manuscript.

Weaknesses:
- The node-path balancing principle lacks rigor and clarity, with the motivating plots suggesting nuances that are not thoroughly explored, particularly regarding variations in classification accuracy among networks with similar balances.
- The motivation for the NPB principle is not sufficiently comprehensive, particularly regarding the entire range of sparsity.
- The connection between sections discussing NAS observations and the NPB principle remains unclear.
- The optimization model for selecting $\alpha$ and $\beta$ is not well-defined, leading to potential confusion for users.
- The clarity of certain figures is insufficient, making it challenging to extract meaningful insights, indicating a need for improved figure design and presentation.

### Suggestions for Improvement
We recommend that the authors improve the rigor and clarity of the node-path balancing principle, possibly by developing a theoretical framework to support it. Additionally, we suggest improving the clarity of the motivation for the NPB principle to encompass the entire range of sparsity rather than focusing solely on the >99% regime. We encourage the authors to explicitly connect the observations from Section 4.1 to the claims made in Section 4.2. It would also be beneficial to provide a more structured approach to the optimization of $\alpha$ and $\beta$, possibly including a visualization of the average path per node as a post-hoc analysis to support the joint optimization argument. To enhance figure clarity, we advise revising figures to ensure that key takeaways are more apparent, such as standardizing scales across plots and ensuring consistent axis ranges. Finally, we encourage the authors to acknowledge the limitations of their proposed method and discuss potential negative societal impacts, as well as outline future work in this area.