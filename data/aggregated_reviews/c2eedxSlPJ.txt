ID: c2eedxSlPJ
Title: Tight Risk Bounds for Gradient Descent on Separable Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of training convex linear models with gradient descent on linearly separable data, establishing both upper and lower bounds on population risk. The authors introduce a broader class of non-negative smooth loss functions, C_{\pi,\beta}, and demonstrate that their results extend beyond traditional logistic regression. The study employs a straightforward proof technique that controls the norm of the GD output and its empirical risk, relying on uniform convergence results tailored for linear models.

### Strengths and Weaknesses
Strengths:  
The paper effectively closes the gap in understanding convex linear models by providing matching upper and lower risk bounds. It is well-written, with clear logic and accessible proofs. The results imply that the bounds on empirical error and the norm of the solution are sufficient for a sharp analysis of population error. The minimal assumptions on the loss function and the use of standard proof techniques enhance the paperâ€™s clarity and impact.

Weaknesses:  
The novelty of the techniques appears limited, as the improvement on the upper bound relies on existing results from Srebro et al. The focus on surrogate loss functions raises concerns about potential overfitting, particularly regarding discrepancies between upper bounds on logistic and 0-1 loss functions. Additionally, the derived upper bounds do not significantly improve upon known results, and the paper lacks a conclusions section discussing limitations and implications.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the novelty of their approach, particularly in relation to existing literature. Clarifying the implications of using diverse tail decays in loss functions would strengthen the motivation for their results. It would be beneficial to provide a classification error bound for C_{\pi,\beta} and to elaborate on the challenges in optimization and generalization analyses. Including a conclusions section that discusses methodology limitations and potential societal impacts would enhance the paper's comprehensiveness.