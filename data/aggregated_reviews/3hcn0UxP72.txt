ID: 3hcn0UxP72
Title: Topological obstruction to the training of shallow ReLU neural networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 6, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 1, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the loss landscape topology in two-layer neural networks with non-negative homogeneous activations, focusing on the obstructions that arise during gradient flow. The authors identify that the loss landscape may consist of disconnected components, preventing certain initial conditions from reaching the global minimum. They derive the Poincar√© polynomial of invariant sets and demonstrate that the number of effective components grows linearly with the number of pathological neurons, which cannot change the sign of their output weight. Additionally, the paper discusses the implications of topological obstructions for training dynamics in deep learning, proposing that discrete gradient descent can bridge these obstructions under specific learning rate conditions, although the characterization of these conditions remains under scrutiny. The analysis is supported by a toy example and numerical experiments, and the authors highlight the relevance of single-hidden-layer networks as foundational components in more complex architectures.

### Strengths and Weaknesses
Strengths:
- The paper employs a novel approach to studying the topology of loss landscapes, integrating optimization dynamics with topological analysis.
- The theoretical results are well-supported by rigorous proofs and clear explanations, making the concepts accessible.
- The authors effectively communicate complex theoretical results with precision and provide valuable insights into the implications of topological obstructions for training dynamics, particularly in the context of two-layer ReLU networks.
- The authors acknowledge limitations and provide a thoughtful discussion of their findings.

Weaknesses:
- The scope is limited to two-layer networks, raising questions about the generalizability of the findings to deeper architectures.
- The main results only apply to networks with scalar outputs, which diminishes their relevance to practical deep learning scenarios.
- The characterization of learning rates that bridge obstructions lacks sufficient empirical support.
- The implications of regularization and biases are not fully addressed in the main text.
- The experimental setup is simplistic, and the authors do not sufficiently explore how their findings extend to more complex models or different activation functions.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the novelty of their work by explicitly comparing it to prior studies, particularly Safran et al. [35] and the emergence of topological obstructions discussed in recent literature. Additionally, the authors should consider extending their analysis to include deeper networks and more complex activation functions to enhance the practical relevance of their findings. It would also be beneficial to clarify how the identified obstructions relate to discrete gradient descent methods and to provide more comprehensive experiments that demonstrate the implications of their results in varied settings. Furthermore, we suggest that the authors improve the characterization of learning rates by conducting additional experiments to clarify the conditions under which discrete gradient descent bridges obstructions. It is essential to explicitly acknowledge the limitations posed by regularization and biases in the main text, potentially promoting material from appendix F. Finally, clearly stating the relevance of their work to practical deep learning scenarios in the introduction or conclusion would emphasize the significance of their findings regarding topological obstructions.