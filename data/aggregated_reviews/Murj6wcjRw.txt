ID: Murj6wcjRw
Title: An Efficient Dataset Condensation Plugin and Its Application to Continual Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 6, 7, 4, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel dataset condensation (DC) technique that utilizes low-rank representations to enhance computational and condensation efficiency. The authors propose decomposing image representations into two low-rank matrices, allowing integration with existing DC methods. They demonstrate that their approach significantly improves state-of-the-art (SOTA) DC methods through extensive experiments on common classification benchmarks, particularly on CIFAR10. The method also shows promise for continual learning by reducing catastrophic forgetting and privacy leakage. The authors claim that their method is orthogonal to existing data parameterization methods and provide experimental evaluations to support their assertions.

### Strengths and Weaknesses
Strengths:
- The approach of leveraging low-rank representations to enhance dataset condensation is innovative and effectively addresses the dimensionality reduction challenge.
- The paper is well-written, with clear motivation and thorough technical details, making it easy to follow.
- The proposed method shows substantial improvements over existing SOTA DC methods and has implications for continual learning.
- The authors have effectively addressed reviewer concerns, leading to an upgraded score from one reviewer.
- The experimental results on CIFAR10 partially validate the claims of the proposed method's effectiveness.

Weaknesses:
- The experimental protocol is a major concern; the input sizes for datasets like MNIST and CIFAR100 are too small to realistically assess the proposed method's effectiveness. Larger input sizes (e.g., at least 512 x 512) should be tested to better understand the method's limitations.
- The evaluation is limited to CIFAR10, and a more extensive assessment across additional datasets and memory budgets is necessary to strengthen the claims further.
- The authors should include results for DC and DSA on TinyImageNet to validate the proposed method's performance on more realistic datasets.
- Some results, such as those on MNIST, appear inconsistent, warranting further explanation from the authors.
- The paper lacks a thorough discussion of the relationship between memory usage and rank, as well as the implications of using low-dimensional representations for real-world applications.

### Suggestions for Improvement
We recommend that the authors improve the experimental protocol by testing larger input sizes to better evaluate the proposed method's effectiveness and limitations. Additionally, including results for DC and DSA on TinyImageNet would provide a more comprehensive understanding of the method's performance. The authors should also clarify the unexpected results on MNIST and provide a detailed analysis of memory usage in relation to different ranks. Furthermore, we suggest that the authors improve the evaluation by including a broader range of datasets and memory budgets to provide more comprehensive support for their claims. Finally, a more extensive discussion on the implications of low-dimensional representations in real-world scenarios and comparisons with existing data parameterization methods would strengthen the paper.