ID: 9STYRIVx6u
Title: Convergence of mean-field Langevin dynamics: time-space discretization, stochastic gradient, and variance reduction
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 6, 8, 7, 8, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the convergence of various algorithms under *Mean-Field Langevin Dynamics* (MFLD), focusing on three aspects: the approximation of the measure $\mu_t$ by the empirical measure $\mu_X$, the discretization of dynamics, and the use of noisy versions of gradients. The authors provide quantitative convergence bounds for $\mu_X$ to a minimum of an entropy-regularized objective, including a one-step bound and more precise bounds based on the version of the noisy gradient considered.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a significant problem in the study of wide neural networks, presenting impressive results that enhance the state of the art.  
- The flow of the paper is decent, with well-explained appendix proofs that facilitate understanding.  
- The theoretical contributions include extensive non-asymptotic convergence rates and a thorough comparison with existing methods.

Weaknesses:  
- The necessity for two regularization terms and their heavy dependency on parameters is a notable drawback.  
- The paper's technical complexity may be excessive for a NeurIPS submission, leading to a dense presentation that could benefit from a longer exposition.  
- Some parts of the appendix, particularly Appendix A, are rushed and lack proofs for necessary conditions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by expanding on key examples, especially the first one, to elucidate the role of the first-variation functional and its comparison with existing results on SGD/mean-field for 2LNNs. Additionally, we suggest including numerical experiments to support the theoretical results and enhance the paper's impact. Furthermore, we encourage the authors to articulate the differences from Nitanda and Chizat more clearly and to provide a clearer motivation for Assumption 4 in the text.