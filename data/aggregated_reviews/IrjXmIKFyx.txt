ID: IrjXmIKFyx
Title: Model-free Posterior Sampling via Learning Rate Randomization
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Randomized Q-learning (RandQL), a novel model-free algorithm aimed at regret minimization in episodic Markov Decision Processes (MDPs). The authors propose learning rate randomization as a means to achieve optimistic exploration without the need for bonuses, demonstrating near-optimal regret bounds in both tabular and non-tabular metric space settings. The theoretical analysis is complemented by empirical results showing RandQL's superior performance compared to existing model-free approaches like OptQL.

### Strengths and Weaknesses
Strengths:
- The concept of learning rate randomization is innovative and facilitates exploration without bonuses.
- The algorithm achieves near-optimal regret upper bounds, indicating its efficiency.
- Empirical results validate the theoretical claims, showing RandQL outperforms existing model-free methods.

Weaknesses:
- The experiments conducted are overly simplistic; additional tests in more complex environments would enhance the research.
- The intuition behind the value of $J$ is not clearly explained, particularly regarding the implications of its size.
- The computation of the policy $Q$ value $\overline{Q}_h(s_h,a_h)$ may incur significant estimation error, raising concerns about practical implementation.
- The reliance on exact computation of covering in the Net-Staged-RandQL could be seen as a limitation, although it aligns with existing methodologies.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including more complex environments beyond gridworld to better demonstrate the algorithm's capabilities. Additionally, providing a clearer explanation of the intuition behind the value of $J$ and its implications would be beneficial. Addressing the potential estimation error in the computation of $\overline{Q}_h(s_h,a_h)$, possibly by incorporating techniques from double Q-learning, could strengthen the practical applicability of RandQL. Finally, including comparisons with ensemble methods like bootstrapped DQN would provide valuable insights into the advantages of learning rate randomization over traditional approaches.