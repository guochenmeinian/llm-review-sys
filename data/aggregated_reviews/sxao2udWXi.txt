ID: sxao2udWXi
Title: A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 3, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the relationship between graph topology and feature evolution in Graph Neural Networks (GNNs), specifically through the lens of Neural Collapse (NC). The authors discuss the phenomenon of NC in instance-wise deep classifiers and extend their analysis to node-wise classification using Stochastic Block Model (SBM) graphs. An empirical study reveals a decrease in within-class variability in GNNs trained on SBMs, although less pronounced than in instance-wise settings. The authors propose a graph-based mathematical model to understand the influence of node neighborhood patterns and community labels on NC dynamics, requiring strict structural conditions for exact variability collapse. Theoretical analyses of gradient dynamics provide explanations for the observed partial collapse in GNNs, and comparisons are made with spectral clustering methods. Additionally, the authors analyze NC metrics, specifically focusing on NC1 and NC2, arguing that NC1 is the most critical component reflecting the reduction of within-class variability, while NC2's significance is less pronounced. They acknowledge the complexity of analyzing NC2 in GNNs compared to plain deep neural networks (DNNs) and highlight the need for future research in this area.

### Strengths and Weaknesses
Strengths:
1. The paper focuses on an interesting topic: Neural Collapse during the training of GNNs.
2. It provides empirical evidence showing a decrease in within-class variability.
3. The authors present unique theoretical results, such as the partial collapse in Theorem 3.1, and the methodology is clearly explained.
4. The paper provides a thorough empirical analysis of NC1 and NC2 metrics, contributing to the understanding of neural collapse in GNNs.
5. The authors effectively argue the primacy of NC1 over NC2, supported by references to existing literature.
6. The discussion on graph rewiring strategies and their potential implications for GNN performance is insightful.

Weaknesses:
1. The analysis is limited to intra-class variability, neglecting inter-class variability, which diminishes the overall significance.
2. The study does not sufficiently differentiate between within-class variability caused by over-smoothing and that due to neural collapse, which could clarify the contribution of this work.
3. The practical utility of the theoretical analysis is limited, and some experimental findings lack robust theoretical support.
4. The evidence presented for NC2 is perceived as insufficient, raising concerns about its empirical validation, especially given the focus on over-smoothing.
5. The assumptions regarding balanced communities in the data model may limit the applicability of the conclusions in real-world scenarios.
6. The presentation of the derivations and the clarity of the proof sketch require significant improvement for better reader comprehension.

### Suggestions for Improvement
We recommend that the authors improve the discussion on inter-class variability to enhance the significance of their findings. Additionally, a clearer distinction between the effects of over-smoothing and neural collapse should be provided to strengthen the paper's contribution. We suggest improving the empirical evidence regarding the existence of NC2 in graph contexts, particularly by showcasing data on smaller real-world graphs to address concerns about its oversight. The authors should also consider incorporating holdout nodes in the proposed graph rewiring task to optimize NC metrics, thereby enhancing the discussion on the generalizability of both NC1 and NC2. Furthermore, we advise the authors to reconsider the assumptions of balanced communities in their data modeling, as this may affect the practical relevance of their conclusions. Lastly, addressing the concerns regarding the proof of Theorem 3.2, refining the presentation by enhancing the narrative in the appendix, and including a proof sketch in the main content will enhance the rigor and readability of the paper.