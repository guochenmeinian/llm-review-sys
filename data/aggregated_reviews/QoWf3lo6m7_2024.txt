ID: QoWf3lo6m7
Title: Towards a Theoretical Understanding of the 'Reversal Curse' via Training Dynamics
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the "reversal curse" phenomenon in large language models (LLMs), where models trained on "A is B" fail to generalize to "B is A." The authors investigate the gradient dynamics of bilinear models and one-layer transformers under cross-entropy loss, revealing a strict separation between training and test errors. They propose that while training data includes both directions, the model's ability to infer reverse relationships is not guaranteed. The analysis highlights that the asymmetry in model weights prevents effective learning of reverse relationships and extends to chain-of-thought reasoning. The authors discuss two rules regarding the relationship between entities: RULE 1, where A→B implies B←A, and RULE 2, where this implication holds only under specific conditions. They argue that the model's failure to infer B←A is due to model weight asymmetry rather than a true understanding of the rules.

### Strengths and Weaknesses
Strengths:
- The exploration of the reversal curse is timely and relevant, providing a pioneering theoretical analysis that may inspire future research.
- The paper is technically sound, with an impressive depth of analysis that contributes significantly to understanding training dynamics.
- The inclusion of in-context learning (ICL) experiments strengthens the paper's arguments.
- The theoretical explanation of the reversal curse is well-articulated and provides insight into model behavior.

Weaknesses:
- The approach does not adequately address the meta-learning capabilities of transformers, potentially limiting the analysis of generalization behavior.
- The motivation for the paper remains unclear, which affects the overall impact of the work.
- The reliance on the asymmetry of model weights feels like a verification of existing theories rather than offering new insights, necessitating a more in-depth discussion of prior empirical studies.
- The conclusion regarding the model's ability to learn reverse relationships is contested, as it does not account for exceptions in human reasoning.
- The assumptions made in the analysis, such as the dimensionality of embeddings and the simplification of the attention mechanism, may oversimplify the complexities of model dynamics.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the meta-learning capabilities of transformers to better contextualize the reversal curse. Additionally, we suggest clarifying the paper's motivation to enhance its impact. The authors should provide more insight into the relationship between their results and the experiments conducted in prior studies, particularly regarding whether the model learns RULE 1 or simply increases the size of the specific set in RULE 2. Further theoretical analysis and experiments are also recommended to validate their intuitions in a broader context. Moreover, the authors should consider extending the analysis to compare pre-training and in-context learning settings, as this could provide valuable insights into overcoming the reversal curse. Lastly, clarifying the implications of the assumptions made regarding dimensionality and attention mechanisms would strengthen the theoretical framework, and improving the clarity and precision of the writing, particularly in defining terms and maintaining consistent notation, would enhance the paper's accessibility.