ID: cOuLbPhOT1
Title: PACE: Marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a consistency regularization method aimed at enhancing the generalization performance of Parameter-Efficient Fine-Tuning (PEFT) techniques. The authors propose a framework that constructs two predictions perturbed by different noises and penalizes the L2 distance between them. Theoretical analysis indicates that this regularization reduces gradient norms, while empirical evaluations across various tasks demonstrate state-of-the-art performance improvements. Additionally, the paper introduces the PACE framework and its variants, such as PACE$\_{lazy}^{half}$ and PACE$\_{fast}$, which aim to enhance model performance while minimizing additional memory requirements. The authors clarify that PACE$\_{lazy}^{half}$ ideally incurs no extra memory, and PACE$\_{fast}$ requires only marginal memory compared to baseline GPU memory. Empirical evidence shows that PACE variants outperform the baseline in terms of accuracy while using significantly less memory and training time. The authors emphasize the theoretical contributions of their work, particularly the relationship between small gradient norms and improved generalization.

### Strengths and Weaknesses
Strengths:  
- Theoretical results are clearly articulated and provide a solid foundation for the proposed method.  
- Extensive empirical evaluation across multiple datasets and tasks supports the effectiveness of the approach.  
- The ablation study is well-executed, validating the design choices made by the authors.  
- The paper effectively addresses reviewer concerns, enhancing clarity and completeness.  
- Empirical results show that PACE variants outperform the baseline with reduced memory and training time.  
- The theoretical insights regarding gradient norms and generalization are well-articulated and contribute to the understanding of neural network behavior.

Weaknesses:  
- **Lack of related work discussion**: The paper does not adequately compare its method with existing techniques like L2-SP, DELTA, and FTP, which limits the evaluation's comprehensiveness.  
- **Increased computational demands**: The necessity for two forward passes during fine-tuning significantly raises computation and memory requirements, which may hinder practical application.  
- **Unclear alignment mechanism**: While $D^{fp}$ encourages alignment, the authors do not sufficiently clarify how $D^{PACE}$ achieves this.  
- **Novelty concerns**: The integration of consistency regularization with PEFT is not sufficiently innovative, as similar approaches exist in the literature.  
- **Experimental baseline ambiguity**: The pre-training method for the ViT-B/16 model is not clarified, raising questions about the impact of supervised versus self-supervised training.  
- The proposed variants are perceived as minor tricks rather than substantial technical advancements.  
- There is a need for a more detailed discussion on similarities and differences with multimodal fine-tuning approaches.  
- The computational complexity associated with multiple forward passes remains a concern.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related works by including comparisons with L2-SP, DELTA, and FTP to provide a more comprehensive evaluation of their method. Additionally, the authors should address the increased computational and memory requirements by providing efficiency metrics alongside performance results. Clarifying how $D^{PACE}$ ensures alignment would strengthen the theoretical foundation of the paper. Furthermore, we suggest extending the experimental evaluation to include comparisons with larger domain shifts and other orthogonal fine-tuning techniques. We also recommend justifying the prolonged fine-tuning duration required for their method and exploring its applicability to language tasks to demonstrate broader generalizability. Additionally, we suggest that the authors improve the discussion on the similarities and differences with multimodal fine-tuning approaches to maintain modal consistency. Lastly, further investigation into methods to reduce the higher computational complexity linked to multiple forward passes could enhance the practicality of the proposed methods.