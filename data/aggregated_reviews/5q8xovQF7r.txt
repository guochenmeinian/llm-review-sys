ID: 5q8xovQF7r
Title: Pairwise Causality Guided Transformers for Event Sequences
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 6, 5, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to enhance transformer-based models for temporal event sequences by incorporating pairwise qualitative causal knowledge. The authors establish a framework for causal inference and provide theoretical justification for their method. Experimental results indicate that their approach outperforms existing models, demonstrating improved prediction accuracy on both synthetic and real datasets.

### Strengths and Weaknesses
Strengths:
- The investigation into incorporating causal knowledge into transformers addresses a relevant and significant topic, representing a valid contribution to the field.
- Extensive experiments validate the proposed approach, showing it outperforms all baselines, albeit sometimes by a small margin.

Weaknesses:
- The structure of the paper could be improved; key contributions, particularly training details and the proposed loss function, are not clearly presented.
- The impact of the choice of the $\alpha$ value is not discussed, and some notations are confusing, leading to potential misunderstandings about the event sequences being modeled.
- The theoretical contributions appear to be straightforward adaptations of existing theories, lacking significant novelty.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper's structure, particularly by emphasizing the training details and the proposed loss function. Additionally, a discussion on the impact of the $\alpha$ value should be included. We suggest clarifying notations to avoid confusion regarding the event sequences and providing a more thorough theoretical analysis to enhance the perceived novelty of the contributions.