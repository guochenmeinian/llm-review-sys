ID: lNCsyA5uS1
Title: Thought of Search: Planning with Language Models Through The Lens of Efficiency
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 4, 7, 6, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper analyzes the use of large language models (LLMs) in planning and proposes writing the successor function and goal test in code rather than solving the problem directly. The authors demonstrate through experiments that using code yields higher accuracy and fewer calls to the LLM compared to traditional LLM-based solutions. The authors also argue that current LLM planning methods waste computational resources and suggest a more efficient approach by employing LLMs to preprocess search algorithms.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, with detailed discussions providing valuable insights.
- It presents a simple yet original idea focused on responsible usage of LLMs for planning, which could enhance performance.
- The methodology is complemented by an extensive survey of existing LLM planning methods, including a summary of their complexities and soundness.

Weaknesses:
- The conclusion lacks surprising insights and does not significantly advance the field, as the idea of using LLMs to write code has been previously explored.
- The experiments rely on classic benchmarks, raising questions about the novelty and effectiveness of the proposed method.
- The paper does not adequately address related works that could strengthen its arguments, particularly in learning for planning and heuristic generation.
- The need for human feedback in code generation raises concerns about the generalizability of the proposed approach.

### Suggestions for Improvement
We recommend that the authors improve the conclusion to provide more novel insights and clearly differentiate their work from existing literature. A more thorough literature review is essential to address related works, particularly those that focus on learning for planning and heuristic generation. Additionally, we suggest that the authors provide statistics on the extent of human feedback required in their experiments to clarify the robustness of their method. Lastly, enhancing the clarity of experimental details and addressing minor formatting issues would improve the overall presentation of the paper.