ID: WY3xgXIZUR
Title: Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 6, 4, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called Visualized In-Context Text Processing (VisInContext) aimed at enhancing multi-modal large language models (MLLMs) by converting lengthy text into visual tokens. This approach seeks to reduce GPU memory usage and computational costs while increasing the context length that can be processed effectively. The authors demonstrate that VisInContext outperforms baseline models in various tasks, particularly in document understanding and in-context few-shot evaluations.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents an innovative concept of transforming text into visual representations to decrease computational expenses.
- VisInContext effectively reduces GPU memory usage and FLOPs, allowing for longer text contexts with lower computational costs.
- The method shows improved performance on downstream benchmarks, particularly in document comprehension tasks.

Weaknesses:
- The approach may introduce complexity due to multiple steps involved, such as text rendering and token masking, which could hinder practical adoption.
- The motivation for certain experiments, particularly those involving text-only adaptations of visual tasks, lacks clarity and relevance.
- There is insufficient exploration of potential information loss during the rendering process and a lack of comparison with other long-context methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental design and the relevance of the tasks chosen for validation. Specifically, the paper should include a thorough investigation of the potential information loss from rendering long texts into images and provide a comparison with other long-context methods, such as compression-into-text approaches. Additionally, we suggest that the authors explore the optimal compression rate by examining the trade-off between the number of images and the number of words per image. Finally, addressing the complexity of the proposed method and its implications for practical implementation would enhance the paper's contribution.