ID: H15KtcyHvn
Title: Training Fully Connected Neural Networks is $\exists\mathbb{R}$-Complete
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 7, 6, 5, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the algorithmic complexity of training fully connected ReLU neural networks, demonstrating that perfectly fitting two-dimensional data with a one-hidden layer network and two output neurons is $\exists\mathbb{R}$-complete. The authors prove the ER-completeness of empirical risk minimization for such networks, contributing significantly to the theoretical understanding of neural network training complexity. The paper is well-structured, though the proofs are relegated to supplementary material.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents its findings clearly, reflecting the authors' thorough effort in discussing results and implications.
- The reduction of training two-layer neural networks to ETR-INV is novel and contributes to the understanding of training complexity.
- The clarity of presentation and discussion of context, limitations, and implications enhances the paper's value.

Weaknesses:
- The focus on perfectly fitting data contrasts with practical interests in achieving sufficiently small training errors.
- The assumption of needing at least 13 classes in two dimensions is limiting, as common practices may circumvent $\exists\mathbb{R}$-completeness.
- The reliance on supplementary material for the main proofs raises concerns about thorough review and understanding of the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theorem statement in line 109 by replacing "even" with "only" to avoid confusion. Additionally, we suggest that the authors elaborate on the proof ideas within the main paper to enhance understanding, particularly regarding the crucial steps of their reductions. Furthermore, we encourage the authors to address the success of gradient-based optimization in training neural networks more explicitly, as this is a significant aspect often overlooked in the discussion. Lastly, we advise shortening cumbersome definitions, such as Definition 2, and using standard matrix notation for neural networks in Definition 1 to improve readability.