ID: dQmEIwRw16
Title: Collision Cross-entropy for Soft Class Labels and Entropy-based Clustering
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the collision cross-entropy (CCE) as an alternative to Shannon cross-entropy (SCE) for handling soft labels in classification, particularly in self-labeling and clustering contexts. The authors argue that CCE addresses SCE's limitations, especially its sensitivity to label noise. They provide theoretical foundations for CCE, describe an EM algorithm for pseudo-label estimation, and demonstrate its superior performance through experiments. Additionally, the authors focus on network training with noisy labels in self-labeled "deep clustering" and propose to clarify the scope of their discussion on generalized entropy measures in information theory while acknowledging the existence of other generalizations outside this context.

### Strengths and Weaknesses
Strengths:  
- The paper is **well-written** and provides a thorough review of relevant theoretical research and practices in sections 1-2.  
- The introduction of CCE is **original** and well-motivated, addressing the need for robust handling of soft labels.  
- The authors present a **solid theoretical foundation** for CCE, including its properties and relationship to other entropy measures.  
- The authors demonstrate a clear understanding of the relationship between entropy in thermodynamics and information theory, proposing to cite relevant literature to enhance their discussion.  
- The authors provide a thoughtful response to reviewer feedback, indicating a willingness to improve the paper based on suggestions.  
- The experimental results show that the proposed method achieves better robustness to label uncertainty, which is significant for self-labeled clustering.  

Weaknesses:  
- The contributions are **conceptually limited**, primarily focusing on a single alternative to Shannon entropy without exploring other generalized entropies.  
- The paper lacks a **comprehensive theoretical analysis** of CCE and its EM algorithm, particularly regarding convergence.  
- The paper lacks theoretical justification for the new loss function and does not sufficiently compare its performance to state-of-the-art methods, leading to concerns about its contribution.  
- The experimental evaluation relies on **outdated architectures** and small datasets, raising concerns about the generalizability of the results.  
- The authors' reliance on older models may not meet the performance standards established in prior work, as noted by multiple reviewers.  
- The presentation of the EM algorithm in section 4 is **less clear** compared to other sections, potentially confusing readers unfamiliar with such algorithms.  

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of the collision cross-entropy, particularly regarding its convergence properties and how it compares to other generalized entropies. Additionally, we suggest conducting experiments on larger, more modern datasets and architectures to validate the robustness of CCE. We also recommend that the authors improve the theoretical grounding of their new loss function by providing justifications for its use and expanding their literature review to include more recent works that explore entropy beyond Shannon's, particularly in the context of clustering and machine learning. Clarifying the relationship between the proposed loss function and the EM algorithm would strengthen the paper's claims. Furthermore, we encourage the authors to expand their discussion of the implications of CCE in real-world applications, particularly in areas like distillation and teacher-student training. Lastly, addressing the limitations of the work more explicitly and considering the potential impact of soft label quality on performance would enhance the paper's depth. Additionally, we advise clarifying the use of bold font in tables to avoid misinterpretation regarding statistical significance.