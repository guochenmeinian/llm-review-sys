ID: rafVvthuxD
Title: EM Distillation for One-step Diffusion Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 6, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents EM Distillation (EMD), a method that distills a pretrained diffusion model into a one-step generator using a maximum likelihood approach based on the Expectation-Maximization (EM) algorithm. The authors propose an alternative to naive EM adaptation, which is computationally expensive, by employing MCMC sampling on the joint distribution \( p(x,z) \) to generate samples. The method incorporates a reparametrization trick and Langevin dynamics, aiming to stabilize training through noise cancellation. EMD shows promising results in conditional image generation tasks, generalizing previous methods like Variational Score Distillation (VSD) and Diff-Instruct.

### Strengths and Weaknesses
Strengths:
1. The qualitative results for one-step generation with EMD are impressive, with quantitative metrics showing comparable or superior performance on datasets like ImageNet and MS-COCO.
2. The method generates high-quality images in one step and improves the diversity of generated images, indicating enhanced mode coverage.
3. The innovative use of stochastic Langevin updates contributes to better mode coverage while addressing noise for smoother training.

Weaknesses:
1. EMD requires more calls to the teacher model than baseline methods like Diff-Instruct, leading to increased training time and computational overhead, which should be discussed in detail.
2. Significant typos and missing multiplicative factors in equations raise concerns about the correctness of experimental observations, necessitating clarification from the authors.
3. The introduction of additional hyperparameters lacks corresponding ablation studies, particularly regarding the sensitivity of performance to parameters like \( K \) and \( t^\star \).

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the computational overhead associated with additional calls to the teacher model, particularly how it scales with \( K \). Additionally, addressing the significant typos and providing corrections in Section 3.2 is crucial for validating experimental results. We suggest including ablation studies to analyze hyperparameter sensitivity, especially for \( K \) and \( t^\star \). Furthermore, a theoretical justification for the noise cancellation technique should be provided, alongside empirical evidence of its impact on optimization variance. Lastly, we encourage the authors to clarify the implementation of forward KL divergence with a discriminator as an alternative to MCMC.