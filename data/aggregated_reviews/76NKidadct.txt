ID: 76NKidadct
Title: Improved Particle Approximation Error for Mean Field Neural Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 7, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an improved finite particle approximation error bound for mean-field Langevin dynamics (MFLD) that eliminates the dependency on the log-Sobolev inequality (LSI) constant. The authors establish a $O(1/N)$ gap between the original and $N$-particle objective, enhancing convergence rates, sampling guarantees, and uniform-in-time propagation of chaos results. The applicability of their results is demonstrated through three examples, including improved convergence of MFLD and sampling guarantees for the final stationary distribution.

### Strengths and Weaknesses
Strengths:
- The independence of the particle discretization error from the LSI constant is a significant advancement, particularly in low-temperature regimes where $\alpha$ can be large relative to $\lambda^{-1}$.
- The proof technique utilizing the induced Bregman divergence is novel and simplifies previous analyses, making the new error easily applicable to existing MFLD frameworks.
- The paper is well-structured and clearly presented, with key contributions well-motivated and relevant literature effectively summarized.

Weaknesses:
- While Theorem 1 presents a strong result, subsequent findings appear to be straightforward applications of existing analyses, retaining some dependency on the LSI constant.
- The assumption of bounded activation functions and the necessity of LSI conditions may limit the general applicability of the results.
- The authors do not sufficiently motivate why the particle approximation should not depend on the LSI constant, which could enhance the reading experience.

### Suggestions for Improvement
We recommend that the authors improve the motivation for the independence of the particle approximation from the LSI constant to enhance clarity. Additionally, consider addressing the following points:
- Provide a sketch on how the LSI constant $\alpha$ deteriorates with the regularization parameter $\lambda$.
- Include references to "Mean field analysis of neural networks: A law of large numbers" and "Mean field analysis of neural networks: A central limit theorem" by J. Sirignano and K. Spiliopoulos for a more comprehensive literature review.
- Clarify the role of the boundedness of the first variation in Assumption 1 and whether it can be relaxed.
- Ensure that the terminology regarding Lipschitz smoothness is consistent and clear throughout the paper.