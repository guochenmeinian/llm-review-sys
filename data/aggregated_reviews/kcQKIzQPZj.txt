ID: kcQKIzQPZj
Title: Localize, Understand, Collaborate: Semantic-Aware Dragging via Intention Reasoner
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 5, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework called LucidDrag for drag-based image editing, addressing the limitations of current methods in understanding user intentions. The authors propose an intention reasoner utilizing LLMs to infer potential editing intentions, which are then used to provide semantic guidance during the editing process. Additionally, a collaborative guidance sampling mechanism integrates semantic and quality guidance to enhance image fidelity. Experimental results demonstrate the proposed method's superiority over existing techniques in producing semantically coherent and diverse outcomes.

### Strengths and Weaknesses
Strengths:
1. The paper is well-organized and easy to follow.
2. The proposed "what-then-how" paradigm for drag-based editing is novel and sound.
3. The collaborative guidance mechanism is interesting and effective.
4. The authors perform sufficient comparison experiments and ablation studies to support their claims.

Weaknesses:
1. The selection process for experimental results, such as those in Figure 4, raises concerns about potential unfair comparisons if results were manually chosen.
2. There are observable artifacts in synthesized images, indicating quality issues.
3. The utility of the Intention Reasoner is questioned, as it may be more effective for users to provide their true intentions directly.
4. The confidence probabilities of the LLM outputs may not be reliable without fine-tuning, and the efficiency of different methods should be compared.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how single experimental results were selected and consider providing inferred potential intentions, source, and target prompts for generated examples to explain performance. Additionally, addressing the observable artifacts in synthesized images is crucial. We suggest that the authors clarify how the predicted outputs of LLMs align with user intentions and include text/instruction-based methods for a fairer comparison. Lastly, we encourage the authors to discuss the limitations in the main paper rather than the supplementary material and to ensure that the computational complexity of the proposed method is clearly outlined.