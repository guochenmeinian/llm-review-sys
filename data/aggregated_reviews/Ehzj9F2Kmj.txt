ID: Ehzj9F2Kmj
Title: Generative Modelling of Stochastic Actions with Arbitrary Constraints in Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an algorithm designed to enhance reinforcement learning (RL) efficiency in large discrete action spaces, where only certain actions are valid for specific states. The authors propose using normalizing flows to learn a policy without modeling the entire action probability distribution, which is challenging to estimate. Key algorithmic details include a sandwich bound for improved action log probability estimation and an oracle action validator to manage invalid actions. The paper evaluates its method against control tasks and resource allocation tasks, demonstrating its effectiveness. Additionally, the authors analyze an autoregressive (AR) approach for resource allocation, focusing on the allocation of three resources across nine areas on a graph, with constraints requiring that resources R2 and R3 be within two hops of each other. They propose a policy network where the allocation of R2 depends on R1, and R3 depends on both R1 and R2. The paper shows that the AR approach struggles with constraint satisfaction, leading to suboptimal allocations compared to their proposed method.

### Strengths and Weaknesses
Strengths:
- The paper addresses significant issues in RL related to large discrete action spaces without assuming any specific structure among actions.
- The approach leverages established algorithms like normalizing flows and presents a sound theoretical foundation, supported by empirical evaluations.
- The overall performance of the proposed method surpasses existing benchmarks, indicating its effectiveness.
- The paper provides a clear experimental framework and detailed analysis of the AR approach, highlighting its limitations in handling constraints.
- The authors effectively illustrate the performance gap between their approach and the AR method through well-structured tables and examples.

Weaknesses:
- The abstract fails to adequately summarize the paper's content and appears disconnected from the main text.
- The assertion that the optimal policy must be stochastic is unclear and seems irrelevant to the core problem.
- The comparison with Wol-DDPG is questionable, as it assumes spatial structure among discrete actions, which the authors do not adequately address.
- The sample-and-reject method for action validity is somewhat unsatisfactory, although acknowledged as a limitation by the authors.
- The discussion on constraint satisfaction and its impact on performance could be more concise and clearer.
- The AR model's dependency structure may not be adequately addressed, leading to confusion regarding its implications for constraint handling.

### Suggestions for Improvement
We recommend that the authors improve the abstract to better reflect the paper's content and significance. Clarifying the necessity of a stochastic policy in the context of the proposed method would strengthen the argument. Additionally, we suggest benchmarking against decomposed action decision methods to provide a more comprehensive comparison. The authors should also ensure that the comparison with Wol-DDPG is valid and address the flat learning curves observed. Furthermore, we recommend improving the clarity of the discussion on constraint satisfaction, emphasizing its role in the performance gap between the AR and IAR-A2C approaches. Including a concise version of this discussion in the paper would enhance understanding. We also suggest extending the experimental section to include comparisons with relevant baselines, as this would significantly enhance the paper's value. Finally, we encourage the authors to explore the reparameterization of the policy, as this could lead to better results compared to the baselines and improve the overall quality of the paper.