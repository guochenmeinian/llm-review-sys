ID: 9bu627mTfs
Title: Context and Geometry Aware Voxel Transformer for Semantic Scene Completion
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CGFormer, a Context and Geometry Aware Voxel Transformer for semantic scene completion. It generates unique voxel queries for different input images, utilizing a 3D deformable cross-attention mechanism to enhance feature sampling and improve depth accuracy through a depth refinement strategy. The architecture achieves state-of-the-art performance on benchmarks like SemanticKITTI and SSC-Bench-KITTI360.

### Strengths and Weaknesses
Strengths:
1. The code is provided in the supplementary material with clear guidelines.
2. The proposed blocks demonstrate significant performance improvements, supported by thorough experiments.
3. CGFormer outperforms previous methods on key benchmarks.
4. The paper is well-organized, with a clear motivation.

Weaknesses:
1. The use of EfficientNetB7 introduces more parameters than ResNet50 in prior methods, making comparisons less fair.
2. There is a text error in Figure 1.
3. While qualitative results are strong, the inclusion of failure cases would enhance the analysis.
4. The empirical results do not convincingly support claims about context-aware queries, particularly in categories with finer details.
5. The details of the Context-Aware Query Generator (CAQG) are insufficiently elaborated in Section 3.2.

### Suggestions for Improvement
We recommend that the authors improve the comparison of CGFormer with other methods by using consistent settings, particularly regarding parameter counts. Additionally, please correct the text error in Figure 1 and consider including failure cases to provide a more comprehensive evaluation of the model's performance. We suggest elaborating on the CAQG in Section 3.2 to emphasize its novelty and importance. Finally, we encourage the authors to analyze the performance discrepancies observed in Table 3, particularly regarding the TPV branch and its impact on mIoU and IoU metrics.