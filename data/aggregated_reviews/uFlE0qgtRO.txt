ID: uFlE0qgtRO
Title: Focus Your Attention when Few-Shot Classification
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 4, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel few-shot learning paradigm by adapting large-scale pretrained models directly to classification tasks through fine-tuning on few-shot examples. The authors propose a method called FORT, which utilizes attention and gradient information to identify important entities, termed position prompts, and introduces a new auxiliary loss to enhance attention focus on these prompts. Extensive experiments demonstrate that the proposed method consistently improves performance across various datasets and model architectures.

### Strengths and Weaknesses
Strengths:
1. The approach of using "large-scale pretraining --> few-shot learning" without base training is meaningful and realistic.
2. The focus on key entities through attention mechanisms is a reasonable strategy for few-shot adaptation.
3. The experiments validate the method, showing consistent improvements over multiple baselines.

Weaknesses:
1. The rationale for adding an identity matrix in Eqn. 6 is unclear, raising questions about its necessity.
2. The attention graph calculations differ significantly between columnar and pyramidal architectures, and ablation studies are needed to explore these differences.
3. Results analysis lacks clarity, with multiple observations condensed into single paragraphs, making it difficult to correlate observations with specific results.
4. The visualization in Fig. 1 is misleading, as it does not adequately represent the useful foreground, unlike Fig. 4.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the rationale behind the identity matrix in Eqn. 6. Additionally, conducting ablation studies to investigate the attention graph design across different architectures would enhance understanding. The authors should also clarify the results analysis section to ensure that observations are explicitly linked to corresponding results in the tables. Finally, we suggest revisiting the visualization in Fig. 1 to ensure it accurately represents the attention focus on relevant foreground elements.