ID: hwuUBsMlBf
Title: CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 5, 7, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CuMo, a method aimed at enhancing multimodal large language models (MLLMs) by integrating sparsely-gated Mixture-of-Experts (MoE) blocks into the vision encoder and MLP connector. CuMo addresses the computational inefficiencies of traditional scaling approaches and employs a three-stage training process, achieving notable performance across various benchmarks. The authors explore MoE integration strategies and introduce auxiliary losses for expert load balancing, demonstrating CuMo's effectiveness in improving model capabilities while managing computational efficiency.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written, with a clear and simple presentation of the proposed method.
2. CuMo addresses the significant issue of computational expense in traditional MoE-based MLLMs, particularly regarding the vision encoder.
3. The experimental evaluation is thorough, showcasing CuMo's competitive performance against state-of-the-art models across multiple benchmarks.

Weaknesses:
1. The authors do not adequately explain how the MoE approach enhances model capabilities, particularly in the vision encoder and connector.
2. The expansion of trainable parameters due to the MoE method raises questions about whether merely increasing parameters in the vision encoder will enhance model capabilities.
3. CuMo's evaluation is primarily limited to Mistral-8Ã—7B, lacking comparisons with other LLM backbones like Qwen and LLaMA to clarify that improvements are not solely due to inherent LLM capabilities.
4. The three-stage training process may lead to unfair comparisons with other baselines, such as LLaVA1.5, due to increased training data.
5. There is limited discussion on the computational efficiency and training time required for integrating MoE blocks, which is crucial given potential additional costs.
6. The interpretability of MoE decisions within CuMo is not addressed, which is important for real-world applications.

### Suggestions for Improvement
We recommend that the authors improve the explanation of how the MoE method enhances model capabilities, particularly in the vision encoder and connector. Additionally, consider discussing whether merely increasing the parameters of the vision encoder will enhance model capabilities, providing readers with valuable insights. We suggest including comparisons with other LLM backbones, such as Qwen and LLaMA, to demonstrate that improvements are not solely due to differences in inherent LLM capabilities. To ensure fair comparisons, clarify how the increased training data in the three-stage training process affects results relative to other baselines. Furthermore, we encourage the authors to expand the discussion on the computational efficiency and training time associated with MoE integration, as well as the interpretability of MoE decisions within CuMo.