ID: uwSaDHLlYc
Title: Diversity-Driven Synthesis: Enhancing Dataset Distillation through Directed Weight Adjustment
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for dataset distillation through Directed Weight Adjustment (DWA), aimed at enhancing the diversity of synthesized datasets. The authors propose a dynamic weight adjustment technique that incorporates decoupled coefficients for batch normalization loss, initialization with real images, and random perturbation of model weights. Extensive experiments across various datasets, including CIFAR, Tiny-ImageNet, and ImageNet-1K, demonstrate that the proposed method significantly outperforms existing approaches while maintaining minimal computational expense.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and clearly written, providing a solid theoretical foundation and extensive empirical validation.
- The innovative dynamic adjustment mechanism effectively enhances the diversity of synthesized datasets, addressing a key challenge in dataset distillation.
- Detailed ablation studies illustrate the contributions of each component of the proposed method, showcasing its robustness across different hyperparameter settings and datasets.

Weaknesses:
- The effectiveness of the proposed method is not adequately compared with recent state-of-the-art dataset distillation methods, particularly those extending from SRe2L and diffusion-based approaches.
- The reliance on batch normalization statistics may limit applicability to recent models like Vision Transformers that do not utilize batch normalization.
- The paper lacks a comprehensive ablation analysis that evaluates the individual contributions of the decoupled $L_{var}$ coefficient, DWA scheme, and real initialization.
- The results on CIFAR-10 using ConvNet are significantly worse than the current state of the art, and the authors do not clarify the computational overhead associated with the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the comparison with recent state-of-the-art dataset distillation methods, particularly those derived from SRe2L and diffusion-based techniques. Additionally, the authors should clarify the applicability of the proposed method to Vision Transformer models and provide insights into the computational cost of parameter tuning for different architectures. A more detailed ablation study is necessary to separately evaluate the contributions of the key components of DWA. Lastly, addressing the performance discrepancies on CIFAR-10 and providing a clearer explanation of the computational overhead would strengthen the submission.