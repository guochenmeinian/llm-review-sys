ID: b6FeLpKKjl
Title: Convergence of Alternating Gradient Descent for Matrix Factorization
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 4, 8, 8, 8, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the convergence of alternating gradient descent (AGD) for the low-rank matrix factorization problem, specifically minimizing the function \( f(X,Y) = ||XY - A||_F^2 \). The authors propose an asymmetric initialization strategy that significantly improves convergence rates, achieving an error of \( \epsilon \) in \( O(\kappa^2 \log \frac{||A||_F}{\epsilon}) \) iterations, where \( \kappa \) is the pseudo-condition number of \( A \). This approach does not require computing an SVD of \( A \), marking a notable advancement in the field.

### Strengths and Weaknesses
Strengths:
- The introduction of an asymmetric warm starting rule is a novel technical contribution that enhances the theoretical understanding of matrix factorization.
- The paper is well-written, with rigorous proofs and interesting experimental results demonstrating the efficacy of the proposed method.
- The theoretical analysis advances the state of the art in optimization algorithms relevant to machine learning.

Weaknesses:
- The proposed initialization appears similar to trivial solutions, raising questions about its novelty and the significance of the improvements in convergence rates.
- The assumption regarding the independence of entries in line 454 is potentially incorrect, which may affect the validity of the results.
- The comparison with related works may be perceived as unfair, particularly regarding the initialization strategies used in previous studies.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the motivations for their initialization strategy, clarifying how it differs from trivial solutions like \( X = \frac{1}{\sqrt{\eta}} A \) and \( Y = \sqrt{\eta} Id \). Additionally, we suggest addressing the assumption about \( V^\top \Phi_1 \) having i.i.d entries and considering a different lemma if necessary. It would also be beneficial to compare algorithms using the best fixed step size for each, rather than a uniform step size, to ensure fair evaluations. Furthermore, we encourage the authors to explore the generalizability of their findings to more complex models where gradient descent is essential, as well as to provide more intuition on the significance of imbalanced scaling in their experiments. Lastly, we suggest including a logarithmic y-axis in error plots due to the exponential convergence rates observed.