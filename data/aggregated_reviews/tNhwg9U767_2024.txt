ID: tNhwg9U767
Title: Microstructures and Accuracy of Graph Recall by Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive evaluation of large language models' (LLMs) ability to recall graph (sub)structures using natural language as an interface. Through extensive experiments across various domains, it highlights LLMs' underperformance in graph recall tasks and provides novel insights that encourage further research in this area. The authors propose a method to assess graph recall and examine factors influencing this ability, although the focus is primarily on link prediction.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and motivated, integrating computational social science with LLMs and comparing their behavior to human performance.
2. It addresses an important problem by evaluating LLMs' capability to memorize graph structures, contributing valuable insights to the field.
3. The evaluation design is robust, supported by social science methodologies.
4. Code implementation is provided, ensuring reproducibility.
5. The authors suggest several potential research directions.

Weaknesses:
1. The study primarily uses natural language as the interface, while recent works suggest that graph encoders and multi-modal projectors may better inject graph-related knowledge into LLMs.
2. Focusing on k=5 limits the generalizability of conclusions; LLMs' recall capabilities may vary with input length and larger graphs.
3. The formulas used to derive results for Table 1 are not provided.
4. Some phenomena discussed lack corresponding explanations.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by exploring the use of multi-modal LLMs and graph encoders. Additionally, consider varying the input length and graph size to assess the impact on recall capabilities. Clarifying the formulas used for Table 1 and providing explanations for unexplained phenomena, such as the one mentioned in line 327, would enhance the paper's clarity. Finally, discussing the relationship between their findings and recent theoretical analyses on transformer reasoning capabilities would provide valuable context.