ID: QwQ5HhhSNo
Title: Is Distance Matrix Enough for Geometric Deep Learning?
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents k-DisGNNs, which utilize distances to represent higher-order interactions in geometric graphs, bridging geometric deep learning and traditional graph representation learning. The authors conduct experiments primarily on the MD17 dataset, achieving state-of-the-art results with 2F-Dis and 3E-Dis models. However, performance diminishes on the QM9 dataset, where 2F-Dis underperforms compared to existing methods, and 3E-Dis is computationally infeasible. The paper also introduces a method to generate counterexamples for geometric graphs that cannot be distinguished by MPNNs. Additionally, the authors propose a framework for DisGNNs that addresses the trade-off between completeness and efficiency in distance matrices, noting that while their current implementation uses a dense n x n matrix, the models can be adapted for sparse graphs using sparse data structures and scatter operations in PyTorch. They acknowledge the need for further research into the potential of sparsifying distance matrices without losing completeness.

### Strengths and Weaknesses
Strengths:
1. The first work to use distances alone for higher-order geometric information representation.
2. Establishes a connection between geometric deep learning and traditional graph representation learning.
3. Strong performance of the k=2 model on the MD17 dataset.
4. Well-written with informative figures.
5. Clear understanding of the trade-offs involved in using distance matrices and a thoughtful approach to adapting models for sparse settings.
6. Detailed explanation of the implementation process for sparse-version DisGNNs, including the use of tensors and scatter operations.

Weaknesses:
1. Prohibitive computational costs for k > 2, making the model infeasible for larger datasets like QM9.
2. Lack of scalability and performance issues when transitioning from MD17 to QM9, with 2F-Dis underperforming against prior approaches.
3. Clarity issues in the writing regarding the message passing mechanism.
4. The theoretical framework may not be directly applicable to sparse settings, indicating a potential gap in the current research.
5. Initial misunderstanding regarding the application of distance matrices to sparse graphs suggests a need for clearer communication of the model's capabilities.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the message passing mechanism by explicitly writing the message passing block rather than discussing its differences from existing equations. Additionally, we suggest including the explicit equations for k-E-DisGNN in the main paper. It would also be beneficial to reiterate the limitations regarding the scalability of their models and the applicability of their theoretical framework to sparse graphs. We recommend that the authors improve the theoretical framework to better address the application of their models in sparse settings and provide more formal evidence supporting the potential for sparsifying distance matrices while maintaining completeness. Clarifying the adaptability of their models to various input data structures would enhance the overall understanding of their work. Finally, addressing the ambiguity in the claims about geometric deep learning's scope would enhance the paper's precision.