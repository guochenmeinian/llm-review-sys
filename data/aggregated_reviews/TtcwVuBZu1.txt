ID: TtcwVuBZu1
Title: QuadMamba: Learning Quadtree-based Selective Scan for Visual State Space Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 9, 8, 5, 5, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents QuadMamba, a novel Mamba architecture designed for visual tasks, including image classification and dense predictions. The authors introduce a learnable quad-tree partition strategy that adaptively generates multi-scale visual sequences, enhancing the model's ability to capture relevant information through a coarse-to-fine approach. The implementation utilizes a differentiable splitting operator, allowing for effective training. Experimental results demonstrate that QuadMamba achieves state-of-the-art performance across standard benchmarks such as ImageNet, COCO, and ADE20K.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with clear figures that facilitate understanding.
- The coarse-to-fine scanning approach effectively captures relevant information.
- The proposed method consistently outperforms classic CNNs and Vision Transformers on key tasks.
- Strong motivation and applicability of adapting Mamba models to vision tasks.
- Good experimental results, with extensive ablation studies supporting the findings.

Weaknesses:
- The lightweight module for predicting 2D locality needs clarification on whether it is shared across layers or specific to each layer.
- The potential for more than two levels of partitioning in remote sensing images remains unexplored.
- The significance of the contribution appears incremental compared to existing methods, and the performance trade-offs in lighter variants are not clearly articulated.
- Some experimental comparisons lack rigor, particularly regarding the training overhead and the impact of hyper-parameters.

### Suggestions for Improvement
We recommend that the authors clarify whether the lightweight module for predicting 2D locality is shared across layers or specific to each layer, as this could impact the model's performance. Additionally, we suggest conducting experiments with more than two levels of partitioning to explore its potential benefits. It would also be beneficial to provide detailed analyses of hyper-parameter impacts and to include comparisons of training overhead between QuadVSS and VSS blocks. Finally, we encourage the authors to enhance clarity in the presentation of results, particularly regarding the significance of performance trade-offs and the rationale behind architectural choices.