ID: NPkkvrv2Vp
Title: Who is Speaking? Speaker-Aware Multiparty Dialogue Act Classification
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a GNN-based approach to dialogue act recognition, incorporating speaker turn tokens, a RoBERTa-based utterance encoder, and a graph neural network that models speaker and utterance nodes. The experimental evaluation on MRDA and SwDA indicates that the speaker turn token significantly enhances performance, while the GNN component shows limited effectiveness and can sometimes degrade results. The authors aim to improve dialogue act classification by leveraging explicit speaker representations.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and logically structured, with a thorough literature review.
- It provides detailed parameter exploration and rich experimental details.
- The proposed GNN approach is relevant and has the potential to improve dialogue act classification.

Weaknesses:
- The novelty of the approach is questionable, as many related works exist that utilize similar methods for encoding speaker information.
- The experimental comparisons are insufficient, lacking a broader range of baseline systems.
- The minority class analysis does not adequately consider sample size, and the significance of the reported improvements is unclear.

### Suggestions for Improvement
We recommend that the authors improve the novelty justification by discussing related works in Emotion Recognition in Conversations (ERC) and providing a more comprehensive experimental comparison with recent methods. Additionally, we suggest that the authors include significance test results for their findings, clarify the RGAT function, and ensure that all important results are referenced appropriately in the main body. Lastly, addressing the underspecified parameter settings and making the code open-source would enhance reproducibility.