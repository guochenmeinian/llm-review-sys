ID: WmpyDkTHvI
Title: Text2Tree: Aligning Text Representation to the Label Tree Hierarchy for Imbalanced Medical Classification
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Hierarchy Aligned Representation Learning (HARL) for medical text classification, addressing the imbalanced problem where certain classes lack sufficient training data. HARL integrates two components: 1) Similarity Surrogate Learning, which employs supervised contrastive learning with all samples in a batch as weighted positive samples; and 2) Dissimilarity Mixup Learning, which utilizes weighted mixup samples. The authors employ similarity scores of label representations as weights, learned through a cascade attention module. The effectiveness of HARL is demonstrated through experiments on five biomedical datasets.

### Strengths and Weaknesses
Strengths:
- The paper explores medical text classification from a data imbalance perspective, proposing a general solution adaptable to various scenarios.
- The experiments validate the utility of each model component, showing promising results on rare labels.
- The methodology includes a thorough description of experimental settings, facilitating reproducibility.

Weaknesses:
- Section 3.2 is incomplete, and strong baseline comparisons are lacking; stronger hierarchical text classification models should be included.
- The ablation results do not sufficiently demonstrate how HARL components outperform existing solutions.
- The method's description is not comprehensive, lacking an overall framework diagram to clarify module interactions.

### Suggestions for Improvement
We recommend that the authors improve Section 3.2 to provide a complete description of the proposed methods. Additionally, include comparisons with stronger hierarchical text classification models, such as HPT and Jiang et al.'s work, to substantiate claims of performance. We suggest enhancing the ablation study by explicitly comparing HARL components against existing methods, such as normal contrastive learning loss and standard mixup strategies. Furthermore, we advise the authors to include a framework diagram to illustrate the interactions among the three modules (HLR, SSL, and DML) for clarity. Lastly, addressing the writing quality and ensuring comprehensive explanations of the cascade attention module's advantages over conventional graph neural networks would strengthen the paper.