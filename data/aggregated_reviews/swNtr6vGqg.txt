ID: swNtr6vGqg
Title: The noise level in linear regression with dependent data
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 4, 3, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on least-squares linear regression with dependent data, specifically addressing the case of $\beta$-mixing data. The authors propose a blocking-based argument to manage dependence, allowing for a theoretical framework that provides upper bounds on the ordinary least squares (OLS) error. The main result, Theorem 3.1, asserts that after a burn-in phase, OLS achieves a risk bound similar to that of independent data, with the dependence on mixing behavior relegated to smaller order terms. The paper emphasizes the importance of the blocking technique and offers a detailed analysis of the non-asymptotic guarantees without requiring realizability assumptions.

### Strengths and Weaknesses
Strengths:
- The paper is exceptionally well-written, with clear notation and intuitive explanations of complex ideas.
- The main technical result (Theorem 3.1) fills a significant gap in the theoretical literature regarding linear regression with dependent data.
- The proof structure is logically coherent, effectively utilizing the blocking technique to manage dependence.

Weaknesses:
- The approach of combining blocking with concentration inequalities is somewhat traditional and lacks novelty, as similar methods have been previously established in the literature.
- There is insufficient empirical validation; the absence of numerical experiments limits the practical evaluation of the proposed method.
- Clarity issues arise regarding the definitions of "instance-specific" and "instance-optimal" performance guarantees, as well as the methodology and comparisons with prior work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions surrounding "instance-specific" and "instance-optimal" performance guarantees by providing a more explicit explanation of the "global" complexity in previous works. Additionally, we suggest including a definition for $\widehat{M}$ analogous to $M_{\star}$ in (2.1) to enhance accessibility for readers familiar with empirical risk minimization (ERM). To strengthen the paper, we encourage the authors to conduct numerical experiments to illustrate the effectiveness of their method and to provide a comparative analysis with existing methods. Furthermore, a more thorough discussion of the differences between the proposed methods and martingale techniques, as well as a detailed examination of the second condition in (3.3), would enhance the paper's rigor and clarity. Lastly, we advise the authors to address the organization of the manuscript and correct any typographical errors to improve overall presentation.