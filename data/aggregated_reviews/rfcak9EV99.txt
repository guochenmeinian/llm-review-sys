ID: rfcak9EV99
Title: Policy Optimization for Continuous Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 5, 5, 7, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to continuous reinforcement learning (RL), focusing on deriving a continuous-time analog to key concepts such as the performance difference lemma and policy gradient. The authors propose a continuous version of the TRPO/PPO algorithms and evaluate their performance on synthetic linear-quadratic stochastic control problems and a two-dimensional optimal pair trading problem. The paper aims to address the challenges of applying RL in continuous time, particularly for high-frequency applications.

### Strengths and Weaknesses
Strengths:
- Continuous-time RL is an under-explored area with significant potential for high-frequency control systems and adaptive discretization.
- The work is pioneering in proposing a continuous-time analog in a stochastic setting for the performance difference lemma and deriving a continuous PPO algorithm.
- The paper is well-organized, clearly written, and effectively explains its results.

Weaknesses:
- There is a lack of discussion or experimental results demonstrating the advantages of the continuous-time formulation over discrete-time RL, and comparisons to existing discrete-time methods are missing.
- The evaluation is limited to toy examples, which do not adequately address the scalability of the proposed methods; more complex scenarios are needed.
- The discussion of related work is insufficient, leaving unclear how this research fits within the continuous-time RL literature.
- Implementation details, such as hyperparameters and random seeds, are not provided, hindering reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including comparisons with discrete-time RL methods to illustrate the advantages of the continuous-time formulation. Experiments should highlight scenarios where continuous-time RL outperforms discrete-time approaches, particularly in cases of unevenly-spaced observations. Additionally, the authors should explore the impact of hyperparameters specific to the continuous-time setting on the optimization process and provide a thorough discussion of the limitations of their approach, including assumptions made in the proofs. Finally, we suggest enhancing the discussion of related work to clarify the positioning of this research within the broader continuous-time RL literature.