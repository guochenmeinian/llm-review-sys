ID: 19AgWnmyoV
Title: Instructing Goal-Conditioned Reinforcement Learning Agents with Temporal Logic Objectives
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 5, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for instructing goal-conditioned reinforcement learning (RL) agents to follow specifications expressed in Linear Temporal Logic (LTL) formulae. The authors propose constructing a Buchi automaton from the LTL specification, converting it into a directed graph, and employing a weighted-graph search algorithm to derive a high-level plan that satisfies the LTL specification. The method is evaluated across three benchmark environments: LetterWorld, ZoneEnv, and Ant-16rooms, demonstrating superior performance compared to two baseline methods for learning LTL-satisfying policies. Additionally, the authors plan to enhance the main body of the paper by integrating material from the appendix, including results comparing their algorithm with hierarchical and compositional reinforcement learning approaches, and addressing limitations related to LTL properties and planning dynamics.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant problem in the field, focusing on how to learn policies that generalize to complex task specifications with minimal additional training.
- The use of a Buchi automaton and weighted graph search to solve high-level plans for LTL tasks is technically interesting and novel.
- Experimental results indicate that the proposed approach outperforms existing methods and generalizes well to out-of-distribution tasks.
- The authors plan to include detailed pseudocode for their task-planning algorithm, enhancing clarity.
- Experimental results will be integrated, providing a robust comparison with existing methods.
- The authors acknowledge and plan to address limitations, demonstrating a commitment to transparency and improvement.

Weaknesses:
- The experimental setup lacks appropriate baselines that also assume given low-level policies, which undermines the evaluation of the proposed method.
- Clarity in writing could be improved; a brief introduction of the method and a summary of experimental results in the introduction would enhance understanding.
- The paper does not adequately discuss limitations, particularly regarding scenarios where tasks cannot be divided into high-level LTL solving and low-level goal achieving.
- The current version lacks integration of appendix material into the main text, which is crucial for clarity and completeness.
- There is a need for a clearer explanation of the algorithm's operation and its implications for the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by including baselines that operate under the same assumptions regarding low-level policies. Additionally, enhancing the clarity of the paper through a more structured introduction and summarizing results would benefit readers. It is also crucial to discuss the limitations of the approach, particularly in cases where task specifications may be unsatisfiable or context-dependent. We suggest that the authors improve the integration of appendix material into the main body to ensure clarity and coherence, providing a concrete set of edits planned for this integration. Furthermore, we recommend that the authors include a high-level explanation of their method and extend the related work section to better contextualize their contributions.