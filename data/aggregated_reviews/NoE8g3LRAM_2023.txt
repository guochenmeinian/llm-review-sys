ID: NoE8g3LRAM
Title: Benchmarking Encoder-Decoder Architectures for Biplanar X-ray to 3D Bone Shape Reconstruction
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 6, 6, 8, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for evaluating 3D bone shape reconstruction models from biplanar X-ray images, systematically categorizing existing architectures and conducting comprehensive experiments. The authors propose a platform that includes curated data, models, and scripts, facilitating the evaluation of various encoder-decoder architectures against both segmentation-based metrics and clinically relevant criteria. The framework emphasizes the importance of clinical metrics alongside traditional image-based metrics like Dice Similarity Coefficient (DSC), advocating that improvements in DSC do not necessarily correlate with better clinical outcomes. The authors also address reproducibility issues in existing literature by providing open-source reference implementations and curated datasets. Additionally, they discuss the challenges faced with the Rib dataset, attributing poor results to factors such as background/foreground ratio and the unique topology of ribs.

### Strengths and Weaknesses
Strengths:  
- The proposed benchmark has significant potential for advancing 3D bone shape reconstruction models.  
- The systematic categorization and thorough comparison of architectures enhance understanding of the current state-of-the-art.  
- The open-source platform provides a complete package of data, scripts, and models, which is valuable for the research community.  
- The focus on clinically relevant metrics is a notable contribution.  
- The authors provide a clear and detailed explanation of hyperparameter tuning, enhancing reproducibility.  
- The discussion on the Rib dataset's challenges demonstrates a thoughtful analysis of the results.  
- The availability of source code promotes transparency and accessibility.

Weaknesses:  
- Inconsistent performance of the OneDConcat model between figures raises concerns.  
- The writing contains grammatical errors and missing details, such as the contributions section.  
- The novelty of the work is perceived as weak, and the focus on encoder-decoder architectures is narrow.  
- The resolution adaptation of datasets may hinder performance in certain applications.  
- The initial explanation of the relationship between DSC and clinical metrics was unclear, leading to potential misunderstandings.  
- The reliance on existing clinical parameter estimation methods, which were validated on limited datasets, may affect the robustness of the findings.  
- The reasons for poor results on the Rib dataset remain speculative and lack comprehensive investigation.  
- The attempt to improve results through higher resolution images did not yield positive outcomes, indicating a need for further exploration.  
- Some figures were criticized for clarity and relevance, indicating a need for better visual representation.

### Suggestions for Improvement
We recommend that the authors investigate the reasons behind the inconsistent performances of the OneDConcat model between Fig. 5 and Fig. 7. Additionally, presenting average scores for each model in Table 1 would enhance the overview. To improve clarity, we suggest resizing the dots in Fig. 4 and Fig. 7 for better visual comparison. The authors should also provide more details on defining configuration files and running scripts, as well as a more comprehensive discussion of the clinical implications and limitations of the benchmarking framework. Furthermore, we advise addressing the ambiguous aspects of Figures 5 and 6 to clarify the relationship between Dice scores and clinical metrics. We recommend improving the clarity of the relationship between DSC and clinical metrics in the revised manuscript, ensuring that the nuances of their findings are well articulated. Additionally, consider expanding the discussion on the limitations of the clinical parameter estimation methods used, particularly regarding their validation on diverse datasets. It would also be beneficial to conduct further experiments to evaluate the model's performance under misalignment scenarios, as suggested by reviewers. Finally, we recommend that the authors improve the investigation into the poor results on the Rib dataset by conducting more targeted experiments to identify specific factors contributing to the challenges, and exploring alternative methodologies or preprocessing techniques that could enhance the quality of rib segmentation.