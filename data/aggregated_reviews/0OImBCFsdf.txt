ID: 0OImBCFsdf
Title: SaVeNet: A Scalable Vector Network for Enhanced Molecular Representation Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 6, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SaVeNet, an SE(3)-equivariant model designed for geometric representation learning of molecules. The authors propose a framework that integrates directional noise and vector activation functions, enhancing both efficiency and expressiveness. The model addresses the dual challenges of scalability and effectiveness, with directional noise serving as a transient training mechanism that improves model robustness without compromising SE(3)-equivariance during inference. The performance of SaVeNet is validated through experiments on multiple datasets, including ablation studies that demonstrate the significant impact of proposed modules on scalability, particularly in the context of the Molecule3D dataset.

### Strengths and Weaknesses
Strengths:
- SaVeNet achieves outstanding performance across various synthetic and real-world datasets.
- The proposed scalar-based message passing scheme shows better efficiency and performance compared to existing baselines.
- The empirical analysis on efficiency is thorough, covering time for forward/backward passes, FLOPs, MACs, and memory consumption.
- The response clarifies the motivation behind direction noise and its role in maintaining SE(3)-equivariance.
- The ablation studies provide empirical evidence of the importance of specific modules in enhancing scalability and performance.

Weaknesses:
- The presentation of the method is unclear, with several symbols and equations poorly defined, such as the notation in Eq. 4 and Eq. 5. The authors should clarify the substitution of the multiplication symbol and consider adding a figure to illustrate the message passing process.
- Claims regarding numerical stability and convergence speed due to vector initialization lack theoretical analysis and empirical evidence.
- The core differences between SaVeNet and previous networks like PaiNN are not sufficiently highlighted, and the overall novelty of SaVeNet is limited, as many techniques have been previously studied.
- There remains some ambiguity regarding the symmetrical properties of the direction noise module, particularly its behavior during inference.
- The feasibility of conducting comprehensive scaling-up experiments for different SaVeNet variants is questioned.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by defining all symbols and equations more rigorously, particularly in the methodology section. Additionally, the authors should provide theoretical analysis or empirical evidence to support claims about numerical stability and convergence speed. To enhance the understanding of SaVeNet's contributions, we suggest that the authors clearly delineate the differences between their model and existing approaches like PaiNN. Furthermore, we recommend improving clarity regarding the behavior of the direction noise module during inference to alleviate concerns about SE(3)-equivariance. Finally, conducting parallel scaling-up experiments for different SaVeNet variants while removing some novel operations would provide clearer insights into the model's scalability. Including a visual representation of the SaVeNet architecture would also aid in conveying the model's structure and components effectively.