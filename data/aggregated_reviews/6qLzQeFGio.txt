ID: 6qLzQeFGio
Title: Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 8, 7, 5, 2, -1, -1, -1
Original Confidences: 4, 5, 5, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive study of pre-trained visual encoders for Embodied AI, introducing a new pre-trained ViT-L encoder (VC-1) developed through Masked Auto-Encoding on a dataset that combines an expanded collection of egocentric video and ImageNet. The authors propose CortexBench, a benchmark consisting of 17 task types covering dexterous manipulation, locomotion, mobile manipulation, and navigation. The findings indicate that no visual encoder is universally superior, and simply increasing the size and diversity of the pre-training dataset does not guarantee consistent improvements across benchmarks. The paper includes experiments on real hardware, demonstrating VC-1's performance relative to existing Masked Visual Pre-training encoders.

### Strengths and Weaknesses
Strengths:
- The study provides a thorough examination of pre-trained visual encoders for various Embodied AI tasks.
- VC-1 serves as a high-performance reference model across all task types considered.
- The introduction of the CortexBench benchmark is significant and relevant.
- The paper is well-written and easy to follow.

Weaknesses:
- The lack of a universally high-performing pre-trained model without adaptation is acknowledged, but raises questions about the potential for future models with larger datasets.
- There are inconsistencies in reported results, such as the performance of TriFinger, which contradicts findings in Section 6.
- The omission of prompting/conditioning VC-1 as an adaptation method is noted, despite its potential as a cost-effective alternative.
- The paper does not explore the performance of R3M and CLIP alongside VC-1, limiting the comparative analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the results by addressing the discrepancies noted, particularly regarding TriFinger performance. Additionally, we suggest including a discussion on why prompting/conditioning VC-1 was not considered as an adaptation method, as this could provide valuable insights. It would also be beneficial to conduct experiments comparing VC-1 with R3M and CLIP to enhance the comparative framework of the study. Lastly, we encourage the authors to expand the analysis of the tasks in CortexBench to better understand their relationships and importance in achieving a robust visual cortex representation.