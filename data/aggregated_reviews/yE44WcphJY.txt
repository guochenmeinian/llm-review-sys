ID: yE44WcphJY
Title: Dissecting In-Context Learning of Translations in GPT-3
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration into the integration of Large Language Models (LLMs) such as GPT-3 for Machine Translation (MT), focusing on in-context learning and the role of demonstration attributes. The authors find that perturbing the source side has minimal impact, while perturbing the target side significantly reduces translation quality, leading to the hypothesis that the output text distribution is a primary learning signal during in-context translation learning. Additionally, the paper proposes a zero-shot-context method for translating with LLMs that does not require in-context examples, showing some benefits over vanilla zero-shot performance.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and supported by experiments that validate the hypothesis.
- It addresses an open question regarding the impact of prompts on translation performance.
- The analysis of robustness in LLMs when prompted to translate with in-context examples is valuable.

Weaknesses:
- The experimental setup has relevant issues, such as using development set samples for in-context examples without variance studies, which is critical for few-shot examples.
- The evidence presented is predominantly for "en-xx; out-of-English" directions, neglecting the different characteristics of LLMs when translating into English.
- The choice of a quality estimation system over a neural metric for evaluation is questionable, especially given the paper's focus.
- The analysis of perturbation impact is limited to quality metrics, lacking deeper insights like error analysis.
- Section 4 lacks clarity regarding the "context generation" step and its utility compared to providing in-context examples.

### Suggestions for Improvement
We recommend that the authors improve the experimental setup by conducting variance studies with multiple sets of examples. Additionally, consider replicating results with open large language models to enhance reproducibility. We suggest providing qualitative analyses of translations under different perturbations to offer deeper insights beyond quality metrics. Clarifying the rationale behind the context generation process in Section 4 and providing examples of generated context would also strengthen the paper. Finally, addressing the concerns raised by reviewers regarding the clarity of findings and evaluation metrics will enhance the overall coherence and impact of the work.