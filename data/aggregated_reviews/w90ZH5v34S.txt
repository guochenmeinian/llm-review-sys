ID: w90ZH5v34S
Title: Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 7, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a large-scale dataset and benchmarks aimed at enhancing AI's ability to generate humorous captions, comprising over 250 million human ratings on 2.2 million captions from The New Yorkerâ€™s cartoon caption contest. The dataset captures human preferences for cartoon captions, enabling fine-tuning of Vision Language Models (VLMs) and Language Models (LLMs) to generate humorous captions and better estimate human humor preferences. The authors analyze state-of-the-art models, such as GPT-4 and Claude, revealing their limitations in humor generation compared to human contestants and experts. The study also examines various alignment strategies, including Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), to better align AI outputs with human preferences. The authors propose a novel ranking evaluation that integrates human judgments with LLM results, emphasizing that while LLMs outperform laypersons in humor understanding, they still lag behind experts. The findings reveal that current fine-tuning methods do not adequately address the challenge of generating humorous captions.

### Strengths and Weaknesses
Strengths:
- The introduction of a unique, extensive dataset provides a valuable resource for studying humor in AI, significantly larger than existing datasets.
- The data was voluntarily generated over eight years, likely resulting in higher quality than datasets created by laypersons under forced conditions.
- The paper offers a comprehensive analysis of state-of-the-art models and alignment methods, supported by empirical results.
- It establishes new benchmarks for evaluating AI-generated humor, contributing significantly to the field of creative AI.
- The open-source availability of the dataset and code encourages further research and collaboration.

Weaknesses:
- The writing style is overly complex and LLM-heavy, which detracts from clarity and precision.
- There are concerns regarding the fairness of comparisons between language-only models and multimodal models due to potential biases.
- The paper lacks definitional clarity on key concepts such as "alignment" and "humor," leading to vague and general statements.
- The small sample size of human evaluators raises questions about the reliability of the findings.
- The dataset contains only 365 unique images, limiting diversity in humor styles and topics.
- The novelty of the dataset is somewhat limited, as the format of cartoons, captions, and ratings is not new to the community, despite its larger scale.

### Suggestions for Improvement
We recommend that the authors improve the clarity and precision of the writing by simplifying overly complex language and avoiding vague claims. It is essential to provide clear definitions of key terms such as "alignment" and "humor" as operationalized in the study. Additionally, we suggest addressing the fairness of model comparisons by clarifying the conditions under which evaluations are conducted. The authors should also consider expanding the evaluation to include other creative tasks and humor genres to enhance the generalizability of their findings. We recommend improving the depth of the analysis regarding VLMs, specifically exploring the conditions under which LLMs outperform VLMs, and investigating whether this is influenced by the visual complexity of images or specific characteristics of captions. Clarifying the exact number of contests in the dataset early in the paper would enhance clarity. Finally, we encourage a careful discussion of the psychological and sociological basis for defining "funniness" at a population level, including potential consequences of training models to align with a "mean" human perspective.