ID: 1wxFznQWhp
Title: Delving into the Reversal Curse: How Far Can Large Language Models Generalize?
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the reversal curse, where LLMs trained on "A is B" fail to generalize to "B is A." The authors extend this analysis to "Name is Description" and "Description is Name" formats, focusing on two tasks: answer completion and multiple-choice answering. Key findings include: 
1) In multiple-choice settings, models can effectively reverse information from "Name is Description" to answer questions about "Description."
2) Conversely, models trained on "Description is Name" do not outperform random guessing, indicating a "thinking bias."
3) Analysis of chain-of-thought reasoning and saliency maps reveals models favor names over descriptions, explaining their poor performance on description-based questions.
4) Further training and synthetic augmentation do not alleviate these issues.

### Strengths and Weaknesses
Strengths:
- The paper provides conceptual clarity and extends the understanding of the reversal curse, revealing additional biases in LLMs.
- Figure 1 effectively illustrates biases, and experiments are conducted across various open-weight models (Llama, Vicuna, Mistral).
- The book-story dataset results in the appendix are compelling and should be highlighted earlier in the paper.

Weaknesses:
- The results raise questions about whether observed biases are artifacts of model size, as similar models were used in prior reversal curse studies. Without a scaling analysis, the claims may lack robustness.
- The presentation of results could be clearer, particularly in distinguishing the core contributions from the reversal curse, as the main findings relate more to multiple-choice QA and thinking bias.
- Some figures, such as Figure 3, are confusing and lack clarity in their color coding.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by explicitly stating in the abstract and introduction that training on "Name is Description" is effective, while "Description is Name" consistently fails. Additionally, we suggest including a scaling plot or analysis to address whether biases are model size-dependent. The authors should also clarify the novelty of their contributions in the introduction and ensure that limitations and future work are discussed in the main text rather than the appendix. Finally, enhancing the clarity of figures, particularly Figure 5, and addressing the confusion surrounding the saliency scores would strengthen the presentation.