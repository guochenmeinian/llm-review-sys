ID: 5c1hh8AeHv
Title: MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 8, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MultiTrust, a comprehensive benchmark for evaluating the trustworthiness of Multimodal Large Language Models (MLLMs) across five key aspects: truthfulness, safety, robustness, fairness, and privacy. The authors evaluate 21 representative MLLMs using 32 diverse tasks, revealing significant insights into model performance, including vulnerabilities to multimodal jailbreaking and biases. The code and datasets have been open-sourced, enhancing community access.

### Strengths and Weaknesses
Strengths:
- The topic of trustworthiness evaluation for MLLMs is timely and meaningful.
- The paper is well-written, with clear presentation and organization.
- The experimental results are comprehensive and provide valuable insights.
- The proposed benchmark is novel and considers various aspects of trustworthiness.

Weaknesses:
- Key aspects such as truthfulness, safety, robustness, fairness, and privacy lack precise definitions and appear scattered.
- The relationship between input images and text in safety and privacy evaluations is unclear.
- Some figures, particularly Figure 2, require better labeling and clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of key concepts by providing precise definitions of truthfulness, safety, robustness, fairness, and privacy in a dedicated section. Additionally, the authors should enhance the discussion linking diverse task design to trustworthiness aspects, justifying their relevance as benchmarks. A comprehensive description of the benchmarks and a systematic discussion of metrics, distinguishing between subjective and objective metrics, would also strengthen the paper. Finally, addressing the labeling issues in Figure 2 and considering alternative models for data generation would improve the evaluation's fairness.