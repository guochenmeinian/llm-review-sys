ID: J8rOw29df2
Title: On the Stability and Generalization of Meta-Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 4, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a stability analysis for meta-learning, introducing the concept of uniform meta-stability, which accounts for changes in both tasks and examples at test time. The authors provide high-probability bounds of the order of $\beta + O(1/\sqrt{mn})$, where $\beta$ is the stability parameter, $m$ is the number of tasks, and $n$ is the number of examples per task. The analysis includes a prox meta-learning algorithm with two task-specific algorithms, establishing stability bounds across various problem settings, including convex, Lipschitz, and weakly-convex problems. Applications to stochastic optimization and robust adversarial proximal meta-learning are also discussed.

### Strengths and Weaknesses
Strengths:
- The introduction of a new stability probability measure for meta-learning is innovative, providing both high-probability and expectation bounds for algorithms meeting uniform meta-stability.
- The paper establishes stability bounds for two algorithms under different problem settings, demonstrating improvements over existing bounds by incorporating $1/\sqrt{mn}$, which highlights the advantages of considering multiple tasks simultaneously.
- The writing is clear, with main contributions well-articulated.

Weaknesses:
- Theorem 4.4 presents risk bounds that may not be meaningful if $\lambda$ is small, as suggested by the bounds in Theorem 4.7. The choice of $\lambda=O(1/\sqrt{n})$ leads to excess risk bounds that diminish the role of the term $\sqrt{1/(mn)}$ in generalization.
- The proof of Theorem B.3 is questioned for its rigor, as the identity used appears incorrect.
- The assumptions regarding compactness of $\mathcal{W}$ and the requirement for $\lambda$ to be large in certain theorems are seen as overly restrictive.

### Suggestions for Improvement
We recommend that the authors improve the rigor of the proof for Theorem B.3 by verifying the correctness of the identity used. Additionally, the authors should consider relaxing the assumptions on $\lambda$ to allow for more practical choices, as the current requirements may limit applicability. A clearer comparison with existing literature, particularly regarding the advantages and disadvantages of their approach relative to works such as those by Denevi et al. and Maurer, would enhance the paper's contribution. Lastly, addressing the implications of the new bounds in a more detailed manner would provide valuable insights into their significance in the context of meta-learning.