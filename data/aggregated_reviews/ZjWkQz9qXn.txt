ID: ZjWkQz9qXn
Title: Query-based Image Captioning from Multi-context 360Â° Images
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new task, Query-based Image Captioning for 360-degree images (QuIC-360), where the model generates captions based on a panoramic image and a specific query. The authors introduce a novel dataset comprising 3,940 360-degree images and 18,459 query-caption pairs. The paper demonstrates that fine-tuning existing models on this dataset can enhance caption diversity and relevance to the queries.

### Strengths and Weaknesses
Strengths:  
- The introduction of the QuIC dataset enhances the field of panoramic image captioning and provides a potential benchmark for future research.  
- Comprehensive testing and evaluation of the dataset are conducted, including human evaluations, which add credibility to the findings.  
- The analysis of model performance, particularly with fine-tuning, is high-quality and valuable for the community.

Weaknesses:  
- The task may not be sufficiently distinct from existing controllable image captioning methods, as the unique aspects of 360-degree images are not clearly articulated.  
- The experimentation is limited to a small number of models, and the dataset lacks scenarios where the query object is absent, which would reflect real-world conditions.  
- The impact of pre-training on this dataset for other image captioning tasks remains unexplored.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the unique benefits of using 360-degree images compared to standard images. Additionally, expanding the experimentation to include a more diverse set of models would strengthen the findings. Incorporating mismatched cases in the dataset, such as when query objects are absent, would enhance its applicability. Furthermore, we suggest providing explicit definitions for terms like "(image) context" and clarifying the relationship between the proposed model and the specific challenges of 360-degree images. Lastly, including examples to illustrate claims about performance improvements would make the arguments more convincing.