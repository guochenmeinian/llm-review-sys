ID: SnFmGmKTn1
Title: KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for knowledge graph completion (KGC) that integrates a large language model (LLM) with a triple-based retriever, specifically focusing on link prediction tasks. The authors propose KICGPT, which employs in-context learning through a method termed Knowledge Prompt, demonstrating state-of-the-art performance on benchmarks like FB15k-237 and WN18RR. The approach includes a multi-prompt interaction strategy with ChatGPT and a prompt-engineering methodology for generating fluent textual descriptions.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, clearly articulating its ideas and experimental design.
- It introduces an innovative method for coupling triple-based approaches with LLMs, showing promising results.
- The integration of Analogy Pool and Supplement Pool for entity reordering is a notable contribution.

Weaknesses:
- The manuscript lacks sufficient justification for the choice of RotatE as the retriever and does not adequately address the long-tail problem in KGC.
- Details regarding the experimental setup and methodology are insufficient, particularly concerning the handling of unexpected outputs from ChatGPT and the specifics of the Text Self-Alignment process.
- The improvements over baseline methods appear modest, raising questions about the effectiveness of the proposed prompt engineering.

### Suggestions for Improvement
We recommend that the authors improve the justification for using RotatE as the retriever and provide a comparative analysis with additional KG embedding models. Additionally, the authors should clarify the handling of unexpected outputs from ChatGPT during multi-round prompting and elaborate on the Text Self-Alignment process to enhance reproducibility. Lastly, we suggest that the authors consider validating their approach on few-shot KGC tasks to strengthen their claims regarding the long-tail problem.