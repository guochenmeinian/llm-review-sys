ID: G7QS68ICPJ
Title: Nimbus: Secure and Efficient Two-Party Inference for Transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 5, 6, 5, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents Nimbus, a secure two-party computation (2PC) protocol for Transformer models aimed at enhancing the efficiency of large matrix multiplication and non-linear layer approximation during inference. The authors propose client-side outer product and output compact techniques to reduce computational and communication overhead. They also consider input distributions to improve the approximation of GELU and softmax functions using lower-order piecewise polynomials. Comprehensive experiments demonstrate that Nimbus significantly outperforms existing protocols like BumbleBee, Iron, and BOLT in terms of performance and efficiency.

### Strengths and Weaknesses
Strengths:
- Nimbus introduces client-side outer product (COP) and output compact techniques, effectively reducing computing and communication overhead.
- The protocol accounts for input distribution, simplifying polynomial approximations for GELU and softmax.
- The authors provide thorough discussions on Nimbus's efficiency, feasibility, and complexity analyses, making the results convincing.
- Extensive evaluations validate that Nimbus enhances secure 2PC performance for Transformer models.

Weaknesses:
- The accuracy and feasibility of estimating input distribution through sampling are major concerns, with insufficient details on the sampling process, such as batch size and whether all training data or just a subset is used.
- Nimbus appears to assume similar distributions for training and test data, raising questions about performance with significantly skewed distributions.
- The piecewise polynomial approximations are empirically defined, which may lead to inaccuracies if input distributions vary.
- The nonlinear approximation may leak private information, and the system could impose significant storage overhead on the client side.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the sampling process for estimating input distributions, including details on batch size and whether all training data is utilized. Additionally, the authors should address the assumption that training and test data distributions are similar and explore the implications of differing distributions. We suggest providing a security proof for the Client-side Outer Product Protocol and clarifying the rationale for using HE + Add-SS for 2PC instead of direct multi-party computing. Lastly, we encourage the authors to enhance the discussion on function privacy and the potential storage overhead on the client side.