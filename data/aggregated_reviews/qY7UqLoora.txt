ID: qY7UqLoora
Title: Are GATs Out of Balance?
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 6, 6, 7, 7, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the initialization of Graph Attention Networks (GATs) and its impact on performance, particularly regarding the performance versus depth aspect. The authors derive a conservation law for GATs with positive homogeneous activation functions, providing theoretical support for a parameter norm balancedness-based initialization method. Extensive experiments demonstrate the effectiveness and fast convergence of this approach, showing that deep GATs initialized with the proposed method outperform those initialized with standard techniques.

### Strengths and Weaknesses
Strengths:
- The problem of training GATs, especially with many layers, is well identified and addressed.
- The proposed theories are solid and effectively support the problem.
- The initialization method is simple yet effective, improving accuracy and speeding convergence.
- The paper is well-written in terms of technical details and presents compelling experimental results.

Weaknesses:
- Some explanations lack detail, potentially causing confusion, particularly in equations presented.
- The organization could be improved for better readability, with more paragraphs or subsections.
- The motivation for the balanced initialization and its specific benefits for deep networks is not clearly articulated.
- The paper lacks comparisons with other methods and relevant citations related to graph neural networks as gradient flow.

### Suggestions for Improvement
We recommend that the authors improve the clarity of explanations, particularly in equations on lines 140-142, and enhance the organization of the paper for better readability. It would be beneficial to include comparisons with other methods and address the limitations of the proposed approach, particularly regarding its generalization beyond GATs. Additionally, we suggest that the authors clarify the motivation for the balanced initialization and its specific advantages for deep networks, as well as explore the applicability of the proposed initialization to other types of message-passing neural networks (MPNNs).