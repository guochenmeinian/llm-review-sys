ID: sKdsBUAnts
Title: Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach for constructing persona-consistent dialogue agents using offline reinforcement learning (RL). The authors propose the use of offline RL to train models without generating additional data and to apply rewards and punishments to specific utterances. They introduce the "VaRMI Importance Sampling" technique to reduce variance in importance sampling, which is crucial due to the high variance often associated with offline RL. The method is evaluated by fine-tuning BlenderBot3 (BB3) using the Dialogue Natural Language Inference (DNLI) dataset and PersonaChat, demonstrating performance improvements across various metrics, although some results indicate that GOLD outperforms VaRMI in persona consistency.

### Strengths and Weaknesses
Strengths:
- The paper addresses the important topic of persona consistency in dialogue agents using offline RL, which is a commendable approach.
- The authors provide code for reproducibility, enhancing the study's applicability.
- The proposed method shows promising results, outperforming BB3 in several metrics.

Weaknesses:
- The paper lacks a clear and coherent structure, making it difficult to follow.
- The motivation appears contradictory, as it addresses high training costs of online RL while requiring manually annotated dialogues.
- The necessity of the new importance sampling method is not well justified, and baseline comparisons are insufficient.
- Concerns about the assumptions related to importance sampling and the validity of the proposed method are raised.

### Suggestions for Improvement
We recommend that the authors improve the paper's structure to enhance clarity and understanding. Additionally, the authors should clarify the motivation behind their approach, particularly regarding the reliance on manually annotated dialogues. It is essential to justify the necessity of the proposed importance sampling method and to include appropriate baseline comparisons with other methods addressing similar issues. We also suggest addressing the concerns regarding the assumptions of importance sampling and providing empirical validation for these assumptions. Lastly, the authors should consider including more detailed explanations of the offline RL framework and the calculations involved in their experiments.