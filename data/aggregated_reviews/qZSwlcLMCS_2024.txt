ID: qZSwlcLMCS
Title: Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Kaleido Diffusion, which enhances the diversity of text-to-image generation by utilizing autoregressive latent priors. The authors propose that conventional diffusion models (DM) often restrict diversity due to high classifier-free guidance (CFG) settings. By first modeling latent tokens with an autoregressive approach, the method captures a broader distribution of modes, allowing for more varied outputs while maintaining quality. Experimental results support the effectiveness of this approach in improving image diversity.

### Strengths and Weaknesses
Strengths:
1. The approach of generating abstractions to expand condition diversity before proceeding with generation is reasonable and enhances interpretability and customization.
2. The writing is clear, and the qualitative results appear promising.
3. The incorporation of autoregressive modeling to construct fine-grained priors is a novel contribution.

Weaknesses:
1. The formulation in Section 3.1 is somewhat unintuitive; the authors could clarify why their method deviates from the original CFG formulation.
2. Quantitative results are limited, with only MDM as a baseline; more comparisons with state-of-the-art diffusion models are needed.
3. The evaluation lacks numerical data, and the context extractor (MLLM) used in experiments is not clearly defined.
4. The choice of autoregressive modeling as the first-stage distribution learner and the specific types of latent tokens appears ad-hoc, raising questions about scalability and complexity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the formulation in Section 3.1 by justifying the deviation from the original CFG formulation. Additionally, the authors should include more quantitative results and comparisons with other state-of-the-art models to strengthen their claims. Clarifying the context extractor used in experiments and providing more implementation details would enhance reproducibility. Finally, discussing the implications of using autoregressive modeling and the choice of latent tokens in terms of scalability would address concerns regarding the complexity of the pipeline.