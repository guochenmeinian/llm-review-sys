ID: q4oWkMHkQx
Title: Task-Level Thinking Steps Help Large Language Models for Challenging Classification Task
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to classification tasks by introducing task-level thinking steps and a progressive revision framework aimed at enhancing large language models (LLMs). The authors propose that these task-specific thinking steps reduce bias from demonstrations and clarify reasoning processes. The framework involves a teacher LLM and a student LLM that iteratively refine thinking steps based on feedback from challenging examples. The effectiveness of the proposed Progressively Revised Thinking Steps (PRTS) method is demonstrated across various classification tasks, showing superior performance compared to existing baselines in zero-shot, few-shot, and few-shot-CoT settings.

### Strengths and Weaknesses
Strengths:
- The proposed method outperforms traditional approaches and demonstrates significant bias reduction in classification tasks.
- The progressive revision framework enhances LLM capabilities through iterative feedback, leading to improved chain-of-thought explanations.
- Comprehensive experiments across diverse classification tasks provide robust evidence of the method's effectiveness.

Weaknesses:
- The paper lacks comparisons with key baselines such as Madaan et al. (2023) and other relevant prompt engineering techniques, which limits the strength of the claims.
- Concerns regarding the iterative framework's high token costs and the potential difficulty in finding challenging demonstrations in small training sets are not adequately addressed.
- The reproducibility of results is hindered by the absence of code and insufficient methodological details.

### Suggestions for Improvement
We recommend that the authors improve the comparison with Madaan et al. (2023) to validate the effectiveness of hard demonstrations. Additionally, clarify how the task-level thinking steps are preserved during the iterative process to prevent overwriting. To enhance reproducibility, please include standard deviations for results and provide code for others to replicate the study. We also suggest discussing the implications of prompt template generalizability and conducting a cost analysis of the iterative framework, particularly in scenarios with limited training data.