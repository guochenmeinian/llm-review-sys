ID: 5DwqmoCE1N
Title: On the Surprising Effectiveness of Attention Transfer for Vision Transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 6, 3, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to fine-tuning Vision Transformers (ViTs) by introducing Attention Transfer, which emphasizes the significance of pre-trained attention maps over pre-trained features. The authors propose two methods: Attention Copy, which directly transfers attention maps from a teacher network, and Attention Distillation, which allows the student network to learn its own attention maps through a distillation objective. The results demonstrate that these methods can achieve performance comparable to traditional fine-tuning, particularly on ImageNet-1k, while also providing insights into the role of attention maps in model performance.

### Strengths and Weaknesses
Strengths:
- The work highlights the critical role of pre-trained attention maps in ViT fine-tuning, offering important implications for the field.
- The potential for using Attention Transfer with ensembles is intriguing and may enhance self-supervised ViT performance.
- The paper is well-structured, with comprehensive analyses across various configurations and a clear presentation of results.

Weaknesses:
- Attention Transfer does not consistently match the performance of standard fine-tuning, especially in cases with domain gaps, and is more computationally expensive.
- The analysis lacks a direct comparison of Attention Distillation versus Attention Copy in scenarios with domain gaps.
- Some visualizations are difficult to interpret due to color choices and background images, which could hinder clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of visualizations in Figures 7, 10, and 11 to enhance interpretability. Additionally, it would be beneficial to include a baseline comparison of distillation on features to provide a more comprehensive understanding of the proposed methods. We suggest that the authors explore the performance of Attention Distillation in domain gap scenarios, as it may yield better results than Attention Copy. Furthermore, we encourage the authors to address the implications of transferring Q features more thoroughly, as this could provide deeper insights into the relationship between attention transfer and model performance.