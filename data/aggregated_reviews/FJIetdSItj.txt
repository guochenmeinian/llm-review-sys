ID: FJIetdSItj
Title: LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 5, 7, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LV-Eval, a benchmark designed to evaluate the long-context understanding capabilities of large language models (LLMs), featuring five distinct length levels from 16k to 256k words. The authors propose three key techniques: Confusing Facts Insertion (CFI), Keyword and Phrase Replacement (KPR), and a keyword-recall-based metric. A comprehensive evaluation of 12 LLMs, including commercial and open-source models, is conducted, alongside ablation studies to assess the impact of the benchmarking techniques. The paper also introduces a bilingual Long-Context QA Benchmark in English and Chinese, addressing knowledge leakage and enhancing evaluation precision.

### Strengths and Weaknesses
Strengths:
1. The benchmark effectively mitigates knowledge leakage and introduces a more objective evaluation metric.
2. The framework for constructing the benchmark is clearly articulated, serving as a guide for future developments.
3. The evaluation encompasses a wide range of LLMs, providing a comprehensive overview of the state-of-the-art.
4. The paper is well-structured and clearly presented, enhancing accessibility.

Weaknesses:
1. The benchmark is limited to QA tasks, excluding other NLP tasks like summarization.
2. A detailed error analysis is lacking, which could elucidate specific challenges faced by LLMs.
3. There is no clear measure of alignment between the keyword-recall-based metric and human evaluation, potentially affecting result objectivity.
4. The scalability of the CFI technique may be constrained due to reliance on human annotators.

### Suggestions for Improvement
We recommend that the authors improve the benchmark's applicability by including other NLP tasks, such as summarization. A detailed error analysis should be conducted to provide insights into LLM performance challenges. Additionally, the authors should clarify the alignment between the keyword-recall-based metric and human evaluation to strengthen the validity of their results. Finally, we suggest exploring the inclusion of larger parameter open-source LLMs in future experiments to enhance the comprehensiveness of the evaluation.