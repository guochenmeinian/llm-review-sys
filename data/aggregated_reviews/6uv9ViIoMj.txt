ID: 6uv9ViIoMj
Title: Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 5, 5, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel layer-wise post-training quantization (PTQ) algorithm called aespa, aimed at quantizing large transformer models while balancing accuracy and efficiency. The authors propose refined quantization objectives for the attention layer, which accelerates the quantization process through pre-computation. Extensive experiments demonstrate that aespa outperforms existing PTQ methods, particularly in low-bit quantization scenarios.

### Strengths and Weaknesses
Strengths:
1. The mathematical foundation of aespa allows for easy application to other transformer models.
2. The algorithm effectively addresses cross-layer dependencies by quantizing query, key, and value separately within attention modules, reducing computational load.
3. Comprehensive experiments on various language models validate aespa's performance, showing significant speed advantages over traditional methods.

Weaknesses:
1. The focus on the attention module limits the consideration of cross-layer dependencies across the entire model.
2. The experimental results lack explicit comparisons of accuracy against original floating-point models, with only one table addressing zero-shot performance.
3. Some results, particularly for baseline comparisons, appear inconsistent with prior literature, warranting further discussion.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the trade-off between accuracy and efficiency in the introduction. Additionally, we suggest conducting further experiments on LLMs, particularly with the LLaMa 2 & 3 families, and providing more detailed implementation specifics, such as the number of iterations required for updating Q, K, and V in the attention module. It would also be beneficial to include ablation studies on group sizes and to organize the experimental results more effectively, ensuring that key findings are presented in the main text rather than solely in the appendix. Finally, we encourage the authors to discuss the accuracy of aespa compared to other methods in greater detail.