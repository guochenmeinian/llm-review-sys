ID: lpXDZKiAnt
Title: Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning Attack
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Vaccine, a technique designed to enhance the security of Large Language Models (LLMs) by implementing perturbation-aware alignment during fine-tuning. Vaccine addresses the "alignment destruction effect," where even minimal harmful data can disrupt model alignment. The method effectively maintains alignment robustness against harmful prompts while preserving reasoning capabilities on benign prompts. Empirical results demonstrate that Vaccine significantly reduces harmful output probability (up to 9.8%) across various models and tasks.

### Strengths and Weaknesses
Strengths:  
- The authors provide empirical evidence showing that minimal harmful data can significantly impact LLM embeddings, leading to misalignment.
- Vaccine has been validated on popular LLMs, demonstrating substantial improvements in resilience against malicious prompts while maintaining performance on non-malicious prompts.
- The methodology is clearly articulated, and the approach is simple and generalizable, allowing for plug-and-play application across different scenarios.

Weaknesses:  
- The computational overhead of Vaccine scales linearly with model size, potentially doubling training time, which may be prohibitive for users in a "finetuning as a service" context.
- The method's performance diminishes when the harmful data ratio is high, and the selection of the optimal noise intensity (ρ) is not straightforward.
- The paper lacks a discussion on the model's generative ability post-application of the Vaccine method and does not compare with a broader range of baselines.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational overhead by comparing runtime and memory usage against traditional fine-tuning and other baseline methods. Additionally, further guidelines on selecting the hyper-parameter ρ would enhance practical applicability. We encourage the authors to explore related acceleration methods to mitigate the increased training time and to provide a more intuitive visualization of the embedding drift phenomenon, such as t-SNE. Lastly, addressing the implications of using a black box moderation model for harmful score calculations would strengthen the paper.