ID: 9K1urVN7ti
Title: DueT: Image-Text Contrastive Transfer Learning with Dual-adapter Tuning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DueT, an adapter-based method for aligning pre-trained vision and text encoders (ViT and BERT) by incorporating gated Adapter units within the frozen encoders and training them using a contrastive loss. The authors demonstrate that DueT achieves superior performance and parameter efficiency in image-text retrieval tasks across both English and Japanese datasets. The methodology is well-structured, detailing the design choices and conducting extensive ablation studies to validate the effectiveness of the proposed approach.

### Strengths and Weaknesses
Strengths:
- The proposed method is simple, effective, and technically valid, enhancing the image-text contrast learning framework.
- The paper is well-written and easy to understand, with a clear description of the network design and architecture of the gated adapter unit.
- Comprehensive experiments are conducted on both English and Japanese datasets, showcasing the method's applicability to non-English languages.
- The ablation studies are well-motivated, addressing various aspects such as the size of the bottleneck layer and the utility of GAUs.

Weaknesses:
- The introduction of GAUs increases training parameters significantly, which may raise concerns about efficiency.
- Some reviewers noted a lack of originality in using pre-trained encoders for contrastive learning and suggested that testing on a limited number of languages may not sufficiently support the claims.

### Suggestions for Improvement
We recommend that the authors improve the reproducibility of their results by providing more detailed parameter settings and clarifying the hashing algorithm used. Additionally, the authors should explain the choice of m=1536 in the experimental setup, as its performance relative to lower values is unclear. Finally, releasing the code and model would facilitate future research and enhance the transparency of the findings.