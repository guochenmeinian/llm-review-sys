ID: 73XPopmbXH
Title: Smoothing the Landscape Boosts the Signal for SGD: Optimal Sample Complexity for Learning Single Index Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 8, 8, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a modified learning algorithm that bridges the gap between the established lower bound on learning a single index model with Gaussian inputs and the sample complexity of stochastic gradient descent (SGD). The authors demonstrate that smoothing the gradients allows for larger step sizes, improving sample complexity, and they connect their findings to tensor decomposition. The analysis reveals that the convergence to error Îµ occurs in \(d^{k^*/2} + d/\epsilon\) iterations, enhancing understanding of SGD's dynamics.

### Strengths and Weaknesses
Strengths:  
- The alteration to SGD is both simple and clever, with clear arguments and a well-structured proof sketch that elucidates the impact of smoothing on the drift term.  
- The paper is well-written, effectively explaining the technical contributions and the connection to related work, particularly Tensor PCA.  
- It achieves a significant result by meeting the CSQ lower bound, enhancing the understanding of SGD's sample complexity.

Weaknesses:  
- The experiments appear to utilize an analytic formulation of the smoothed loss, which may not reflect practical scenarios, raising computational concerns.  
- There are several areas needing clarification, such as the ease of computing smoothed gradients, the rigor of the CSQ lower bound, and the implications of using correlation loss over square loss.  
- The discussion on the analysis of \(E[|v|^2]\) lacks depth, and the paper does not adequately address the limitations of using correlation loss for learning single index functions.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the computation of smoothed gradients, specifying whether closed-form formulas exist or if sampling is required. Additionally, the authors should elaborate on how SGD qualifies as a CSQ algorithm and clarify the rigor of the claims regarding optimality. It would be beneficial to include more intuition or examples regarding the analysis of \(E[|v|^2]\) and to discuss the limitations of using correlation loss in practical applications. Finally, addressing the minor remarks and ensuring all technical terms are well-defined will enhance the paper's accessibility.