ID: ZjgcYMkCmX
Title: How does Inverse RL Scale to Large State Spaces? A Provably Efficient Approach
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 5, 8, 5, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on inverse reinforcement learning (IRL) within linear Markov Decision Processes (MDPs), demonstrating that the feasible reward set cannot be efficiently learned in large state and action spaces. The authors propose a new framework termed "reward compatibility," which reformulates IRL as a classification problem, allowing for the development of a sample-efficient algorithm called CATY-IRL. This algorithm employs reward-free exploration (RFE) and evaluates reward compatibility with expert demonstrations. The authors also establish a tight minimax lower bound for the tabular setting and introduce the concept of objective-free exploration, extending RFE to arbitrary tasks.

### Strengths and Weaknesses
Strengths:
1. The paper makes a significant and original contribution to the complex problem of IRL in large state-action spaces, particularly through the introduction of reward compatibility and a sample-efficient algorithm.
2. The technical quality is high, with clear notation and comprehensive proofs provided in the appendix.
3. The analysis of sample complexity for CATY-IRL across various settings is thorough, and the introduction of a tight minimax lower bound is particularly noteworthy.

Weaknesses:
1. The motivation for learning a reward compatibility classifier is not clearly articulated, leaving its practical applications ambiguous.
2. The writing lacks clarity, making it difficult to understand on the first read; more emphasis on motivation and intuitive examples is needed.
3. The novelty of the reward compatibility framework is questionable, as it requires a predefined reward function, which aligns closely with standard reinforcement learning practices.
4. The terminology used, such as "online IRL," may cause confusion due to existing definitions in the literature.
5. The paper lacks empirical evaluation and practical application discussions, which limits its applicability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the reward compatibility classifier and its potential applications. Additionally, enhancing the writing style to facilitate understanding, particularly through intuitive examples, would be beneficial. We suggest revising the terminology used to avoid confusion with established definitions in the literature. Furthermore, including empirical evaluations to support theoretical claims would strengthen the paper's contributions. Lastly, a more extensive discussion on the practical limitations and the computational complexity of the proposed algorithms is warranted.