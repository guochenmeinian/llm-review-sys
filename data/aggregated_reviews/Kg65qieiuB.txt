ID: Kg65qieiuB
Title: Demystifying Oversmoothing in Attention-Based Graph Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 7, 7, 6, 8, -1, -1
Original Confidences: 4, 3, 5, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the oversmoothing problem in Graph Neural Networks (GNNs), specifically focusing on attention-based models. The authors demonstrate that the graph attention mechanism does not mitigate oversmoothing and leads to an exponential loss of expressive power. They utilize Ergodicity analysis of infinite products of matrices to establish bounds on convergence and provide suggestions regarding activation functions to alleviate oversmoothing. The methodology is versatile, applicable to various GNN types, and supported by empirical evidence.

### Strengths and Weaknesses
Strengths:
- The approach to the oversmoothing problem through Ergodicity is innovative.
- The paper is well-written, making complex mathematical concepts accessible.
- The theoretical claims are clear, and the proofs are valid.
- The results have implications for a broader range of GNN models beyond just attention-based ones.

Weaknesses:
- The paper lacks experimental validation, particularly a plot showing JSR.
- There is insufficient discussion on alternative methods to alleviate oversmoothing.
- The analysis primarily considers homophily graphs, neglecting heterophily datasets, which limits the generalizability of the findings.
- Some prior work on reducing oversmoothing is not adequately referenced or discussed.

### Suggestions for Improvement
We recommend that the authors improve the paper by including an experiment that directly shows JSR to validate their theoretical claims. Additionally, the authors should discuss alternative strategies for alleviating oversmoothing beyond scaling, as suggested in [1]. Incorporating literature that addresses the controversy surrounding the effectiveness of attention-based GNNs in mitigating oversmoothing would enhance the reliability of their motivation. Furthermore, considering heterophily datasets and conducting experiments on larger datasets would strengthen the significance of their contributions. Lastly, clarifying the criteria for selecting negative slope values and addressing the sensitivity of hyperparameters to performance would improve the paper's rigor.