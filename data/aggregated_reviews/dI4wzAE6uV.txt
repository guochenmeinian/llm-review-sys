ID: dI4wzAE6uV
Title: Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs
Conference: NeurIPS
Year: 2023
Number of Reviews: 25
Original Ratings: 9, 8, 7, 8, 8, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents BIRD, a novel benchmark for evaluating text-to-SQL methods, which includes a large-scale dataset of 12,751 text-to-SQL pairs and 95 databases totaling 33.4 GB across 37 domains. The dataset aims to enhance the understanding of database schemas and values through external knowledge, addressing a significant gap in current benchmarks. The authors propose two efficiency metrics: Execution Accuracy (EX) and Valid Efficiency Score (VES), to evaluate both the correctness and efficiency of generated SQL queries. Experimental results indicate that current LLMs, such as ChatGPT and GPT-4, significantly underperform compared to human-generated queries, achieving only 40% execution accuracy compared to 92.86% for humans. The benchmark introduces three challenges reflecting real-world scenarios: managing large-scale databases with noisy values, understanding external knowledge evidence, and generating efficient SQL queries.

### Strengths and Weaknesses
Strengths:
- A new large-scale collection of realistic text-to-SQL dataset.
- Incorporation of external knowledge paired with the database.
- Introduction of SQL efficiency evaluation for the first time in text-to-SQL assessment.
- The authors address the need for a general-purpose, large-scale cross-domain benchmark in the text-to-SQL community, fulfilling a significant gap.
- The rigorous annotation process and detailed experimental analysis indicate high quality and careful effort in the research.
- The comparison of model performance with human performance provides valuable context for evaluating advancements in the field.

Weaknesses:
- Minor typos present (e.g., line 195: tex-to-SQL).
- Lack of an ablation study to illustrate the impact of external knowledge categories on model performance.
- Unclear consumption of external knowledge in T5 models during training.
- Insufficient justification for the manual split of datasets into train/test/dev sets.
- Lack of clarity regarding the definitions of EX and VES metrics.
- The dataset construction process lacks some details, particularly regarding distinguishing features between DuSQL and the 20% of databases in BIRD.
- Missing numerical details about the annotation process, such as the number of applicants and annotators, hinder a full understanding of the dataset's construction.
- Concerns about the stability and definition of the VES metric, including the range of functions and outlier removal methods, need clarification.
- The rationale for using the square root function in VES evaluation is not adequately explained, potentially compressing efficiency score distinctions.
- There is an imbalance in database sizes across domains, which may affect the evaluation of model performance.
- The paper lacks detailed benchmarking results categorized by question types, which could provide deeper insights into model performance.
- Some sections, such as the explanation of two-stage optimization and "Chat with Database," require further elaboration for clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions for EX and VES, possibly by including examples. Additionally, we suggest providing a better justification for the manual splitting of datasets and clarifying how the number of tokens was determined in the SQL statistics section. The authors should also consider including benchmarking results for state-of-the-art text-to-SQL methods to enhance the analysis. Furthermore, it would be beneficial to track the number of LLM-generated queries requiring human intervention and the effort involved in correcting them. Lastly, we encourage the authors to address the ethical implications of crowdsourcing, particularly regarding compensation and the platform used for data collection. Improving the alignment of database sizes across domains will provide a more balanced evaluation. Including detailed benchmarking results categorized by question types, both macro- and micro-categories, would enhance the understanding of model performance. Further elaboration on the significance and efficacy of the "Chat with Database" method, as well as the two-stage optimization process, would also benefit the clarity of the paper. Lastly, addressing reader-friendliness issues will enhance the overall accessibility of the work.