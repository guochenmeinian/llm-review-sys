ID: gKMTM1i8Ew
Title: Optimal Multi-Fidelity Best-Arm Identification
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 4, 7, 7, -1, -1
Original Confidences: 3, 3, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for efficient batch multiobjective Bayesian optimization in the context of best-arm identification (BAI) with multi-fidelity bandits. The authors derive a tight instance-dependent cost lower bound and propose an algorithm that asymptotically matches this bound. The method integrates concepts from Thompson sampling, Gaussian process surrogates, and evolutionary algorithms, providing theoretical analysis and empirical performance on synthetic and real-world problems.

### Strengths and Weaknesses
Strengths:  
- Theoretical contributions include a tight lower bound on the cost complexity of multi-fidelity BAI, enhancing understanding of fundamental limits in this area.  
- The proposed algorithm is designed to achieve optimal performance in the high-confidence regime, showcasing significant advancements in multi-fidelity BAI.

Weaknesses:  
- The experimental evaluation is limited, primarily focusing on synthetic problems and a few real-world examples; broader testing on diverse applications would enhance practical relevance.  
- The connection to real-world problems is not clearly articulated, necessitating more discussion on practical implications.  
- Some assumptions, particularly Assumption 5 in Theorem 4.1, may not hold in practice, requiring further exploration of their implications.  
- The focus on best-arm identification may limit the scope compared to broader multi-fidelity Bayesian optimization problems.  
- Clarity in gradient computation with respect to ω and μ is lacking, particularly given their discrete nature in bandit problems.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by including more diverse real-world applications, such as neural architecture search or classic multi-fidelity Bayesian optimization problems. Additionally, we suggest that the authors elaborate on how their work relates to broader multi-fidelity Bayesian optimization settings and provide more intuition or examples demonstrating the practical implications of their theoretical results. A thorough discussion on the realism of Assumption 5 in Theorem 4.1, including scenarios where it may not hold, would be beneficial. Furthermore, enhancing the clarity of gradient computation with respect to ω and μ, including detailed explanations and precise notation, would improve understanding. Lastly, a comprehensive discussion of the limitations of the best-arm identification framework compared to more general multi-fidelity Bayesian optimization problems would add depth to the paper.