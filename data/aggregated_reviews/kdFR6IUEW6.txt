ID: kdFR6IUEW6
Title: Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents POMP, a vision-language pre-training method designed to address GPU memory limitations in existing methods like CoOp when handling a large number of classes. POMP employs local contrast and local correction strategies to enhance memory efficiency and mitigate sampling bias. The authors demonstrate the effectiveness of POMP through comprehensive experiments across various visual recognition tasks, including image classification, semantic segmentation, and object detection.

### Strengths and Weaknesses
Strengths:
1. The local contrast strategy is reasonable, easy to implement, and effective for large class numbers.
2. The paper is well-organized, and the framework is simple.
3. Comprehensive experiments are conducted, and the code is provided, contributing to the open-source community.
4. POMP achieves state-of-the-art performance across multiple datasets and tasks, establishing a new benchmark for prompt tuning.

Weaknesses:
1. The local correction strategy resembles a general approach in contrastive learning, and its claim to solve the sampling bias problem is questionable, as bias may still persist.
2. The performance of open vocabulary object detection is not significantly improved compared to existing methods, raising concerns about POMP's effectiveness in detection benchmarks.
3. The training process for Stage 1 is similar to CLIP, but it is unclear why CLIP does not excel in downstream tasks compared to tuning on ImageNet-22K.
4. Figures may mislead regarding the zero-shot transfer capabilities of POMP, necessitating clearer explanations of the training process.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the local correction strategy and address the potential persistence of sampling bias. Additionally, the authors should clarify the performance differences between POMP and CLIP, particularly regarding the impact of the dataset size on downstream tasks. It would also be beneficial to provide explicit details about the additional training required for zero-shot transfer to avoid misconceptions from the figures presented.