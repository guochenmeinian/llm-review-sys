ID: Xt1JbFofwP
Title: TabPrompt: Graph-based Pre-training and Prompting for Few-shot Table Understanding
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TabPrompt, a prompt-based method for table understanding that employs graph contrastive learning to generate prompts using both text and topological information. The authors propose that their method achieves state-of-the-art results on cell type classification (CTC) and table type classification (TTC) through experimental evaluations on benchmark datasets. The work opens new avenues for utilizing prompts in tabular data and highlights the significance of topological observations for future research.

### Strengths and Weaknesses
Strengths:  
1. The experimental results are impressive, indicating the utility of the TabPrompt method.  
2. The approach of leveraging topological information in tabular data is novel and technically sound.  
3. The writing is clear, and the ablation study demonstrates the effectiveness of various components.  
4. The paper provides sufficient support for its major claims and includes various benchmark datasets for reproducibility.

Weaknesses:  
1. The motivation for the problem is weak, as labeled tabular data is not scarce for CTC and TTC.  
2. There is a lack of direct evaluation results in Tables 1 and 2, making comparisons to baseline methods less fair.  
3. The paper does not adequately compare against state-of-the-art methods, limiting the assessment of its performance.  
4. The experiments are limited in scope, missing combinations of datasets, baselines, and tasks, and references are outdated.

### Suggestions for Improvement
We recommend that the authors improve the motivation for their work by addressing the abundance of labeled tabular data. Additionally, the authors should include more direct evaluation results in Tables 1 and 2 to facilitate fair comparisons. It would be beneficial to expand the experimental scope by evaluating more tasks, such as question answering with datasets like WTQ and HiTab. Furthermore, the authors should incorporate comparisons with state-of-the-art methods and conduct significance tests, such as t-tests, to validate their findings. Lastly, updating the references to include more recent works would enhance the paper's relevance.