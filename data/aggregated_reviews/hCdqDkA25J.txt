ID: hCdqDkA25J
Title: Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 7, 7, 7, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 2, 2, 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents optimization algorithms that achieve optimal convergence rates and reproducibility in convex optimization and convex-concave minimax settings. The authors provide upper bounds on convergence and reproducibility that align closely with known lower bounds, addressing open questions from prior work. The work introduces new methods, particularly focusing on the reproducibility problem, and extends results to minimax optimization.

### Strengths and Weaknesses
Strengths:
- Clear presentation and readability.
- Novel technical contributions with strong theoretical results.
- The paper effectively demonstrates the use of L2 regularization to enhance reproducibility and convergence.

Weaknesses:
- Some details regarding the Inexact-EG method are lacking, particularly its relation to prior work.
- The writing could be improved to better motivate the relevance of reproducibility in modern machine learning.
- Certain technical aspects, such as the definition of "optimal reproducibility" and the implications of assumptions, are not sufficiently explained.

### Suggestions for Improvement
We recommend that the authors improve the discussion around the Inexact-EG method to clarify its connection to existing methods. Additionally, enhancing the motivation for reproducibility within the context of modern machine learning would strengthen the paper. Providing more intuition behind the bounds and discussing the implications of assumptions in the preliminaries would also be beneficial. Lastly, including experiments to demonstrate the practical usefulness of the proposed framework could significantly enhance the paper's impact.