ID: iD18l6prA7
Title: $C^2M^3$: Cycle-Consistent Multi-Model Merging
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a cycle-consistent method for merging multiple neural networks in weight space by solving permutation-based merge processes simultaneously. The authors propose using a "universe" space to enhance cycle consistency when merging more than two models, demonstrating substantial benefits across various architectures and datasets. The method employs the Frank-Wolfe algorithm to optimize neuron permutations globally, addressing inter-layer dependencies and ensuring that cyclic permutations yield the identity map.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel alignment algorithm that ensures cycle consistency for merging multiple models, utilizing a universe space effectively.
- It provides a comprehensive comparison with existing methods across different datasets and settings, including loss landscape analysis.
- The compatibility of the method with techniques like REPAIR enhances its practical performance.

Weaknesses:
- The theoretical foundations regarding neuron permutations are heavily reliant on prior work, lacking independent analysis.
- The paper does not adequately analyze the implications of simultaneous global optimization, particularly the computational cost associated with the Frank-Wolfe process.
- There is insufficient exploration of the method's scalability to larger models and datasets, and the convergence properties are not thoroughly discussed.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of the global optimization process, specifically addressing the ease of optimization and its scalability with wider or deeper networks. Clarifying the merging factor $\alpha$ when using REPAIR for multiple models would also enhance understanding. Additionally, conducting experiments on larger networks and datasets, as well as providing case studies of real-world applications, would strengthen the paper's contributions. Finally, addressing the convergence speed in comparison to other algorithms, such as MergeMany, would provide valuable insights into the method's efficiency.