ID: vU0KbvQ91x
Title: Learning to Abstract with Nonparametric Variational Information Bottleneck
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 7
Original Ratings: -1, -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel language representation model that applies the Nonparametric Variational Information Bottleneck (NVIB) regularizer to self-attention layers in a Transformer encoder, enabling the model to learn to compress information to varying levels of linguistic abstraction. The authors demonstrate that this approach enhances robustness to adversarial perturbations and provides insights into the hierarchy of abstraction within the model. The contributions include the innovative application of NVIB, comprehensive experiments, and analyses that validate the model's capabilities across various NLP tasks.

### Strengths and Weaknesses
Strengths:
- The integration of NVIB into self-attention layers broadens its utility in natural language processing tasks.
- The authors utilize implicit reparameterization gradients, leading to efficient learning processes.
- Experimental results support the efficacy of the proposed method across several NLP tasks, with clear presentation of findings.

Weaknesses:
- The paper lacks a brief introduction to NVIB, which may hinder understanding for readers unfamiliar with the concept.
- The experimental setup is limited, focusing solely on character-level models and a single language, which restricts the generalizability of the findings.
- Important methodological details are relegated to the appendix, making the paper less self-contained.

### Suggestions for Improvement
We recommend that the authors improve the accessibility of the paper by providing a brief introduction to NVIB for readers unfamiliar with the concept. Additionally, the authors should consider expanding the experimental setup to include comparisons with more baseline models, particularly sub-word based methods, and explore the performance of their model on languages with larger character sizes, such as Japanese and Chinese. Furthermore, we suggest that the authors justify their design choices in adapting NVIB to self-attention and provide a more detailed discussion on the implications of these choices.