ID: au9VfbABDO
Title: Diffusion Model-Augmented Behavioral Cloning
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 3, 3, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm for behavior cloning (BC) that modifies the BC learning objective using a diffusion modeling loss to model the joint state-action distribution of expert data. The authors demonstrate the advantages of modeling both conditional and joint probabilities, supported by extensive experiments across various continuous control tasks. The proposed method, Diffusion Model-Augmented Behavioral Cloning (DBC), shows improved performance compared to baseline methods, including BC and Implicit BC, while addressing issues like manifold overfitting and inference efficiency. Additionally, the paper explores manifold overfitting in the context of diffusion models and proposes adding noise to expert data as a potential solution. Experiments evaluate the impact of varying noise levels on model performance, particularly focusing on the success rates of policies trained with different loss functions, including the diffusion model loss and BC loss. The findings indicate that a specific noise level can improve the model's ability to generalize to unseen states, although the authors acknowledge that the issue of manifold overfitting is not entirely resolved.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and easy to understand, with a clear motivation for the problem.
- Extensive experiments demonstrate the effectiveness of DBC across diverse tasks, showing significant performance improvements over baselines.
- The approach combines the strengths of behavioral cloning and diffusion models, potentially encouraging further research in this area.
- The authors provide a thoughtful response to reviewer concerns, clarifying the theoretical underpinnings of their approach.
- Experimental results demonstrate that a noise level of 0.005 yields the best fit for expert data and improves policy success rates compared to models without noise.

Weaknesses:
- Limitations of the proposed approach are primarily discussed in the appendix, and a brief mention in the main text would enhance completeness.
- The mathematical notation and clarity in the presentation could be improved, particularly regarding expectations and the optimization process.
- The analysis of the $\lambda$ parameter's sensitivity is limited to a single environment, and the method's performance across different datasets remains unclear.
- The rationale for combining the diffusion model loss and BC loss lacks a solid theoretical foundation, leading to skepticism about the benefits of using both objectives.
- There are unresolved questions regarding the agent's performance on unseen states when trained solely with the diffusion model loss, raising doubts about the effectiveness of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the clarity of mathematical notation and explicitly state where expectations are taken in the equations. Additionally, we suggest including a brief mention of the limitations in the main paper for completeness. To strengthen the analysis, we encourage the authors to investigate the performance of DBC across various environments and dataset sizes, and to compare it with other state-of-the-art methods, such as Action Chunking with Transformers (ACT) and GAIL. Furthermore, we recommend that the authors improve the theoretical justification for the combination of the diffusion model loss and BC loss, addressing the concerns raised about their redundancy. We also suggest conducting further experiments to clarify the differences in agent performance when trained with either loss function, particularly regarding generalization to unseen states. Finally, we advise conducting experiments with more random seeds to ensure statistical significance in the results and to engage with all reviewer feedback comprehensively to enhance the clarity and robustness of their arguments.