ID: 19sGqVUxQw
Title: Inverse Scaling Can Become U-Shaped
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the inverse scaling behavior of language models, revealing that performance can exhibit a U-shaped curve as model size increases. The authors hypothesize that this is due to distractors affecting medium-sized models and propose mitigation strategies, including one-shot prompting and chain-of-thought prompting, which effectively transform inverse scaling performances into a U-shape. The experiments conducted are extensive and support the findings across various tasks.

### Strengths and Weaknesses
Strengths:  
- The identification of U-shaped performance is significant and challenges existing notions of inverse scaling.  
- The authors provide a plausible explanation for the observed behavior and suggest effective mitigation strategies.  
- The presentation of the paper is clear, and the experimental results are convincing.

Weaknesses:  
- The prompting methods rely heavily on manual construction, which may limit reproducibility.  
- There is a lack of direct support for the proposed explanation, particularly regarding qualitative analysis of success and failure cases.  
- The use of proprietary models like PaLM hinders reproducibility and broader applicability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the reproducibility of their results by incorporating non-proprietary models and providing more detailed descriptions of the model settings and training data. Additionally, we suggest including qualitative analyses of success and failure cases to support the hypothesis regarding distractor and true tasks. It would also be beneficial to discuss the prompt engineering efforts that went into constructing the chain-of-thought prompts, as this information is crucial for understanding the applicability of the proposed strategies. Finally, exploring the effects of instruction tuning and zero-shot chain-of-thought prompting could provide further insights into overcoming distractor tasks.