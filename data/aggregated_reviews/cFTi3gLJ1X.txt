ID: cFTi3gLJ1X
Title: Depth Anything V2
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Depth Anything V2, an advanced monocular depth estimation model that enhances its predecessor by utilizing synthetic images and large-scale pseudo-labeled real images for training. The results indicate that discriminative models can achieve fine-grained depth estimation, and the model demonstrates strong generalization capabilities. The authors also introduce a new benchmark, DA-2K, to improve evaluation diversity and accuracy in future research.

### Strengths and Weaknesses
Strengths:
1. The paper effectively enhances the existing discriminative depth estimation model, DepthAnything, using synthetic datasets and large-scale pseudo labels, which is both simple and effective.
2. Extensive experiments and a comprehensive analysis of recent depth foundation models are provided, showcasing strong generalization performance and real-time capabilities.

Weaknesses:
1. There is a lack of concrete metrics to quantitatively define fine-grained depth, making it difficult to compare fairly with generative-based models.
2. The pseudo labels are limited to relative depth, which may not be directly applicable for metric depth estimation.
3. The absence of dataset ablation experiments raises questions about the necessity of using all 62M pseudo labels, as performance with a subset remains unverified.
4. The novelty of the approach is limited, as it primarily focuses on data analysis without introducing new methods or insights.

### Suggestions for Improvement
We recommend that the authors improve the quantitative evaluation of fine-grained depth by incorporating concrete metrics. Additionally, conducting dataset ablation experiments would clarify the necessity of the total 62M pseudo labels. The authors should also elaborate on the pipeline and loss function formulation for better clarity. Including comparisons with more recent state-of-the-art methods, such as UniDepth, ZoeDepth, and Metric3D, would enhance the analysis of the proposed model's performance. Finally, providing visualizations of results back-projected into 3D would help assess depth map quality more effectively.