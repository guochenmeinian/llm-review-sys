ID: Zi1KKzh5Aj
Title: Collapsed Inference for Bayesian Deep Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for calculating Bayesian integrals, specifically Bayesian model averaging (BMA), by reformulating it as a weighted volume computation (WVC) problem. The authors propose using weighted model integration (WMI) to approximate the intractable WVC, employing approximations like a triangular distribution for Gaussian likelihoods. Additionally, the paper introduces a lower variance approach to estimating the posterior predictive distribution by aggregating over a finite number of samples, which is described as equivalent to an expectation under a discrete uniform distribution. The results indicate superior performance compared to SGD-based Bayesian methods and comparable results to Bayesian last-layer methods. The paper also provides a closed-form approximation for the posterior predictive distribution in Bayesian deep learning.

### Strengths and Weaknesses
Strengths:
* The paper offers a novel characterization of BMA as a volume computation problem.
* It introduces an innovative approach to approximating BMA using WMI.
* The authors provide practical methods to integrate WMI into the Bayesian learning framework.
* The use of a motivating example enhances clarity.
* The manuscript is acknowledged for its originality and interesting approach.
* Reviewers appreciate the authors' willingness to clarify and expand on complex points in the revised version.
* The writing is generally clear and well-structured.

Weaknesses:
* The assumption of a uniform posterior lacks adequate justification, particularly in the context of empirical results.
* The performance claims are exaggerated, as the method is not compared against all relevant Bayesian methods, and improvements are often mild.
* Key experimental details, such as validation splits and hyper-parameter settings, are insufficiently reported.
* The computational cost of WMI solvers is not quantified, raising concerns about reproducibility.
* The manuscript lacks explicit discussion of "bias," which could enhance clarity.
* Limitations regarding the approximations used in the methodology should be more openly discussed.
* The paper lacks a sensitivity analysis regarding the number of samples and does not discuss limitations adequately.

### Suggestions for Improvement
We recommend that the authors improve the justification for the uniform posterior assumption, providing clearer intuition and empirical support. Additionally, the authors should include comparisons against a broader range of Bayesian methods to substantiate performance claims. A detailed sensitivity analysis regarding the number of samples and a quantification of computational costs should be added to enhance reproducibility. Furthermore, we suggest including more experimental details, such as validation splits and hyper-parameter settings, to strengthen the paper's rigor. We also recommend that the authors improve clarity by explicitly mentioning "bias" in the manuscript and openly discuss the limitations of the approximations used in estimating the posterior predictive distribution. Including full explanations as previously suggested will further strengthen the revised version. Lastly, a discussion of the limitations of the proposed method should be incorporated to provide a balanced perspective.