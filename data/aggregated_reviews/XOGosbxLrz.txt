ID: XOGosbxLrz
Title: dopanim: A Dataset of Doppelganger Animals with Noisy Annotations from Multiple Humans
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 7, 6, 9, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new dataset, Dopanim, focused on doppelganger animals with multiple annotations per image, aiming to enhance research in machine learning with noisy annotations. The authors propose that their dataset includes unique features such as soft per-person uncertainties, annotation times, and annotator metadata. The evaluation highlights various research questions that can be explored using this data.

### Strengths and Weaknesses
Strengths:
- The concept of doppelganger animals is innovative and refreshing.
- The paper is well-structured, with clear outlines and key insights that enhance readability.
- Transparency regarding related work from the authors' group is commendable.
- The methodology is sound, and evaluations are generally robust.

Weaknesses:
- The distinction between Dopanim and previous datasets is not sufficiently clear, raising questions about its uniqueness.
- Comparisons with similar datasets, particularly in Table 1 and the appendix, appear arbitrary and could be improved.
- The paper lacks adequate motivation for certain decisions, such as the choice of categories and the annotation interface design.
- The utility of annotation time is questioned, and the paper does not adequately address the implications of individual annotator uncertainties.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the differences between Dopanim and previous works, particularly by integrating more relevant datasets into Table 1. Additionally, please verify the claims regarding annotator metadata availability and clarify the implications of this on your dataset's uniqueness. In Section 5.1, consider comparing individual human uncertainty to averaged uncertainty methods like CIFARH, and discuss the trade-offs between annotation costs and performance gains. 

Furthermore, we suggest providing brief explanations of the methods listed in Table 4 to facilitate comparisons. Establish a baseline using averaged soft labels instead of majority vote aggregation, and increase the size of all tables and figures for better readability. Lastly, elaborate on how the metadata is utilized in Section 5.2 to ensure the paper is self-contained.