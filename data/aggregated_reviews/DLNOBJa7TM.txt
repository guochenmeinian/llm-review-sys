ID: DLNOBJa7TM
Title: Efficient Federated Learning against Heterogeneous and Non-stationary Client Unavailability
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on heterogeneous and non-stationary client availability in federated learning (FL), demonstrating its significant impact on the performance of FedAvg. The authors propose the FedAWE algorithm, which compensates for missed computations due to client unavailability with minimal overhead and is agnostic to non-stationary dynamics. The work includes a convergence proof in a non-convex setting and numerical experiments to validate the proposed solution's effectiveness.

### Strengths and Weaknesses
Strengths:
1. **Importance of the research problem:** The work addresses a critical issue of heterogeneous and non-stationary client participation patterns, particularly relevant in FL with edge devices.
2. **Simple, principled, and convenient approach:** The proposed method is straightforward to implement, involving the calculation of a discount factor on clients' pseudo gradients and delaying model updates.
3. **Good results:** Experimental results effectively demonstrate the approach's efficacy in managing non-stationary and heterogeneous client availability, with code provided for reproducibility.

Weaknesses:
1. **Presentation issues:** The writing lacks clarity, making it difficult for readers to grasp the intuitions behind the technical innovations. There are excessive references to supplementary material, which should be minimized.
2. **Algorithmic concerns:** The model update weight modification may lead to unreasonable amplification of gradients under low client participation rates. Additionally, the convergence rate does not clearly show linear speedup concerning local steps and active clients.
3. **Dimensional inconsistencies:** The term $(1 + L^2)$ raises concerns regarding dimensional unity, suggesting potential issues in the proof.

### Suggestions for Improvement
We recommend that the authors improve the manuscript's clarity by highlighting the most important information and reducing reliance on supplementary material. Additionally, we suggest providing a more in-depth analysis of the algorithm's complexity and addressing the concerns regarding gradient amplification and the dimensionality of $(1 + L^2)$. Lastly, we encourage the authors to discuss the limitations of their approach more thoroughly in section 8.