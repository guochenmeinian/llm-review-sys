ID: ywrPcBEXdC
Title: Revisiting Adversarial Robustness Distillation from the Perspective of Robust Fairness
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 5, 7, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on fairness in adversarial robustness distillation, highlighting the exacerbation of accuracy disparity between classes in student models due to the capacity gap with teacher models and equal class importance during distillation. The authors propose the Fair Adversarial Robustness Distillation (Fair-ARD) framework, which employs a class re-weighting mechanism based on a geometric measure of class difficulty, specifically the Least PGD Steps (LPS). Extensive experiments demonstrate that Fair-ARD improves both average and worst-group accuracy compared to existing methods.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a significant issue in the fairness-robustness trade-off, initiating an important direction for research.
2. The proposed Fair-ARD framework is simple, intuitive, and can be integrated into existing ARD methods.
3. The empirical evaluation is thorough, with clear writing and well-structured mathematical notations.

Weaknesses:
1. The novelty of the approach is limited, as robust fairness has been widely studied, and the re-weighting method is not fundamentally new.
2. The metric for class difficulty lacks theoretical grounding, raising concerns about the reliability of the experimental results, particularly on CIFAR-100.
3. The experiments primarily utilize CIFAR datasets, which may not reflect performance on more challenging datasets.
4. The dynamic nature of the re-weighting strategy may introduce computational overhead and stability issues.

### Suggestions for Improvement
We recommend that the authors improve the theoretical foundation of the class difficulty metric to enhance the credibility of their results. Additionally, it would be beneficial to discuss existing fair-oriented distillation methods and their potential to address the robust fairness problem. We suggest including more challenging datasets beyond CIFAR to validate the generality of the proposed method. Furthermore, clarifying the stability of the re-weighting approach and its sensitivity to PGD parameters would strengthen the paper. Lastly, addressing the implications of class imbalance in the training data would provide a more comprehensive understanding of the method's applicability.