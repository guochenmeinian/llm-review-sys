ID: NPu7Cdk2f9
Title: Adaptive Depth Networks with Skippable Sub-Paths
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 5, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an easy-to-train supernet that allows adaptive changes in network depth by disabling certain blocks (skippable sub-paths) while maintaining mandatory sub-paths. The authors propose that the lengths of mandatory and skippable sub-paths should be equal for optimal accuracy and computational efficiency. The training involves two forward-backward passes: training the entire supernet for task objectives and self-distilling intermediate feature maps to a base network using KL-divergence loss. The approach employs Switchable LayerNorm or BatchNorm in mandatory sub-paths and claims to provide a theoretical basis for predictable depth adaptation with minimal performance loss.

### Strengths and Weaknesses
Strengths:
- The resulting base/student networks achieve higher accuracy than separately trained networks with equivalent flops, depth, and structure.
- The supernet outperforms other state-of-the-art methods in terms of accuracy and computational complexity, as shown in Figure 4(b).
- The method demonstrates flexibility, allowing for quick adjustments in speed and accuracy at test time.
- Extensive experiments validate the approach across various networks, including CNNs and transformers.

Weaknesses:
- The proposed method is outperformed by SpViT-Swin-Ti in terms of accuracy and flops.
- There is a lack of Out-of-Domain (OoD) comparisons, which are crucial for assessing robustness in real-world applications.
- Most comparisons focus on computational complexity (Flops), with insufficient emphasis on latency, which is more relevant for practical scenarios.
- The claim of being the first approach to adaptive networks may be overstated, as prior works on subnetwork distillation exist.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by including Out-of-Domain comparisons to demonstrate robustness across different datasets. Additionally, we suggest incorporating more charts that compare Out-of-Domain accuracy against latency to better reflect real-world applicability. It would also be beneficial to clarify the differences between their approach and existing methods, particularly in terms of performance. Lastly, we encourage the authors to quantify claims regarding the compactness of representations and to provide clearer explanations and captions for tables and figures to enhance understanding.