ID: rbaK24KnIO
Title: Combining Denoising Autoencoders with Contrastive Learning to fine-tune Transformer Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a three-phase fine-tuning approach to adapt a pre-trained model for supervised classification tasks, which includes denoising autoencoder training, supervised contrastive learning, and final fine-tuning. The authors propose an unbalance correction method and demonstrate that their approach outperforms classical fine-tuning through extensive experiments across multiple datasets. The methodology is well-explained, ensuring reproducibility and task independence.

### Strengths and Weaknesses
Strengths:
- The proposed fine-tuning algorithm shows improved performance over classical methods.
- The unbalance correction method is a valuable contribution.
- Detailed explanations of each phase enhance understanding.
- Extensive experiments and ablation studies support the claims.

Weaknesses:
- The novelty of combining existing techniques may be questioned, as it lacks surprising elements.
- The paper does not adequately address the training costs associated with the proposed method.
- Some hyperparameters were not thoroughly explored, which could affect results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the introduction to better outline their procedure and related work. Additionally, consider adding a denoising pretraining phase and a contrastive pretraining phase to enhance novelty. Address the training costs and their implications for cost-performance trade-offs. Finally, ensure that all hyperparameters, particularly learning rates and maximum epochs, are thoroughly searched and reported.