ID: 4CLNFKi12w
Title: Task Arithmetic with LoRA for Continual Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: 6, -1, -1, -1
Original Confidences: 5, 4, 4, 3

Aggregated Review:
### Key Points
This paper presents a novel approach to continual learning by utilizing low-rank adaptation (LoRA) combined with task arithmetic to address catastrophic forgetting and reduce computational costs. The authors demonstrate that their method, supported by a small memory of 10 samples per class, achieves performance levels comparable to full-set fine-tuning across three datasets.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- The proposed method is intuitive and practically applicable.
- Strong evaluation across multiple datasets and relevant baselines, showing promising results that reduce computational requirements by a factor of 4-5.

Weaknesses:
- The approach is of incremental novelty, with limited contribution.
- The evaluation datasets, such as CIFAR10 and Flowers-102, may be too simple.
- The ablation study is insufficient, lacking experiments on full-rank fine-tuning and the number of samples per class for fine-tuning.
- Comparisons with newer state-of-the-art methods are needed, particularly in the related work section.

### Suggestions for Improvement
We recommend that the authors improve the ablation studies by including experiments on full-rank fine-tuning and varying the number of samples per class for fine-tuning. Additionally, comparisons with more recent methods should be incorporated to strengthen the evaluation. We suggest restructuring the related work section for clarity. Furthermore, addressing formatting issues, such as improving equation presentation and table formatting, would enhance the overall quality of the paper. Adding a footnote in Section 3.3 to clarify that lambda in Equation 5 is not learnable and specifying the value used in experiments would also aid readability.