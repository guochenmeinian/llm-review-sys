ID: SVUQX1W7RL
Title: DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall's Rank Correlation
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 4, 5, 3, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel similarity metric for few-shot learning, specifically Kendall’s rank correlation, motivated by the observation that channel values in novel datasets exhibit a more uniform distribution compared to base datasets. The authors propose a differentiable approximation to address the non-differentiability of Kendall’s rank correlation, demonstrating its utility through experiments. The contributions include the introduction of feature channel importance ranking, the effectiveness of Kendall's rank correlation in enhancing few-shot learning performance, and a differentiable loss function for meta-training. The method achieves state-of-the-art (SOTA) results in the 1-shot setting for mini-ImageNet and tiered-ImageNet datasets while maintaining competitive performance in the 5-shot setting. The authors emphasize the simplicity and generality of their method, which can enhance existing cosine-based approaches without additional training costs.

### Strengths and Weaknesses
Strengths:
- The idea of using ranking information from channels instead of values to compute similarity is intriguing.
- The proposed method achieves SOTA results in the 1-shot setting and competitive results in the 5-shot setting.
- The simplicity and broad applicability of the method are significant advantages.
- The paper is well-structured, clearly written, and presents a straightforward method.
- The integration of Kendall's rank correlation shows consistent improvements across various datasets.
- The analysis in Section 5.4 aids in understanding the impact of channel value magnitude on performance.

Weaknesses:
- The proposed Kendall’s rank correlation is limited to few-shot learning methods that rely on similarity metrics, restricting its applicability.
- Comparisons in Section 4.2 are unconvincing, as only cosine similarity is considered without evaluating other popular metrics.
- The motivation and theoretical justification for using Kendall's rank correlation over cosine distance are not clearly articulated.
- The clarity of results in Table 1 is questionable, particularly regarding the significant performance drop compared to previous studies.
- The comparison methods in Tables 2 and 3 do not represent state-of-the-art techniques, undermining claims of superiority.
- Some reviewers feel that the novelty and significance of the method are limited due to reliance on an existing metric without sufficient explanation.

### Suggestions for Improvement
We recommend that the authors improve the comparison in Section 4.2 by including additional baselines that utilize other similarity metrics, such as Square Euclidean Distance and learnable metrics. Clarifying the results in Table 1 is essential, particularly addressing the discrepancies with previous studies. The authors should also ensure that the comparison methods are indeed state-of-the-art to substantiate claims of performance superiority. Additionally, we recommend enhancing the theoretical explanation of why Kendall's rank correlation can effectively replace cosine distance, as this would enhance the perceived novelty and significance of the work. Providing a clearer analysis of the performance variations across different backbone architectures could strengthen the manuscript. Lastly, addressing the reviewers' concerns regarding the motivation behind the algorithm more comprehensively would further clarify the contribution of the paper.