ID: BGY6LWN8bh
Title: How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompt
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 5, 8, 6, 7
Original Confidences: 3, 5, 4, 3

Aggregated Review:
### Key Points
This paper presents MAD-Bench, a benchmark designed to evaluate the robustness of multimodal large language models (MLLMs) against deceptive prompts, consisting of 1000 image-prompt pairs categorized into five types of deception. The authors assess 19 state-of-the-art MLLMs, with GPT-4V achieving the highest performance. They propose a simple remedy of adding an additional prompt paragraph to enhance model performance.

### Strengths and Weaknesses
Strengths:
- The evaluation encompasses a wide range of MLLMs, including both open-source and proprietary models, providing a comprehensive overview of the current state-of-the-art.
- MAD-Bench features well-thought-out categories of deception, facilitating a nuanced assessment of model capabilities. The automated evaluation using GPT-4o, validated by human checks, is a practical approach for large-scale assessments.
- The paper is well-structured and easy to follow, with thorough experiments that leave no major questions unanswered.

Weaknesses:
- The analysis lacks significant technical innovations in model architecture or training methods to address the identified issues.
- Despite its diversity, the benchmark is limited to 1000 samples and 5 categories, suggesting that a more extensive benchmark could yield additional insights.
- The proposed remedy is relatively simple, and a more comprehensive exploration of potential solutions is warranted.
- The novelty of the technical approach appears limited, as prior works have also addressed deceptive prompts, and the authors could better highlight unique insights gained from their dataset.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contribution by further emphasizing the unique insights derived from their dataset compared to prior work. Illustrating that the insights can only be obtained through the inclusion of prompts starting with “What, How, Where” could strengthen their argument for the study's importance. Additionally, expanding the benchmark beyond 1000 samples and 5 categories may provide deeper insights into model performance.