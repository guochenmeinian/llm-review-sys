ID: hfZKiBh4zS
Title: MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Multi-level Prompt Tuning (MPrompt), a method designed to enhance the fine-tuning of pre-trained language models (PLMs) for reading comprehension tasks. MPrompt introduces dynamically generated prompts, including domain-specific and context-specific prompts, to improve performance on the machine reading comprehension (MRC) task. The authors argue that this multi-level approach leads to better learning outcomes, demonstrating superior results compared to previous methods such as prefix/prompt tuning and XPrompt across various MRC datasets.

### Strengths and Weaknesses
Strengths:
- The proposed MPrompt method is intuitive and shows performance improvements across multiple MRC datasets.
- The paper is well-structured, providing a solid experimental foundation with necessary comparisons to fine-tuning and prompt tuning baselines.
- The analysis of prompt tuning hyperparameters for MPrompt is thorough.

Weaknesses:
- Concerns exist regarding the generalization ability of MPrompt, particularly its reliance on clustering for domain determination, which may hinder performance on out-of-distribution examples.
- The efficiency of MPrompt compared to baselines is unclear, particularly regarding the introduction of additional trainable parameters and the use of an extra language model for prompt generation.
- The novelty of MPrompt is questioned, as it appears to adapt existing instance-level prompt techniques without significant innovation.

### Suggestions for Improvement
We recommend that the authors improve the clarity on how MPrompt generalizes to new domains not present in the training data and provide a strategy for selecting domain prompts for these new contexts. Additionally, we suggest including a detailed comparison of the training resource costs between MPrompt and baseline methods to clarify its efficiency. Finally, we encourage the authors to incorporate more case studies and visualizations to enhance the understanding of their approach, as well as to reference and discuss recent parameter-efficient tuning techniques such as LoRA, DyLoRA, and AdaLora to contextualize their contributions within the broader landscape of prompt-based learning.