ID: YvA8UF0I37
Title: PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 7, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel QAT algorithm called PV-tuning for extreme compression of LLMs, utilizing discrete weight representations such as vector and uniform quantization. Unlike traditional optimization methods that depend on straight-through estimation (STE), PV-tuning employs an alternating optimization approach reminiscent of the EM algorithm. The algorithm consists of a P step, where the scale parameter is updated via backpropagation, and a V step, which rounds to the nearest scaled integer while updating only a few coordinates to avoid sub-optimal solutions. Experimental results indicate that PV-tuning outperforms existing PTQ techniques like QuIP, AQLM, and GPTQ, particularly for 2-bit compression of Llama 2 7B, although it is 1.5x slower than standard fine-tuning.

### Strengths and Weaknesses
Strengths:
- The paper addresses the critical issue of improving STE, which is essential for advancing extreme compression of LLMs.
- The proposed algorithm has a theoretical foundation and demonstrates superior performance in empirical evaluations across various models.

Weaknesses:
- **Clarity**: The presentation, especially regarding algorithmic details, requires significant improvement. The use of non-uniform quantization examples complicates understanding; a focus on uniform or vector quantization would enhance clarity.
- **Experimental Evaluation**: The experimental setup lacks robust comparisons, particularly against QAT techniques using STE, which are more relevant than PTQ methods.
- **Related Work**: The paper does not adequately discuss existing QAT techniques that enhance STE for uniform quantization, which is surprising given their relevance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by focusing on more relevant examples, such as uniform quantization or vector quantization, to unify the weight representations. Additionally, we suggest enhancing the experimental evaluation by including comparisons against QAT techniques using STE as baselines. It would also be beneficial to discuss existing QAT methods that improve upon STE to provide a comprehensive context. Furthermore, addressing the notation confusion and providing clearer explanations of the algorithm's mechanics would aid in comprehension. Lastly, we encourage the authors to explore the implications of their findings regarding the performance of PV-tuning compared to STE in more detail, including potential empirical evidence supporting their claims.