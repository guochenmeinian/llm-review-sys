ID: fPAAgjISu0
Title: In Defense of Softmax Parametrization for Calibrated and Consistent Learning to Defer
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to learning to defer by introducing an asymmetric softmax-based surrogate loss that addresses the calibration issues associated with symmetric loss functions. The authors theoretically prove that asymmetric losses are necessary for bounded probability estimates (Theorem 1) and demonstrate the consistency of their proposed loss (Theorem 2). They also provide a risk transfer bound to the original zero-one loss (Theorem 3). Empirical results indicate that the proposed loss function achieves low error rates and good calibration properties across various datasets.

### Strengths and Weaknesses
Strengths:
- Theoretical contributions are robust, particularly Theorem 1, which could guide the design of other softmax-based estimators for non-classic learning tasks.
- The proposed asymmetric softmax-parameterized loss is straightforward to implement, introducing minimal computational overhead.
- Empirical performance is strong, consistently outperforming tested baselines.
- The paper is well-written, effectively managing complex notation.

Weaknesses:
- The rationale for preferring softmax-based losses over non-softmax alternatives is unclear, necessitating a more detailed explanation or reference.
- Empirical validation is limited, primarily focusing on synthetic CIFAR100 data, with insufficient results for CIFAR10H and a lack of broader dataset comparisons.
- The presentation could be improved due to heavy notation, making it challenging to follow.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the rationale for using softmax-based losses, providing a more explicit justification for their advantages. Additionally, we suggest expanding the empirical validation to include more datasets beyond CIFAR100, such as CIFAR10H, CheXpert, and synthetic datasets, to strengthen the paper's claims. Explicit numerical results for CIFAR10H should also be included to verify baseline implementations and tuning. Finally, consider adding a common notation section at the beginning to aid reader comprehension.