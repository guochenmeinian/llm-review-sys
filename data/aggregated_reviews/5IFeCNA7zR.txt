ID: 5IFeCNA7zR
Title: DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework, Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graphs (DARG), which dynamically generates evaluation data with controlled complexity, addressing limitations of static benchmarks such as data contamination and lack of adaptability to evolving LLM capabilities. The authors evaluate multiple LLMs across four reasoning tasks and observe consistent performance drops, suggesting that previous static evaluations may have been misleading.

### Strengths and Weaknesses
Strengths:
1. The method of dynamically generating evaluation data based on adaptive reasoning graphs is promising and effectively addresses critical limitations of static benchmarks.
2. The evaluation encompasses a wide range of LLMs and tasks, providing valuable insights into model robustness and generalization capabilities.
3. The paper highlights how biases in LLMs can be exacerbated under complex testing conditions, contributing significantly to the discourse on ethical AI.
4. The use of reasoning graphs to represent problem-solving processes ensures that generated data retains linguistic diversity and is representative of real-world scenarios.

Weaknesses:
1. The ranking order among various models remains consistent as complexity metrics increase, contradicting the conclusion regarding the unreliable assessment of LLMs' capabilities using static benchmarks.
2. The applicability of DARG to other task types, particularly those that do not lend themselves to graph-based representations, is unclear.
3. The graph extraction and data generation process relies heavily on closed-source LLMs, raising questions about the reproducibility of the proposed framework.
4. The rule-based function of DARG is not clearly explained, which is important for ensuring the quality of reasoning graph generation.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the observed inconsistencies between model rankings and stated conclusions. Additionally, the authors should provide preliminary results or case studies demonstrating DARG's performance on non-reasoning tasks. It would also be beneficial to explore using only open-source models for graph extraction and data generation to assess DARG's effectiveness without reliance on closed-source LLMs. Finally, expanding on the fine-tuning experiments, particularly regarding the integration of GSM8K training data with newly generated samples, would enhance the paper's contributions.