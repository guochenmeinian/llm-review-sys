ID: 9622QfVSAb
Title: Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 5, 9, 7, -1, -1, -1
Original Confidences: 4, 2, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of the Implicit Multimodal Alignment (IMA) phenomenon in frozen Large Language Models (LLMs), demonstrating that perceptual tokens are implicitly aligned with text tokens despite the LLMs being trained solely on text. The study argues that this alignment is crucial for understanding task performance and model visual faithfulness. The authors propose architectural changes to reduce inference costs by skipping specific operations on perceptual tokens. The analysis spans various setups and modalities, validating the IMA effect and discussing its implications for computational efficiency and model performance.

### Strengths and Weaknesses
Strengths:
- The work provides original insights into the mechanisms of multimodal LLMs, potentially enhancing visual faithfulness in existing models.
- The extensive series of experiments validate the hypotheses and offer meaningful results regarding the IMA effect.
- The study can facilitate the extension of large semantic models to multimodal domains, reducing computational costs.

Weaknesses:
- The presentation quality is poor, with pixelated figures that hinder understanding; many figures lack clarity and detail.
- Some claims lack sufficient backing, and inconsistencies in terminology (e.g., "P" for both Prompt and Perceptual) create confusion.
- The evaluation of the Multi Task setup is incomplete, lacking common benchmarks and implementation details.

### Suggestions for Improvement
We recommend that the authors improve the presentation quality by utilizing vectorized figures or tools like TikZ to enhance readability. It is crucial to clarify terminology and ensure consistency throughout the paper, particularly regarding the definitions of "Prompt" and "Perceptual." We suggest including additional evaluation metrics and benchmarks, such as LLaVABench-in-the-Wild and MMHALBench, to strengthen the findings. Furthermore, addressing the clarity of textual descriptions and providing more comprehensive explanations of the subnet and its performance implications would enhance the overall understanding of the work.