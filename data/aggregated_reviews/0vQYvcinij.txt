ID: 0vQYvcinij
Title: DrawEduMath: Evaluating Vision Language Models with Expert-Annotated Studentsâ€™ Hand-Drawn Math Images
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 4, 6, 7
Original Confidences: 5, 5, 4

Aggregated Review:
### Key Points
This paper presents a new dataset named DrawEduMath, comprising handwritten questions and answers from 188 K-12 math problems, collected by research assistants. The dataset includes diverse question types, metadata on grade levels, descriptions, and topic categories. The authors evaluate the dataset using large language models (LLMs) GPT-4o and Gemini-1.5 Pro, finding that GPT-4o outperforms Gemini-1.5 Pro across all metrics. The authors assert that this dataset enhances the capabilities of Vision Language Models (VLMs) in addressing various mathematical problems.

### Strengths and Weaknesses
Strengths:  
- The dataset DrawEduMath is a valuable contribution, providing a benchmark for evaluating VLMs in educational contexts.  
- The inclusion of diverse information types in each image datapoint is commendable.  
- The comparison between GPT-4o and Gemini-1.5 Pro is well-executed and informative.  

Weaknesses:  
- The implications and motivations for the work are not clearly articulated, making it difficult to understand the specific shortcomings in the models.  
- Table 4's interpretation is challenging, as it compares VLM responses to expert tutor summaries without clear benchmarks for real-world applications.  
- The correlation between human judgments and automatic metrics is low, raising concerns about the reliability of the results in Table 4.  
- The dataset could benefit from an increased number of QA pairs and problems to enhance its utility for fine-tuning.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their work's implications and motivations, specifically addressing the shortcomings of current models in tutoring contexts. Distinguishing between different types of difficulties, such as arithmetic or understanding drawings, would provide valuable insights for researchers. Additionally, incorporating fine-tuning strategies, such as with Llava, could enhance model performance. To address concerns regarding the reliability of Table 4, consider decomposing VLM responses into factual and non-factual questions, potentially implementing a binary measure for factual correctness. Finally, increasing the number of QA pairs and problems in the dataset would significantly enhance its applicability for fine-tuning purposes.