ID: tbHe97ENFD
Title: Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the FiLM model, a financial language model initialized from RoBERTa, which is pretrained on a diverse corpus of financial texts. The authors investigate whether this diversity leads to improved performance compared to existing models, which are often pretrained on less varied data. The results indicate that FiLM, trained on 2.4B tokens, outperforms models trained on over 3B tokens from less diverse sources. The study emphasizes the significance of data diversity in enhancing model performance across various financial tasks.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents strong experimental support for its claims.
- It highlights the importance of diverse datasets in training financial language models, demonstrating that FiLM outperforms existing models, including BloombergGPT.
- The introduction of a new dataset that encompasses five major groups of financial literature is a notable contribution.

Weaknesses:
- The choice to initialize FiLM from RoBERTa rather than BERT raises questions about the fairness of comparisons with other models.
- The paper lacks clarity regarding the source of the 5.5B token corpus mentioned in the results.
- There is insufficient analysis of the model's performance on out-of-domain tasks, and the novelty of the findings is questioned due to the established understanding of data diversity benefits.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup, including details on the variance and mean of fine-tuning performance across tasks. Additionally, the authors should provide a more equitable comparison by including experiments with FLANG-RoBERTa. It would be beneficial to report confidence intervals and clarify the results in Table 2, particularly regarding the 5.5B token corpus. A qualitative error analysis demonstrating why FiLM outperforms other models would enhance the paper's contributions. Lastly, we suggest proofreading to improve the overall presentation and conciseness of the text.