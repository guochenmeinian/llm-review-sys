ID: DNGfCVBOnU
Title: Pretraining with Random Noise for Fast and Robust Learning without Weight Transport
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 5, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the hypothesis that networks trained with feedback alignment can benefit from pre-training using random input-output pairs, leading to faster learning and improved generalization. The authors demonstrate that pre-training reduces the effective dimensionality of network activity, which may enhance performance. The experiments show that networks with pre-training converge comparably to those using traditional backpropagation and generalize better across tasks.

### Strengths and Weaknesses
Strengths:  
The paper introduces a novel approach to accelerate training with feedback alignment, providing convincing results regarding faster learning. The clarity of the theoretical explanations and empirical analyses is commendable, particularly the exploration of various metrics over time.

Weaknesses:  
A critical limitation is the lack of training until convergence for both random pre-training and task-related data, which raises concerns about potential performance issues. The paper does not present numerical performance metrics, making it difficult to assess the adequacy of training. Additionally, the definition of key concepts, such as the "cosine angle," lacks clarity, and the reliance on a single hidden layer network limits the generalizability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the training methodology by ensuring convergence for both random pre-training and task-related data to validate their claims fully. It would be beneficial to include numerical performance metrics to clarify the training adequacy. We suggest revising the definition in lines 147-149 for clarity regarding the "cosine angle" and its implications. Furthermore, consider adding comparisons with backpropagation-trained networks in the appendix to strengthen the analysis. Lastly, we encourage the authors to explore the implications of their findings on deeper architectures and investigate the potential benefits of pre-training on traditional backpropagation networks.