ID: 4vpsQdRBlK
Title: Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified framework for Cross-Lingual Medical Vision-Language Pre-Training (Med-UniC), addressing the bias caused by different languages in medical datasets. The authors propose a Cross-lingual Text Alignment Regularization (CTR) to unify semantic representations of medical reports from English and Spanish. The experiments demonstrate that CTR effectively mitigates bias and achieves superior performance across multiple medical image tasks and datasets.

### Strengths and Weaknesses
Strengths:
1. The paper tackles an important issue of bias in medical visual language pretraining, making it relevant and practical.
2. The method is straightforward, enhancing the paper's clarity and comprehension.
3. The CTR loss is an innovative approach that shows promise in addressing community bias in multilingual pre-training.

Weaknesses:
1. The design of the visual language model closely resembles existing MLM and CLIP-based methods, necessitating more analysis on its differentiation.
2. The experimental setting lacks clarity, particularly regarding the implementation and training data of state-of-the-art (SOTA) methods.
3. The ablation study is incomplete, limiting insights into the contributions of the CTR loss and the impact of increased data quantity.
4. The bias analysis section is brief, lacking depth in identifying sources of bias or providing illustrative examples.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup by detailing the implementation and training datasets of the SOTA methods. Additionally, we suggest enhancing the ablation study to include more settings, allowing for a clearer understanding of the contributions from the CTR loss and data quantity. Furthermore, we encourage the authors to expand the bias analysis section to include more comprehensive insights into the sources of bias and to provide examples of language-agnostic content with differing embeddings.