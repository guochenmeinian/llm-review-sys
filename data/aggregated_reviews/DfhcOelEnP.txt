ID: DfhcOelEnP
Title: cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 8, 7, 6, -1, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents cPAPERS, a novel dataset aimed at developing conversational AI systems for multimodal interactions regarding scientific papers. It includes 5,030 question-answer pairs grounded in figures, tables, and equations from NeurIPS and ICLR papers (2020-2023). The authors propose a robust methodology for data collection and processing, leveraging APIs and regex filtering to ensure accurate extraction of relevant content. Additionally, the paper benchmarks the performance of LLAMA-2-70B in zero-shot and few-shot settings.

### Strengths and Weaknesses
Strengths:  
- The dataset addresses a significant need for multimodal conversational AI in scientific research, enabling nuanced interactions based on visual and tabular data.  
- The methodology is well-documented and systematic, ensuring high-quality, contextually rich question-answer pairs.  
- The integration of diverse data types (text, equations, figures, tables) provides a comprehensive resource for future research.  
- The presentation of baseline approaches using LLMs offers valuable insights into the dataset's applicability.

Weaknesses:  
- There is a potential bias in question-answer pairs due to the expert-level nature of the questions arising from the peer review process, which may not reflect real-world scenarios.  
- The dataset's focus on NeurIPS and ICLR papers may limit its generalizability across different scientific fields.  
- The paper lacks detailed discussions on dataset usage and evaluation metrics, which could enhance understanding and reproducibility.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the filtering of "clarification questions" mentioned in line 168 and provide a rationale for this choice. Additionally, the authors should clarify the reporting of the ANOVA results on line 255, including independent and dependent variables, assumptions, and post-hoc tests. We suggest addressing the hypothesis in line 246 about the utility of referring text for tables, as it raises questions about the necessity of the summaries. 

Furthermore, we encourage the authors to implement enhanced matching mechanisms to address mismatches between figures, tables, or equations and question-answer pairs due to manuscript revisions. Expanding the dataset to include a broader range of scientific papers could improve its applicability. We also recommend introducing additional quality control measures to verify the accuracy of extracted question-answer pairs.

Lastly, we urge the authors to provide more detailed documentation on the dataset creation process, including the preprocessing pipeline and evaluation code, to enhance reproducibility and transparency.