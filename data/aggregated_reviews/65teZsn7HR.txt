ID: 65teZsn7HR
Title: Exploring Linguistic Properties of Monolingual BERTs with Typological Classification among Languages
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into how morphology and syntax are encoded in pre-trained monolingual BERT models. The authors utilize Centered Kernel Alignment to analyze the similarity between weight matrices, concluding that syntax is primarily encoded in the middle layers, while morphology is less strongly represented in the attention layer. The methodology developed for assessing linguistic phenomena is validated across multiple languages, including low-resource ones, and corroborates findings from Tenney et al. (2019) using a different approach. Additionally, the authors contribute missing features to the WALS database.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and poses a clear research question, addressing it empirically.
- It confirms previous findings by Tenney et al. (2019) through a novel methodological approach.
- The experimental setup is innovative, particularly the use of WALS for language clustering.
- The methodology has broader applicability beyond BERT, which could be emphasized more prominently.

Weaknesses:
- The paper lacks detailed discussion on how syntactic and morphological features are represented as vectors and does not adequately explain lang2vec.
- There is insufficient exploration of the impact of different training corpora on the findings.
- The significance of the research and its implications for future studies are not thoroughly articulated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the term "syntactic typological similarity" on line 077 by providing examples or concise explanations. Additionally, we suggest addressing the assumption made on line 144 regarding mBERT's exploitation of typological similarities, clarifying whether this is enforced in the training process. It would be beneficial to include a brief explanation of lang2vec on line 200, or focus on how the authors' approach differs from it. We also encourage the authors to mention any contributions made to the WALS project regarding the missing features. On line 379, the authors should discuss the potential variations due to different training corpora. Finally, we recommend providing more insight into the importance of the findings on lines 604-608, elaborating on how these results could be leveraged in future research.