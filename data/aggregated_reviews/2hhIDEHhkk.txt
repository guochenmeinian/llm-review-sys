ID: 2hhIDEHhkk
Title: Estimating Propensity for Causality-based Recommendation without Exposure Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 3, 8, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for estimating causal effects in scenarios lacking observation of the treatment variable, utilizing interactions to approximate missing exposure or propensity data. The authors propose a propensity estimation model that leverages item popularity as a strong confounder, relying on several key assumptions, including a monotonic relationship between popularity and exposure. The paper reviews existing literature thoroughly and includes a comprehensive simulation section comparing the proposed method with baselines and oracles. 

### Strengths and Weaknesses
Strengths:  
- The proposed method is innovative, effectively using strong intuitions to impute the exposure variable through a learnable objective.  
- The paper is well-written, presenting a clear motivation and thorough literature review.  
- The experimental analysis is comprehensive, providing insights into causal performance and the quality of estimated propensity scores.  

Weaknesses:  
- The evaluation framework is limited, relying on a single DLCE model and weak baselines; more advanced models should be included for comparison.  
- The technical contributions are somewhat constrained, as the relationship between popularity and propensity is similar to existing works.  
- The datasets used are small, raising concerns about the scalability and performance of the proposed method in larger contexts.  

### Suggestions for Improvement
We recommend that the authors improve the evaluation framework by including a wider range of advanced baselines and backbone models to enhance the robustness of their comparisons. Additionally, we suggest clarifying the relationship between popularity and propensity, explicitly stating whether it is correlation or causality. The authors should also consider conducting experiments on larger datasets to assess the scalability of the proposed method. Finally, providing more detailed explanations of the case study and the role of the KL-divergence-based regularization in propensity training would strengthen the paper.