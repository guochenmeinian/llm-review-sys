ID: 7FitZnnnu8
Title: Learning Directed Graphical Models with Optimal Transport
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an optimal transport-based method for learning the parameters of a directed acyclic graph (DAG) structure in Bayesian networks, accommodating incomplete data. The authors propose a framework that minimizes the Wasserstein distance between the data distribution and the model distribution, utilizing stochastic backward mappings for optimization. The method is illustrated through experiments on various probabilistic models, including LDA and HMM, demonstrating its effectiveness. Additionally, the authors provide a comparison with Variational Autoencoders (VAE) and Wasserstein Autoencoders (WAE), emphasizing the importance of exploring the independence of push-forward distributions from numeric inputs. Empirical evidence from Poisson time-series segmentation supports their theoretical claims, showing alignment between inferred and true states, and a detailed analysis of the impact of the tuning parameter $\eta$ on reconstruction loss and parameter estimation is included.

### Strengths and Weaknesses
Strengths:
1. The generalization of optimal transport to Bayesian networks is novel and interesting.
2. The paper is well-written, particularly in the initial sections.
3. The proposed method is reasonable and straightforward to implement, with a clear objective function based on established theory.
4. The evaluation on diverse datasets showcases the method's versatility and effectiveness compared to existing approaches.
5. The comparison with VAE/WAE enhances the paper's value.
6. Empirical evidence supports the theoretical claims, particularly in Poisson time-series segmentation.
7. A detailed analysis of the impact of the tuning parameter $\eta$ on reconstruction loss and parameter estimation is provided.

Weaknesses:
1. The method is limited to parameter learning and does not address learning the DAG structure itself.
2. The description of the algorithm lacks detail, particularly in Section 3, where key components like the cost function and divergence measure are not adequately explained.
3. Concerns exist regarding the similarity of the proposed method to Wasserstein autoencoders (WAE), particularly in the theoretical framework.
4. The experimental datasets are relatively simple, raising questions about the method's scalability to more complex data.
5. There is a need for deeper experimental exploration regarding the independence of push-forward distributions.
6. The paper could benefit from more comprehensive verification of the discussed intuitions.

### Suggestions for Improvement
We recommend that the authors improve the clarity and detail in the description of the proposed algorithm, particularly in Section 3, by incorporating essential information from the supplementary material into the main text. Additionally, the authors should explicitly discuss the connections and differences between their method and WAE to clarify its novelty. We also suggest considering more complex datasets in the experiments to better demonstrate the method's applicability and robustness. Furthermore, we recommend enhancing the experimental exploration of the independence of push-forward distributions $\phi(X_c)$ from numeric inputs $X_c$, and verifying the scale of the reconstruction loss and regularizer in relation to the tuning parameter $\eta$. Expanding on these aspects will strengthen the paper's findings and provide clearer insights into the model's performance. Lastly, the authors should provide a more thorough discussion of the limitations and potential shortcomings of their approach, particularly in relation to the performance of baseline methods.