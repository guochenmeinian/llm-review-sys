ID: xUjBZR6b1T
Title: ReVideo: Remake a Video with Motion and Content Control
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ReVideo, a novel approach for precise local video editing that modifies both content and motion. It introduces a three-stage training strategy and a spatiotemporal adaptive fusion module to effectively integrate edits across frames and locations. The method allows for complex editing tasks, such as changing content while maintaining motion and customizing motion trajectories. The authors also explore motion coupling between the foreground (editing region) and the background, proposing a motion decoupling training method to address the model's tendency to estimate the motion state of the foreground based on background motion. The experimental results demonstrate that ReVideo can achieve photorealistic editing while adhering to user-specified motion trajectories, showcasing impressive video quality with minimal artifacts.

### Strengths and Weaknesses
Strengths:
- The paper is the first to explore local editing of both content and motion in videos using diffusion models, showcasing a novel capability.
- The three-stage training strategy enhances robustness and effectiveness, supported by experimental validation showing superior performance compared to existing methods.
- The writing is clear and well-organized, effectively explaining complex concepts.
- The results showcase impressive video quality with minimal artifacts, and concerns raised by reviewers were adequately addressed.

Weaknesses:
- The method lacks an end-to-end editing pipeline, as the authors do not explain how the first frame is edited.
- Some original video motion is not preserved in the edited output, leading to unrealistic results in certain scenarios.
- The training process is complex and may overfit the dataset, with insufficient details on implementation specifics and computational demands.
- There is a lack of quantitative comparisons and qualitative video comparisons with other methods, limiting the evaluation of the proposed approach.
- Some reviewers perceive the work as somewhat incremental, primarily due to the combination of a ControlNet-like motion injection module with augmented training data.
- Clarity regarding the motion decoupling training method may need enhancement.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the editing pipeline by providing a detailed explanation of how the first frame is edited. Additionally, addressing the preservation of original motion in edited videos, particularly for non-rigid motions, would enhance the method's applicability. We suggest including qualitative comparisons with other methods and providing more detailed implementation specifics, including parameter settings and architecture of the spatiotemporal adaptive fusion module. Furthermore, a discussion on the computational demands and scalability of the method, along with a quantitative evaluation of the ablation studies, would strengthen the paper. Lastly, we recommend that the authors improve the clarity of the motion decoupling training method to better convey its significance and emphasize the novel aspects of the approach to address the perception of incremental contributions. Exploring the method's performance in editing multiple objects simultaneously and its applicability to text-to-video generation models would also broaden its scope.