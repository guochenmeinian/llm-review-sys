ID: EY4OHikuBm
Title: Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 7, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of the implementation details from Dreamer-v3 within the context of the Proximal Policy Optimization (PPO) algorithm, assessing the generality of these techniques in Deep Reinforcement Learning (DeepRL). The authors identify five unique implementation tricks and conduct thorough evaluations across continuous and discrete control environments, specifically the DeepMind Control Suite and Atari games. They provide evidence regarding the effectiveness of these tricks, particularly in environments with poorly normalized, unbounded rewards.

### Strengths and Weaknesses
Strengths:
- The paper contributes original research by investigating the transfer of implementation tricks from an on-policy model-based algorithm to a model-free one, which is valuable given the prevalence of such tricks in DeepRL.
- The evaluation is comprehensive, covering extensive benchmark domains, and the writing is clear, effectively communicating the authors' objectives and findings.
- The per-environment learning curves presented in the supplementary material democratize access to important insights for researchers with limited computational resources.

Weaknesses:
- The originality of the paper is limited, as the primary innovation is the adaptation of Dreamer-v3 tricks to PPO, which may not constitute a significant contribution on its own.
- The authors do not explore hyperparameter ranges for each implementation detail, which would enhance the understanding of their sensitivity.
- There is a lack of deeper analysis into subcategories of tasks within the benchmark suites, which could provide insights into why certain tricks were effective or ineffective based on task features.
- The choice of PPO as the sole algorithm for testing may restrict the generalizability of the findings, and the authors do not adequately justify this choice over alternatives like A2C.

### Suggestions for Improvement
We recommend that the authors improve the paper by including hyperparameter ranges for each implementation detail to provide clearer insights into their sensitivity. Additionally, we suggest conducting a deeper analysis of task subcategories within the benchmark suites to elucidate the relationship between task features and the effectiveness of the implementation tricks. Furthermore, exploring the application of these tricks to other algorithms, such as A2C, would strengthen the paper's contributions and generalizability. Lastly, we encourage the authors to consider reproducing the Dreamer-v3 results on the original algorithm to validate their implementation and support their claims.