ID: 3EREVfwALz
Title: Multiclass Transductive Online Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 8, 7, 7, 6, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of transductive online learning in the multiclass setting with unbounded label spaces. The authors extend the results of Hanneke et al. [2023] by characterizing the optimal mistake bound in this context. They introduce two new combinatorial parameters, the Level-constrained Littlestone dimension \(D(C)\) and the Level-constrained Branching dimension \(B(C)\), which are crucial for characterizing transductive online learning. The paper establishes a trichotomy of possible minimax rates for the realizable setting and achieves optimal rates in the agnostic setting, determined by the level-wise Littlestone dimension.

### Strengths and Weaknesses
Strengths:
1. The paper effectively characterizes the optimal mistake bounds for transductive online learners in the unbounded label space, addressing both realizable and agnostic cases.
2. The introduction of \(D(C)\) and \(B(C)\) provides new insights into transductive online learning, with comparisons to existing combinatorial parameters.
3. The technical modifications to the Halving technique and the standard optimal algorithm are noteworthy contributions that may interest the learning theory community.
4. The work significantly advances the literature on regret and mistake bounds in transductive online learning.

Weaknesses:
1. The definitions of expected regret and expected cumulative mistakes in section 2.2 lack clarity regarding the probability distribution over the unbounded label space, which is a critical aspect of the work.
2. The proof technique for the agnostic setting heavily relies on previous works, limiting the novelty of Theorem 4.
3. The relationship between \(D(C)\), \(B(C)\), and existing dimensions like the DS dimension is not sufficiently clarified in the main text, which could enhance understanding of the new parameters.
4. The proofs in sections 3.1 and 3.2 could better highlight the Halving technique's power and may benefit from deferring some details to the appendix.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions of expected regret and expected cumulative mistakes in section 2.2 by rigorously defining the probability distribution over the unbounded label space. Additionally, we suggest that the authors provide a more substantial discussion on the novelty of Theorem 4, emphasizing its contributions beyond previous works. It would also be beneficial to explicitly state in the main text why \(D(C)\) and \(B(C)\) are necessary compared to the DS dimension. Furthermore, we encourage the authors to rewrite the proofs in sections 3.1 and 3.2 to better showcase the Halving technique's effectiveness and consider moving some proof details to the appendix for improved readability.