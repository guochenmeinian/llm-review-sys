ID: DPeBX79eNz
Title: Transfer Learning with Affine Model Transformation
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 6, 4, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 2, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive investigation of transfer learning through an affine model transfer framework. The authors summarize various transfer learning approaches under affine model transformation and establish theoretical properties, including generalization error and excess risk. The proposed methods demonstrate comparable performance to baseline methods in tasks like robot arm trajectory prediction and scientific document score prediction. The authors motivate the affine model as optimal for squared loss tasks and provide an estimation algorithm using kernel methods, alongside generalization and excess risk bounds.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important topic, providing valuable insights into transfer learning, domain generalization, and model adaptation.
- The theoretical framework for affine model transfer is well-structured, and the derived generalization bounds enhance the analysis.
- The paper is well-organized and effectively communicates the intricacies of the proposed framework.

Weaknesses:
- The experimental evaluation is limited in scope; it would benefit from validation on standard transfer learning benchmarks like the MIT Indoors and Caltech 256 datasets.
- Observed improvements in experiments are marginal, with the proposed method performing worse than Fine-tuning in the Torque 7 prediction task.
- The paper lacks a discussion on the specific properties of the functions $g_1, g_2$, and $g_3$, which would aid in understanding their design and implementation.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by including widely recognized benchmark datasets to validate the proposed method's effectiveness. Additionally, clarifying the advantages of the proposed method over Fine-tuning, especially in scenarios where it performs worse, would enhance the paper's contribution. We also suggest providing insights or guidelines regarding the design of the functions $g_1, g_2$, and $g_3$, as well as specific neural network architectures, to assist readers in implementing the proposed method effectively.