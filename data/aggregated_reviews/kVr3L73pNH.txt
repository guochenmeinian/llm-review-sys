ID: kVr3L73pNH
Title: Data Attribution for Text-to-Image Models by Unlearning Synthesized Images
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for data attribution in text-to-image diffusion models, focusing on unlearning synthesized images to identify influential training examples. The authors utilize elastic weight consolidation to mitigate catastrophic forgetting and evaluate their approach through rigorous counterfactual experiments on MSCOCO, demonstrating that their method outperforms existing baselines like influence functions and feature matching approaches.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, clearly structured, and effectively motivates the problem of data attribution.
- The proposed approach is innovative and validated through extensive experiments, including both qualitative and quantitative evaluations.
- The evaluation metrics and benchmarks are well-described, showcasing the effectiveness of the method.

Weaknesses:
- The theoretical justification for how unlearning impacts overall model utility is unclear.
- The evaluations are limited to medium-scale datasets, raising questions about scalability to larger datasets.
- The results section lacks clarity, with quantitative results primarily in supplementary materials and insufficient context for interpretation.
- Some claims, such as the prevention of catastrophic forgetting, are not qualitatively verified.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the results section by including raw numbers and tables for quantitative comparisons, rather than relying solely on bar plots. Additionally, we suggest providing qualitative verification for claims regarding catastrophic forgetting and ensuring that the evaluation of the unlearning method is robust across various dataset sizes. It would also be beneficial to clarify the use of DDPM loss and its application in the methods section. Finally, we encourage the authors to incorporate textual captions in visual results to enhance comprehension.