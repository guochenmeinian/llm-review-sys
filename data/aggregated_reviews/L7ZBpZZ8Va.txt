ID: L7ZBpZZ8Va
Title: Orthogonal Subspace Learning for Language Model Continual Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents O-LoRA, a method designed to mitigate catastrophic forgetting in continual learning by employing orthogonal low-rank adaptation for sequential tasks. The authors argue that by maintaining orthogonality among task-specific LoRA parameters, they can minimize interference and enhance performance across tasks. The experimental results indicate that O-LoRA outperforms various continual learning methods, demonstrating its effectiveness in addressing the challenges of sequential task learning.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant issue in continual learning, particularly in pre-trained models, and proposes a well-structured solution that is easy to follow.
- The method shows promising results, outperforming existing continual learning approaches and demonstrating parameter efficiency without requiring task replay.

Weaknesses:
- The paper lacks a thorough comparison with state-of-the-art continual learning methods, particularly in terms of hyper-parameter optimization and sensitivity analysis.
- There is insufficient detail regarding the implementation specifics, such as dataset sizes and training parameters, which raises concerns about reproducibility.
- The claims regarding the method's effectiveness in handling unseen tasks are not adequately supported, and the training methodology inconsistencies may skew results.

### Suggestions for Improvement
We recommend that the authors improve the rigor of their experimental design by conducting an ablation study to isolate the contributions of instruction tuning and orthogonal LoRA. Additionally, please ensure that comparisons with state-of-the-art methods, such as EWC and A-GEM, are included, and clarify the hyper-parameter optimization process for all methods. To enhance reproducibility, provide detailed information on dataset sizes, training specifics, and hyperparameters for the considered baselines. Finally, consider evaluating O-LoRA's performance without instruction tuning to better understand its impact on catastrophic forgetting.