ID: rheCTpRrxI
Title: DreamHuman: Animatable 3D Avatars from Text
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 6, 8, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for generating animatable 3D human avatars from text using a pretrained 2D diffusion model and Score Distillation Sampling (SDS). The authors propose a novel approach that utilizes an implicit 3D human model to establish correspondence and condition the canonical representation on pose parameters, employs per-part optimization, and incorporates a physics-based shading formulation to optimize environment lighting. These innovations aim to enhance deformation for loose clothing, improve detail reconstruction for faces and hands, and achieve more realistic colors. Comparisons with AvatarClip and DreamFusion indicate the advantages of the proposed method.

### Strengths and Weaknesses
Strengths:
- The method demonstrates substantially better visual quality compared to AvatarClip and DreamFusion, although evidence is limited.
- The use of imGHUM enhances clothing diversity.
- Part-based optimization visibly improves visual quality.

Weaknesses:
- Concurrent works such as "Dreamavatar" and "AvatarCraft" are not mentioned in the related work.
- The comparison with AvatarClip lacks sufficient detail; results should be included in supplemental material.
- The pose-dependent nature of NeRF in canonical space raises concerns about overfitting; performance on unseen poses is unclear.
- The variability of the shape parameter $\beta$ is inadequately explained.
- The benefits of shading and optimization of spherical harmonics for environment light are insufficiently elaborated, including the albedo before shading and the modeling of irradiance.
- Animations are presented from a fixed view, making it difficult to assess the deformation of loose clothing.

### Suggestions for Improvement
We recommend that the authors improve the related work section by including concurrent papers such as "Dreamavatar" and "AvatarCraft." Additionally, the authors should provide a more comprehensive comparison with AvatarClip, including results in the supplemental material. Clarification on the performance of the method for unseen poses is necessary, as is a better explanation of the shape parameter $\beta$. The authors should elaborate on the shading and optimization of spherical harmonics, detailing the albedo and irradiance modeling. We also suggest providing visual examples of avatars with loose clothing in motion from a frontal view and conducting a user study on visual quality to further validate the results.