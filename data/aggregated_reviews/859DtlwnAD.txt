ID: 859DtlwnAD
Title: Pin-Tuning: Parameter-Efficient In-Context Tuning for Few-Shot Molecular Property Prediction
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 4, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Pin-Tuning method, a parameter-efficient in-context tuning approach for few-shot molecular property prediction (FSMPP). It introduces techniques such as the context adapter in GNN, context graph, and weight consolidation to enhance the contextual perceptiveness of pre-trained molecular encoders. The authors propose separate strategies for embedding and message passing layers, supported by Bayesian weight consolidation constraints and bottleneck adapters. Additionally, the paper provides a comprehensive analysis of molecular encoders, focusing on the effectiveness of GNNs and MLPs as state-of-the-art (SOTA) models for encoding molecular data. The authors introduce the MP-Adapter, which enhances message passing mechanisms in GNNs by integrating molecular context, and discuss the CHEF method for fine-tuning GNN-based molecular encoders. Experimental results demonstrate significant improvements in few-shot performances across various benchmarks, as well as the effectiveness of the proposed methods in fine-tuning GNNs.

### Strengths and Weaknesses
Strengths:
- FSMPP tasks are of great research value, and the Pin-Tuning system is well-designed and evaluated.
- The motivation is convincingly derived from pilot experiments, indicating the need for effective tuning methods.
- The integration of context-aware capabilities into the adapter enhances the model's performance on limited data samples.
- The paper effectively discusses the advantages of GNNs and MLPs as SOTA molecular encoders.
- The introduction of the MP-Adapter and its integration with molecular context is a significant contribution.
- The experimental results demonstrate the effectiveness of the proposed methods, particularly in fine-tuning GNNs.

Weaknesses:
- The use of adapter-based parameter-efficient tuning is not novel, as it has been previously applied in various tasks.
- The paper lacks details on the pilot experiment, including featurization and model architectures, which are necessary for assessing fairness.
- The findings regarding the Identity matrix approximation in Emb-BWC appear counterintuitive, as it overlooks parameter correlations.
- The FS-Mol experiment shows that the PAR model underperforms compared to other studies, and the choice of baseline models could be improved.
- Some variants presented in the experiments are outperformed by simpler models, indicating potential limitations in the proposed approach.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the pilot experiment by providing details on featurization, model architectures, and pre-training strategies. Additionally, it would be beneficial to include a row in Table 4 for the "size of the part that needs to be tuned" to clarify the advantages of the proposed method. The authors should also address the counterintuitive findings regarding the Identity matrix approximation in Emb-BWC and consider discussing the significance of parameter correlations in molecular property prediction tasks. Furthermore, we recommend improving the FS-Mol experiment by comparing the PAR model's performance with more suitable hyperparameters, as differences in performance are minimal. Exploring alternative backbone models, such as a GIN-encoder based ProtoNet or Neural Similarity Search version, may yield better results. Lastly, expanding the discussion on context modeling to clarify the unique contributions of their approach and enhancing the presentation of the context encoder's procedure in Figure 2 while standardizing mathematical notation would improve the manuscript's clarity and impact.