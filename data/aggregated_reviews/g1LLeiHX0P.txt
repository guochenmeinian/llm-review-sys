ID: g1LLeiHX0P
Title: Representative Demonstration Selection for In-Context Learning with Two-Stage Determinantal Point Process
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on In-Context Learning (ICL) with Large Language Models (LLMs), focusing on the critical issue of selecting a representative subset of demonstrations. The authors propose a two-stage Determinantal Point Process (DPP) method that emphasizes quality and diversity in demonstration selection. The experimental results indicate that this method significantly enhances model performance, particularly in terms of inference time and token usage compared to retrieval-based methods.

### Strengths and Weaknesses
Strengths:
- The paper addresses a timely and crucial issue in the field of large language models.
- It is well-written, clear, and easy to follow, with a persuasive analysis supporting the proposed method.
- The two-stage DPP method is simple yet effective, validated by robust experimental design and execution.

Weaknesses:
- The baseline method compared is outdated, lacking contemporary comparisons that would strengthen the results.
- The paper does not open source the code, which diminishes its contribution to the research community.
- There is insufficient exploration of the proportion of the selected subset and the potential loss of useful samples.
- The motivation for the representative-based method compared to retrieval-based approaches lacks clarity.

### Suggestions for Improvement
We recommend that the authors improve the paper by incorporating newer baseline comparisons, such as those from Xiaonan Li et al. (ACL 2023) and Zhang et al. (2022a). Additionally, we suggest that the authors provide a clearer analysis of the selected subset's proportion and investigate the potential exclusion of useful samples. Including a model frame diagram to illustrate the entire process of the proposed method would enhance understanding. Finally, we encourage the authors to make their code publicly available to bolster the paper's credibility and reproducibility.