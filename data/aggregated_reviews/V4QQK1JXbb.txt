ID: V4QQK1JXbb
Title: Graph Fairness Learning under Distribution Shifts
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on graph fairness learning under distribution shift, addressing a novel research problem. The authors provide theoretical analysis indicating that distributional shifts in training and test graphs can impact fairness. They propose an adversarial learning framework combined with a graph generation module to achieve fairness under these conditions. Experimental results demonstrate the effectiveness of their approach across various datasets.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized, with clear logic and structure, facilitating reader comprehension.
- The authors provide a thorough theoretical analysis and detailed proofs, enhancing the framework's credibility.
- Extensive experiments validate the model's effectiveness in practical scenarios, showing improved fairness on unknown-distribution datasets.

Weaknesses:
- The datasets used do not convincingly demonstrate a distribution shift; they appear somewhat artificial, lacking real-world application relevance.
- The evaluation metrics lack detailed explanations, particularly regarding their scaling and normalization.
- The paper focuses on binary classification, which may not reflect practical applications; the framework's performance in multi-classification scenarios remains uncertain.
- The ablation study lacks sufficient analysis, particularly regarding the results from adversarial training.
- Comparisons with other out-of-distribution GNN models are limited, as only one model (EERM) is evaluated.

### Suggestions for Improvement
We recommend that the authors improve the demonstration of how the datasets reflect a true distribution shift rather than a transductive learning setting. The authors should consider using datasets with real application use-cases for learning under distribution shift. Additionally, we suggest providing a more detailed explanation of the evaluation metrics and their scaling. The authors should expand their evaluation to include multi-classification tasks and compare their method against a broader range of out-of-distribution GNN models. Finally, we encourage the authors to enhance the analysis in the ablation study to clarify the impact of adversarial training on accuracy.