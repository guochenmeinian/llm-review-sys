ID: WOBhJs9gqU
Title: Dual-frame Fluid Motion Estimation with Test-time Optimization and Zero-divergence Loss
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a self-supervised method for 3D particle tracking and modeling turbulent fluid flow, utilizing a zero-divergence loss function and a splat-based implementation. The authors propose a graph-based network that combines feature extraction with test-time optimization through a Dynamic Velocimetry Enhancer (DVE) module. The methodology aims to address the challenge of large data dependency by achieving effective performance with only 1% of training samples compared to supervised methods. The approach is evaluated on synthetic data and demonstrates generalization capabilities on dissimilar datasets.

### Strengths and Weaknesses
Strengths:
- The method can train with significantly smaller datasets than current state-of-the-art methods while achieving better performance.
- Test-time optimization for particle tracking is effectively integrated.
- The use of a physically motivated loss term supports a self-supervised learning framework.
- The paper is well organized and presents a novel approach to solving the data dependency problem.

Weaknesses:
- The evaluation lacks statistical significance and ablation studies, raising concerns about the robustness of results.
- The divergence-free loss term is not consistently utilized or evaluated, particularly during test-time optimization.
- The paper's formatting is inconsistent, with odd table layouts and highlighting issues.
- There is insufficient clarity on how the 1% data subset was chosen and its impact on results.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including statistical significance tests and addressing the influence of seed initialization on results. Clarifying the role of the divergence-free loss term during test-time optimization is essential, as is providing a more thorough investigation of the method's performance on real-world datasets. Additionally, we suggest that the authors standardize the formatting of tables and figures for better readability and remove unnecessary bolding in the main text. Lastly, a clearer explanation of how the lambda parameters were chosen and the implications of using different data densities during training and inference would enhance the paper's clarity.