ID: AYLlZMmUbo
Title: Two Heads are Better Than One: A Simple Exploration Framework for Efficient Multi-Agent Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 4, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel and compute-efficient exploration method, COIN, which integrates curiosity-based and influence-based exploration strategies in multi-agent reinforcement learning (MARL). COIN quantifies the impact of each agent's actions on others to derive intrinsic rewards for influence-based exploration, while curiosity-based exploration is driven by prediction errors of local observations and global states. The effectiveness of COIN is demonstrated through experiments on three benchmarks: StarCraft II, MACO, and Google Football.

### Strengths and Weaknesses
Strengths:
1. The design of a MARL framework that effectively combines two primary exploration methods, achieving efficient exploration with minimal computational costs.
2. The proposed exploration method is adaptable to other multi-agent methods.
3. The overall clarity and comprehensibility of the paper's ideas.

Weaknesses:
1. The paper lacks novelty, primarily relying on established methods like Qmix, MI, and Prediction Error without significant innovation.
2. There is insufficient explanation on extending the method to other MARL frameworks.
3. Experimental results are inadequate, lacking final convergence states and quantitative expressions such as reward values or case studies.
4. The paper's language and presentation require significant improvement, with numerous spelling and grammar issues.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the work by providing more original contributions beyond the combination of existing methods. Additionally, the authors should clarify how to extend COIN to other MARL methods. We suggest enhancing the experimental section by including final convergence states, quantitative measures of performance, and a more thorough investigation of hyperparameter effects. Furthermore, we advise refining the language and presentation of the paper to eliminate grammatical errors and improve clarity.