ID: 4lGPSbGe11
Title: Is Cross-validation the Gold Standard to Estimate Out-of-sample Model Performance?
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 8, 5, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of bias and confidence intervals from various model evaluation methods, specifically comparing cross-validation (CV) to the plug-in estimator (training loss). The authors evaluate these methods based on their convergence to the true out-of-sample error and the validity of their confidence intervals. The main conclusions are that $K$-fold CV offers no advantage over the plug-in estimator and that the utility of leave-one-out CV (LOOCV) is questionable due to its high computational cost despite slightly better asymptotic performance.

### Strengths and Weaknesses
Strengths:
- The paper contributes significantly to the theoretical understanding of CV, which has previously lacked robust foundations.
- It presents surprising insights that challenge common practices, potentially prompting reevaluation of $K$-fold CV's utility.
- The writing is clear and concepts are well-defined, making the technical content accessible.

Weaknesses:
- The paper claims to identify necessary conditions for low bias and valid coverage but does not sufficiently justify the necessity of its assumptions.
- The implications regarding the superiority of plug-in over $K$-fold CV are nuanced and may mislead practitioners, particularly in high-stakes applications.
- The empirical results rely on a limited experimental setup that does not adequately represent practical scenarios, raising concerns about their validity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding necessary conditions in the introduction and abstract, ensuring they accurately reflect the analysis presented. Additionally, we suggest providing a more nuanced discussion of the implications of their findings for practitioners, particularly in high-stakes contexts. The authors should also consider expanding their empirical results to include more representative scenarios, such as varying the number of folds in cross-validation and exploring cases where $p > n$. Finally, enhancing the documentation of the supplementary code and addressing the concerns regarding the interval estimates would strengthen the reproducibility of their results.