ID: dB4lvScPIj
Title: SmooSeg: Smoothness Prior for Unsupervised Semantic Segmentation
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 4, 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SmooSeg, an unsupervised semantic segmentation method that groups pixels into semantic clusters without manual annotation. The authors propose a smoothness prior, enforcing that adjacent features in metric space share the same semantics, and utilize a teacher-student style predictor to update pseudo labels for smoother segmentation predictions. The approach leverages self-supervised representation learning (DINO) and introduces a pairwise smoothness loss, demonstrating state-of-the-art performance in pixel accuracy across three datasets: COCOStuff, Cityscapes, and Potsdam with a ViT-S backbone.

### Strengths and Weaknesses
Strengths:
- The method effectively utilizes advancements in self-supervised learning to generate dense representations, modeling relationships between image patches using high-level features from DINO.
- The formulation of the energy minimization objective function and the incorporation of the smoothness prior are notable contributions.
- The smoothness prior allows for direct optimization of segmentation maps without requiring K-means, leading to more coherent results.
- The paper is well-structured, and the methodology and experimental analysis are convincing, demonstrating the method's applicability and robustness across diverse datasets.

Weaknesses:
- The originality of the method is questioned as it closely resembles STEGO and SlotCon, particularly in the loss formulation.
- The architecture description is convoluted, and Figure 1 is overly complex; the term "prototypes" in the teacher-student paradigm is also confusing.
- The reliance on hyperparameters, such as CRF weights and the number of classes, raises concerns about the method's robustness and scalability in practical applications.
- The experiments are limited in scope, focusing on a restricted number of classes and lacking exploration of semi-supervised setups or different backbone architectures.
- There are minor contradictions in the text and a lack of references to supplementary material that contains important details. The authors have not adequately addressed the limitations of their method, which should be included in a dedicated section.

### Suggestions for Improvement
We recommend that the authors improve the originality by clearly distinguishing their method from STEGO and SlotCon, perhaps by conducting a more detailed analysis of the differences. Additionally, the authors should address the robustness of their approach by providing experiments that explore the sensitivity to hyperparameters and the impact of dataset-specific parameters. We encourage the authors to explicitly address the limitations of their method in a dedicated section and consider discussing ideas for future work. Expanding the scope of experiments to include more classes and semi-supervised settings would enhance the practical relevance of the findings. Furthermore, we suggest clarifying the architecture description, simplifying Figure 1, and rephrasing the term "prototypes" to avoid confusion. It is essential to reference the supplementary material in the main submission and ensure all important details are included. Lastly, including linear probing results and over-clustering experiments would strengthen the paper, as would providing semi-supervised results to enhance practical applicability.