ID: SOsiObSdU2
Title: Automatically Learning Hybrid Digital Twins of Dynamical Systems
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 8, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a neurosymbolic approach to model dynamical systems using LLMs and gradient-based optimization, demonstrating competitive performance against reported baselines. The human modeller defines the problem, priors, and target metrics in text, leading to a two-step process where the first LLM generates Python models optimized with SGD, and a second LLM distills results to generate new candidate models. Performance is evaluated on synthetic data, with the proposed model outperforming the baselines. Additionally, the paper describes an LLM-powered evolutionary multi-agent algorithm for automating hybrid digital twin composition, validated on various datasets, showing favorable performance in generalizability and sample efficiency.

### Strengths and Weaknesses
Strengths:
- The model integrates domain knowledge through LLMs.
- The proposed model outperforms reported baselines.
- The paper is well-written and includes comprehensive experimental details.
- The approach is novel and has high potential impact in hybrid modeling.

Weaknesses:
- Uncertainty exists regarding the computational budget comparison between the proposed method and baselines.
- No code is provided for replication.
- The reliance on a closed-source model (GPT-4) raises concerns about performance variability.
- The writing lacks clarity on the search space and design of hybrid models.
- The experimental evaluation primarily uses synthetic data, limiting the applicability of results.

### Suggestions for Improvement
We recommend that the authors improve clarity by specifying whether the computational budget of the baselines is similar to that of the proposed method. Additionally, providing code for replication would enhance the paper's impact. The authors should clarify the necessity of using two LLMs and explore whether a single LLM could suffice. It would be beneficial to include comparisons with existing hybrid models in the experiments and to justify the choice of polynomial order in reported results. Furthermore, we suggest discussing the implications of using LLMs and evolutionary search in critical domains, particularly in medical applications. Lastly, a more detailed explanation of the evolutionary search's computational complexity and its variability across runs would strengthen the paper.