ID: Pnv8C0bU9t
Title: LoQT: Low-Rank Adapters for Quantized Pretraining
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 4, 6, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LoQT, a method designed to efficiently train quantized models by utilizing low-rank decomposition of gradient matrices. LoQT builds on concepts from GaLore and LoRA, introducing two main contributions: 1) a low-rank factors initialization based on gradient matrix and quantization error, and 2) the formulation of low-rank gradients as adapters. The experimental results demonstrate that LoQT outperforms existing baselines on the GLUE task.

### Strengths and Weaknesses
Strengths:
1. LoQT effectively addresses memory concerns associated with optimizer states in large neural network training through low-rank decomposition of gradient matrices, while also incorporating weight quantization.
2. The authors successfully demonstrate that the GaLore strategy can be reformulated into a low-rank adapter framework, allowing for efficient training with gradient descent on one low-rank factor.
3. The proposed strategy for periodically updating pre-trained weights eliminates the need for full-rank optimizer states, requiring only the states of the low-rank factors.

Weaknesses:
1. LoQT primarily extends GaLore without providing new mathematical analysis, as the projection steps (SVD) are identical to those in GaLore.
2. The novelty of LoQT lies mainly in its initialization to compensate for quantization error, yet it reuses the NF4 strategy from QLoRA without discussing alternative methods for adapting low-rank factors to quantization.
3. The experimental evaluation adheres closely to GaLore's benchmarks, lacking assessments on standard LLMs like LLaMA-7B, which would better illustrate LoQT's computational advantages.

### Suggestions for Improvement
We recommend that the authors improve the mathematical analysis of LoQT to differentiate it more clearly from GaLore. Additionally, we suggest incorporating discussions on alternative strategies for adapting low-rank factors to quantization, such as ApiQ, Loftq, and LQLoRA. Expanding the experimental scope to include standard LLMs would provide a more comprehensive evaluation of LoQT's performance. Finally, clarifying the differences between LoQT's error compensation mechanism and existing strategies would enhance the paper's contribution.