ID: RbU10yvkk6
Title: Scaling the Codebook Size of VQ-GAN to 100,000 with a Utilization Rate of 99%
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 7, 6, -1, -1, -1, -1
Original Confidences: 5, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VQGAN-LC (Large Codebook), an innovative image quantization model that significantly expands the codebook size to 100,000, achieving a utilization rate of over 99%. Unlike traditional methods that optimize individual codebook entries, VQGAN-LC initializes its codebook with 100,000 feature centers from a pre-trained vision encoder and trains a projector to align these features with the encoder's distributions. This approach allows for substantial improvements in image reconstruction, classification, and generative tasks compared to earlier models like VQGAN-FC and VQGAN-EMA.

### Strengths and Weaknesses
Strengths:
1. The extensive number of experiments conducted, with diverse datasets and metrics, effectively demonstrates the proposed method's effectiveness.
2. The introduction of a trainable projector that ensures nearly full codebook utilization throughout training is a significant contribution.
3. The ability to scale the codebook size to over 100,000 entries without incurring significant computational costs is critical.

Weaknesses:
1. The focus on scaling codebook size and utilization rates may be seen as an incremental advance rather than a novel approach, heavily relying on existing architectures.
2. The related work section lacks comprehensive coverage of significant contributions addressing codebook usage and collapse, which diminishes the literature review's depth.
3. There is no hypothesis explaining why the proposed method works or how it increases utilization rates, leading to a lack of theoretical foundation.
4. The paper does not adequately address the quality and representational span of the codebooks, which raises questions about the robustness of the proposed method.
5. Inconsistent step sizes in experiments and a lack of comparisons with other methods utilizing similar codebook sizes weaken the analysis.

### Suggestions for Improvement
We recommend that the authors improve the related work section by including recent papers addressing codebook utilization and collapse, such as [1] and [2]. Additionally, providing a theoretical foundation for the method, including a hypothesis on its effectiveness, would enhance the paper's rigor. We suggest conducting experiments to analyze the coverage and distribution of codebook entries, possibly through t-SNE visualizations, to demonstrate the quality of representations. Furthermore, clarifying the rationale behind the step size choices in experiments and comparing VQGAN-LC with other methods at similar codebook sizes would strengthen the analysis. Lastly, addressing the scalability of the method for larger datasets and higher resolutions would provide a more comprehensive understanding of its applicability.