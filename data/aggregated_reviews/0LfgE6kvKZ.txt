ID: 0LfgE6kvKZ
Title: Local Superior Soups: A Catalyst for Model Merging in Cross-Silo Federated Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 5, 5, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Local Superior Soups (LSS), a method designed to minimize communication rounds in federated learning (FL) by utilizing pre-trained models. LSS addresses data heterogeneity through sequential model interpolation, maintaining connectivity, and incorporating diversity and affinity regularization terms. The approach enhances training efficiency, making it suitable for edge computing applications. The authors conduct rigorous experiments across multiple datasets to demonstrate LSS's effectiveness.

### Strengths and Weaknesses
Strengths:
1. The proposed method effectively reduces communication rounds in federated learning using pre-trained models.
2. The method appears sound and is well articulated in the paper.
3. Extensive experiments illustrate the effectiveness of LSS across various datasets.

Weaknesses:
1. The experiments are limited to two small-scale image datasets; more large-scale datasets and other modalities should be included.
2. The distinction between LSS and similar methods like FedProx is not clearly articulated.
3. The notation in section 3 is unclear, leading to potential confusion.
4. The mathematical details in subsection 3.3.1 regarding random interpolation are insufficient.
5. The requirement for clients to receive the interpolated model pool could introduce significant communication overheads.
6. The connection between Theorem 3.1 and the diversity and affinity terms lacks clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of notation in section 3 to avoid confusion, particularly distinguishing between the number of data and the number of averaged models. Additionally, we suggest that the authors explore more pre-trained models and incorporate a wider range of tasks beyond image classification in their experiments. It would be beneficial to clarify the relationship between LSS and FedProx, as well as provide more mathematical detail in subsection 3.3.1. Furthermore, addressing the potential communication overheads associated with the interpolated model pool would strengthen the paper. Lastly, we encourage the authors to reference relevant literature or conduct preliminary experiments to support their claims regarding the limitations of previous model soup methods.