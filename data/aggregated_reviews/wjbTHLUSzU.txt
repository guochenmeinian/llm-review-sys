ID: wjbTHLUSzU
Title: TSDS: Data Selection for Task-Specific Model Finetuning
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 3, 5, 8, 5, -1
Original Confidences: 5, 3, 2, 3, -1

Aggregated Review:
### Key Points
This paper presents a method for data selection in foundation model fine-tuning, framing it as an optimization problem based on optimal transport for distribution alignment. The authors propose a distribution alignment loss, a regularizer for diversity, and kernel density estimation to mitigate the impact of near-duplicates. The experimental results indicate that their method is competitive with state-of-the-art approaches for task-specific instruction fine-tuning and domain-specific continued pretraining.

### Strengths and Weaknesses
Strengths:
- The proposal addresses the important issue of data selection for fine-tuning foundation models, which is crucial for improving data quality.
- The formulation of data selection as an optimal transport problem is well-defined, and the introduction of KNN-based algorithms demonstrates superior performance compared to baseline systems.
- The paper is well-structured, with clear theoretical motivation and strong empirical results, particularly for small sample sizes.

Weaknesses:
- The novelty and contribution of the proposed method are limited, as data selection is a well-studied area in machine learning, and the authors do not provide a comprehensive comparison with existing methods.
- The connection between data selection and the optimal transport problem is not clearly established, leaving some aspects of the methodology ambiguous.
- The experimental section lacks clarity regarding parameter settings and comparisons with various baseline systems, which could hinder understanding of the results.
- The authors only test their method on language models, raising questions about its applicability to other foundation models.
- There is a need for additional empirical comparisons to contextualize efficiency claims against state-of-the-art methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the connection between data selection and the optimal transport problem, possibly by providing more intuitive explanations. Additionally, the authors should conduct a comprehensive comparison with existing data selection methods and include more baseline systems in their experiments. To enhance the robustness of their findings, we suggest testing the proposed method on a wider range of foundation models beyond language models. Furthermore, reporting the time cost associated with fine-tuning using both the full dataset and the selected dataset would provide valuable insights into the efficiency of the proposed approach. Lastly, including discussions on other efficient regularized optimal transport algorithms, such as Sinkhorn distances, could enrich the theoretical framework of the paper.