ID: yh0OkiUk5h
Title: FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 3, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a contrastive learning method for graph representation, named FIGURe, which aims to learn node embeddings in an unsupervised manner by maximizing mutual information between local and global representations. The authors propose using multiple filters to capture different aspects of graph structure and combine these representations for downstream tasks, while also exploring compact representations using Random Fourier Features (RFF) to mitigate accuracy loss. The method shows consistent performance across various datasets, outperforming several state-of-the-art (SOTA) methods on 4 out of 8 datasets, although it is slower than some competing methods. The supplementary material includes a comparative analysis with models like GPRGNN and H2GCN, highlighting their performance across various datasets. The authors also demonstrate that their approach disentangles the representation learning process from the acquisition of combination coefficients.

### Strengths and Weaknesses
Strengths:
- The results indicate improvements over other self-supervised approaches, achieving high average performance across datasets despite not ranking first in every benchmark.
- Smaller representation versions of FIGURe yield good results, demonstrating the effectiveness of the authors' dimensionality reduction strategy.
- The method shows superior performance among unsupervised learning techniques, outperforming SOTA methods on multiple datasets.
- The authors provide extensive experimental results and analyses, including additional experiments on encoder layers and alternative projection techniques.
- The paper is well-written, and the preliminary section is concise and informative.

Weaknesses:
- The study is limited to a 1-layer GCN, leaving questions about performance with deeper architectures.
- Justifications for the need for smaller representations due to the cost of contrastive learning are insufficient; details on training epochs compared to baselines are lacking.
- The shared encoder is presented as an efficiency measure, but performance improvements suggest it may also enhance results.
- The experimental section lacks detail, particularly regarding training and evaluation recipes, which raises concerns about the fairness of comparisons and reproducibility.
- Some reviewers noted that the contributions may appear incremental despite clarifications provided during the rebuttal.
- There is inconsistency in the use of the term "augmentation," which requires clearer explanation.

### Suggestions for Improvement
We recommend that the authors improve the justification for the need for lower-dimensional representations by providing detailed comparisons of training epochs with baselines. Additionally, clarifying the influence of the number of filters on training time and ensuring that baselines are evaluated under similar conditions would enhance the fairness of comparisons. We suggest expanding the experimental section to include more details on the training and evaluation processes, as well as conducting ablation studies to clarify the contributions of RFF and filter augmentations. Furthermore, we recommend improving the clarity of the term "augmentation" by using it consistently throughout the paper and providing a detailed explanation in Section 5. Finally, addressing the mathematical errors and inconsistencies in notation throughout the paper is crucial for improving clarity and comprehension.