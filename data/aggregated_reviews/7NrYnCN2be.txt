ID: 7NrYnCN2be
Title: Boosting Semi-Supervised Scene Text Recognition via Viewing and Summarizing
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 7, 6, 6, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a ViSu framework aimed at enhancing text recognition through a viewing and summarizing paradigm. The authors generate diverse text images to guide the model's focus on various text styles during the viewing process. They identify a significant flaw in existing character consistency loss and introduce a Character Unidirectional Alignment (CUA) loss to cluster characters across different styles. The proposed framework demonstrates superior performance compared to existing methods across multiple datasets.

### Strengths and Weaknesses
Strengths:
1. The design of the Online Generation Strategy (OGS) achieves notable improvements without incurring additional human costs.
2. The authors effectively highlight a critical theoretical flaw in existing metric learning methods regarding consistency loss and provide a solution through the CUA loss.
3. The proposed method achieves state-of-the-art performance on both common and challenging datasets.
4. The framework can be seamlessly integrated with various text recognition models.

Weaknesses:
1. To further validate the effectiveness of OGS, the authors should compare its performance against that of normal synthetic data.
2. Table 5 lacks a comparison with the baseline model.
3. In Table 5, MAERec-B appears to outperform on the Union-14m dataset.
4. There is insufficient clarity regarding the setup and training of CRNN-ViSu and TRBA-ViSu in Table 1.
5. Although the method shows high average accuracy on some benchmarks, it does not achieve state-of-the-art results on others, such as SVT and CUTE.
6. The method's approach to recognizing text with vertical reading order lacks clarity on how to determine the rotation direction.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the descriptions regarding the setup and training of CRNN-ViSu and TRBA-ViSu in Table 1. Additionally, we suggest incorporating a comparison of OGS with normal synthetic data to further validate its effectiveness. The authors should also address the performance discrepancies noted in Table 5 and clarify the method for handling vertical text recognition. Finally, enhancing the writing quality and providing more detailed explanations of OGS and Unified Representation Forms would strengthen the paper.