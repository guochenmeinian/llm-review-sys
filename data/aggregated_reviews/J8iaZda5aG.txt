ID: J8iaZda5aG
Title: Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the impact of tokenization methods on the predictability of word reading times in large language models (LLMs), specifically examining whether LLM-based predictions using subword tokenization differ from those using morphological segmentation. The authors compare surprisal estimates derived from orthographic, morphological, and BPE tokenization against reading time data from various corpora. The findings indicate that the predictive power of surprisal across different tokenization schemes is not significantly different, suggesting that subword tokenization does not have catastrophic implications for psycholinguistic research. The authors advocate for further exploration of morphological segmentation, particularly in morphologically richer languages.

### Strengths and Weaknesses
Strengths:
- The paper addresses a relevant research question regarding the influence of tokenization on reading time predictability, appealing to both cognitive scientists and the NLP community.
- The methodology is presented clearly, facilitating replication and future research.
- The findings have significant implications for understanding cognitive processes in LLMs and their application in psycholinguistic studies.

Weaknesses:
- Concerns are raised about the judgment criteria used to evaluate the impact of tokenization methods, suggesting a need for more nuanced evaluation metrics.
- The overall predictive power of the surprisal models is questioned, with calls for a deeper exploration of the theoretical implications of the findings.
- The paper lacks a discussion on the cognitive plausibility of LLMs in predicting reading times, particularly given their differences from human processing.
- The scope is limited to English, reducing the generalizability of the findings; further research in typologically different languages is recommended.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the theoretical implications of their findings, particularly addressing the cognitive plausibility of LLMs in predicting reading times. It would be beneficial to clarify the choice of tokenization methods and explore additional evaluation criteria for the impact of tokenization on reading time predictions. Additionally, we suggest extending the research to include typologically different languages with richer morphology to enhance the generalizability of the results. Finally, addressing the concerns regarding the low predictive power of the surprisal models would strengthen the paper's contributions to the field.