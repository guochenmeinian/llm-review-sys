ID: h3k2NXu5bJ
Title: Certified Machine Unlearning via Noisy Stochastic Gradient Descent
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 4, 7, 6
Original Confidences: 2, 3, 4

Aggregated Review:
### Key Points
This paper presents a machine unlearning algorithm based on projected noisy gradient descent (PSGD) that addresses a significant problem in the field. The authors establish theoretical guarantees for unlearning under the assumption of convex and smooth losses, demonstrating that the privacy parameter Îµ decreases exponentially with each epoch. The algorithm employs the same batches for both learning and unlearning, utilizing Gaussian noise, and can handle multiple unlearning requests effectively.

### Strengths and Weaknesses
Strengths:  
1. The algorithm is simple and easy to understand, with effective experimental validation showing it outperforms baseline methods with reduced gradient complexity.  
2. The theoretical analysis provides a solid foundation for unlearning guarantees in convex smooth losses, highlighting the privacy-utility-complexity trade-off related to mini-batch size.  
3. The paper is well-written and accessible, with good illustrations of concepts.

Weaknesses:  
1. The requirement for smoothness in the loss function may limit the algorithm's applicability.  
2. The use of the same batches for both learning and unlearning raises concerns about the validity of the unlearning process.  
3. There is a lack of utility bounds, making it unclear what utility the algorithm achieves after multiple unlearning iterations.  
4. Several technical aspects, such as the definition of certain notations and parameters, are unclear or missing, which could hinder understanding.

### Suggestions for Improvement
We recommend that the authors improve clarity by defining all notations explicitly, such as the quantity \( h_{\#}\mu \) in Lemma F.2 and the parameters of Algorithm 1, including \( \eta \) and \( \sigma \). Additionally, we suggest that the authors address the conceptual concern regarding the triviality of convergence to the optimum in strongly convex cases and provide utility bounds to clarify the algorithm's performance after K iterations of unlearning. It would also be beneficial to explain the rationale behind using the same batches for both processes and to consider extending the theoretical guarantees to broader classes of losses beyond convex and smooth functions. Lastly, we recommend renaming "contractive noisy iteration" to a more conventional term like Projected SGD for clarity.