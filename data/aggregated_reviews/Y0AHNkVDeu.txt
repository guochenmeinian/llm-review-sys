ID: Y0AHNkVDeu
Title: Efficient Parallelization Layouts for Large-Scale Distributed Model Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 5
Original Ratings: 7, -1, -1, -1, -1
Original Confidences: 3, 5, 5, 4, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive analysis of various distributed training strategies for large language models (LLMs), focusing on the efficiency of different parallelization techniques, including tensor and pipeline parallelism. The authors propose insights into the integration of FlashAttention with these strategies, although no original algorithm is introduced. The experiments are conducted on a substantial platform, utilizing 32 NVIDIA DGX nodes, which enhances the relevance of the findings.

### Strengths and Weaknesses
Strengths:
- The manuscript is well-structured and clearly written, with a thorough documentation of findings.
- The range of distributed training strategies considered is extensive and beneficial for guiding LLM training.
- The insights on combining efficient training techniques with FlashAttention are valuable for practitioners.

Weaknesses:
- The comparison of the MFU with established frameworks lacks rigor, and the balance between tensor and pipeline parallelism is not well defined.
- The search space does not include ZERO-1,2,3, which may impact the evaluation of activation checkpointing/model parallelism.
- The paper does not provide algorithmic contributions or an easy way to configure various parameters, relying instead on brute-force exploration of configurations.

### Suggestions for Improvement
We recommend that the authors improve the consistency in nomenclature, particularly regarding the distinction between tensor and pipeline parallelism, as noted in Section 4.4. To enhance the conclusions, we suggest increasing the scope of configurations in Fig. 4 and including additional models to clarify trends. The MFU comparisons in Tab. 2 should be based on consistent configurations, such as the number of GPUs and batch size. We advise the authors to provide additional results with a higher number of GPUs and consider adding scaling plots to illustrate performance impacts. Furthermore, a discussion on the communication library used in the study is essential, as it significantly affects parallel efficiency. Lastly, including brief descriptions of ZERO and FlashAttention, as well as illustrations of pipelined model parallelism, would improve the paper's accessibility and clarity.