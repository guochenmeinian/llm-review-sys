ID: 6HO33urpaI
Title: Open-Book Neural Algorithmic Reasoning
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 6, 6, 6, -1
Original Confidences: 2, 2, 4, -1

Aggregated Review:
### Key Points
This paper presents an open-book learning framework that enhances algorithmic reasoning by allowing networks to utilize the entire training dataset during reasoning. The authors propose a general framework for open-book Neural Algorithmic Reasoning (NAR), demonstrating improved performance on the CLRS Algorithmic Reasoning Benchmark and revealing intrinsic connections between tasks through an attention-based mechanism. The framework also shows promise in multi-task training, achieving performance comparable to the best existing multi-task NAR algorithms.

### Strengths and Weaknesses
Strengths:
- The innovation of using training datasets to enhance algorithmic reasoning tasks is notable.
- The paper is well-structured, clear, and easy to read, with a smooth flow of ideas.
- The experimental analysis is rigorous, providing sufficient support for the claims made.

Weaknesses:
- The introduction of additional memory may lead to significant storage overhead with large training sets.
- The proposal bears similarities to retrieval augmented generation (RAG) in NLP, which requires further discussion.
- Concerns exist regarding the robustness and generalization of the open-book NAR model, particularly its performance on out-of-domain data.
- The related work section is too brief, lacking references to key literature.
- There is insufficient ablation for architectural design decisions and hyper-parameter descriptions should be included in the appendix.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the implications of testing with unseen datasets and address concerns regarding the potential overfitting of features learned during training. Additionally, we suggest including a comparison with the algorithm and dataset presented in `[3]` to validate generalization claims. The authors should also expand the related work section, provide more detailed ablation studies, and include hyper-parameter descriptions in the appendix. Finally, reiterating the metrics used and exploring the learning of specific algorithms in multi-task setups would enhance the paper's depth.