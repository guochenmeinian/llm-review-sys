ID: jfaJdk29k4
Title: Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for Question Generation over Knowledge Bases (KBQG) utilizing Large Language Models (LLMs) and Chain-of-Thought (CoT) prompting. The authors propose a two-step approach: (1) Supportive Logic Form Selection, where structural knowledge is encoded and diverse logic forms are selected using K-Means clustering, and (2) Prompt Construction, where reasoning chains are created from these forms to facilitate LLM in-context learning for KBQG tasks. The work is notable for being among the first to explore LLM capabilities in KBQG.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a timely challenge in KBQG, particularly in low-resource settings, and presents a novel approach leveraging CoT prompting.  
- Empirical results demonstrate effectiveness, surpassing established benchmarks on the PathQuestions dataset, providing robust evidence for the proposed techniques.

Weaknesses:  
- The performance gains of KQG-CoT over existing methods like Auto-CoT are marginal, raising questions about the necessity of the logic form approach.  
- The methodology appears incremental, with prior works having proposed similar clustering algorithms and discussed the importance of demonstration order.  
- The effectiveness of the Structured Encoding and Clustering requires further evaluation, and the differentiation from existing approaches is insufficiently articulated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their methodological advancements to ensure a more substantial contribution to the field. Additionally, a more thorough and well-documented human evaluation should be included in the next version to better assess effectiveness beyond string overlap-based methods. We also suggest that the authors address the following questions:  
- For Prompt Construction, consider comparing the bottom-up approach with a top-down approach as seen in previous works.  
- Provide supportive quantitative metrics or visualizations to demonstrate the diversity of sampled k structures.  
- Clarify how semantic diversity/similarity is measured.  
- Ensure that the specific contributions of this paper in relation to Auto-CoT are clearly articulated.