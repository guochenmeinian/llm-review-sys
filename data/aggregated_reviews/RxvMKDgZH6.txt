ID: RxvMKDgZH6
Title: Accelerating Multiple Intent Detection and Slot Filling via Targeted Knowledge Distillation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Targeted Knowledge Distillation Framework (TKDF) aimed at improving non-autoregressive Spoken Language Understanding (SLU) models by leveraging knowledge from a multi-intent autoregressive teacher model. The authors demonstrate that their approach not only accelerates inference speed but also achieves performance comparable to state-of-the-art models, with the distilled student model outperforming baseline models trained on the entire original dataset using only 4% of the original data.

### Strengths and Weaknesses
Strengths:
1. The proposed curriculum learning method effectively selects high-quality data to mitigate the multi-modality problem, leading to competitive performance.
2. The paper is well-written and motivates the approach clearly, showing significant speedup over autoregressive SLU models.

Weaknesses:
1. The framework details are difficult to follow, requiring additional rewriting and clarification, particularly regarding the definitions of original, distilled, and selected data.
2. The performance improvements are not convincingly significant, and the connection between multi-modality motivation and results is weak, with a lack of detailed ablation studies.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the framework by providing clearer definitions and explanations of key terms, such as original data, distilled data, and selected data. Additionally, we suggest including metrics for the four situations in Table 2 to substantiate performance claims and conducting a more thorough analysis on MixSNIPS to demonstrate the necessity of the teacher-student model. A detailed ablation study is also needed to strengthen the novelty of the approach and its relation to the multi-modality issue.