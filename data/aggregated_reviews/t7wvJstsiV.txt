ID: t7wvJstsiV
Title: SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 5, 4, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Self-Evolution Decoding (SED), a novel decoding method designed to enhance the factuality of large language models (LLMs) without relying on external knowledge or fine-tuning. SED estimates the "inner knowledge distribution" by analyzing the differences between logits from various layers and the final layer, which is then used to adjust the model's output logits. Experiments across multiple datasets demonstrate that SED significantly improves factual accuracy compared to existing methods. Additionally, the authors provide a comprehensive response to reviewer feedback, detailing enhancements made to the methodology, including additional ablation studies and results on realistic open-ended generation scenarios. They emphasize their commitment to addressing all concerns raised, particularly regarding computational costs, theoretical analysis, and the reliability of their method through discussions of error bars and statistical significance.

### Strengths and Weaknesses
Strengths:
- The paper introduces an innovative approach to improving LLM factuality without external data or fine-tuning.
- Comprehensive evaluations across various benchmarks show that SED outperforms existing methods like DoLa, achieving notable improvements in factual accuracy.
- The authors have effectively incorporated detailed explanations and additional results based on reviewer suggestions.
- The commitment to enhancing clarity and addressing concerns demonstrates a proactive approach to improving the manuscript.

Weaknesses:
- The theoretical justification for using logit differences as an approximation for the gradient of KL divergence is questionable, as it lacks rigorous support.
- The paper does not adequately explore the inner workings of SED, focusing primarily on task performance rather than providing experimental insights into its mechanisms.
- The structure of the paper is confusing, with inconsistent notation and unclear explanations of key concepts.
- Some reviewers still perceive significant concerns remaining, indicating that further revisions may be necessary.
- The need for substantial editing and polishing has been highlighted, suggesting that the current version may not meet the expected standards.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of SED to provide a clearer understanding of its mechanisms and justifications for the approximations used. Additionally, including more detailed experimental results that explore the accuracy and limitations of the inner knowledge distribution would strengthen their argument. We also suggest enhancing the paper's structure and clarity, particularly in addressing the confusing notation and concepts, to facilitate better comprehension for readers. Furthermore, we recommend improving the clarity and presentation of their methodology by integrating key formulas and discussions more seamlessly into the main content. Prioritizing the inclusion of new results and ablation studies while moving less critical details to the appendix would ensure that all important results are clearly cited. Finally, we encourage the authors to address any remaining concerns from reviewers comprehensively to enhance the manuscript's overall quality and report error bars or measures of statistical significance to enhance the robustness of the findings.