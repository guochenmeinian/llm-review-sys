ID: 3BNPUDvqMt
Title: Better by default: Strong pre-tuned MLPs and boosted trees on tabular data
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 4, 7, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RealMLP, an enhanced multilayer perceptron (MLP) designed for tabular data, along with optimized default parameters for both RealMLP and gradient-boosted decision trees (GBDTs). The authors conduct extensive benchmarking across various datasets, claiming that RealMLP achieves a superior time-accuracy tradeoff compared to other neural networks and remains competitive with GBDTs. Additionally, the paper provides a comprehensive analysis of various neural network architectures, focusing on the importance of meta-learning and default hyperparameters. The authors propose enhancements to the MLP model and evaluate its performance against other methods, including TabR-S-D, while acknowledging the challenges of comparing different architectures and the limitations of their benchmarks. They also discuss the use of label smoothing and its implications for model performance.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and provides sufficient detail for reproducibility.
- Comprehensive benchmarking on a large set of datasets enhances the credibility of the findings.
- The authors' improvements in RealMLP are described in detail, allowing for potential recreation of the method.
- The paper provides extensive results and a thorough exploration of the impact of meta-learning on model performance.
- The authors have addressed several reviewer concerns and clarified the rationale behind their methodological choices.
- The inclusion of diverse benchmarks enhances the robustness of the findings.

Weaknesses:
- The claim that RealMLP outperforms other neural networks and is competitive with GBDTs is inconsistently supported by the experimental results, particularly in Fig. 2.
- The necessity of creating a new benchmark suite is not adequately justified, given existing robust benchmarks in the literature.
- The choice of shifted geometric mean error as a metric is problematic, and multiple metrics should be reported for clarity.
- The comparison of default, tuned-default, and hyperparameter optimization (HPO) configurations lacks a comprehensive overview, particularly regarding the performance of tuned defaults.
- There are inconsistencies in baseline usage, which may lead to misleading interpretations of the results.
- Some components, such as label smoothing, are deemed unusual in the tabular context and require further clarification.
- The authors do not strive for fairness in comparisons between different architectures, which may limit the applicability of their findings.

### Suggestions for Improvement
We recommend that the authors improve the justification for the necessity of the new benchmark suite and clarify why it is the fairest choice for their methods. Additionally, we suggest including a broader range of deep tabular methods in the core experiments to strengthen the comparisons. The authors should also consider reporting multiple metrics instead of relying solely on shifted geometric mean error. Furthermore, enhancing the visibility of the 95% confidence intervals in the figures would improve clarity. We recommend that the authors improve the clarity of their baseline comparisons by ensuring that all included methods are consistently evaluated across benchmarks. Additionally, we suggest that the authors provide a more detailed discussion on the implications of using label smoothing and consider framing their findings as a general recipe for deep learning methods. It would also be beneficial to explicitly mention the use of random search in the paper to justify their methodological choices. Lastly, we encourage the authors to explore the use of spider charts to visually represent the performance of different algorithms across various metrics.