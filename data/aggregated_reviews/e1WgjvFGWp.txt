ID: e1WgjvFGWp
Title: Large Language Models of Code Fail at Completing Code with Potential Bugs
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 7, 4, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into code completion tasks for large language models (LLMs) when faced with buggy prefixes. The authors introduce two benchmarks: buggy-HumanEval, which incorporates synthetic bugs, and buggy-FixEval, derived from real user submissions. The evaluation reveals that performance declines significantly with buggy prompts, and the authors explore various mitigation techniques to address this issue. Additionally, the study emphasizes the challenges posed by data imbalance and domain shifts between synthetic and real bugs, proposing a two-stage training approach to mitigate these issues. The authors also plan to refine the definition of "potential bugs" for clarity.

### Strengths and Weaknesses
Strengths:
- The authors introduce two valuable datasets that can serve as benchmarks for buggy code completion, enhancing research in this area.
- The paper addresses an important and relevant problem, demonstrating a solid understanding of the challenges faced by LLMs in real-world scenarios.
- The evaluation is thorough, providing insights into the performance of different models and mitigation strategies.
- The authors provide a thorough discussion on the implications of data imbalance and domain shifts, referencing relevant literature.
- The inclusion of new experimental results enhances the understanding of model performance across different completion tasks.

Weaknesses:
- The methodology for evaluating code completion with buggy prefixes is flawed, as it conflates bug fixing with code completion, making it difficult to isolate the effects of the buggy prefix.
- The post-hoc mitigation techniques utilize a BERT-based model, which may not be as effective as more advanced models, potentially limiting performance improvements.
- The paper lacks a theoretical analysis of the conditions under which buggy prefixes may or may not satisfy completion requirements, and it does not adequately address data imbalance in real-world bug distributions.
- The results for clean prefix + buggy-based completion in FixEval are unexpectedly low compared to HumanEval, raising questions about model adaptability.
- The definitions used in the paper, while set to be refined, may still lack precision.

### Suggestions for Improvement
We recommend that the authors improve the evaluation methodology by decoupling the bug fixing and completion tasks, possibly by using clean prefixes for testing to assess completion accuracy independently. Additionally, consider employing a more powerful model for the post-hoc mitigation techniques to potentially enhance performance. The authors should also provide a theoretical framework for understanding the relationship between buggy prefixes and completion success, and address the implications of data imbalance in their results. Furthermore, we suggest including a more detailed analysis of the low performance in FixEval for clean prefix + buggy-based completion, as well as incorporating the "clean prefix + buggy-based completion" results in the final tables for stronger models. Finally, improving the clarity of the definition of "potential bugs" would enhance understanding.