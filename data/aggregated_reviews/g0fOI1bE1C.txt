ID: g0fOI1bE1C
Title: Transformers are efficient hierarchical chemical graph learners
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: 6, 6, -1
Original Confidences: 4, 3, 4

Aggregated Review:
### Key Points
This paper presents a novel molecular graph representation learning architecture called SubFormer, which integrates message-passing neural networks (MPNNs) and Graph Transformers to enhance computational efficiency and address challenges like over-smoothing and over-squashing. The SubFormer operates in two stages: first, it aggregates local graph-level information using a shallow MPNN to create a coarse-grained junction tree; second, it applies a Graph Transformer to this representation to learn long-range interactions while reducing computational costs. The authors demonstrate that SubFormer performs competitively with GraphGPS and surpasses other Graph Transformer and MPNN architectures in molecule property prediction benchmarks, achieving this at a lower computational cost.

### Strengths and Weaknesses
Strengths:
- The approach is simple and intuitive, effectively addressing computational efficiency and long-range interaction learning.
- The model's attention weights correspond to chemically meaningful fragments, confirming its alignment with chemical intuition.
- The innovative combination of GNNs, clustering, and Transformers shows promise in tackling molecular analysis challenges.

Weaknesses:
- The paper lacks quantification of the computational cost reduction, which is essential to substantiate claims of efficiency compared to other architectures.
- Over-smoothing remains a concern for SubFormer, unlike in GraphGPS, and potential future solutions are not discussed.
- The justification for using Graph Transformers over other architectures for learning on the junction tree is unclear, and further experimentation is suggested.
- The organization of the paper is disjointed, particularly the transition between sections, and figures are poorly presented and difficult to read.

### Suggestions for Improvement
We recommend that the authors improve the paper's organization by merging Section 2 with the methodology to create a cohesive narrative. Additionally, we encourage the authors to quantify the computational complexity of SubFormer compared to state-of-the-art models using metrics such as FLOPS, parameters, execution time, and memory utilization. Addressing the over-smoothing issue with potential strategies while maintaining interpretability would enhance the paper. Furthermore, we suggest providing a clearer justification for the use of junction trees and exploring alternatives. Lastly, improving the clarity and legibility of figures is essential for better presentation.