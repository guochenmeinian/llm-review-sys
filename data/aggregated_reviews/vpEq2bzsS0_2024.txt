ID: vpEq2bzsS0
Title: MoTE: Reconciling Generalization with Specialization for Visual-Language to Video Knowledge Transfer
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the MoTE (Mixture-of-Temporal-Experts) framework aimed at enhancing the generalization and specialization capabilities of visual-language models (VLMs) for video tasks. The authors propose several key contributions: the introduction of Weight Merging Regularization to balance generalization and specialization, the use of temporal feature modulation to improve generalization during inference, and the application of a Mixture of Experts (MoE) approach to effectively learn domain-specific knowledge. The framework demonstrates competitive results across various video datasets, including Kinetics-400, Kinetics-600, UCF-101, and HMDB-51.

### Strengths and Weaknesses
Strengths:
- The introduction of Weight Merging Regularization and temporal feature modulation offers a novel approach to balancing generalization and specialization in video recognition.
- The experimental results are comprehensive, showcasing the effectiveness of the proposed methods across multiple datasets.
- The manuscript is well-written and presents a clear motivation for the use of a mixture of experts to enhance knowledge acquisition.

Weaknesses:
- The framework's reliance on video category names limits the richness of textual representations; expanding the semantic space with large-scale generative models could enhance performance.
- The method explores limited forms of additional parameters, and extending it to other forms could improve generality and versatility.
- There is ambiguity in symbol usage, particularly with the symbol L representing both the loss function and the number of layers, which could confuse readers.
- The model's performance on more diverse datasets requires further validation, and the additional complexity from Weight Merging Regularization may increase training time, posing challenges for real-time applications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of symbol usage to avoid confusion, particularly regarding the representation of the loss function and the number of layers. Additionally, we suggest providing more details on how expanding the text space with large-scale generative models might enhance model performance. It would also be beneficial to include experiments validating the plug-and-play characteristic of the modulation module and to explore the model's performance with alternative network architectures. Finally, addressing the computational overhead introduced by additional components and providing a deeper analysis of the specific actions each expert excels at recognizing would enhance the paper's comprehensiveness.