ID: 7UvOkmrB8V
Title: Approximating CKY with Transformers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the feasibility of implementing the CKY algorithm using Transformers. The authors conduct two experiments: training a neural constituency parser and training on a synthesis dataset generated by a predefined PCFG. They achieve competitive performance compared to span-based parsers with CKY while demonstrating faster parsing speed. However, they find that Transformers struggle with highly ambiguous grammars, indicating they cannot fully implement CKY. The authors propose techniques such as weight sharing across layers, copy gates, and gradient decoding to enhance performance. The study's contributions include a novel gradient decoding strategy and thorough evaluations across multiple languages and datasets.

### Strengths and Weaknesses
Strengths:
- The research addresses a significant question regarding Transformers and CKY, providing valuable insights into their operational mechanisms.
- The proposed gradient decoding strategy is innovative, and the experiments are comprehensive, covering diverse languages and synthetic datasets.

Weaknesses:
- The primary contribution of the first experiment is diminished due to existing literature demonstrating similar results with local classification loss and greedy decoding.
- The experiments in constituency parsing may be unnecessary, as the focus on speed advantages does not align with the core motivation of the study.
- The paper lacks citations for key references, such as the earliest known usage of local span classification loss for constituency parsing.

### Suggestions for Improvement
We recommend that the authors improve the depth of their analysis regarding whether Transformers can learn dynamic programming algorithms like CKY, as the current exploration appears superficial. Additionally, addressing the distribution of constituency parse trees and its implications on the study's relevance would strengthen the paper. It would also be beneficial to include missing references, particularly the work on local span classification loss. Lastly, expanding the evaluation to include all language treebanks in SPMRL 2013 could enhance the robustness of the findings.