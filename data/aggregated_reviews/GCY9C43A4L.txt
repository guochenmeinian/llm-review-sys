ID: GCY9C43A4L
Title: TaskMet: Task-driven Metric Learning for Model Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 5, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called TaskMet for learning a metric to enhance the performance of prediction models on downstream tasks. The authors propose using task loss to guide the metric learning process, employing a gradient-based approach to optimize the metric parameters. TaskMet aims to improve predictions in various applications, including portfolio optimization and model-based reinforcement learning.

### Strengths and Weaknesses
Strengths:
- The manuscript is well-structured, with clear motivations and extensive experimental results supporting the claims.
- TaskMet allows for the integration of knowledge about downstream tasks while leveraging existing training data for the base prediction task.
- The proposed method is novel and has the potential for wide applicability.

Weaknesses:
- The clarity of the presentation could be significantly improved, particularly in distinguishing between "task learning" and "prediction."
- The experimental results in section 5.2 are relatively weak, with TaskMet not showing improved performance in any of the benchmark tasks.
- The sensitivity of results to hyperparameter choices is unclear, and the methodology section is difficult to understand.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by providing clearer distinctions between "task learning" and "prediction." Additionally, we suggest including a more detailed analysis of hyperparameter sensitivity to address concerns regarding the high standard deviations observed in the results. The authors should also enhance the methodology section with more illustrations and clearer explanations. Furthermore, we advise providing a comparison of TaskMet against other potential metrics beyond the generalized Mahalanobis distance-based loss to enrich the discussion on metric choices. Lastly, we encourage the authors to ensure that all tables and figures are formatted consistently and clearly annotated to enhance readability.