ID: 3RS2T9EPjI
Title: In-Image Neural Machine Translation with Segmented Pixel Sequence-to-Sequence Model
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to In-Image Machine Translation (IIMT) by proposing an end-to-end model that translates text images directly, bypassing the traditional OCR and translation pipeline. The authors convert input and output images into pixel sequences, mapping grayscale values to discrete symbols, and utilize a sequence-to-sequence Transformer model for translation. Experimental results indicate that the proposed method outperforms the cascaded OCR+MT baseline, demonstrating its effectiveness on a created benchmark.

### Strengths and Weaknesses
Strengths:
- The proposed framework effectively translates images without textual supervision, addressing error propagation issues inherent in cascade methods.
- The paper is well-written, with detailed analyses supporting its claims.
- The constructed dataset is a valuable resource for future research.

Weaknesses:
- The dataset primarily consists of images with black text on white backgrounds, which may not represent diverse real-world scenarios.
- The description of the pixel sequence transformation process is unclear, and critical details regarding training and inference are missing.
- Some experimental results lack clarity, particularly regarding the method's performance on incomplete texts compared to pipeline methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the pixel sequence transformation process, including how images are converted into pixel sequences and the method for transforming RGB to grayscale. Additionally, providing more details on the training and inference processes, as well as the optimization objectives, would enhance the paper's comprehensibility. To strengthen the generalizability of the findings, we suggest conducting experiments with more realistic datasets that include varied text image effects, such as different fonts and backgrounds. Furthermore, including comparisons with vanilla patch-based image encoding methods would provide vital baseline insights. Lastly, addressing the concerns regarding the experimental results and clarifying the relationship between error propagation in cascade methods and the proposed end-to-end approach would significantly improve the paper.