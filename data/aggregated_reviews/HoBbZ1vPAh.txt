ID: HoBbZ1vPAh
Title: Ensemble-based Deep Reinforcement Learning for Vehicle Routing Problems under Distribution Shift
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel ensemble-based deep reinforcement learning (DRL) algorithm, EL-DRL, aimed at addressing vehicle routing problems under distribution shifts. The method combines multiple sub-policies, employs distinct losses, and utilizes regularization techniques to enhance generalization. The authors demonstrate that sub-policy diversity improves out-of-distribution performance on various instances of the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP). 

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant issue in neural combinatorial optimization, showing that the proposed ensemble approach outperforms existing methods.
- The methodology is well-articulated, and the empirical results indicate effective performance across diverse benchmarks.
- The writing is clear and accessible, making the concepts easy to follow.

Weaknesses:
- The discussion lacks depth regarding comparisons with existing literature and alternative approaches, such as multi-agent reinforcement learning.
- The choice of using the REINFORCE algorithm is questioned; more modern baselines like PPO or PPG could be considered.
- The theoretical motivation and clarity surrounding the proposed regularization terms, particularly the Theil index and parameter dissimilarity, are insufficiently explained.
- The experimental setup lacks detail, particularly in how training instances are generated and the distribution sampling process.
- The runtime of EL-DRL is significantly slower than POMO, complicating direct performance comparisons.

### Suggestions for Improvement
We recommend that the authors improve the literature review by incorporating recent advancements in mitigating distributional shifts, such as Sym-NCO, ROCO, and Meta-SAGE. Additionally, the authors should clarify the necessity of ensemble learning versus multi-agent approaches and consider using more contemporary algorithms like PPO or PPG as baselines. A more thorough explanation of the regularization terms and their theoretical underpinnings is essential. Furthermore, we suggest conducting additional experiments to evaluate the impact of ensemble size, regularization weights, and the diversity of model outputs. Lastly, the authors should provide clearer details on the experimental setup, including the generation of training instances and the sampling of distributions.