ID: Dsi8Ibxg9H
Title: Language Model as Visual Explainer
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 8, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for interpreting image classifiers through a tree of attributes constructed using large language models (LLMs). The authors propose a framework that dynamically adjusts this tree by querying the LLM, allowing for the addition and removal of attributes based on the model's representations. The resulting hierarchical visual embedding tree is evaluated using new metrics for plausibility, faithfulness, and stability, demonstrating improved performance over previous decision tree approaches like NBDT. Additionally, the authors introduce a novel faithfulness evaluation metric for classifiers, which aims to address the interpretability of complex models and claims to handle co-occurring attributes effectively, despite concerns regarding its applicability to non-linear classifiers. The authors also plan to enhance the paper by including additional analytic and ablation experiments to support their arguments.

### Strengths and Weaknesses
Strengths:
- The introduction of new datasets with rich attributes is valuable for the community, although the necessity of such extensive attribute sets requires further justification.
- The method effectively outperforms previous neural network-based decision trees by utilizing rich attributes.
- The paper is well-documented, with clear objectives and comprehensive state-of-the-art reviews.
- The authors make a compelling case for their interpretability method based on previous works.
- The planned revisions include valuable insights and analyses that will strengthen the paper's arguments.

Weaknesses:
- The explanation provided by the attribute tree may primarily reflect the training set rather than the internal mechanisms of the network, as it relies heavily on the last layer's embeddings.
- The focus on quantitative metrics may overlook the qualitative insights that could enhance human understanding of the explanations.
- The necessity of the extensive attribute set is questioned, and the paper lacks a thorough analysis of the limitations of prior works.
- The faithfulness evaluation metric may not accurately reflect the behavior of classifiers, particularly in cases involving co-occurring attributes.
- The paper's readability is hindered by dense content and small figures, which require significant revisions for clarity.

### Suggestions for Improvement
We recommend that the authors improve the title to better reflect the scope of the paper, emphasizing the construction of hierarchical attributes and decision trees. Additionally, the authors should discuss the advantages and drawbacks of their method compared to other approaches that utilize language for interpreting vision models. A deeper qualitative analysis of individual data points and their explanations is necessary to provide meaningful insights. Furthermore, the authors should clarify the definitions of evaluation metrics and address how their approach can handle fine-grained datasets. A more detailed discussion on the necessity and justification for the large attribute sets is essential. To enhance readability, we suggest that the authors streamline dense sections and increase the font size of figures. Additionally, clarifying the mathematical arguments related to the faithfulness evaluation metric, particularly in the context of non-linear classifiers, will improve the paper's overall quality. Including all additional explanations and insights in the paper will also enhance its overall quality.