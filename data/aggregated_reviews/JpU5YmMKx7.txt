ID: JpU5YmMKx7
Title: Attentive Transfer Entropy to Exploit Transient Emergence of Coupling Effect
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 8, 6, 7, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extension of transfer entropy through the introduction of attentive transfer entropy (ATEn), aimed at inferring connectivity in both real and synthetic coupling networks. The authors demonstrate that ATEn effectively captures transient coupling effects within realistic biological networks by applying it to connectomes and conducting ablation studies on simple connectivity models. The method incorporates a neural estimator of mutual information and an attention mechanism to focus on critical moments of influence.

### Strengths and Weaknesses
Strengths:
- The paper provides convincing empirical evidence for the utility of attentive transfer entropy in understanding coupling effects in biological networks.
- The method shows higher performance and requires fewer data compared to other approaches.
- The use of attention to identify sparse coupling effects is an original contribution, and the text is well-written and clear.

Weaknesses:
- The introduction of the attention mechanism lacks a solid mathematical justification, and its theoretical grounding is insufficient.
- The performance of the classifier with a conventional attentional mechanism is comparable to ATEn, raising questions about ATEn's effectiveness in sparse networks.
- The study relies on predetermined dynamics, which may not reflect real-world network behavior, and the application of the method to only simulated datasets limits its real-world applicability.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the attention mechanism to enhance its credibility. Additionally, it would be beneficial to compare the performance of ATEn and the classifier in more detail, particularly in real-world networks. We suggest that the authors elaborate on the training details, including the total number of samples used in their experiments. Addressing the potential limitations of using predetermined dynamics and exploring the application of their method to actual neural recordings would significantly strengthen the paper. Finally, clarifying the inputs to the neural networks and discussing overfitting prevention strategies in the ATEn method would enhance the experimental setup's clarity.