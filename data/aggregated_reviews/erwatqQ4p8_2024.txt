ID: erwatqQ4p8
Title: Mixture of Experts Meets Prompt-Based Continual Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "Mixture of Experts Meets Prompt-Based Continual Learning," which integrates prompt-based continual learning methods with mixture of experts (MoE) architectures. The authors propose a novel gating mechanism called Non-linear Residual Gates (NoRGa) to enhance performance by leveraging theoretical insights and empirical evidence. The work aims to improve within-task prediction accuracy and overall continual learning performance while maintaining parameter efficiency.

### Strengths and Weaknesses
Strengths:
1. The paper offers a novel connection between prompt-based tuning and mixture of experts, providing a fresh perspective on continual learning approaches.
2. The theoretical framework connecting self-attention mechanisms to MoE models significantly advances understanding in this area.
3. The empirical experiments across diverse benchmarks demonstrate robustness and reliability, with clear explanations aided by concrete examples.

Weaknesses:
1. The paper lacks comparisons of NoRGa with state-of-the-art continual learning methods that do not use prompts, which would highlight its specific advantages.
2. There is insufficient clarity regarding the core elements of prompt-based methods and how prompts are defined and utilized.
3. The performance improvements on some datasets are limited, and the authors do not provide efficiency tests or discuss the implications of added hyperparameters.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the explanations regarding prompt definitions and their utilization within the framework. Additionally, we suggest that the authors include comparisons with other parameter-efficient fine-tuning methods and address the robustness of NoRGa concerning the introduced hyperparameters. Incorporating visual aids, such as diagrams illustrating the architecture of NoRGa and its relationship with prompts, could enhance accessibility and comprehension. Finally, we encourage the authors to provide more extensive evaluations across a broader range of benchmarks to substantiate their claims regarding performance improvements.