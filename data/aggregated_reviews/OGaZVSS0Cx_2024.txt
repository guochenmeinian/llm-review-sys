ID: OGaZVSS0Cx
Title: Mini-batch kernel $k$-means
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 5, 5, 4, -1, -1
Original Confidences: 4, 4, 3, 2, -1, -1

Aggregated Review:
### Key Points
This paper presents the first mini-batch kernel k-means algorithm, significantly improving the running time from $O(n^2)$ to $O(n(k+b))$ per iteration. The authors provide theoretical guarantees for convergence within $O(\gamma^2/\epsilon)$ iterations with high probability, where $\gamma$ is the bound on the norm of points in the feature space. The algorithm maintains the same approximation guarantee as the original k-means and is designed to handle the challenges of high-dimensional Hilbert spaces by bounding the Hilbert norm of points using normalized kernels. Experimental evaluations demonstrate that the mini-batch kernel k-means algorithm performs faster than its full-batch counterpart while preserving solution quality.

### Strengths and Weaknesses
Strengths:
- The algorithm drastically reduces running time, making it feasible for large datasets.
- Theoretical analysis ensures termination and provides an approximation ratio when initialized with the $k$-means++ method.
- The algorithm's flexibility with popular normalized kernels enhances its applicability.
- Detailed experiments show favorable comparisons with prior works, particularly in ARI and NMI scores across various datasets.

Weaknesses:
- The techniques employed are largely adaptations of existing work, lacking novelty in theoretical contributions.
- Some proofs could be moved to the appendix for clarity, as they do not enhance the overall understanding.
- The paper lacks a thorough discussion of experimental results and practical limitations.
- The performance is sensitive to parameters such as batch size and learning rate, which require careful tuning.

### Suggestions for Improvement
We recommend that the authors improve the discussion of experimental results, including a more thorough evaluation with additional parameters and datasets. Additionally, addressing the practical limitations of the algorithm in the main body would enhance its relevance. Clarifying the assumptions regarding the computation time of the kernel function and elaborating on the influence of batch size on running time in the experimental results would also be beneficial. Finally, we suggest that the authors provide a more detailed comparison with related works to highlight the unique contributions of their approach.