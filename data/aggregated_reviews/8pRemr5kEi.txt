ID: 8pRemr5kEi
Title: Visual Prompt Tuning in Null Space for Continual Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 8, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to continual learning through visual prompt tuning, introducing orthogonal projection to mitigate catastrophic forgetting. The authors propose two sufficient consistency conditions for self-attention and an invariant prompt distribution constraint for LayerNorm, leading to a null-space-based approximation solution for implementing prompt gradient orthogonal projection. Extensive experiments validate the effectiveness of the proposed method across four class-incremental benchmarks, demonstrating superior performance compared to state-of-the-art techniques.

### Strengths and Weaknesses
Strengths:  
1) The research motivation is well-founded, with solid theoretical proof supporting the orthogonal constraint to prevent knowledge forgetting.  
2) The experimental results are extensive and demonstrate significant performance improvements over existing methods.  
3) The paper is well-organized and clearly presented, facilitating reader comprehension.

Weaknesses:  
1) There are potential errors in Eq.(8) and a lack of clarity regarding the implications of orthogonal weight updates on the model's discriminative ability.  
2) The complexity and running time of the proposed model compared to baseline VPT are not discussed.  
3) The derivation process in the methods section is lengthy, leading to insufficient detail in the experimental section.  
4) The omission of certain leading methods in comparisons weakens the contribution of this work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Eq.(8) and address the potential impact of orthogonal updates on the model's performance. Additionally, providing a detailed comparison of model complexity and running time with baseline VPT would enhance the paper. We suggest streamlining the derivation process and expanding the experimental section to include more comprehensive comparisons with leading methods. Lastly, we encourage the authors to consider the implications of their orthogonal projection approach on knowledge transfer between tasks, as this could further strengthen their contributions to continual learning.