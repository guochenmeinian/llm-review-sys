ID: lSJ8VjjimZ
Title: HOLMES & WATSON: A Robust and Lightweight HTTPS Website Fingerprinting through HTTP Version Parallelism
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel website fingerprinting method called HOMES & WATSON, which enhances website identification from encrypted traffic by exploiting the unique sequences of HTTP version requests made by websites. The methodology was evaluated using a large dataset of 80,000 popular websites, demonstrating high reliability for tracking visits in encrypted HTTP traffic. HOLMES improves robustness through HTTP version parallelism, extracting application-layer features that yield up to 4.28 bits of information, significantly outperforming previous techniques in stability and information gain. The authors introduce WATSON, a lightweight classification method based on lazy learning, which reduces reliance on large training datasets and computational resources. Together, HOLMES and WATSON achieve an average accuracy of 87.7%, surpassing state-of-the-art methods by over 15% with only a single sample per website, thus demonstrating high efficiency and robustness for practical deployments.

### Strengths and Weaknesses
Strengths:
- The approach is innovative, leveraging HTTP version parallelism for fingerprinting and effectively addressing critical challenges in website fingerprinting, particularly regarding robustness and efficiency in encrypted traffic.
- The comprehensive evaluation across various scenarios shows superior performance compared to existing methods, with impressive performance improvements and over 98% stability across varying network conditions.
- The authors provide an open dataset and source code, enhancing reproducibility and community engagement.
- The methodology is well-explained, ensuring transparency and reproducibility.

Weaknesses:
- The study lacks consideration of mobile users and the Safari browser, which are significant in web traffic.
- Important details regarding the study setup and limitations are missing or inadequately discussed, including the threat model's lack of discussion on the attacker's computational capabilities, particularly regarding state actors.
- The choice of K-NN with a Gini coefficient for efficiency is not justified against other fair classification approaches.
- Details on fine-tuning algorithms to the new dataset are insufficient, raising questions about the validity of adjustments.
- The discussion on real-world complications, such as TLS and IP versions, is limited and could be more comprehensive.
- Scalability and generalization of HOLMES and WATSON to larger datasets or real-time environments are not adequately explored.
- The results and their implications are not thoroughly justified, and some tables are confusing.

### Suggestions for Improvement
We recommend that the authors improve the justification of their study setup by including mobile users and the Safari browser. Additionally, please clarify the dataset collection timeline, the specific Tranco dataset used, and the server locations for crawling. A limitations section should be added to critically discuss factors such as the impact of server geolocation and browser vendor on fingerprinting accuracy.

We suggest enhancing the discussion of results to better articulate their significance for various stakeholders, including developers and decision-makers. The authors should clarify the hyperparameter tuning process and the implications of the classifier's performance across different browsers and versions, as this is currently only mentioned in the appendix.

To improve table clarity, consider including a simplified version in the main text and placing the full table in the appendix. Lastly, we encourage the authors to provide a more detailed explanation of padding-based defense strategies and their potential impact on the proposed method. Additionally, clarify the rationale for using the Gini coefficient over other fair classification methods, such as disparate impact, and provide detailed insights into the fine-tuning process of algorithms to ensure fair analysis. Expand the discussion on real-world complications, particularly regarding TLS and IP versions, and address scalability and generalization challenges, including potential limitations and future research directions related to concept drift.