ID: 7vXufiEzSy
Title: Self-Guided Masked Autoencoder
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 5, 6, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel masking strategy, Self-Guided Informed Masking, aimed at enhancing the Masked Autoencoder (MAE) approach in self-supervised learning for computer vision. The authors identify that MAE learns pattern-based patch-level clustering early in pre-training, which informs the generation of effective masks that cover main objects in images. This method improves learning efficiency without external models, leading to better feature embeddings and performance across various tasks. Additionally, the authors conduct an in-depth analysis of MAEâ€™s intrinsic learning properties, demonstrating that MAE can autonomously generate informed masks early in training. They provide qualitative and quantitative comparisons with MoCo and ViT, as well as analyses of bi-partitioning, KL divergence, and an "exploitation rate" to determine when MAE effectively clusters patches. The authors clarify that "accelerated pre-training" refers to achieving similar downstream task performance with fewer pre-training epochs, and they demonstrate that their method, while using only the top 75% of tokens, outperforms MAE despite a slight degradation in performance.

### Strengths and Weaknesses
Strengths:
1. The self-guided masked autoencoder is innovative, generating informed masks based on the model's learning progress, supported by sound motivations.
2. The paper provides a clear rationale for the informed masking strategy, emphasizing its focus on object-centric regions to improve patch clustering.
3. The analyses presented are thorough, showcasing MAE's capabilities and the effectiveness of the proposed method.
4. The authors effectively address reviewer concerns and clarify key contributions, demonstrating superior performance over MAE with fewer tokens.

Weaknesses:
1. The method's success depends on the accuracy of initial patch clustering; poorly formed clusters may lead to suboptimal masks and reduced performance.
2. The contribution of the masking strategy compared to existing methods like SemMAE and HPM remains unclear, as these methods show superior performance.
3. The bi-partitioning process adds computational overhead, potentially increasing training time and resource consumption; training time or FLOPS should be included.
4. The justification for using 400 epochs for analysis instead of 800 epochs lacks sufficient support, raising concerns about the depth of analysis.
5. The overall presentation of the paper lacks clarity, and the authors' claims need further elaboration.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manuscript by explicitly comparing the performance of their method with SemMAE and HPM, particularly regarding the impact of the masking strategy on MAE's learning. Additionally, we suggest incorporating scalability experiments to assess the method's performance across different training epochs, ideally using 800 epochs for a more in-depth analysis. It would also be beneficial to provide detailed information on the training time overhead compared to vanilla MAE for various epochs, as this would enhance the understanding of the method's efficiency. Furthermore, we recommend that the authors improve the clarity of the connections between the analyses in Section 3 and the proposed method in Section 4. Lastly, updating Table B to reflect a unified training time for a fair comparison with SemMAE is essential, and addressing the scalability of the method with accuracy curves and results on larger models would enhance the paper's contribution.