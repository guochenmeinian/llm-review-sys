ID: 6lgugutkin
Title: Language Model Alignment with Elastic Reset
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called Elastic Reset to mitigate language drift and reward model overfitting during reinforcement learning with human feedback (RLHF). The authors propose periodically resetting the model to an exponentially moving average (EMA) of its previous checkpoints, demonstrating its effectiveness across various tasks, including QA with LLaMA-7B. The approach aims to balance maximizing reward while minimizing drift from the initial model. Additionally, the authors conduct a thorough investigation of Elastic Reset, confirming its effectiveness compared to traditional methods like PPO and Supervised + PPO. They provide additional experiments that confirm the consistency of results across multiple reward models and clarify distinctions between their methods and those in prior work, particularly regarding hyperparameter choices and model performance metrics.

### Strengths and Weaknesses
Strengths:  
- The proposed method is simple and easy to implement, requiring only the storage of previous checkpoints.  
- The empirical results indicate that Elastic Reset outperforms traditional methods like PPO and Supervised + PPO across multiple tasks.  
- The authors conducted additional experiments that enhance the robustness of their evaluation.  
- Clear explanations regarding Elastic Reset, KL penalties, and hyperparameter choices are appreciated.  
- The paper is well-written, with a clear definition of language drift and thorough experimental evaluation.  

Weaknesses:  
- The evaluation protocol relies on the same reward model used for training, which may lead to overfitting and does not reflect human evaluations. Alternative reward models should be considered for more robust assessments.  
- The novelty of the method is questionable, as similar reset mechanisms are common in continual learning, and the drift problem lacks clear definition and justification.  
- The experiments are limited to specific tasks and datasets, which may not convincingly represent the broader applicability of the method.  
- The evaluation could be further strengthened by incorporating different model sizes and architectures, rather than solely relying on variations in training seeds.  
- There remains some ambiguity in comparing the performance of different methods, particularly regarding the drift in model outputs.  

### Suggestions for Improvement
We recommend that the authors improve the evaluation protocol by incorporating a different reward model for assessment, such as a model trained on a different dataset or a separate holdout set. Additionally, including experiments with varying reset intervals across all tasks would provide insights into the sensitivity of the model to this parameter. It would also be beneficial to explore the effects of resetting only certain parameters, such as the last few layers, to better understand the method's flexibility. Furthermore, we suggest improving the evaluation by using different model sizes and architectures or training on diverse datasets to enhance robustness. Clarifying the distinctions between the performance of Elastic Reset and other methods would address any remaining ambiguities. Finally, including Figure 1(c) in the supplementary material would be beneficial for illustrating key points.