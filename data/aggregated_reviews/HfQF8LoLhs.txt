ID: HfQF8LoLhs
Title: Asymptotics of Alpha-Divergence Variational Inference Algorithms with Exponential Families
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 8, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical study of variational inference using alpha-divergences within the exponential family framework. The authors prove a geometric convergence rate for the algorithm proposed in [9] and introduce an alternative unbiased optimization algorithm, providing convergence guarantees. The paper also demonstrates applications of the proposed algorithm in variational autoencoders (VAEs) and includes both simulated and real-data experiments to support its theoretical claims. Furthermore, the authors clarify that the function \( q_{\eta} \) should be expressed as \( q_{\eta}(\cdot|x) \) and emphasize the need for revisions in Section 5 to accurately reflect the relationship between the algorithms MAX and UNB and the variational bounds they optimize, aiming to enhance clarity regarding the training algorithms for VAEs.

### Strengths and Weaknesses
Strengths:
- The derivation of the asymptotic convergence rate for the algorithm in [9] and the introduction of an unbiased algorithm with convergence guarantees are significant contributions.
- The assumptions (H1)-(H3) are well-explained, and the applications to VAEs are relevant and insightful.
- The authors provide a clear framework for understanding the connection between the algorithms and variational bounds.
- The paper is well-written and clear despite its mathematical rigor, addressing an important problem in alpha-divergence minimization.

Weaknesses:
- The analysis is limited to exponential family variational distributions, which restricts the generalizability of the findings.
- The convergence results are only proven in an asymptotic sense, and the clarity of Section 5 is lacking, particularly regarding the assumptions made about the encoder's independence from the input.
- The original formulation in Section 5 was misleading, necessitating significant clarification, and some reviewers still find Section 5 unclear and request further revisions.
- Some technical notations and assumptions could benefit from further clarification, and the empirical results lack significance in certain cases.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 5 by explicitly stating the algorithms derived and addressing the assumptions regarding the encoder's independence from the input. A rewritten Section 5 should incorporate the clarifying comments discussed in the reviews to ensure that the relationship between the algorithms and the variational bounds is comprehensible. Additionally, consider providing a table of notation to aid readability and delineate definitions more clearly. It would be beneficial to extend the empirical analysis to include more diverse experiments, such as Bayesian logistic regression, and to explore the impact of the number of samples used in the sample-based algorithm. Finally, we suggest that the authors clarify the relationship between unbiasedness and computational intensity in their analysis.