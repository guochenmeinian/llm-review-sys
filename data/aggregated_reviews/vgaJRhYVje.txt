ID: vgaJRhYVje
Title: Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MosaiCLIP, a framework designed to enhance the compositional reasoning ability of vision-language models (VLMs) through a coarse-to-fine contrastive learning approach. The authors propose three major technical innovations: (1) **Scene graph guided text decomposition**, which converts image captions into graph structures for contrastive learning; (2) **Negative sub-graph creation**, utilizing operations like node swapping and edge replacement; and (3) **Curriculum and robust fine-tuning**, employing a two-stage strategy for associating images with positive and negative sub-graphs. Empirical studies indicate that MosaiCLIP outperforms existing CLIP-based VLMs, particularly in handling noisy training data.

### Strengths and Weaknesses
Strengths:
- The coarse-to-fine contrastive learning framework is intuitive, with well-designed components.
- Comprehensive experiments validate the model's superiority in visio-linguistic compositional reasoning.
- The analysis explaining MosaiCLIPâ€™s improvements is instructive.
- Limitations are acknowledged and discussed appropriately.
- The paper is well-written and easy to follow.

Weaknesses:
- Uncertainty exists regarding the method's effectiveness on more advanced VLMs like BLIP.
- There is a lack of evaluation on standard image-text retrieval tasks, such as those on COCO.
- The success of the method is contingent on the quality of scene graph generation, which may introduce inaccuracies.
- The computational resources required for extracting scene graphs could hinder performance in complex scenes.
- Comparisons with other state-of-the-art models are insufficient, particularly regarding negative sample types.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including standard image-text retrieval tasks to provide a more comprehensive assessment of MosaiCLIP's performance. Additionally, clarifying the templates used when multiple nodes exist in the sub-graphs would enhance understanding. It is crucial to ensure the reliability of scene graph generation to mitigate potential inaccuracies. Lastly, we suggest including comparisons with models like DeCLIP and ensuring that batch sizes are consistent across methods to validate performance improvements accurately.