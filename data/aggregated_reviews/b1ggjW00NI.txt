ID: b1ggjW00NI
Title: Don't Look Twice: Faster Video Transformers with Run-Length Tokenization
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 4, 7, 8, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents run-length tokenization (RLT), an efficient video patch merging strategy designed to enhance the speed of video transformers. The authors propose a method to omit tubelets that do not change significantly between frames, optionally incorporating a run-length embedding to indicate the duration of token stability. This approach aims to improve processing efficiency without sacrificing performance in human action recognition tasks.

### Strengths and Weaknesses
Strengths:
- The method is well-motivated, addressing the inefficiencies of processing static backgrounds in dynamic videos.
- RLT ensures that, in the worst-case scenario, performance matches that of non-RLT baselines, maintaining efficiency for both static and dynamic videos.
- Minimal architectural modifications are required, allowing for fine-tuning of existing video models rather than necessitating complete retraining.
- The novelty of the run-length "positional" embedding is acknowledged.
- Empirical results demonstrate that RLT can maintain performance while significantly speeding up training compared to random approaches.

Weaknesses:
- The experimental section lacks robustness, with only two simple baselines (Token Merging and random masking) that do not convincingly evaluate the proposed method.
- Details regarding the token merging baseline are insufficient, particularly concerning its implementation across frames versus temporal dimensions.
- The simplicity of the proposed method, which merges patches based on pixel similarity, raises concerns about its depth.
- Conflicting results in Table 3 regarding the utility of length encoding for RLT are confusing and warrant clarification.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by incorporating more diverse and established baselines to enhance the evaluation of the proposed method. Additionally, clarifying the implementation details of the token merging baseline would strengthen the analysis. The authors should also address the conflicting results in Table 3, providing a clearer explanation of the impact of length encoding on performance. Furthermore, exploring alternative methods for calculating patch differences could enhance the robustness of the approach. Lastly, it would be beneficial to investigate the sensitivity of the thresholding approach and its implications for different datasets.