ID: RdAfUp4LcD
Title: Linear Mode Connectivity in Differentiable Tree Ensembles
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 5, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of Linear Mode Connectivity (LMC) in differentiable tree ensembles (DTE) and soft trees, extending concepts from neural networks (NNs) and focusing on model merging in tabular data benchmarks. The authors identify that permutation invariance alone is insufficient for LMC in DTE, introducing two additional tree-specific invariances: subtree flip invariance and splitting order invariance. They propose a modified DTE architecture that retains LMC with only permutation invariance and develop algorithms for establishing LMC between independently trained DTEs, supported by comprehensive empirical evaluations. The paper also discusses the theoretical implications of increasing the permutation search space to reduce barriers between models, while acknowledging the NP-hard nature of finding optimal permutations.

### Strengths and Weaknesses
Strengths:  
- The paper is well-structured and accessible, even for readers unfamiliar with decision trees.  
- It presents a novel contribution by demonstrating the relevance of LMC in DTE models, supported by detailed empirical evaluations across multiple datasets.  
- The study of soft trees is well-motivated and relevant to practical applications, as acknowledged by reviewers.  
- The authors have conducted an ablation study, providing insights into model behavior with respect to tree depth and ensemble size.  
- The exploration of additional invariances beyond permutation invariance is a significant aspect of the work, contributing valuable perspectives on model merging and LMC.

Weaknesses:  
- The paper lacks theoretical support and practical implications, similar to limitations noted in LMC studies of neural networks.  
- There is a lack of theoretical justification regarding when model merging is effective and when low barriers are achieved.  
- Key methodological components, such as algorithms and LAP formulation, are inadequately explained and relegated to the appendix.  
- Some experimental results, particularly regarding MLP performance, appear counterintuitive and warrant further clarification.  
- There are minor issues with text clarity, including typos and undefined terms, such as the value $D$ in line 127.  
- The contribution may be perceived as incremental, primarily showcasing that different architectures require different invariances for LMC analysis.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for LMC in DTE and explore its practical applications more thoroughly. Additionally, we suggest enhancing the clarity of the introduction to avoid confusion regarding the nature of the model. We encourage the authors to improve the theoretical analysis of model merging, specifically addressing when and why it is effective. Furthermore, we suggest enhancing the clarity of the explanations for key methods and algorithms, ensuring they are more accessible in the main text rather than hidden in the appendix. Addressing minor textual inaccuracies and providing a more detailed explanation of the algorithms for weights and activation matching in the main text would also strengthen the paper. Finally, including a related work section would help contextualize the contributions within existing literature and providing a more thorough discussion of the experimental results would clarify any unexpected findings, particularly regarding MLP performance.