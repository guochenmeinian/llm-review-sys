ID: 4hiJ3KPDYe
Title: Tackling Unconditional Generation for Highly Multimodal Distributions with Hat Diffusion EBM
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 4, 6, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Hat Diffusion Energy-Based Model (HDEBM), a hybrid model that integrates a distilled truncated diffusion model as a generator within the Hat EBM framework, aimed at unconditional image generation. The authors define a joint energy function that combines the generator's latent space with a residual image space, allowing for image generation through a two-stage training process. The first stage learns the distribution of residual images conditioned on latent variables, while the second stage learns the joint distribution of the generator latents and residuals. Empirical results demonstrate competitive performance on datasets such as CIFAR-10, CelebA, and ImageNet, particularly achieving state-of-the-art Fr√©chet Inception Distance (FID) scores on ImageNet $128\times 128$. The generative procedure involves an implicit generator, a pretrained distilled diffusion corrector, and energy-based model Langevin dynamics.

### Strengths and Weaknesses
Strengths:
- The introduction and related work sections are well-organized, clearly outlining the HDEBM framework and its connections to existing literature.
- The paper discusses technical limitations and potential social impacts in detail.
- HDEBM shows significant improvements over Hat EBM in FID scores, demonstrating strong performance in unconditional image generation.
- The integration of diffusion models into the EBM framework presents an interesting avenue for future research.
- The methodological contributions include feasible gradient training through a distilled diffusion and reparameterization of the truncated diffusion implicit generator.

Weaknesses:
- The novelty of HDEBM is questionable, as it closely resembles the Hat EBM framework, primarily differing in the generator's parameterization.
- The motivation for using the HDEBM framework is unclear, particularly in comparison to simpler generative models like diffusion models.
- Technical details regarding the diffusion model are unclear, particularly the use of both forward and reverse processes for refinement, which lacks justification.
- The complexity of the framework, requiring multiple components and a three-stage training pipeline, may hinder implementation.
- There are factual inaccuracies in claims about related works, and the design of combining two generator networks appears unnecessarily complex.
- The background section contains non-novel content and should be relocated to improve clarity.
- There are inconsistencies in notation and terminology throughout the paper, which could confuse readers.
- The discussion of Proposition 3.2.1 lacks clarity regarding the implications of a "perfect reverse process."
- The necessity of two training stages over a single fused stage is not sufficiently justified.
- The experimental evaluation is limited to three datasets, and broader applications of the model are not explored.
- Inconsistent performance on CIFAR-10 compared to excellent results on ImageNet-128 raises questions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology section by consolidating details from the appendix into the main text. Additionally, we suggest that the authors provide experimental results comparing HDEBM with recent diffusion models on CIFAR-10 and CelebA, as well as clarify the rationale behind the challenges of multimodal modeling with diffusion models. Furthermore, addressing the factual inaccuracies regarding related works and simplifying the generator design could enhance the paper's overall quality. We also recommend that the authors improve the motivation for the HDEBM approach, clearly articulating its advantages over simpler generative models. A more thorough comparison of the progressive distillation component in isolation to clarify its contributions would be beneficial. Simplifying the framework or providing clearer delineation of the training stages would enhance implementation feasibility. The background section should be moved to a more appropriate location, and notation inconsistencies should be addressed for clarity. We also suggest that the authors clarify the implications of Proposition 3.2.1 and justify the necessity of two training stages. Lastly, expanding the experimental evaluation to include a wider range of tasks and datasets would provide a more comprehensive understanding of the model's capabilities.