ID: M80WgiO2Lb
Title: Scaling Sign Language Translation
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 6, 6, 7, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to improve open-domain sign language translation (SLT) by scaling pretraining data, model size, and translation directions. The authors leverage a mixture of noisy multilingual YouTube SLT data, parallel corpora, and augmented SLT data to enhance pretraining. Experiments with (m/By)T5 models demonstrate substantial improvements over baselines, achieving state-of-the-art results across multiple benchmarks. The study emphasizes the importance of data scaling, model scaling, and cross-lingual/cross-modal transfer in enhancing SLT performance.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and provides valuable insights into the significance of improving SLT in open-domain settings.
- The experiments are thoughtfully designed, yielding compelling results relative to state-of-the-art methods.
- The exploration of large-scale data and model scaling is a novel contribution to the SLT field.

Weaknesses:
- The legend in Figure 7 is blocked, hindering clarity.
- There is no open-source code or model available, making reproduction of the work resource-intensive.
- Missing references, particularly regarding the FLEURS-ASL dataset, raise concerns about the generalizability of results.
- The paper lacks a comparative analysis of other recent SLT methods, limiting its argumentation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 7 by ensuring the legend is fully visible. Additionally, releasing the code and model would facilitate reproduction and further research. We suggest addressing the missing references and providing a detailed characterization of the FLEURS-ASL dataset to clarify its content and biases. A more in-depth comparative analysis of recent SLT methods would strengthen the paper's argumentation. Finally, we encourage the authors to explore the impact of different model architectures and training strategies on SLT performance to enhance understanding of scalability and generalization.