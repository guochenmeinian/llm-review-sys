ID: d4f40zJJIS
Title: Structural Pruning for Diffusion Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 7, 5, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Diff-Pruning, an efficient compression method for learning lightweight diffusion models from pre-existing ones. The authors propose a Taylor expansion over pruned timesteps to eliminate non-contributory diffusion steps and identify important weights. Empirical evaluations on four diverse datasets demonstrate two primary benefits: a 50% reduction in FLOPs at a fraction of the original training cost and consistency in generative behavior with pre-trained models.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel method, Diff-Pruning, specifically designed for compressing diffusion models.
- Empirical assessments across four datasets provide a comprehensive analysis of the proposed method's performance, demonstrating its efficiency and consistency.
- The writing is clear and well-structured.

Weaknesses:
- Additional details about the pruning process are needed; specific explanations or pseudocode would enhance clarity.
- There are inconsistencies in the notation used in equations, particularly regarding the symbols $|\cdot|$, $||\cdot||$, and $||\cdot||_0$, which need verification.
- The derivation and implications of Eq. (6) from Eq. (5) require clarification, particularly its relationship with model performance.
- The authors should explain why the pruned model outperforms the pre-trained model.
- Sensitivity of FID and SSIM metrics to the pruned model should be assessed.
- The MACs metric's ability to demonstrate performance acceleration needs exploration, particularly regarding training or inference speed under different GPU settings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the pruning process by providing detailed descriptions or pseudocode. Additionally, please verify the consistency of notation in equations and clarify the derivation of Eq. (6) and its significance. It is essential to explain the observed performance of the pruned model compared to the pre-trained model and assess the sensitivity of FID and SSIM metrics. Finally, we suggest exploring the MACs metric to demonstrate the proposed method's effectiveness in terms of training or inference speed across various GPU configurations. Expanding the limitations section will also help contextualize the applicability of the proposed method.