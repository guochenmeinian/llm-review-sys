ID: VQQeyiAqtv
Title: Data Selection Curriculum for Abstractive Text Summarization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Data Selection Curriculum for Abstractive Text Summarization, proposing a learning difficulty metric based on a Margin-aware List-wise Ranking (MLR) loss that incorporates a dynamic margin to enhance contrastive learning. The authors utilize this difficulty metric to selectively train models by discarding simple and complex instances, demonstrating promising results on the CNN/DailyMail dataset.

### Strengths and Weaknesses
Strengths:  
- The motivation for the study is clearly articulated, and the integration of a dynamic margin in the loss function is a significant advancement.  
- Experimental results indicate that the proposed method effectively improves summarization performance, achieving state-of-the-art results with only 20% of the training data.  
- The paper is well-written and easy to follow, providing necessary background and insights into recent literature.

Weaknesses:  
- The method has only been tested on the CNN/DailyMail dataset, raising concerns about its generalizability to other datasets.  
- The explanation of the difficulty metric's design is insufficient, particularly regarding the formula in section 2.3.  
- The claim that optimal results can be achieved with less than 20% of the data lacks rigorous validation across different metrics, particularly ROUGE.  
- There is a lack of detail in the methodology, especially concerning the parameters used in the difficulty metric and how data is added to the training set.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the difficulty metric's design and its application in curriculum learning by providing a more detailed explanation. Additionally, conducting experiments on other datasets, such as XSum, would strengthen the claims regarding generalizability. We also suggest validating the optimal data percentage claim using ROUGE metrics alongside BertScore to provide a more comprehensive evaluation of performance. Finally, including results from different pre-trained checkpoints could enhance the robustness of the findings.