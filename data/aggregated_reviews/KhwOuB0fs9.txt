ID: KhwOuB0fs9
Title: EffiLearner: Enhancing Efficiency of Generated Code via Self-Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 24
Original Ratings: 4, 8, 6, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called Self-Optimization based on OverheAd Profile (SOAP) aimed at addressing the inefficiencies in code generated by Large Language Models (LLMs) regarding execution time and memory consumption. The authors propose an iterative refinement process that utilizes execution overhead profiles to optimize LLM-generated code. Preliminary experiments demonstrate that SOAP outperforms other baselines in various tasks, although significant discrepancies in improvement ratios across different tables suggest that task correctness impacts efficiency. The authors clarify that SOAP's performance is evaluated based on its ability to pass public test cases, ensuring minimal decreases in pass@1 rates. They also address the relationship between SOAP and other methods, noting that the tasks and execution times differ significantly, which may affect the evaluation metrics.

### Strengths and Weaknesses
Strengths:
- The paper is well presented, with a clear motivation for addressing code inefficiency.
- The evaluation encompasses a diverse range of Python coding tasks and LLMs.
- The authors commit to open-sourcing the code, enhancing reproducibility.
- The incorporation of profiling results from the `line_profiler` library enhances the optimization process.
- The authors provide a clear methodology for ensuring the correctness of SOAP-optimized code through public test case coverage.
- The authors have conducted a substantial volume of preliminary experiments in a short timeframe, addressing several weaknesses identified in previous reviews.

Weaknesses:
- The baseline selection is inadequate; it relies solely on LLMs generating code without optimization prompts. Additional baselines, such as prompting for efficient code generation and direct optimization, should be included.
- The evaluation methodology lacks consistency, particularly in comparing LLM efficiency across different task sets, leading to potentially unreasonable efficiency scores.
- Tables 1-4 require better alignment for an "apple-to-apple" comparison of models under the same tasks.
- The efficiency evaluation metrics are potentially noisy; the authors should consider using architecture simulators or more defined metrics to improve reproducibility.
- The proposed technique is specialized for Python, and its applicability to performance-demanding languages like C/C++ is unaddressed.
- The results, particularly the pass@1 rates and execution times, appear counterintuitive and require further scrutiny.
- The paper lacks a thorough discussion of closely relevant works, such as those mentioned in the PIE paper.

### Suggestions for Improvement
We recommend that the authors improve the baseline selection by including at least two additional baselines: (1) prompting the model to generate efficient code and (2) asking the model to refine its initial generation for performance. Additionally, we suggest improving the consistency of the evaluation methodology by ensuring that comparisons are made on an "apple-to-apple" basis, removing results from inconsistent comparisons, and enforcing correctness alignments. We also recommend aligning the tasks in Tables 1-4 for a fair comparison of models. To enhance the reliability of efficiency evaluations, consider employing architecture simulators or more robust metrics. Furthermore, the authors should explore the applicability of SOAP to languages beyond Python and provide a comprehensive discussion of related works, particularly those that utilize LLMs for code optimization. Lastly, we suggest that the authors improve the clarity of their results by providing more detailed explanations of the experimental settings and the rationale behind the observed pass@1 rates, and address the implications of applying SOAP after PIE, particularly in terms of performance metrics, to clarify the benefits of using both methods in conjunction.