ID: zpw6NmhvKU
Title: RashomonGB: Analyzing the Rashomon Effect and Mitigating Predictive Multiplicity in Gradient Boosting
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 5, 7, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called RashomonGB to estimate the Rashomon sets and predictive multiplicity of gradient boosting models. It estimates multiple models at each stage, effectively performing local exploration, and combines them to construct \( m^T \) models for Rashomon set computation, where \( T \) is the number of boosting iterations. The authors demonstrate that RashomonGB outperforms re-training with \( m \) seeds, showing greater predictive multiplicity at a fixed loss difference level.

### Strengths and Weaknesses
Strengths:  
- The topic of predictive multiplicity is significant, and the paper is generally clear and well-written.  
- RashomonGB is a sensible first method for boosting algorithms, likely to be adopted due to its intuitive and easy implementation.  
- The theoretical foundation and empirical support enhance understanding of the Rashomon effect in gradient boosting.

Weaknesses:  
- The exploration strategy may not align well with the motivation of the Rashomon set, potentially underestimating predictive multiplicity due to model correlation.  
- The computational cost of RashomonGB may not provide a fair comparison against re-training, necessitating an estimate of compute savings.  
- The introduction and abstract are difficult to understand, which may confuse readers familiar with the literature.  
- Important details about the RashomonGB method and empirical setup are either missing or imprecise, affecting clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the introduction and abstract to facilitate understanding. Additionally, we suggest providing a more detailed justification for the sub-Gaussian assumption in the dataset-related Rashomon bound. It would be beneficial to clarify the exploration strategy and how \( \epsilon_t \) is set for each iteration. We also encourage the authors to include a comparison with stronger baselines for measuring predictive uncertainty in gradient-boosted algorithms. Finally, addressing the potential negative impacts of RashomonGB and ensuring immediate open access to code and data would enhance reproducibility and validation.