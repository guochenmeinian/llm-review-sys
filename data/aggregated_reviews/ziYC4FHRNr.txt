ID: ziYC4FHRNr
Title: Entrywise error bounds for low-rank approximations of kernel matrices
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the first entrywise guarantees for low-rank approximation of kernel matrices under polynomial or exponential decay of kernel eigenvalues. Specifically, in the $\alpha$-polynomial decay scenario, the entrywise error scales as $O(n^{-\frac{\alpha-1}{\alpha}} \log n)$ for rank $d = \Omega(n^{1/\alpha})$, while for $(\beta,\gamma)$-exponential decay, the error scales like $O(1/n)$ for $d > \log^{1/\gamma}(n^{1/\beta})$. The authors establish that eigenvectors corresponding to small eigenvalues are delocalized, with bounded entries of size $O(1/\sqrt{n})$. The technical novelty arises from the dependency and non-zero mean of kernel matrix entries.

### Strengths and Weaknesses
Strengths:
1. This is the first result demonstrating entrywise error guarantees for low-rank approximations of kernel matrices.
2. The proof sketches for the main theorems are clear and comprehensible.
3. The strongest technical contribution is the proof in Appendix D, showing that the norm of the projection of vector 1 onto the subspace spanned by the $n-d'$ smallest eigenvalue eigenvectors vanishes rapidly.
4. The experiments effectively complement the theoretical findings.

Weaknesses:
1. Lemma 1, claimed as a novel concentration result, appears to be a slight generalization of Lemma 68 in Tao and Vu [2011], proven using similar arguments.
2. While the proof sketches of Theorems 1 and 2 are appreciated, more detail on the proof in Appendix D would enhance understanding, as it contains the most novel aspects.
3. The necessity and generality of assumption (R) are unclear beyond the two specific cases in Section 3.1.

### Suggestions for Improvement
We recommend that the authors improve the clarity and significance of Lemma 1, ensuring it is distinct from existing results. Additionally, we suggest providing more detailed information in the main text regarding the proof in Appendix D, as it is crucial for understanding the paper's contributions. Furthermore, we encourage the authors to clarify the necessity of assumption (R) and explore whether it can be relaxed, as the final error bound does not depend on $a$ and $b$. Lastly, we advise providing more robust evidence for the double descent observation, including its behavior across various percentiles and kernel functions.