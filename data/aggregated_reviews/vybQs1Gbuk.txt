ID: vybQs1Gbuk
Title: Learning from Rich Semantics and Coarse Locations for Long-tailed Object Detection
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for long-tailed object detection that utilizes the vision-language pre-trained model CLIP to address semantic ambiguity and location sensitivity. The authors propose RichSem, which introduces an additional semantic branch to the detector, leveraging CLIP scores for supervision. Extensive experiments demonstrate that RichSem achieves state-of-the-art results on the LVIS dataset across various categories. The methodology is well-supported by ablation studies and discussions.

### Strengths and Weaknesses
Strengths:
- The paper is the first to apply vision-language pipelines to long-tailed object detection, marking a significant contribution.
- The writing is clear and the presentation is easy to follow.
- The experimental section is exhaustive, effectively supporting the authors' claims.

Weaknesses:
- The work lacks theoretical insight into how the proposed method integrates with long-tailed feature distributions, particularly in the context of the framework and module design.
- The state-of-the-art comparison is insufficient, notably omitting relevant methods that utilize vision-language pre-trained models for generic object detection.
- The performance improvements appear heavily reliant on the Swin-Transformer backbone, with limited effectiveness observed when using the ResNet-50 backbone.

### Suggestions for Improvement
We recommend that the authors improve the theoretical discussion on how their method fits into the long-tailed feature distribution. Additionally, please enhance the state-of-the-art comparison by including relevant methods that utilize vision-language pre-trained models. It would also be beneficial to investigate why the proposed framework is less effective with the ResNet-50 backbone. Furthermore, we suggest providing extensive feature visualizations related to both long-tail and CLIP features, and using \mathcal{} for loss functions and \mathbf{} for tensors and vectors to improve clarity in notation.