ID: 8PNFSDJ3md
Title: Empower Nested Boolean Logic via Self-Supervised Curriculum Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the logical reasoning capabilities of pre-trained language models, particularly their handling of multi-nested boolean logic. The authors propose a new self-supervised learning method called Curriculum Logical Reasoning (CLR), which enhances model training by gradually introducing complexity in logical patterns. The study reveals that existing models exhibit random selection behavior in boolean logic tasks, while CLR significantly improves their performance on complex reasoning tasks. The paper also highlights additional findings, such as the benefits of recalling simpler logic when tackling harder tasks and the stabilizing effects of a progressive sample offering.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and provides a clear, thorough investigation into the limitations of pre-trained language models.
- The proposed CLR method shows strong experimental evidence of enhancing logical reasoning abilities.
- The BoolKill benchmark is a valuable contribution for evaluating nested boolean logic.

Weaknesses:
- The overall cohesion of the paper lacks distinct logical progression, necessitating a reconsideration of its organization.
- Section 7, dedicated to related work, appears disorganized and requires revision for better coherence.
- The evaluation in Section 6.2 is based on a limited sample size of only 10, which may lead to biased findings.
- Concerns were raised regarding the appropriateness of the model size used in the fine-tuning process.

### Suggestions for Improvement
We recommend that the authors improve the organization of the paper to enhance its logical flow and coherence. Additionally, we suggest revising Section 7 to ensure a more structured presentation of related work. The authors should consider expanding the evaluation sample size in Section 6.2 to provide more robust findings. Furthermore, we encourage the authors to clarify the choice of model size in the fine-tuning section and explore the integration of the proposed training process with general pre-training to balance specialized and general capabilities.