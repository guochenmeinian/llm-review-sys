ID: HTATedSTj4
Title: Whatâ€™s in a Query: Polarity-aware Distribution-based Fair Ranking
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel ranking method, DistFaiR, aimed at reducing individual and group unfairness in automated ranking systems by incorporating a distribution-aware fairness measurement. The authors introduce various divergence measures, such as $D_{L_1}$, $D_{L_2^{var}}$, and $D_{W_1}$, to characterize differences in attention and relevance distributions, considering higher-order moments like variance and skewness. Additionally, the paper emphasizes the significance of query polarity in fairness metrics, addressing the risk of fairwashing in ranking systems. The approach is validated through experiments on synthetic and real-world datasets, demonstrating its effectiveness compared to traditional baseline methods.

### Strengths and Weaknesses
Strengths:
1. The paper provides a clear and detailed description of its methodology, supported by mathematical formulations.
2. It effectively addresses limitations of existing fairness measurement methods by incorporating higher-order moments and query polarity.
3. The theoretical foundation is robust, showing that individual fairness can lead to improved group fairness.
4. The work is relevant to high-stakes applications, highlighting the societal implications of fairness in ranking systems.

Weaknesses:
1. The absence of source code complicates reproducibility of results.
2. Certain figures, such as Figure 1 and Figure 2, lack clarity and proper labeling, which may confuse readers.
3. The experimental setup relies solely on the proposed metrics without including recognized benchmarks.
4. The paper does not adequately address the complexities of real-world scenarios, such as overlapping group identities or the dynamic nature of queries.

### Suggestions for Improvement
We recommend that the authors improve the clarity of figures and provide proper labeling, particularly for Figure 1 and Figure 2. Additionally, including recognized metrics in the experimental evaluation would enhance the robustness of the findings. To facilitate reproducibility, the authors should consider sharing the source code. We suggest that the authors explore the computational trade-offs of using integer linear programming for large datasets and discuss how their framework can adapt to real-world complexities, such as overlapping identities and dynamic queries. Finally, incorporating user studies could validate the alignment of their fairness metrics with human perceptions of fairness.