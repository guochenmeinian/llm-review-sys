ID: Dqn715Txgl
Title: Navigating the Pitfalls of Active Learning Evaluation: A Systematic Framework for Meaningful Performance Assessment
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 5, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive evaluation of Active Learning (AL) methods, identifying five critical pitfalls in previous empirical evaluations and proposing a novel framework to address these issues. The authors investigate the interplay between Active Learning and Semi-Supervised and Self-Supervised Learning, providing extensive empirical results and making their codebase publicly available.

### Strengths and Weaknesses
Strengths:  
- The paper systematically identifies and addresses several pitfalls in AL evaluation, contributing significantly to the field.  
- It offers thorough experimental results that enhance understanding of AL methods, particularly in class-imbalanced settings.  
- The writing is clear and the structure is logical, making it accessible to readers new to AL.

Weaknesses:  
- The paper suffers from readability issues, with excessive content in some sections and unclear figures, particularly in Section 4 and Fig 2.  
- Some findings lack novelty, particularly regarding parameter tuning and the performance of self-supervised learning.  
- The experiments are limited to a single architecture (ResNet-18) and a single semi-supervised learning algorithm, which may restrict the generalizability of the results.

### Suggestions for Improvement
We recommend that the authors improve the readability by compressing Section 2.3 and relocating it to the appendices. Additionally, consider moving most of Section 4 to the appendices, replacing it with a concise summary of results and an intuitive, prescriptive section outlining the ideal evaluation framework for active learning. 

We also suggest that the authors provide a clearer explanation of the concept of "rolling-out Active Learning" to unseen datasets, dedicating 2-3 paragraphs to this topic in the main text. Furthermore, the authors should expand their dataset selection beyond CIFAR, incorporating more complex, real-world datasets and considering techniques for handling imbalanced data. 

Lastly, addressing the empirical evaluation's limitations, particularly regarding the CIFAR datasets and the performance metrics used, would strengthen the paper's conclusions.