ID: n01yLUy7Mj
Title: Interpreting and Analysing CLIP's Zero-Shot Image Classification via Mutual Knowledge
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 5, 6, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates Contrastive Language-Image Pretraining (CLIP) for zero-shot image classification by exploring the mutual knowledge between visual and textual modalities. The authors propose a method that analyzes 13 different CLIP models, varying in architecture, size, and pretraining datasets, to gain insights into zero-shot predictions. The study also addresses explainable artificial intelligence (XAI) in a multimodal context by computing mutual information dynamics between visual and textual features.

### Strengths and Weaknesses
Strengths:
- Introducing mutual information is a reasonable approach.
- The method is simple and effective, providing comprehensive experiments and analysis.
- The analysis of explicability through mutual information is original and potentially fruitful, with good performance compared to recent methods.

Weaknesses:
- The introduction and Figure 1 do not clearly explain the motivation, making it difficult to understand the paper's purpose.
- There is a lack of comparison with the latest literature on CLIP's generalizability.
- The presentation of the method is hard to follow, with unclear notations and descriptions, particularly regarding the generation of descriptors and the evaluation metrics.
- Some figures and tables lack clarity and proper formatting.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the introduction and Figure 1 to better convey the motivation behind the study. Additionally, the authors should include a comparison with recent works on CLIP's generalizability to strengthen the literature context. We suggest revising Section 3 for clearer presentation, particularly regarding the generation of descriptors, the specific prompts and LLM used, and the filtering mechanisms for noisy text descriptions. Furthermore, the authors should provide justifications for the performance metrics reported in Table 1 and clarify the evaluation details, including which visual and textual encoders were used. Lastly, addressing broader societal impacts and potential biases in the use of external LLMs would enhance the discussion of limitations.