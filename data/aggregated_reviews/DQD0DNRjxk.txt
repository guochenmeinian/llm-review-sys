ID: DQD0DNRjxk
Title: GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface Reconstruction in Open Scenes
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Gaussian Voxel Kernel Functions (GVKF) for 3D surface reconstruction, leveraging a continuous signed distance function derived from discrete 3D Gaussians. The authors propose a method that combines implicit and explicit representations to achieve high-fidelity open-scene reconstruction, claiming real-time rendering speeds and significant memory efficiency. Additionally, the paper provides a detailed analysis of Gaussian kernel functions and their application in volume rendering, specifically contrasting NeRF and 3DGS methodologies. The authors clarify that the Gaussian kernel does not strictly conform to a normal distribution and emphasize that opacity in their model is derived from Gaussian primitives, which represent continuous opacity along the ray. They aim to optimize the continuous opacity function using discrete Gaussian primitives, addressing the challenges of integrating 3DGS with continuous surface representations. However, the validation of these claims, particularly regarding memory efficiency, is inadequately addressed throughout the paper.

### Strengths and Weaknesses
Strengths:
1. The voxel kernel function is a novel concept that appears promising.
2. The paper provides a thorough mathematical framework that supports its claims and clarifies complex concepts.
3. The evaluation is comprehensive, with experiments conducted on multiple datasets, achieving state-of-the-art results on the MipNeRF360 dataset.
4. The authors effectively address reviewer questions, demonstrating a commitment to improving clarity and understanding.
5. An ablation study is included to analyze the impact of voxel size, and the proposed method shows promising performance improvements over state-of-the-art approaches.

Weaknesses:
1. Claims regarding significant memory savings are made without sufficient discussion or validation, particularly in the abstract.
2. Table 1's statements lack justification and contradict experimental results, particularly regarding implicit and explicit methods.
3. The contribution regarding memory reduction remains inadequately addressed, and the claim that taking maximum values of kernel functions for numerical integration leads to trivial conclusions lacks sufficient novelty.
4. The terminology surrounding 'kernel regression' is confusing and not clearly differentiated from 'differentiable rendering with Gaussian kernels.'
5. The approach is very similar to Gaussian Opacity Fields (GOF) but lacks a comparative analysis, despite apparent awareness of GOF.
6. The mathematical formulation is complex and contains misalignments in definitions, lacking clarity on the progression of equations.
7. Implementation details are insufficient, complicating reproducibility, and it is unclear if neural networks are utilized.
8. The experimental results, while strong, lack a cohesive analysis linking them to the overall narrative of the paper.

### Suggestions for Improvement
We recommend that the authors improve the discussion and validation of memory consumption claims throughout the paper, particularly in the abstract. Clarifying the discrepancies in Table 1 and ensuring consistency with experimental results is crucial. Additionally, we suggest including a comparison with GOF to substantiate the contributions of the proposed method. Enhancing the clarity of the mathematical formulations and providing more detailed implementation information will aid in reproducibility. We also recommend that the authors provide a more robust justification for the novelty of their integration approach, ensuring it is not perceived as trivial. Clarifying the distinction between 'kernel regression' and 'differentiable rendering with Gaussian kernels' would enhance understanding. Finally, we encourage the authors to strengthen the analysis of their experimental results to better connect them with the paper's main arguments and contributions, and to move the limitations from the appendix to the main paper for better visibility.