ID: Fr9d1UMc37
Title: LLM Dataset Inference: Did you train on my dataset?
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 7, 6, 5, 5, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to dataset inference in large language models (LLMs), addressing the limitations of traditional membership inference attacks (MIAs) that struggle to distinguish between training and non-training data from the same distribution. The authors propose a method that combines multiple MIA metrics to detect entire datasets used in model training, achieving statistically significant results with no false positives. The analysis is conducted on the Pythia model suite and the Pile dataset, highlighting the method's effectiveness in real-world scenarios involving copyright concerns.

### Strengths and Weaknesses
Strengths:
1. The paper provides a compelling argument for focusing on dataset-based inference rather than individual string-based MIAs.
2. It is well-structured and clearly written, making complex topics accessible.
3. The extensive testing on the Pythia models and the Pile dataset demonstrates the robustness of the proposed method.

Weaknesses:
1. The validation set's decontamination from training data is questionable, raising concerns about the effectiveness of MIAs.
2. The proposed method may lack novelty, as it primarily builds on existing MIA techniques.
3. The requirement for validation data from the same distribution as test data may not be practical in real-world applications.
4. The computational complexity of the proposed algorithm is not addressed, and its feasibility for larger models remains unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the validation set's decontamination process to enhance the credibility of their findings. Additionally, consider providing a detailed quantitative comparison of the computational complexity of the proposed method. It would be beneficial to explore the generalizability of the method across various models and datasets beyond the Pile dataset. Furthermore, we suggest including a qualitative analysis of samples to distinguish between attack samples with low and high p-values. Lastly, addressing the practical challenges of obtaining IID datasets for validation would strengthen the applicability of the proposed method.