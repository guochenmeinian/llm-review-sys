ID: b1XS87j323
Title: A Multi-Task Dataset for Assessing Discourse Coherence in Chinese Essays: Structure, Theme, and Logic Analysis
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel benchmark for assessing discourse coherence in human-generated texts in the Chinese language, introducing the Chinese Essay Discourse Coherence Corpus (CEDCC). The dataset comprises 501 essays from secondary school students and features a three-fold labeling schema for multi-task settings: discourse coherence grading, topic sentence extraction, and discourse relation recognition. The authors conducted experiments comparing various models across these tasks, revealing that no single model excels universally, advocating for a hybrid approach. The performance of ChatGPT was notably low across all tasks, and explicit discourse relations underperformed compared to implicit ones.

### Strengths and Weaknesses
Strengths:
- The dataset is composed of real-world texts, providing a valuable resource for researchers in Chinese discourse analysis.
- The paper is well-written, self-contained, and presents clear motivations for the work.
- A variety of models were tested, showcasing different approaches and architectures.

Weaknesses:
- The annotation of coherence into three classes may limit the assessment of this complex phenomenon; a wider Likert scale could be more appropriate.
- The paper lacks sufficient explanation for the choice of sentence-level topic annotation over existing topic continuity modeling.
- There is no development set for hyper-parameter tuning, raising concerns about the reliability of evaluations.
- Basic descriptive statistics of the dataset are missing, and the corpus construction details are inadequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the explanation of why sentence-level topic annotation was employed instead of existing topic continuity modeling. Additionally, we suggest integrating a wider Likert scale for coherence grading to capture its complexity more effectively. The authors should also include a development set for hyper-parameter tuning to enhance evaluation reliability. Furthermore, providing basic descriptive statistics of the dataset and clarifying corpus construction details, such as anonymization, would strengthen the paper. Lastly, we encourage the authors to highlight the correlations between the tasks more explicitly to create a cohesive narrative.