ID: xUoNgR1Byy
Title: Interpreting Learned Feedback Patterns in Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 5, 4, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 1, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to measure and interpret the divergence between learned feedback patterns (LFPs) in large language models (LLMs) and the human feedback used during reinforcement learning from human feedback (RLHF) training. The authors identify layers with significant parameter differences and utilize sparse auto-encoders (SAEs) to analyze activations from these layers. Probes are trained to predict feedback signals from the SAE outputs, allowing for comparisons with actual feedback. The study finds varying correlations between the probes and human feedback across different models.

### Strengths and Weaknesses
Strengths:
- The investigation into LFPs provides a novel perspective on understanding how LLMs learn from human feedback, contributing to the field of interpretability.
- The use of synthetic datasets enhances reproducibility and robustness, with datasets made publicly available for further research.
- The validation of probes through comparisons with GPT-4â€™s feature descriptions strengthens the reliability of findings.

Weaknesses:
- The probing method relies on several unverified assumptions, such as the interpretability of SAE outputs and the accuracy of the probes, which lack sufficient experimental evidence.
- The study's focus on a limited number of models and tasks may restrict the generalizability of the findings.
- The reliance on GPT-4 for feature interpretation introduces potential biases, and the paper does not adequately explain how activation deltas are calculated, leading to confusion.
- The results in Table 5 do not convincingly demonstrate the relevance of identified features, as differences before and after ablation are often minimal.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their methodology, particularly in explaining how activation deltas are computed and ensuring consistency in descriptions of probe inputs. Additionally, we suggest providing confidence intervals or standard errors in Table 5 to assess the significance of the differences observed. To enhance the robustness of their findings, the authors should explore other models or methods for validating the identified features beyond GPT-4. Finally, addressing the limitations of their approach regarding the generalizability of results across different LLM architectures and tasks would strengthen the paper.