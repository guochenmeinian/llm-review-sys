ID: CQgmBmRBMb
Title: Don’t Add, don’t Miss: Effective Content Preserving Generation from Pre-Selected Text Spans
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents advancements in the controlled text reduction (CTR) task by proposing a new CTR model that incorporates Highlights-Oriented RL Training and Highlights-Sensitive Decoding, while also enhancing the CTR dataset using GPT-4. The authors demonstrate that their approach achieves significant improvements over baseline models.

### Strengths and Weaknesses
Strengths:  
- The proposed approach is well-motivated and reasonable, achieving significant improvements over baselines.  
- The paper is well-written and easy to follow, with extensive experimentation conducted.  

Weaknesses:  
- The novelty of the work is limited, as many techniques are borrowed from existing literature with only slight modifications.  
- The experimental section is inadequate, lacking thorough human evaluation and failing to analyze the performance decrease when combining the two strategies. Additionally, the evaluation metrics used may not be sufficient to explain the results.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contributions by providing more original insights rather than relying heavily on existing methods. Additionally, we suggest conducting more comprehensive experiments, including human evaluations of the CTR model with both strategies, to clarify the performance results. It would also be beneficial to provide clearer details on the parameter settings and the quality of the training data used.