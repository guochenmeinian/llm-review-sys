ID: bISkJSa5Td
Title: Neural Lad: A Neural Latent Dynamics Framework for Times Series Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new neural ODE framework, Neural Lad, which decomposes the differential function into three components: a neural network differential function, an attention-based network, and a time-dependency function, along with a graph convolution network for spatial correlations. The authors evaluate the method on both univariate and multivariate time series forecasting tasks, demonstrating that it outperforms or matches existing models, including Neural ODEs, time-series transformers, and graph neural networks.

### Strengths and Weaknesses
Strengths:
- The work is a novel combination of established techniques, clearly comparing itself to existing methods like Neural CDEs.
- The related work section is well-organized, providing a comprehensive overview.
- The submission is clearly written, making it easy to follow the proposed method.
- Empirical results indicate improved forecasting performance across various datasets.

Weaknesses:
- The function $h_w(t)$, intended to extract seasonal trends, lacks clarity in both mathematical formulation and experimental validation.
- The attention-based network's relationship with the memory matrix is unclear, and the matrix is not visualized.
- Key experimental details, such as input data points and dataset specifics, are missing.
- The hyperparameter section lacks information on the solver and optimizer used.
- The block residual architecture description is unclear and would benefit from visual representation in the main text.
- There is insufficient analysis of the model's performance across different components and conditions under which $f_\theta(z_t)$ remains valid.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the function $h_w(t)$ and provide experimental evidence demonstrating its effectiveness in capturing seasonal trends. Additionally, please clarify the role of the memory matrix in the attention-based network and include visualizations. We suggest updating equations (1) and (2) to reflect the prediction as $\hat{x_{t:t+H}}$ and ensure that the relationship for the first function $\xi$ in line 92 aligns with equation (3). We also advise including crucial experimental details in the supplementary material, such as input data points and dataset characteristics. Furthermore, please provide more information on the solver and optimizer in the hyperparameter section. Moving the figure illustrating the block residual architecture to the main text would enhance clarity. Lastly, we encourage a more thorough discussion on the computational costs and limitations of Neural Lad, particularly in relation to transformer models.