ID: VIDDZO2f0A
Title: Editing Common Sense in Transformers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the causal relationship between commonsense knowledge and localized, editable parameters in Transformers, specifically investigating whether model parameters can be updated to correct erroneous commonsense knowledge. The authors propose three main contributions: 1) demonstrating strong causal relations between commonsense plausibility judgments and early MLP layers in Transformers; 2) enhancing the MEMIT parameter editing algorithm to MEMIT_{CSK} for commonsense plausibility prediction; and 3) introducing a new dataset for comprehensive evaluation, which includes unaffected and affected neighborhoods, paraphrases, and reasoning challenges.

### Strengths and Weaknesses
Strengths:  
- The paper presents an interesting approach to updating language models by directly editing model parameters without retraining.  
- The improved MEMIT_CSK outperforms fine-tuned baselines by over 10% on two datasets.  
- The organization and presentation of the paper are commendable, with clear visual aids like Fig. 1.  
- The study provides a rich set of experiments and comprehensive discussions on commonsense knowledge editing.

Weaknesses:  
- The paper does not sufficiently highlight the differences between the new dataset and previous ones, particularly in the introduction.  
- The rationale behind the modifications to the MEMIT algorithm is not adequately explained prior to presenting results.  
- The work appears incrementally similar to existing methods like MEMIT and Rome, lacking exploration of larger models beyond GPT-2.  
- The abstract's claim of being a black-box approach contradicts the requirement for probability distribution and gradient information.

### Suggestions for Improvement
We recommend that the authors improve the introduction by clearly delineating the differences between the new dataset and previous datasets. Additionally, we suggest that the authors provide a more thorough explanation of the rationale behind the adaptations made to the MEMIT algorithm before presenting results. It would also be beneficial to explore commonsense editing in larger models and clarify the connection between editing factual and commonsense knowledge. Lastly, we advise including a discussion on the moving average of AIE in the experimental section and addressing how editing can be evaluated in forms other than binary classification.