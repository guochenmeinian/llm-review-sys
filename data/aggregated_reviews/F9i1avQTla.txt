ID: F9i1avQTla
Title: SAM-Guided Masked Token Prediction for 3D Scene Understanding
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a two-stage SAM-guided pre-training method for 3D scene understanding, introducing a group-balanced re-weighting method for long-tail representation distillation and a SAM-guided tokenization method to align 2D and 3D region-level features. The authors demonstrate the effectiveness of their approach through extensive experiments on various downstream tasks.

### Strengths and Weaknesses
Strengths:  
1. The adoption of SAM to guide pre-training is logical, as it aids the model in learning high-quality patterns.  
2. The results significantly surpass previous state-of-the-art methods.  
3. The SAM-guided tokenization method effectively aligns 2D and 3D representations, addressing limitations of traditional KNN-based techniques.  
4. The introduction of a group-balanced re-weighting strategy improves representation of under-represented samples.  

Weaknesses:  
1. Despite the proposed pre-training method, the model's overall mAP for 3D object detection remains lower than state-of-the-art detectors like CAGroup3D and VDETR, necessitating supporting experimental results for application on these architectures.  
2. The exploration of using SAM to guide 3D pre-training has been previously addressed in SEAL, requiring more comparison or discussion.  
3. The paper should compare not only with transformer-based pre-training methods but also with other state-of-the-art techniques like PPT.  
4. The necessity of two-stage training for self-supervised learning is unclear, and the rationale behind group-balanced re-weighting in the presence of noisy pseudo labels is not well justified.  
5. There are typos and inconsistencies in reference formatting, and a visual comparison with current SOTA methods for mask generation would strengthen the justification of its effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the applicability of their method to more advanced architectures, such as CAGroup3D and VDETR, by providing supporting experimental results. Additionally, we suggest including more comparisons with recent methods like CLIP2Scene and SEAL to verify the efficiency of the proposed approach. Clarifying the rationale behind the choice of SAM as the distillation source and elaborating on the necessity of two-stage training would enhance the paper's depth. Furthermore, addressing the identified typos and reference inconsistencies, along with providing visual comparisons for mask generation, would improve the overall presentation. Lastly, we encourage the authors to explore the potential of their method in online 3D perception settings and include relevant visualizations in the revised version.