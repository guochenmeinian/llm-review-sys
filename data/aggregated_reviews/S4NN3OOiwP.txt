ID: S4NN3OOiwP
Title: Efficient Equivariant Transfer Learning from Pretrained Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 6, 7, 5, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents $\lambda$-equitune, a novel method that refines existing strategies for achieving equivariant outputs from non-equivariant neural networks by employing importance weights for feature averaging. The authors validate the effectiveness of $\lambda$-equitune and equizero across diverse applications, including image classification, deep Q-learning, and natural language generation. The paper also demonstrates that $\lambda$-equitune serves as a universal approximator of equivariant functions, with empirical evidence supporting its claims.

### Strengths and Weaknesses
Strengths:  
- **Originality**: The introduction of importance weights for feature averaging addresses limitations in existing equivariance methods, contributing to the originality of the work.  
- **Quality**: The research is well-supported by rigorous theoretical justifications and empirical evaluations, ensuring reliability.  
- **Clarity**: The paper is well-written, presenting complex concepts clearly and concisely, with a logical organization that enhances understanding.  
- **Significance**: The broad applicability of the methods across various domains highlights their potential to advance transfer learning.

Weaknesses:  
- The exploration of continuous groups is limited, as the focus is primarily on finite groups, restricting the generalizability of the proposed algorithms.  
- Sub-Section '3.2 Properties' lacks clarity and requires a more detailed analysis to ensure logical consistency among definitions and theorems.  
- The novelty of the work compared to Basu et al. (2023) needs further clarification, particularly regarding the introduction of extra parameters and fine-tuning processes.  
- The experimental settings for image classification may be overly simplistic, as they primarily involve 90-degree rotations and flips, which may not represent more complex transfer learning tasks.

### Suggestions for Improvement
We recommend that the authors improve the discussion on extending their methods to continuous groups by providing concrete solutions or insights. Additionally, we suggest that the authors clarify the connections between Theorem 1, Definition 1, and Theorem 2 in Sub-Section '3.2 Properties' to enhance logical consistency. It would also be beneficial for the authors to elaborate on the novelty of their work in relation to Basu et al. (2023) and to consider more complex experimental settings in image classification to better reflect realistic transfer learning tasks.