ID: gJxEiRcnao
Title: Biologically Inspired Learning Model for Instructed Vision
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 8, 6, 3, 6, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel biologically plausible alternative to backpropagation, introducing a model that integrates bottom-up (BU) and top-down (TD) networks, symmetric in structure. The authors propose a Counter Hebbian learning rule that updates weights based on the firing sequence of neurons, allowing for task-guided learning through a TD network that serves dual purposes of task instruction and error propagation. Experiments demonstrate that this learning rule approximates backpropagation and achieves competitive performance across various tasks.

### Strengths and Weaknesses
Strengths:  
- The model offers an attractive, biologically inspired alternative to backpropagation, relying on local synaptic learning without computational "tricks."  
- The integration of task-based instruction into visual processing is computationally elegant and biologically realistic.  
- The authors effectively explain complex concepts clearly and simply.

Weaknesses:  
- The reference formatting is difficult to read; a bracketed number or (Author, year) format would improve clarity.  
- There are minor typographical errors and unnecessary abbreviations that could be streamlined.  
- Some discussions, particularly regarding backpropagation in Related Work, lack clarity.  
- Visual representations, such as Figure 3, are hard to interpret, and graphs in the appendix lack axis labels.

### Suggestions for Improvement
We recommend that the authors improve the reference formatting for better readability. Additionally, addressing the typographical errors and reconsidering the use of certain abbreviations would enhance clarity. We suggest clarifying the discussion of backpropagation in Related Work and improving the visual clarity of Figure 3 and the graphs in the appendix by adding axis labels. Furthermore, we encourage the authors to conduct additional ablation experiments to dissect the components of the Counter Hebbian learning rule and to explore the model's scalability and performance on more complex tasks beyond toy-level datasets.