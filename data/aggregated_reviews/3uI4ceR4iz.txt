ID: 3uI4ceR4iz
Title: SA3DIP: Segment Any 3D Instance with Potential 3D Priors
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "SA3DIP: Segment Any 3D Instance with Potential 3D Priors," a novel method for 3D instance segmentation that integrates geometric and textural priors to enhance segmentation accuracy. The authors propose a 3D detector to refine segmentation and introduce the revised ScanNetV2-INS dataset with improved annotations. Experimental results across multiple datasets demonstrate the method's effectiveness and robustness, addressing common issues of under-segmentation and over-segmentation found in existing methods.

### Strengths and Weaknesses
Strengths:  
- The innovative integration of geometric and textural priors significantly reduces initial errors and enhances segmentation quality.  
- The introduction of a 3D detector effectively addresses over-segmentation issues.  
- The revised ScanNetV2-INS dataset provides a more accurate benchmark for evaluating 3D segmentation models, contributing valuable data to the research community.  
- Experimental results convincingly demonstrate the robustness and effectiveness of the proposed method.

Weaknesses:  
- The framework closely resembles SAI3D, with the main difference being the addition of a 3D detector, limiting its perceived innovation.  
- The performance improvement heavily relies on the pre-trained 3D detector, which diminishes the originality of the contribution.  
- Insufficient documentation and analysis of the new annotations for ScanNetV2-INS, making it difficult to assess the dataset's complexity and utility.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition and derivation of 'histogram vectors' in Section 3.1 to enhance understanding of their role in the methodology. Additionally, we suggest including visual comparisons with SAMPro3D in Figure 4 to substantiate the claims of performance. More comprehensive experiments are needed to validate the effectiveness of the Complementary Primitives Generation module, and the authors should further discuss the motivations for using color value similarities as texture priors. Lastly, providing a more detailed analysis of the new annotations for ScanNetV2-INS, including quantitative statistics, would strengthen the paper's contributions.