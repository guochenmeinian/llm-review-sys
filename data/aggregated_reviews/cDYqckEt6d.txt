ID: cDYqckEt6d
Title: DiscoveryWorld: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 7, 6, 8, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DiscoveryWorld, a virtual environment designed for developing and benchmarking AI agents' capabilities in scientific discovery. It includes a suite of 120 tasks across eight scientific domains, requiring agents to hypothesize, experiment, analyze, and conclude. The evaluation framework incorporates metrics for task completion, performance scores for scientific steps, and accuracy of discovered knowledge. The authors demonstrate that even strong baseline agents struggle with many tasks, highlighting the benchmark's potential in advancing AI capabilities in scientific discovery.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and articulates the motivation for studying scientific discovery, which may have broader impacts on the community.
- DiscoveryWorld covers a wide range of real-world scientific scenarios, encouraging the development of discovery skills in agents.
- The inclusion of expert human evaluations provides a clear baseline for assessing agent performance and highlights the gap between current LLM capabilities and human reasoning.

Weaknesses:
- The evaluation of reasoning is conflated with basic navigation and manipulation skills, which may detract from the focus on scientific discovery.
- The benchmark is prohibitively expensive to run, limiting accessibility for researchers outside of well-funded institutions.
- The claim of 120 tasks is misleading; it consists of 8 tasks with 3 difficulty levels and 5 episodes each.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by simplifying the API to reduce the emphasis on navigation and manipulation, which distracts from core scientific tasks. Additionally, incorporating reinforcement learning experiments could provide deeper insights into agent capabilities. To enhance accessibility, consider developing a subset of tasks that are easier to evaluate, thereby lowering the computational and budget barriers. Clarifying the task difficulty variations and providing a clearer explanation of the differences in human versus agent performance would also strengthen the paper. Finally, we suggest exploring the potential for a simplified environment configuration that removes movement requirements to better align with the intended applications in automated lab settings.