ID: XY2qrq7cXM
Title: Gradient Rewiring for Editable Graph Neural Network Training
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a model editing technique for graph neural networks (GNNs) called Gradient Rewiring (GRE), which addresses the challenge of maintaining performance for both target and training nodes during fine-tuning. The authors propose a method that rewires gradients to preserve locality and enhance training, validated through experiments on various datasets. The paper also introduces a more advanced version, GRE+, and evaluates both methods against baseline approaches.

### Strengths and Weaknesses
Strengths:
- The proposed method effectively addresses persistent errors in GNNs without compromising original performance, supported by three metrics: accuracy after editing, the DropDown metric, and the success rate of error correction.
- The authors provide a well-motivated and organized introduction, along with a clear derivation of the methodology.
- Comprehensive empirical evaluations demonstrate the method's scalability and consistent performance across datasets.

Weaknesses:
- The distinction between GRE and GRE+ is unclear, as GRE does not consistently perform well on its own, leading to confusion in the presentation.
- The writing contains numerous grammatical issues and lacks clarity, making it difficult to follow.
- There is no analysis of the cost/complexity of the model editing methods, and the significance of editing GNNs remains ambiguous.
- Some experimental details, such as dataset splitting and time complexity, are missing.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing by revising sections with grammatical issues and confusing phrasing. Specifically, consider rephrasing the introduction and providing a clearer explanation of the experimental process and results. Additionally, we suggest that the authors include a table detailing the memory requirements for different models and analyze the cost/complexity of the model editing methods. It would also be beneficial to consolidate the presentation of GRE and GRE+ into a single method with an ablation study to clarify their relationship. Lastly, addressing the missing experimental details and providing a discussion on the significance of editing GNNs would strengthen the paper.