ID: yHdTscY6Ci
Title: HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 7, 9, 6, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HuggingGPT, a framework that automates the use of large language models (LLMs) to call upon various existing models for solving complex AI tasks. The proposed workflow consists of four stages: task planning, model selection, task execution, and response generation. The authors demonstrate that by leveraging ChatGPT and numerous models from Hugging Face, HuggingGPT can effectively address a wide range of sophisticated tasks across different modalities, achieving notable results. The authors also propose a planning strategy to extend the task scope of LLMs, aiming to parse user requests into task sequences with a reported accuracy of over 50% using ChatGPT or GPT-4. However, concerns are raised regarding the lack of a simple ablation study to validate the proposed approach, questioning the correlation between the planning strategy and task performance.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- It introduces a novel idea supported by robust experimental results.
- The framework effectively connects LLMs with diverse models in the ML community, enhancing the applicability of LLMs for multi-modal tasks.
- The proposed idea is recognized as interesting and valuable, with empirical results demonstrating HuggingGPT's performance against ChatGPT, showing a win rate of 56.5% and 60.9% in evaluations.

Weaknesses:
- The method's reliance on state-of-the-art LLMs like ChatGPT raises practical concerns, such as high API costs, and the potential for under-explored alternatives like open-source LLMs (e.g., Vicuna).
- The evaluation protocol lacks maturity, with insufficient evidence to validate the evaluation metrics used and the selection criteria for models.
- The planning strategy appears simplistic compared to existing iterative planning methods, and the experimental results are limited in scope, lacking comprehensive evaluations across all framework stages.
- The absence of a straightforward ablation study raises doubts about the effectiveness of the proposed planning strategy.
- The reliance on only two examples to support claims about HuggingGPT's performance is deemed insufficient.

### Suggestions for Improvement
We recommend that the authors improve the evaluation protocol by providing clearer criteria for model selection among the top-K models and ensuring consistent evaluation metrics across tables. Additionally, the authors should consider incorporating human evaluations alongside automated metrics to strengthen the experimental results. We suggest that the authors explore the potential of open-source LLMs and provide more details on the demonstration examples used in task parsing. Furthermore, we recommend that the authors conduct a simple ablation study comparing HuggingGPT against solving sequential or graph language-only tasks without any planning, serving as a naive baseline to address concerns regarding HuggingGPT's performance relative to LLMs. Finally, we suggest scaling up the examples provided to strengthen the argument for the proposed planning strategy and its correlation with task performance.