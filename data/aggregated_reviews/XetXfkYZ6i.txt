ID: XetXfkYZ6i
Title: Deep Recurrent Optimal Stopping
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for optimal stopping that generalizes previous approaches by incorporating non-Markovian settings and utilizing a Bayesian network formulation. The authors propose an RNN-based method for optimal stopping, which can be trained via direct optimization or expectation-maximization, demonstrating its superiority over existing methods on common datasets.

### Strengths and Weaknesses
Strengths:
- The authors introduce a reasonable solution to the optimal stopping problem, supported by theoretical justifications.
- The approach is well-motivated, and experimental results indicate that their method outperforms baseline models.
- The paper is well-organized and includes real-world benchmark experiments.

Weaknesses:
- The writing and organization of the paper require significant improvement, with an overuse of abbreviations that complicates readability.
- The presentation of technical details is excessive, with some content better suited for supplementary materials.
- There is a lack of ablation studies to justify design decisions, and the paper does not adequately discuss its limitations due to the absence of a conclusion or discussion section.

### Suggestions for Improvement
We recommend that the authors improve the clarity and conciseness of the writing by reducing the use of abbreviations and reorganizing the content for better flow. It would be beneficial to move some technical details, such as the equivalence of EM and policy gradient, to supplementary materials. Additionally, we suggest including ablation studies to validate key design choices and adding a conclusion or discussion section to address the limitations of the work. Finally, we encourage the authors to test their algorithm on various American options to strengthen their findings.