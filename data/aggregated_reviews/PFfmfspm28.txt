ID: PFfmfspm28
Title: Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 7, 6, 4, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Minigrid and Miniworld libraries, which are simulated environments for reinforcement learning (RL) focused on goal-oriented tasks. The authors propose a minimalistic design philosophy that facilitates easy installation, customization, and visualization of these environments. They highlight that these libraries are suitable for a wide range of research topics, including curriculum learning, exploration, meta-learning, and transfer learning, with a unified API that enhances transfer learning capabilities between different observation spaces. The paper includes case studies demonstrating the advantages of unified APIs and experimental evaluations of transfer learning effects on neural network modules. However, it also discusses the limitations of the environments, particularly their basic nature and lack of complex features compared to other simulation environments. Source code and comprehensive documentation are provided.

### Strengths and Weaknesses
Strengths:
- The paper is well presented and easy to follow.
- Open-sourced code and comprehensive documentation enhance usability.
- The minimalistic design streamlines installation and customization.
- The libraries support diverse research topics and have been widely adopted in the RL community, evidenced by numerous citations.
- The unified API facilitates transfer learning between different observation spaces.

Weaknesses:
- Limited discussion of existing works utilizing Minigrid and Miniworld.
- Lack of detailed comparisons with other related deep reinforcement learning (DRL) environments.
- The simplicity of the environments limits their applicability for more complex RL tasks.
- Transfer learning results could be better illustrated with learning curves.
- The lack of physics engines and advanced object interactions restricts the realism and complexity of simulations.
- The training process may be slower compared to other advanced RL libraries, and there is a need for clearer performance metrics.

### Suggestions for Improvement
We recommend that the authors improve the discussion on existing works that adopt Minigrid and Miniworld, including a wider range of related DRL environments such as DeepMind Lab 2D and Melting Pot. Additionally, we suggest including learning curves in Section 3.1 to provide a clearer understanding of transfer learning results. The authors should clarify the advantages of the new benchmark over previous Minigrid publications and include performance curves for Miniworld tasks to provide clearer metrics on success rates. Furthermore, we encourage the authors to explore the scalability of the environments to more complex tasks and enhance them with features that allow for more complex interactions and physics simulations while maintaining ease of use. Finally, implementing an end-to-end GPU pipeline could improve performance, while balancing complexity for users.