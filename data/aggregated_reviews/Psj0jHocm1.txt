ID: Psj0jHocm1
Title: Contextually Affinitive Neighborhood Refinery for Deep Clustering
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 6, 4, 7, 6, -1, -1, -1
Original Confidences: 4, 5, 4, 5, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Contextually Affinitive Neighborhood Refinery (CoNR) framework, which enhances deep clustering by employing an online re-ranking process to identify informative neighbors and promote cross-view neighborhood consistency. The method incorporates a progressive boundary filtering strategy to mitigate noise near cluster boundaries. Validation of CoNR is conducted on five standard deep clustering benchmarks, demonstrating its effectiveness compared to state-of-the-art methods. Additionally, the authors propose a method for deep clustering that builds on pre-trained models, addressing scalability concerns to large datasets like ImageNet. They acknowledge that while their method requires further training, preliminary experiments on ImageNet demonstrate adaptability despite time constraints. The authors clarify that their evaluation protocols align with prior works and provide additional experiments to justify their results across various metrics and settings, emphasizing that the efficacy of self-supervised methods does not directly correlate with clustering ability, including DINO as a baseline to showcase improvements.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with clear motivations and thorough explanations of each component.
- CoNR shows promising empirical results, surpassing existing methods on multiple benchmarks.
- The framework's design allows for easy integration into other self-supervised learning (SSL) frameworks.
- The authors provide a clear justification for their experimental settings, aligning with standard practices in deep clustering.
- Preliminary results on ImageNet indicate the method's adaptability to large datasets.
- Additional experiments enhance the robustness of their findings across different evaluation protocols.

Weaknesses:
- The paper lacks a thorough analysis of its limitations and potential scenarios where CoNR may not perform optimally.
- Several terms and equations are introduced without adequate explanation, leading to potential confusion (e.g., definitions of $l$ in Eq. 6 and $r$ in Eq. 7).
- The motivation for defining the graph in Eq. (3) and the subsequent message passing in Eq. (6) is missing, necessitating theoretical justification and ablation studies.
- The method's reliance on pre-trained models may limit its applicability to very large datasets without further training.
- The authors do not fully explore the implications of not having access to K during clustering, which could be a significant contribution.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by defining all terms and notations consistently throughout the text. Additionally, a deeper exploration of the limitations of the CoNR framework should be included to provide a balanced perspective. The authors should conduct experiments on larger datasets to validate the scalability of the proposed method and compare it against more recent state-of-the-art self-supervised methods. Furthermore, we suggest adding a theoretical justification for the graph definition and message passing process, along with an ablation study to support the claims made regarding the effectiveness of the boundary filtering strategy. We also recommend that the authors improve the exploration of scalability to very large datasets, such as ImageNet, by conducting more extensive experiments beyond preliminary results. Additionally, we suggest that the authors investigate the implications of automatically determining K during clustering, as this could strengthen their technical contributions. Finally, we encourage the authors to further clarify the distinction between self-supervised learning efficacy and clustering ability to enhance the paper's argumentation.