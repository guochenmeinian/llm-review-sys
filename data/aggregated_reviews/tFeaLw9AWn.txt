ID: tFeaLw9AWn
Title: Single-Call Stochastic Extragradient Methods for Structured Non-monotone Variational Inequalities: Improved Analysis under Weaker Conditions
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 6, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents new convergence results for single-call stochastic extragradient methods applied to quasi-strongly monotone and weak Minty variational inequality (VI) problems. The authors introduce the expected residual (ER) condition, which generalizes existing conditions in stochastic optimization to the VI context, allowing for convergence guarantees without the bounded variance assumption. The paper establishes convergence results for both constant and decreasing step size rules and provides expressions for ER parameters under non-uniform sampling, supported by numerical experiments. The authors argue that their work addresses open questions in the performance of stochastic extragradient methods (SPEG) and emphasizes that their approach is distinct from previous works, particularly in its use of a single oracle call per iteration compared to the two oracle calls in related literature.

### Strengths and Weaknesses
Strengths:
- Clear presentation of results and thorough discussion of the ER condition's background and context.
- First convergence guarantee for stochastic optimistic gradient (SOG) under arbitrary sampling without bounded variance assumptions.
- Comprehensive analysis of convergence for both quasi-strongly monotone and weak Minty VI problems.
- The paper provides a first-time convergence guarantee for extragradient methods applied to weak Minty VIPs without the bounded variance assumption.
- The authors effectively address open questions in the performance of SPEG, showcasing the novelty of their approach.

Weaknesses:
- The practicality of the proposed step size rules is questionable; they may be overly conservative or difficult to determine in practice.
- The expected residual condition appears restrictive, as it is applied directly to stochastic estimators rather than stochastic oracle queries.
- Some reviewers perceive the proof techniques as incremental, questioning the overall novelty of the work.
- The limited scope of the paper is noted, with suggestions that it may not fully address broader applications of adaptive step sizes in optimization.
- The novelty of the contributions is debated, with some reviewers suggesting significant overlap with existing literature.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the practicality of the proposed step size rules. Specifically, consider discussing the feasibility of using more aggressive step sizes that may not strictly adhere to the theorems but could enhance convergence in practice. Additionally, we suggest providing practical examples that satisfy the definitions used, particularly for weak Minty variational problems. Clarifying the stochastic settings—whether finite-sum or infinite-sum—would also enhance the paper's comprehensibility. Furthermore, we recommend that the authors improve the clarity of their contributions by explicitly detailing how their work extends beyond previous research, particularly in the context of proof techniques. Lastly, we encourage the authors to elaborate on the novelty of their contributions in relation to existing works, particularly addressing the overlap with previous studies, and consider discussing the implications of their findings on adaptive step sizes in optimization, even if it is outside the primary scope of the paper, to strengthen their argument regarding practical applications.