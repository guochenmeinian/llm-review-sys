ID: SFxAjB7UXx
Title: Scalable Optimization in the Modular Norm
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 5, 8, 7, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the *modular norm*, a new norm for deep learning that allows for recursive composition, facilitating the control of the Lipschitz constant of the network and loss gradient. The authors propose a method to scale gradient updates using this norm and empirically demonstrate its effectiveness in achieving invariance of optimal hyperparameters to scale. Additionally, the authors define a single normalize function that can be applied to *either* Adam *or* SGD, facilitating good learning rate transfer across optimization algorithms. They acknowledge the need for practical applications and commit to including comparisons with muP in the camera-ready version. The authors highlight the theoretical significance of ensuring Lipschitz smoothness in the modular norm, despite the lack of a clear application, and express hope that this will inspire future research.

### Strengths and Weaknesses
Strengths:  
- The paper tackles a significant problem in neural network optimization, introducing a mathematically rigorous norm that captures essential features for scaling.  
- The concept of an architecture-agnostic norm that scales gracefully in width and depth is innovative and challenges existing practices in hyperparameter tuning.  
- The paper introduces interesting ideas and evaluates them effectively, demonstrating a willingness to engage with reviewers and improve based on feedback.  
- Experimental results show promising robustness in learning rate transfer, and the originality of the work is noted positively.

Weaknesses:  
- The introduction of "masses" feels somewhat ad hoc, shifting the hyperparameter adjustment burden to determining these masses for modules.  
- The term ‘feature learning’ is used ambiguously compared to existing literature, leading to potential confusion.  
- The paper currently lacks clear, well-supported claims and in-depth discussions on key concepts, such as the relevance of Lipschitz smoothness in optimization.  
- Many avenues remain open without sufficient clarity, which may hinder the paper's impact.  
- The presentation could be clearer, particularly in linking motivations and results, and the experimental validation is limited, especially concerning larger transformer architectures.  
- The absence of a formal contribution list at the end of the introduction may confuse some readers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their definitions, particularly regarding ‘feature learning’ and its implications in Proposition 3, to avoid confusion. The authors should consider restructuring Section 2 to provide a clearer motivation for the modular norm and its relationship to Lipschitz continuity. Additionally, we suggest including a comprehensive list of contributions early in the paper to clarify the significance of the work. We recommend that the authors improve the clarity of their claims and provide well-supported discussions on key concepts, particularly regarding the relevance of Lipschitz smoothness according to the modular norm. Expanding the experimental section to include larger transformer models, such as the Pythia 160M, would strengthen the empirical validation of the proposed method. Finally, addressing the tightness of the bounds in the modular norm and comparing it with existing methods like muP would provide valuable insights into its effectiveness and increase the paper's impact.