ID: cAjZ3tMye6
Title: HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 6, 7, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for utilizing external large language models (LLMs) for automatic speech recognition (ASR) error correction, introducing the "Hypothesis Paradise" (HP) dataset, which comprises N-best hypotheses and their corresponding accurate transcriptions. The authors propose several ASR error correction techniques, including self-activated in-context learning and hypotheses-to-transcription training (H2T-ft and H2T-lora), demonstrating their effectiveness in reducing word error rates (WER) across various ASR corpora. Additionally, the paper establishes an open-source and reproducible dataset for generative error correction in ASR, incorporating enhancements such as a Mandarin dataset from AISHELL-1, error correction baselines, and acoustic scores for hypotheses. The authors report results on noise-robust ASR and code-switching tasks, aiming to create benchmarks for generative error correction to address the lack of public datasets in this area.

### Strengths and Weaknesses
Strengths:
- The authors propose innovative approaches, including self-activated in-context learning and hypotheses-to-transcription training (H2T-ft and H2T-lora).
- The HP dataset is extensive, constructed from multiple ASR corpora across nine different domains, and introduces a novel dataset for generative error correction in ASR.
- The authors have made significant modifications based on reviewer feedback, including the addition of error correction baselines and acoustic scores.
- Experimental results indicate a significant reduction in WER, showcasing the effectiveness of the proposed methods, and the incorporation of multilingual capabilities and noise-robust ASR demonstrates the dataset's versatility.

Weaknesses:
- The paper resembles a standard research paper rather than a dedicated work on Track Datasets and Benchmarks, as it primarily focuses on LLMs for ASR error correction.
- Comparisons are limited to the proposed method and LM reranking; broader comparisons with other ASR error correction techniques are lacking.
- The discussion of the data selection process lacks motivation and clarity regarding dataset choices and their implications.
- The paper currently limits its scope to English and Mandarin, with a need for broader language coverage.
- The authors have not yet fully explored the implications of using speech lattices for correction.
- There is a lack of direct comparison with existing error correction models like FastCorrect, as pre-trained models are temporarily unavailable.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the data selection process in section 3.2 by providing clear motivations for each dataset included in the benchmark. Additionally, the authors should enhance the discussion of the ASR system selection process in section 3.1, detailing the rationale behind model choices and configurations. It would be beneficial to assign a single score to models across datasets to facilitate comparison and to explore the performance of in-context LLMs using top-1 predictions. We suggest including additional datasets that reflect noisy conditions and expanding the benchmark to include multiple languages, explicitly addressing the limitations of focusing solely on English and Mandarin. Furthermore, we encourage the authors to explore the use of speech lattices in the HP dataset to leverage richer information for correction. We also recommend providing a more comprehensive comparison with existing error correction models, such as FastCorrect, once the pre-trained models become available. Finally, the authors should clarify the limitations of their approach in the main paper, rather than relegating them to the appendix, and consider providing a reproducibility scheme and evaluation scripts to enhance the benchmark's usability and transparency.