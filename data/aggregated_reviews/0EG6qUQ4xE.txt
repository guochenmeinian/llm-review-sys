ID: 0EG6qUQ4xE
Title: AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 4, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AR-DIFFUSION, a diffusion model that incorporates autoregressive-like generation behavior through a multi-level diffusion strategy, which includes both sentence-level and token-level diffusion. The authors propose a skipping mechanism to expedite the generation process and demonstrate the model's superiority over existing diffusion language models in text generation tasks and inference efficiency.

### Strengths and Weaknesses
Strengths:
1. The methodology is intuitive and well-motivated, integrating autoregressive dependency into diffusion models.
2. The skipping mechanism effectively accelerates the generation process.
3. Comprehensive experiments validate the efficacy of the proposed method across various text-generation tasks.

Weaknesses:
1. The paper's writing is hard to follow, requiring significant improvement.
2. The experiments lack convincing comparisons with a broader range of diffusion model baselines, particularly with models like DiffuSeq, CDCD, and others.
3. The reported performance metrics, particularly the NFE metric, may be misleading as they do not account for the MBR process.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing to enhance readability. Additionally, it is crucial to include a more exhaustive comparison with relevant diffusion model baselines, such as DiffuSeq and CDCD, to substantiate claims regarding the model's performance. Furthermore, we suggest clarifying the discrepancies in performance metrics, particularly regarding the reported scores of GENIE versus AR-DIFFUSION, and consider increasing the parameter size to match GENIE's level for a fair comparison. Lastly, addressing the robustness of AR-Diffusion in infilling tasks and comparing decoding speeds with NAR models would provide valuable insights.