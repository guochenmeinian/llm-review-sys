ID: L1mMK39Z7P
Title: ACES: Generating a Diversity of Challenging Programming Puzzles with Autotelic Generative Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 4, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Autotelic CodE Search (ACES), a method for generating diverse and challenging Python programming puzzles by leveraging state-of-the-art generative models. ACES aims to automate problem generation by optimizing for both diversity and difficulty, utilizing previously generated puzzles as in-context examples. The authors conduct experiments demonstrating that ACES produces puzzles that are more diverse and significantly more challenging than those generated by existing methods and benchmarks. Additionally, the authors propose a method for generating coding puzzles using LLMs, addressing concerns about the reliability of LLM-generated evaluations. They argue that P3 puzzles, defined solely by their test cases, ensure a baseline level of validity as they are solvable if at least one solution exists. The authors acknowledge the subjective nature of puzzle quality but propose using semantic labels to enhance user control over puzzle generation. They also highlight the potential for automated quality assessment through preference models and tournament-style evaluations, suggesting comparisons of LLM performance against human performance on these benchmarks to validate their effectiveness.

### Strengths and Weaknesses
Strengths:
- The presentation, writing, and clarity of the paper are commendable.
- The method creatively integrates evolutionary algorithms, particularly Quality-Diversity algorithms, with LLMs to generate programming puzzles.
- The experiments indicate that ACES effectively generates a variety of challenging puzzles, outperforming existing generative approaches.
- The method allows for a diverse and customizable generation of puzzles, potentially increasing their quality and relevance.
- The authors provide a clear rationale for the validity of their benchmarks, emphasizing the solvability of P3 puzzles.
- The approach of using human-in-the-loop methods for puzzle selection is commendable and aligns with current practices in AI evaluation.

Weaknesses:
- The contribution of the framework is unclear; practical applications and the value of generating diverse puzzles need further justification. Demonstrating downstream tasks, such as fine-tuning models on generated data, would clarify its importance.
- The limitation of discarding unsolvable puzzles may restrict the difficulty of generated problems, potentially undermining the benefits of the new data for model training.
- The reliance on LLMs raises concerns about their imperfections and the lack of mechanisms to improve them, which could lead to misleading evaluations of generated puzzles.
- Concerns remain regarding the reliability of LLM-generated puzzles, as LLMs can produce errors.
- The lack of formal verification for solutions raises questions about the robustness of the evaluation method.
- Comparisons with existing methods, such as AlphaGeometry, highlight limitations in guaranteeing solution correctness.

### Suggestions for Improvement
We recommend that the authors improve the discussion of practical applications and the significance of generating diverse puzzles by providing examples of downstream tasks that could benefit from this framework. Additionally, addressing the limitations of relying on LLMs, including proposing mechanisms for improving their performance, would strengthen the paper. Clarifying the difficulty measure and ensuring the correctness of generated puzzles should also be prioritized. We suggest improving the robustness of the evaluation method by incorporating formal verification processes where feasible. Furthermore, exploring the implementation of automated quality assessment models trained on human preference data to enhance the evaluation of puzzle quality would be beneficial. Lastly, clarifying how the proposed method can ensure the correctness of generated solutions would strengthen the paper's credibility.