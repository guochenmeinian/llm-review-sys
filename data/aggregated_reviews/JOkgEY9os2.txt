ID: JOkgEY9os2
Title: MMD-Fuse: Learning and Combining Kernels for Two-Sample Testing Without Data Splitting
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 7, 8, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two new test statistics, FUSE_N and FUSE_1, for permutation-based two-sample tests related to the Maximum Mean Discrepancy (MMD). The authors theoretically establish that FUSE_1 corresponds to a regularized supremum of the MMD over kernel combinations, while FUSE_N, which shows superior empirical performance, is derived from FUSE_1 with kernels weighted by normalized MMD estimates. The authors conduct a theoretical power analysis and experiments demonstrating competitive power against the state-of-the-art MMDAgg method, with a notable speed-up. Additionally, the paper provides a detailed analysis of MMD-FUSE, emphasizing its advantages over MMDAgg, particularly in the context of multiple testing with an increasing number of kernels. The authors clarify that MMD-FUSE remains well-defined even as the number of kernels approaches infinity, unlike MMDAgg, which lacks a thorough theoretical examination in this limit. The paper also offers insights into test selection strategies for permutation tests, which may benefit newcomers to the field.

### Strengths and Weaknesses
Strengths:
- The experiments are thorough and well-structured, with high-quality code provided for reproducibility.
- The paper is well-written, presenting important aspects of permutation-based tests in an accessible manner.
- The proposed tests exhibit strong power and runtime performance, with empirical evidence supporting MMD-FUSE's superiority over MMDAgg.
- The paper offers a solid theoretical framework for MMD-FUSE, demonstrating its robustness in the limit of many kernels.
- The authors are responsive to feedback and committed to improving the clarity of their proofs and arguments.

Weaknesses:
- The motivation for the proposed test statistic is insufficiently clear, particularly regarding FUSE_N, which lacks theoretical justification.
- The authors do not adequately address the theoretical limitations of kernel selection strategies, such as the non-scale-invariance of maximizing MMD.
- There is confusion regarding the parameter $\lambda$, which is sample size-dependent in theory but set to 1 in experiments without clear justification.
- The clarity of the "Lower Bound" section is lacking, leading to confusion among reviewers.
- The authors did not initially provide sufficient theoretical analysis of MMDAgg's performance in the context of increasing kernels, raising questions about its limiting behavior.
- The paper's presentation could be improved, particularly in clarifying the motivation for certain design choices and simplifying theoretical analyses.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind FUSE_N and provide a theoretical justification for its use. Additionally, the authors should address the limitations of kernel selection strategies in the context of test power. Clarifying the determination of the parameter $\lambda$ in experiments and discussing its practical implications would enhance the paper's rigor. We also suggest revising the presentation to streamline the theoretical analysis and better motivate design choices, potentially merging Sections 2 and 3 for conciseness. Furthermore, we recommend improving the clarity of the "Lower Bound" section to address the confusion noted by reviewers and including the bound for optimal \( t \) in Theorem 5 in the final version for enhanced clarity. Finally, while the additional experiments have strengthened the paper, we encourage the authors to further explore and clarify the theoretical implications of MMDAgg's performance in the limit of many kernels. Including more experimental results in the main body, particularly for promising datasets like CIFAR, would also strengthen the paper.