ID: zWxKYyW9ik
Title: Universality and Limitations of Prompt Tuning
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 5, 5, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of prompt tuning, focusing on its capacity and limitations within transformer models. The authors demonstrate that a sufficiently large transformer can achieve universal approximation for Lipschitz functions. However, they also illustrate the limitations of prompt tuning, particularly with single-layer transformers, by constructing datasets that cannot be memorized, even with an infinite prompt embedding size. The authors compare prompt tuning with LoRA, indicating that prompt tuning may require more tokens for exact memorization.

### Strengths and Weaknesses
Strengths:
- The paper exhibits originality by theoretically studying the capacity and limitations of prompt tuning.
- Clarity is maintained throughout, with well-defined structures and emphasized conclusions.
- The significance of the problem addressed is notable, contributing valuable insights into prompt tuning.

Weaknesses:
- While experiments support the proposed limitations, additional experimental verifications are necessary to reinforce theoretical claims across a broader range of tasks and datasets.
- The simplicity of the constructed datasets raises questions about the necessity of memorization in real-world applications and whether it could lead to overfitting.
- The authors provide mathematical proofs but lack practical guidance for prompt tuning applications, such as scenarios where LoRA might be preferable.
- Minor typographical errors need correction, including references and notation inconsistencies.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including more diverse datasets and tasks to validate their theoretical findings. Additionally, addressing the relevance of memorization in practical applications would enhance the discussion. The authors should clarify the context of "L-lipschitz functions" for better reader comprehension and provide specific scenarios where LoRA outperforms prompt tuning. Lastly, correcting minor typographical errors will improve the overall presentation of the paper.