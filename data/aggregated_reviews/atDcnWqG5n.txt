ID: atDcnWqG5n
Title: Logical characterizations of recurrent graph neural networks with reals and floats
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 8, 8, 8, -1, -1, -1
Original Confidences: 4, 5, 4, 5, -1, -1, -1

Aggregated Review:
### Key Points
This paper examines the relationship between GNN models based on real numbers and those based on floating-point numbers, advancing the study of recurrent GNNs with a focus on termination conditions indicated by designated "accepting" feature vectors. The authors establish connections between the expressive power of these GNNs and logical formalisms such as the graded modal substitution calculus (GMSC) and Monadic Second Order Logic (MSO), proving that recurrent GNNs over floats and GMSC are equally expressive, and that recurrent GNNs over floats and reals share the same expressive power for properties definable in MSO.

### Strengths and Weaknesses
Strengths:
- The paper significantly advances the theoretical study of expressive power in graph representation learning, particularly by addressing the novel consideration of floating-point numbers.
- It is rigorously written, maintaining a high standard of quality and precision.
- The results are non-trivial and can motivate further theoretical exploration of the connections between graph representation learning models and logic.

Weaknesses:
- The paper is primarily accessible to seasoned logicians, limiting its reach within the broader representation learning community. It resembles a submission to LICS rather than NeurIPS.
- There is a lack of empirical evidence demonstrating the practical applicability of the proposed recurrent GNNs, with only generic mentions of standard GNN applications.
- The discussion of related work is insufficient, particularly regarding the contributions relative to [23] and the connection between GMSC and recent logics.
- The proposed termination condition based on accepting feature vectors raises questions about their learnability and the potential for non-termination during recurrent computation.
- The expressivity results do not clarify whether an effective procedure exists for computing the corresponding GMSC theory from a trained recurrent GNN.

### Suggestions for Improvement
We recommend that the authors improve the accessibility of the paper by providing more intuitive explanations, examples, and illustrations to engage a broader audience. Additionally, we suggest that the authors attempt to identify practical needs unmet by standard GNN models that their proposed recurrent models could address. A more detailed discussion on the contributions of this paper relative to [23] and the connection between GMSC and logics with counting terms or Presburger quantifiers would enhance the context. Furthermore, we encourage the authors to comment on the possibility of learning the acceptance vectors and the implications of non-termination in their model. Lastly, clarifying the assumptions regarding floating-point systems in the proof sketch would strengthen the paper's rigor.