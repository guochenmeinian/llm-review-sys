ID: bnZZedw9CM
Title: Decoupled Kullback-Leibler Divergence Loss
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 5, 5, 6, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Decoupled Kullback-Leibler Divergence Loss (DKL), which is mathematically equivalent to KL divergence. The authors propose the Improved KL Divergence (IKL) to address asymmetries in KL/DKL by incorporating class-wise global information and cross-entropy loss for soft labels. The proposed method achieves state-of-the-art adversarial robustness on public leaderboards and demonstrates effectiveness in knowledge distillation tasks.

### Strengths and Weaknesses
Strengths:
1. The motivation for the paper is clear, and the writing is straightforward, making it easy to follow.
2. The proposed method shows improved performance in adversarial robustness and knowledge distillation, validated through comprehensive experiments on standard datasets.

Weaknesses:
1. The empirical study lacks sufficient comparisons between IKL and KL divergence, particularly in various adversarial attack scenarios.
2. The theoretical evaluation is not convincing, with insufficient justification for the choice of the specific anti-derivative formulation in Theorem 1.
3. The paper does not adequately address the computational costs associated with the IKL loss, which is crucial for practical applications.

### Suggestions for Improvement
We recommend that the authors improve the empirical study by applying the IKL approach to other methods such as TRADES or MART to validate its effectiveness further. Additionally, including results from a wider range of adversarial attacks, such as PGD and CW, would enhance understanding of IKL's robustness. We suggest adding experiments comparing IKL with Mean-Teacher in a semi-supervised setting and providing a toy example to illustrate its effectiveness against KL. Clarifying the differences in terms used in equations and improving the overall readability of the paper, including clearer figures and explanations, would also be beneficial. Finally, a more detailed analysis of the computational costs associated with the IKL loss should be included to address concerns regarding model complexity.