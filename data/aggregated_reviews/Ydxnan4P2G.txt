ID: Ydxnan4P2G
Title: Your representations are in the network: composable and parallel adaptation for large scale models
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 6, 5, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on InCA (Introspective Cross-Attention), a method for adapting large pretrained vision models through a compact cross-attention module that does not modify the intermediary representations, thus enhancing memory and computational efficiency. The authors claim that InCA can generalize across various classification tasks and supports class incremental and multi-task learning. InCA emphasizes parameter efficiency and modularity compared to existing methods like FaCT and SSF, which, while efficient in parameter storage, require full fine-tuning, making them computationally expensive and less flexible. Empirical results indicate that InCA performs competitively with full fine-tuning and other adaptation methods, utilizing significantly fewer parameters (1.3% of a ViT-L/16) while maintaining performance signatures that enhance interpretability.

### Strengths and Weaknesses
Strengths:
- The methodical study of adaptation is well-executed, with a simple and reproducible approach.
- InCA demonstrates significant parameter efficiency and modularity, making it suitable for various learning tasks.
- The experiments cover a broad range of datasets, demonstrating the method's applicability.
- The paper provides a detailed comparison with existing methods, clarifying the advantages of InCA over FaCT and SSF.
- InCA shows strong computational benefits and stable performance across different model architectures.

Weaknesses:
- The claim regarding the sufficiency of a single query in InCA raises concerns about the potential for flat attention weights, necessitating further experiments to validate this.
- The experimental design lacks diversity, primarily focusing on computer vision tasks without exploring other domains like language understanding.
- The choice of datasets for evaluation may not align with standard benchmarks like VTAB-1k or FGVC, potentially affecting the fairness of comparisons.
- The paper may contain overclaims regarding the capabilities of ImageNet pretrained models and the use of scientific terminology without precise definitions.
- The clarity of the writing and presentation could be improved, particularly regarding the choice of layers and the dimensions of variables.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in Section 3, by organizing it around heuristics, methodology, and benefits. Additionally, we suggest conducting experiments to validate the claims about class-incremental learning benefits and exploring the use of multiple self-attention layers. It would be beneficial to include comparisons with standard benchmarks such as VTAB-1k or FGVC to strengthen the fairness of evaluations. Furthermore, we advise toning down any overclaims regarding the capabilities of the methods discussed, particularly concerning ImageNet pretrained models, and ensuring that all scientific terms used are clearly defined to maintain scientific rigor. Addressing the computational costs associated with full fine-tuning and elaborating on the selection process for layer activations would enhance the overall rigor of the study.