ID: R9XUQ0hWTy
Title: Reinforcement Learning for Locally Checkable Labeling Problems
Conference: AAAI
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 6, 7
Original Confidences: 4, 3, 3

Aggregated Review:
### Key Points
This paper presents VARL, a reinforcement learning-based framework designed to solve Locally Checkable Labeling (LCL) problems on graphs by utilizing local validators instead of relying on ground truth labels. This approach mitigates algorithmic biases and accommodates multiple valid solutions, demonstrating effective generalization across various tasks and outperforming traditional supervised learning methods.

### Strengths and Weaknesses
Strengths:  
- The writing is clear and well-structured.  
- The proposed framework achieves impressive performance across diverse LCL problems, significantly surpassing supervised methods.  
- VARL is novel, eliminating the need for ground-truth labels or predefined algorithms, and effectively handles problems with multiple valid solutions.  
- The technical derivations are sound, employing a multi-agent reinforcement learning framework and graph convolution layers.  

Weaknesses:  
- The time complexity of the approach requires further analysis, as the exploration of all valid solutions may lead to inefficiencies.  
- The paper lacks a theoretical analysis of time complexity and does not provide detailed descriptions of the local validators, which may limit general applicability.  
- Experiments are conducted solely on small graphs of size 16, raising questions about scalability and the practical applicability of the method.  
- Clarification is needed regarding the dataset generation process for training, validation, and testing.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of time complexity to assess practical applicability. Additionally, testing on larger-scale graph structures (e.g., with 100+ nodes) would provide insights into training efficiency and validator performance. We suggest incorporating other reinforcement learning methods and traditional heuristic-based algorithms as additional baselines for a more comprehensive evaluation. Furthermore, detailed descriptions of the local validators, including pseudocode or process flows, should be provided to clarify their computational requirements. Lastly, we advise moving the description of the reinforcement learning algorithm from the appendix to the main text for better accessibility.