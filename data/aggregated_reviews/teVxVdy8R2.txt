ID: teVxVdy8R2
Title: Prediction with Action: Visual Policy Learning via Joint Denoising Process
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 4, 6, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework called PAD, which utilizes a diffusion transformer (DiT) to jointly denoise future image frames (RGB/Depth) and generate actions. The authors propose that this joint learning process enhances scalability and improves success rates in various robotics tasks. The framework supports co-training on robotic demonstrations and large-scale video datasets, demonstrating superior generalization to unseen tasks on the MetaWorld benchmark and in real-world scenarios. Additionally, the authors conducted experiments on the CALVIN benchmark, showing that PAD outperforms GR-1 in sample efficiency, particularly in a reduced dataset scenario. They also addressed concerns regarding task selection in Meta-World, clarifying the categorization of tasks and their relevance to the evaluation of learned policies.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and presents a straightforward yet effective idea.
- The proposed method shows strong results on generalization benchmarks, with solid ablation studies validating its hypotheses.
- The implementation details are well described, and the provision of code facilitates reproducibility.
- The paper makes a valid technical contribution by exploring a unified diffusion architecture for robotic learning.
- Experimental results indicate that PAD is more sample-efficient than GR-1, particularly in the 10% dataset regimen.
- The authors effectively addressed reviewer concerns regarding the relevance of their work to existing literature.

Weaknesses:
- The performance improvement on real data is marginal, raising questions about its effectiveness in practical applications.
- The evaluation lacks comparisons against important baselines and prior works, particularly regarding the DBC method and GR-1, which could provide better context for its contributions.
- The paper does not adequately address the latency implications of running image diffusion at every time step, which may limit practical adoption.
- The evaluation on Meta-World may not adequately demonstrate PAD's handling of uncertainty in dynamic environments, as the tasks do not involve significant complexity or ambiguity.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including comparisons against additional strong baselines, such as DBC and GR-1, and discussing the differences with prior works. Additionally, it would be beneficial to conduct ablation studies on the co-training data sources to understand their effectiveness. We suggest that the authors conduct further experiments on the ABC->D split in CALVIN to better showcase the advantages of joint video training. Finally, we encourage the authors to enhance the evaluation of PAD's performance in more complex scenarios that involve unpredictable object movements and occlusions, as well as addressing the latency concerns associated with the proposed method and providing a clearer take-home message to enhance the paper's impact.