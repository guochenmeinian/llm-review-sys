ID: QbPHYPZKJI
Title: Learning Distributions on Manifolds with Free-Form Flows
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 7, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extension of free-form flows to Riemannian manifolds, introducing M-FFF, a single-step generative model. The authors adapt the existing framework to allow for faster generation on non-Euclidean spaces, demonstrating that M-FFF can generate high-quality samples and learn distributions effectively across various datasets. The method relies on a projection function from an embedding space and employs a unique encoder-decoder architecture, differing from traditional injective manifold flows.

### Strengths and Weaknesses
Strengths:
- The adaptation of free-form flows to Riemannian manifolds is novel, enabling easier learning and significantly faster inference.
- M-FFF shows competitive performance against both single-step and multi-step methods across multiple benchmarks.

Weaknesses:
- The extension to Riemannian manifolds lacks novelty as it follows a well-trodden path in recent research, although the theoretical contributions are solid.
- The method's performance is evaluated under limited conditions (dimensionality ~10 or less), and there is insufficient discussion on its limitations, particularly regarding high-dimensional spaces.
- The reliance on approximations may lead to inflated likelihood values, as the model might not map to the entire data space, raising concerns about the validity of likelihood comparisons with exact flows.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of their method, particularly regarding dimensionality and the potential for inflated likelihood values. A thorough runtime comparison with state-of-the-art approaches should be included to substantiate claims of speed advantages. Additionally, we suggest that the authors empirically investigate how variance scales with dimensions and clarify the practical implications of the reconstruction error and other loss terms in their experiments. Finally, addressing the discrepancies in the claims regarding the necessity of the projection function and the evaluation metrics used would enhance the clarity and rigor of the paper.