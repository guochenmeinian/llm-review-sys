ID: JvUuutHa2s
Title: Clean-label Backdoor Attacks by Selectively Poisoning with Limited Information from Target Class
Conference: NeurIPS
Year: 2023
Number of Reviews: 2
Original Ratings: 6, 4
Original Confidences: 4, 5

Aggregated Review:
### Key Points
This paper presents a method for selecting critical samples for poisoning to enhance the effectiveness of clean-label backdoor attacks. The authors utilize a pre-trained self-supervised model to extract features from training samples and identify outliers as hard samples for poisoning. The method is evaluated on the CIFAR and GTSRB datasets, demonstrating its resistance to two backdoor defenses.

### Strengths and Weaknesses
Strengths:
1. The paper is well written, and the idea is easy to follow.
2. The topic is significant and of interest to the workshop audience.
3. The method does not require model training, enhancing efficiency.

Weaknesses:
1. The authors should provide a real-world case where attackers can only supply samples from a single class.
2. The comparison of their method with existing works, such as [8] and [31], is lacking.
3. There is insufficient discussion regarding the method's effectiveness when the domain of the pre-trained self-supervised model differs significantly from that of the training dataset.
4. The experimental setup assumes access to a large number of data points, which may not be realistic for attackers in real-world scenarios.
5. The baseline comparison with random selection is inadequate; a relevant baseline would be the first work on clean-label attacks, cited as [29].
6. The high poisoning percentages used in experiments may not reflect realistic scenarios, as evidenced by Carlini's work on lower poisoning rates achieving significant ASR.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing a real-world case where attackers can only use samples from a single class. Additionally, the authors should compare their method with existing works, particularly citing [8] and [31], and include a discussion on the effectiveness of their approach when the pre-trained model's domain significantly differs from the training dataset. Furthermore, we suggest revising the experimental setup to reflect more realistic conditions regarding the number of data points available to attackers. Lastly, the authors should consider using a more relevant baseline than random selection, such as the work cited as [29], and address the concerns regarding the high poisoning percentages by referencing Carlini's findings on lower rates.