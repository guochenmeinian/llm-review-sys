ID: gLoe70Tn8V
Title: Fine-Grained Dynamic Framework for Bias-Variance Joint Optimization on Data Missing Not at Random
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 6, 7, 5, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the relationship between bias, variance, and generalization bounds in estimators with data missing not at random. The authors propose a quantitative bias-variance joint optimization method aimed at achieving bounded variance. They highlight the limitations of existing regularization techniques and introduce a dynamic learning framework that adaptively selects estimators for user-item pairs, ensuring reduced and bounded variances with theoretical guarantees. Extensive experiments validate the proposed methods using two real-world datasets.

### Strengths and Weaknesses
Strengths:
- The research question is significant, addressing debiasing methods in real-world applications.
- The theoretical contributions are robust, with sound mathematical derivations and proofs.
- The paper is well-structured, facilitating comprehension despite its complexity.
- The experimental results are comprehensive and validate the proposed methods effectively.

Weaknesses:
- The organization and expression of the article require improvement, as the layout is crowded, hindering readability.
- The discussion on limitations of previous methods lacks precision, particularly regarding the boundedness of variances.
- The proof of Theorem 3.1 needs further elaboration, especially concerning the validity of the final inequality.
- The validation of the method relies on only two datasets, which may not be sufficiently persuasive; additional validation using the KuaiRec dataset is recommended.
- The paper contains several typos and unclear expressions that detract from its overall quality.

### Suggestions for Improvement
We recommend that the authors improve the organization and clarity of the paper by restructuring figures, tables, and equations to enhance readability. The authors should provide a more precise discussion on the limitations of previous methods, particularly clarifying the relationship between variances and estimated propensities. Further elaboration on the proof of Theorem 3.1 is necessary to address concerns regarding the final inequality. We suggest validating the findings with additional datasets, such as the KuaiRec dataset, to strengthen the experimental results. Additionally, the authors should correct typos and unclear expressions throughout the paper to improve overall presentation.