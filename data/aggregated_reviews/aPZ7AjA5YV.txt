ID: aPZ7AjA5YV
Title: Revisiting Large Language Models as Zero-shot Relation Extractors
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel prompting scheme for zero-shot relation extraction, termed SUMASK, which involves a four-step process: 1) summarizing the context, 2) rewriting the RE triple as a natural question, 3) answering the question based on the simplified context, and 4) approximating the probability of each relation uncertainty. The authors demonstrate that this methodology significantly enhances the performance of LLMs compared to traditional prompting methods. 

### Strengths and Weaknesses
Strengths:
- The methodology effectively leverages multiple steps of LLM prompting while addressing prediction uncertainty.
- Strong empirical evidence supports the effectiveness of the proposed method across various LLMs and datasets.
- The writing is clear and accessible, making the paper easy to understand.

Weaknesses:
- Several relevant baselines are missing from the main experiments, particularly NLI-DeBERTa and QA4RE, which raises concerns about the completeness of the evaluation.
- The inference complexity of the method is a concern due to the need to enumerate all possible triples.
- There is inadequate discussion regarding the uncertainty estimation technique, particularly its implications and operational mechanisms.

### Suggestions for Improvement
We recommend that the authors improve the inclusion of relevant baselines in the main experiments, particularly NLI-DeBERTa and QA4RE, to clarify the comparative effectiveness of their method. Additionally, we encourage the authors to provide a more detailed explanation of Figure 4 and to elaborate on the uncertainty estimation technique, especially regarding cases of high uncertainty. Finally, we suggest enhancing the discussion on how SUMASK prompting can improve logical reasoning capabilities within LLMs.