ID: 3lQgEPRxeu
Title: Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 7, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 2, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a primal-dual policy gradient algorithm for infinite horizon average-reward constrained Markov Decision Processes (MDPs) with general policy parametrization. The authors propose a method that simultaneously minimizes regret and manages constraints, achieving sub-linear bounds of \(O(T^{4/5})\) for both regret and constraint violations. The work significantly contributes to the theory of reinforcement learning by addressing the complexities of average-reward settings, which are more challenging than discounted counterparts.

### Strengths and Weaknesses
Strengths:
1. The authors explore reinforcement learning in average-reward constrained MDPs, proposing novel techniques to tackle inherent challenges.
2. The paper is well-structured, clearly articulating assumptions and discussing relevant parameters.
3. The theoretical analysis is robust, providing global convergence results and bounds on expected regret and constraint violation.
4. The work fills a notable gap in the literature regarding average-reward constrained MDPs under general policy parametrization.

Weaknesses:
1. The assumption that all policies induce aperiodic irreducible Markov chains may be overly stringent; the authors should consider relaxing this assumption.
2. The claim of \(O(T^{4/5})\) regret in Table 1 may be misleading due to linear-in-T terms influenced by the transfer error \(\epsilon_{bias}\).
3. The reliance on the knowledge of mixing and hitting times may limit the applicability of the proposed algorithm; alternative techniques that do not require this knowledge should be discussed.
4. The paper lacks numerical experiments to validate the proposed algorithm, which is crucial for bridging theory and practice.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their assumptions regarding the mixing and hitting times, possibly by exploring techniques that achieve sublinear guarantees without this knowledge. Additionally, the authors should elaborate on the bias of single trajectory-based estimations and clarify how regret and constraint violation are calculated in a prominent section of the paper. Including experimental results to demonstrate the validity of the proposed approach would significantly enhance the manuscript. Lastly, consider addressing the potential to relax the assumption of aperiodic irreducibility in the Markov chain to broaden the algorithm's applicability.