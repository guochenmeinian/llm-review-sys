ID: lPDxPVS6ix
Title: SPEAR: Exact Gradient Inversion of Batches in Federated Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 5, 7, 7, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method, SPEAR, for recovering batched input data from gradients in fully-connected networks and federated learning contexts. The authors exploit low-rank and sparsity properties of gradients, particularly with ReLU activations, to achieve high-probability reconstruction of input data from gradients. The method combines matrix decomposition with an optimization-based filtering procedure, demonstrating significant improvements over previous methods, especially for larger batch sizes.

### Strengths and Weaknesses
Strengths:
- The paper introduces a significant advancement in gradient inversion attacks, enabling exact reconstruction for batch sizes greater than one.
- The authors provide a strong theoretical foundation, including proofs of the low-rank nature of gradients and the sparsity induced by ReLU activations.
- The writing is clear and well-structured, with good empirical evaluations and comparisons to previous methods.

Weaknesses:
- The effectiveness of the algorithm is limited to fully connected networks and small batch sizes, with performance dropping for larger batches.
- The theoretical analysis primarily considers ReLU and fully connected layers, restricting applicability to popular convolutional networks.
- The method's running time is exponential in batch size, raising concerns about its practicality for high-resolution datasets.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of the method, particularly regarding its reliance on ReLU activations and its applicability to convolutional networks. Additionally, we suggest conducting experiments on different linear layers beyond the first one to demonstrate the method's versatility. It would also be beneficial to explore trade-offs between fidelity and running time when increasing batch sizes, as well as providing more comprehensive visualizations of reconstruction results across various layers and datasets. Lastly, we advise double-checking for typos and clarifying the implementation details regarding the computation of \( Q_{\text{opt}} \) to avoid confusion.