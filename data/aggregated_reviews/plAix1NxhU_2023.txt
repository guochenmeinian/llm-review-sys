ID: plAix1NxhU
Title: TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 7, 8, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TpuGraphs, a large-scale dataset for performance prediction of tensor computation graphs on TPUs, comprising approximately 40 million data points. It is 25 times larger than existing datasets and features graphs that are 770 times larger on average. The dataset aims to assist compilers in heuristic decision-making and autotuners in optimizing configurations, focusing on layout and tile configurations for maximum performance gain. The authors propose a model that integrates learned performance predictions to enhance compiler efficiency, acknowledging the importance of graph diversity for model generalizability. They have included variations of existing models and are committed to improving the dataset by incorporating more graphs and expanding model architectures in future releases. The paper provides a comprehensive evaluation of the dataset, its collection process, evaluation metrics, and potential applications in machine learning.

### Strengths and Weaknesses
**Strengths:**
1. The dataset's size and comprehensive coverage of major neural network architectures enhance its utility for performance prediction.
2. The inclusion of real-world graphs provides valuable insights into compiler optimizations, improving the model's applicability to production environments.
3. The paper is well-written, offering detailed insights into the dataset collection process and runtime measurement methods, contributing to transparency.
4. The unique inclusion of whole tensor program information allows for global optimization strategies, which is a significant advancement over previous datasets.

**Weaknesses:**
1. The dataset lacks support for dynamic shape tensors, limiting its applicability to certain machine learning models.
2. The focus on TPUs may restrict the dataset's relevance for performance prediction across other hardware platforms like GPUs and CPUs.
3. The dataset is based on a limited number of core graphs, which may affect its generalizability to unseen models.
4. There is a lack of diversity in graph structures, which may further limit the model's generalizability.
5. The reliance on the XLA compiler raises concerns about the dataset's applicability to other compilers and hardware platforms.
6. The authors did not observe improvements from their data augmentation attempts, indicating potential limitations in their current approach.

### Suggestions for Improvement
1. We recommend that the authors explain the necessity of different models for various optimization domains, such as tile size and layout, to clarify the rationale behind their approach.
2. We suggest emphasizing the importance and complexity of layout optimization to highlight its significance in the context of the dataset.
3. We encourage the authors to consider incorporating dynamic shape support and expanding the dataset to include diverse hardware configurations beyond TPUs to enhance its applicability.
4. We recommend improving the diversity of graphs in the dataset by exploring advanced graph generation techniques that produce diverse yet realistic structures.
5. We suggest considering other data augmentation methods, such as graph augmentation via node and edge insertion, deletion, and modification.
6. We recommend adding a section that discusses the design choice of using open-source models versus randomly generated graphs, including the advantages and disadvantages of each approach.
7. We advise clarifying the dataset's limitations regarding its focus on the XLA compiler to address concerns about its broader applicability.
8. We recommend establishing standardized benchmarks for performance prediction models to facilitate fair comparisons and advancements in the field.