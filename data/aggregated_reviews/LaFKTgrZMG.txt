ID: LaFKTgrZMG
Title: DataPerf: Benchmarks for Data-Centric AI Development
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 8, 6, 6, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DataPerf, a benchmark suite and competition designed for evaluating data-centric AI methods, featuring an initial set of five challenges selected for their diversity in tasks and modalities. The authors emphasize the importance of a unified GitHub repository for hosting challenge code and documentation, aiming to foster community engagement and contributions. The work is positioned as a significant advancement for the data-centric AI/ML community, providing practical value and a structured environment for research while clarifying the distinctions between challenges, benchmarks, and baselines.

### Strengths and Weaknesses
Strengths:
- The DataPerf project is a pioneering effort in data-centric AI, managed by a strong team, and facilitates community involvement.
- It includes a diverse range of tasks and well-developed tools for participation, which are relevant to real-world applications.
- The authors have made significant improvements in response to reviewer feedback, including a unified GitHub repository and expanded sections on related work and benchmark evaluations.
- The paper is well-written, clear, and provides a good motivation for the importance of data-centric deep learning.

Weaknesses:
- The current presentation lacks organization, making it appear as a collection of individual benchmarks without clear motivation for the selected tasks.
- The selection of only five challenges seems arbitrary, and the relationship to existing benchmarks is not adequately discussed.
- The integration of the Adversarial Nibbler challenge is questioned, as it is still under development and may weaken the paper's overall impact.
- Baseline methods are insufficiently detailed, limiting the ability to assess the benchmarks' effectiveness and practical implications.
- The evaluations of the benchmarks are perceived as insufficiently detailed, raising concerns about the clarity of the paper's goals.

### Suggestions for Improvement
We recommend that the authors improve the organization of the paper to clarify the motivation behind the selection of the five benchmarks and how they collectively address the field of data-centric AI. Additionally, we suggest providing more insights gained from the benchmark suite to guide future research directions. 

We encourage the authors to improve the API design of the unified GitHub repository to ensure scalability as more data-centric tasks are introduced. Furthermore, it would be beneficial to unify the code repositories for the benchmarks to enhance usability and ensure consistent APIs. The authors should also include more detailed descriptions of baseline methods, ideally providing at least five methods per benchmark to facilitate better comparisons. 

Lastly, we suggest that the authors provide more comprehensive evaluations for each benchmark, potentially considering separate papers for each task to enhance clarity and depth. A more thorough discussion of the relationship between DataPerf and existing benchmarks, including potential unification opportunities, would strengthen the paper's contribution to the field.