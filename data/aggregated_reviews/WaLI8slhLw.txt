ID: WaLI8slhLw
Title: DeWave: Discrete Encoding of EEG Waves for EEG to Text Translation
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel EEG-to-text model, "DeWave," which decodes text strings from EEG data recorded during reading. The model employs a discrete codebook to manage noise and processes continuous EEG data, enhancing real-world applicability. It is evaluated on the ZuCo datasets, demonstrating improved performance over previous methods in both word-aligned and continuous settings. The authors propose a two-stage optimization: (1) matching EEG to text via contrastive learning with codex quantization, and (2) fine-tuning a pre-trained language model (BART) for decoding.

### Strengths and Weaknesses
Strengths:
- The EEG-to-text application is intriguing and challenging, leveraging recent advancements in contrastive learning and signal quantization.
- The paper is well-organized and easy to read, with a clear presentation of related works.
- The model shows promising performance on the ZuCo datasets, achieving state-of-the-art results.

Weaknesses:
- Concerns exist regarding the training data volume for the CLIP model, as the ZuCo dataset may be insufficient.
- The utility of the discrete codex module is unclear; further experiments are needed to assess the impact of using raw continuous signals.
- BART may not be the most suitable language model for this application; alternatives like BioBERT could be more appropriate given the limited fine-tuning data.
- Additional datasets and baseline models are required to rigorously validate the proposed model's superiority.

### Suggestions for Improvement
We recommend that the authors improve their discussion on the data volume for training the CLIP model and provide explanations regarding its sufficiency. Additionally, we suggest conducting more experiments to evaluate the discrete codex's effectiveness compared to using raw continuous signals. Consideration of alternative language models, such as BioBERT, may enhance the model's applicability in the medical domain. Finally, incorporating more datasets and baseline models will help systematically demonstrate the proposed model's advantages.