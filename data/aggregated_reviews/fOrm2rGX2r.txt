ID: fOrm2rGX2r
Title: C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 8, 7, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents C-EVAL, a comprehensive Chinese benchmark designed to evaluate large language models (LLMs) on advanced knowledge and reasoning abilities. The benchmark consists of multiple-choice questions across four difficulty levels and 52 diverse disciplines, including STEM, Humanities, and Social Sciences. C-EVAL also includes a challenging subset, C-EVAL HARD, aimed at assessing advanced reasoning. The authors report that current LLMs show moderate performance on C-EVAL, indicating significant room for improvement.

### Strengths and Weaknesses
**Strengths:**
- C-EVAL is a comprehensive evaluation suite tailored for the Chinese context, covering a wide range of domains and difficulty levels.
- The benchmark is constructed from high-quality, real-world exam questions, ensuring relevance and reliability in assessing model performance.
- The paper is clear and well-structured, with adequate validation of LLMs' abilities through experiments.

**Weaknesses:**
- The benchmark's novelty is limited, as it resembles the MMLU benchmark, raising questions about its technical depth.
- The reliance on multiple-choice questions may not fully capture the diverse tasks LLMs are expected to perform in real-world applications.
- The evaluation metrics, primarily accuracy, may overlook the influence of prerequisite knowledge and option bias, potentially skewing results.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by addressing the following points:
1. Consider incorporating a wider variety of evaluation tasks beyond multiple-choice questions to better reflect real-world applications of LLMs.
2. Discuss the impact of prerequisite knowledge on model performance and the adequacy of using accuracy as the sole evaluation metric.
3. Explore the inclusion of more English-oriented models in the evaluation to provide a broader perspective on LLM performance.
4. Investigate and report on potential option bias in the benchmark, and consider employing a permutation-invariant metric to enhance evaluation robustness.