ID: g49s1N5nmO
Title: Transformers over Directed Acyclic Graphs
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 4, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an attention mechanism tailored for directed acyclic graphs (DAGs), focusing on the reachability of nodes and masking out unreachable nodes. The authors propose a new positional encoding (PE) based on the maximum depth from root nodes and evaluate their method on source code and citation datasets, demonstrating improved performance over existing graph transformers. 

### Strengths and Weaknesses
Strengths:
1. The method achieves state-of-the-art results when integrated into transformer architectures on the datasets used.
2. The approach is claimed to be parallelizable, resulting in faster training compared to asynchronous methods.
3. The proposed method is efficient and versatile, improving performance across various baseline transformers.

Weaknesses:
1. The selection of the parameter k for limiting the receptive field raises concerns, as it appears heuristic and dataset-dependent, which diminishes the novelty of the work.
2. The contribution seems limited since the method primarily restricts attention to reachable nodes and employs topological depth as PE, lacking significant innovation.
3. The proposed attention mechanism may encounter learning issues when node labels depend on non-reachable nodes, potentially undermining the advantages of transformers over graph neural networks (GNNs).

### Suggestions for Improvement
We recommend that the authors improve the clarity on how to select the parameter k in a principled manner rather than relying on heuristics. Additionally, we suggest including experimental results on larger datasets to validate the efficiency and versatility claims. The authors should also address the missing citations of related works that perform similar tasks, as this would enhance the comprehensiveness of the literature review. Lastly, we encourage the authors to clarify the implications of the choice of root nodes and the potential learning issues associated with the proposed attention mechanism.