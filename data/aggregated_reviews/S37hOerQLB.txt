ID: S37hOerQLB
Title: Self-Refine: Iterative Refinement with Self-Feedback
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Self-Refine, a framework that utilizes the same LLM to generate feedback and iteratively refine its outputs without additional training. The authors evaluate Self-Refine across seven tasks, demonstrating that it outperforms one-step generation methods in both human preference and automatic metrics. The method is structured into three steps: initial generation, feedback, and refinement, leveraging task-specific in-context learning.

### Strengths and Weaknesses
Strengths:
1. The Self-Refine framework is conceptually clear and can be integrated with various LLMs, effectively decoupling output refinement into iterative steps.
2. Experimental results indicate that Self-Refine is particularly effective on challenging tasks, as shown in Table 1.

Weaknesses:
1. The framework relies heavily on prompting for feedback generation, which requires significant design effort that is not thoroughly explained. Additionally, Appendix S appears incomplete.
2. The stopping condition for the iterative process is not clearly defined, and various design choices necessitate control experiments for validation.
3. The contribution of Self-Refine may be more about enhancing prompting techniques rather than genuinely improving LLM capabilities, as evidenced by its limited performance in tasks like Math Reasoning.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the prompt design process and complete Appendix S. Additionally, the authors should clearly articulate the stopping condition for the iterative framework and conduct necessary control experiments to validate different design choices. To enhance the evaluation, we suggest addressing potential biases in human evaluations and considering the use of alternative LLMs for evaluation to mitigate order bias. Finally, including results from additional reasoning tasks based on automatic metrics would strengthen the reliability of the evaluation setting.