ID: TfbzX6I14i
Title: Sparse Modular Activation for Efficient Sequence Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 6, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Sailboat, a framework that builds upon MEGA by integrating linear state space models (SSMs) and Gated Attention Units (GAUs) through a modular approach. Sailboat employs a mixture of experts (MoE) strategy, where each layer combines outputs from multiple EMA + GAU modules, and introduces Sailboat-mem, which utilizes local attention with a sliding window for improved efficiency. Additionally, the authors propose a novel Sparse Modular Attention (SMA) mechanism aimed at enhancing efficiency in sequence modeling through adaptive input sparsification and dynamic routing. The experimental results indicate that the SMA mechanism outperforms traditional efficient attention strategies, particularly in terms of accuracy and efficiency trade-offs, although the empirical validation of its full potential remains limited. The proposed method demonstrates better speed-accuracy trade-offs compared to strong baselines, particularly in LRA tasks.

### Strengths and Weaknesses
Strengths:
1. The routing of a subset of inputs for attention per module simplifies the complexity of soft module selection, presenting a straightforward strategy for modular networks.
2. The exploration of hybridizing SSMs and Transformers is an intriguing area of research.
3. The SMA mechanism offers a promising approach to modular activation, simplifying attention complexity by creating input subsets.
4. Empirical performance shows decent improvements in some tasks, with efficiency gains over MEGA, and the experimental results demonstrate the superiority of adaptive input sparsification over basic efficient attention baselines.
5. The authors acknowledge the theoretical contributions of their framework to the fields of adaptive computation and mixture of experts.

Weaknesses:
1. Several technical unclarities remain, particularly regarding the computation of module parameters and hyperparameter setups.
2. The paper lacks sufficient ablation studies to explore simpler alternatives of input subset selection.
3. The focus on M=1 limits the exploration of the modular aspect, making much of the theoretical discussion less relevant.
4. The main technical contribution appears to focus on modular architectures, with insufficient comparison to existing MoE strategies and layer-skipping methods.
5. The value of the proposed strategy in efficient attention remains unclear, especially when compared to other methods like BigBird or Performer.
6. Empirical results show limited efficiency gains over MEGA, with performance on some tasks being comparable.
7. Uncontrolled variables, such as the feed-forward MLP after the GAU block, may confound comparisons.

### Suggestions for Improvement
We recommend that the authors improve clarity by defining all variables and notations before their usage, particularly in the method section. Additionally, we suggest including more ablation studies to explore the impact of selective attention without modularity and comparing against simpler input selection methods. It would be beneficial to provide a more comprehensive discussion on the differences between Sailboat and existing efficient attention strategies, including GShard and Flash attention. Furthermore, we encourage the authors to clarify the empirical results and their implications, particularly regarding the performance discrepancies observed in different tasks. We also recommend that the authors focus solely on the adaptive sparsification strategy and compare it to other efficient transformers, such as local attention and BigBird, to allow for a clearer understanding of the SMA's advantages. Additionally, we suggest that the authors explicitly state how the module selection mechanism works, particularly how \( a_t^i \) is computed for module sparsification. Lastly, considering empirical validation of SMA with multiple modules in future work would strengthen their claims about its effectiveness. Addressing presentation issues, such as ensuring consistency in the introduction and main text, will enhance the overall readability of the paper.