ID: ob2vgX4aCH
Title: Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel knowledge graph-instructed generation strategy and introduces a dataset called ExpTime, which contains 26k samples with temporal reasoning paths. The authors propose a new series of LLMs, TimeLlaMA, fine-tuned for explainable temporal reasoning, demonstrating the ability to predict event occurrences. The methodology includes constructing the dataset based on temporal knowledge graphs and utilizing instruction tuning to enhance LLM performance in temporal reasoning tasks.

### Strengths and Weaknesses
Strengths:
- The paper introduces a comprehensive approach to temporal reasoning using knowledge graphs and LLMs.
- The dataset ExpTime is well-constructed, containing a significant number of samples for evaluation.
- The fine-tuning of LLMs for explainable temporal reasoning is a novel and interesting contribution.

Weaknesses:
- The discussion on fine-tuning Llama-7b/13b models is limited, with insufficient details on methods and parameters.
- There is a lack of evidence demonstrating that LLMs can understand complex temporal expressions.
- The presentation of the paper is unclear in some areas, with imprecise terminology and formatting issues affecting readability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the fine-tuning process for Llama-7b/13b models by providing more detailed methods and parameters. It would be beneficial to include evidence of LLMs' understanding of temporal expressions and to clarify the defined time span used in their configurations. Additionally, we suggest revising the presentation of baseline models for better readability and addressing the imprecise terminology used throughout the paper. Lastly, we encourage the authors to explore and discuss more traditional methods in their comparisons, as well as clarify any ambiguous terms such as "instruction tuning" and "Polish prompt."