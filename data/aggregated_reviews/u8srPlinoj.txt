ID: u8srPlinoj
Title: ReDS: Offline RL With Heteroskedastic Datasets via Support Constraints
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ReDS, a novel approach for offline reinforcement learning (RL) that addresses challenges posed by heteroskedastic datasets. The authors transform distributional constraints into support-based constraints, enhancing the performance of conservative Q-learning (CQL) through a reweighting mechanism. The paper includes a thorough exploration of offline RL challenges, particularly focusing on the definition of \(C^\pi_\text{diff}\) and its implications for heteroskedasticity in offline RL problems. The authors report on experiments conducted with SQL, EQL, and XQL-C on heteroskedastic antmaze datasets, demonstrating the effectiveness of the CQL (ReDS) algorithm relative to other approaches.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, with high-quality examples and thorough discussions.
- It is theoretically sound and supported by extensive evaluations across various tasks, showcasing significant performance improvements.
- The authors effectively address reviewer concerns regarding the definition of \(C^\pi_\text{diff}\) and its relevance to offline RL.
- The experimental results provide valuable comparisons across various methods, showcasing the performance of their approach.

Weaknesses:
- The novelty of the proposed method is questionable, as similar ideas of reweighting samples and support matching have been previously explored in works [1] and [2], which are not adequately referenced.
- The experiments lack coverage of important domains, such as medium-level locomotion tasks and original antmaze tasks, limiting the clarity of ReDS's performance.
- The heuristic design of the reweighted behavior policy lacks justification, and the experimental settings may not convincingly demonstrate the method's advantages.
- Some reviewers express reservations about the heuristic design of the coefficient and the need for further experiments to clarify certain aspects.
- There is a suggestion that additional analyses could enhance the paper's robustness, particularly regarding the implementation of in-sample learning methods.

### Suggestions for Improvement
We recommend that the authors improve the novelty aspect by thoroughly discussing related works [1] and [2] and clarifying how ReDS differs from these approaches in terms of performance and applicability. Additionally, we suggest expanding the experimental evaluation to include more diverse and relevant tasks, such as medium-level locomotion and autonomous driving datasets, to better validate the method's effectiveness. Furthermore, clarifying the rationale behind the heuristic design of the reweighted behavior policy and the coefficient in Definition 3.1, as well as providing more intuitive explanations for the experimental results, would enhance the paper's rigor. We also recommend conducting further experiments to address the additional questions raised by reviewers, particularly regarding the implementation of in-sample learning methods. Lastly, addressing minor issues like spelling errors and ensuring figures are self-explanatory would improve overall presentation quality.