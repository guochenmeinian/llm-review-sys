ID: zJNSbgl4UA
Title: Slicing Vision Transformer for Flexible Inference
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 4, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Scala, a framework designed to scale down Vision Transformers (ViTs) for environments with fluctuating resource constraints. The authors propose that smaller ViTs can act as sub-networks within a larger ViT, utilizing innovations such as Isolated Activation and Scale Coordination to enhance learning objectives. Empirical results demonstrate that Scala achieves scalable representation with one-shot training, matching the performance of Separate Training while using fewer parameters. The framework is validated across various tasks, showing improvements on ImageNet-1K.

### Strengths and Weaknesses
Strengths:
1. The problem addressed is significant for practical applications.
2. The experimental results are robust, showcasing Scala's effectiveness in various settings.
3. The paper is well-organized and clearly written, making it accessible.

Weaknesses:
1. The novelty of Scala is limited, as similar aims can be achieved with multi-exit networks, which are not adequately discussed or compared in the paper.
2. Key techniques like Isolated Activation and Knowledge Distillation lack originality, being widely adopted in prior works.
3. The term "scalable representation learning" may be misleading, as it does not align with common usage in the literature.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related works, particularly multi-exit networks, to provide a more comprehensive context for their contributions. Additionally, clarifying the differences between Noise Calibration and standard knowledge distillation would enhance the paper's originality. We suggest including results from a standard 300-epoch ViT training schedule and extending experiments to larger architectures and NLP tasks to demonstrate broader applicability. Finally, reconsidering the terminology used to describe Scala may help avoid confusion regarding its scalability claims.