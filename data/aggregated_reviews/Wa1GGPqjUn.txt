ID: Wa1GGPqjUn
Title: Online learning of long-range dependencies
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 6, 6, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an online learning algorithm for recurrent neural networks (RNNs), focusing on long-range dependencies. The authors propose using linear recurrent units (LRUs) and independent recurrent modules, which simplify the computation of exact gradients for single-layer networks and approximate gradients for multi-layer networks. The algorithm shows improved performance on long-range dependency tasks compared to traditional backpropagation through time (BPTT) and other online learning methods.

### Strengths and Weaknesses
Strengths:
- The paper combines insights from recent developments in LRUs and online learning, demonstrating effective results on long-range dependency tasks.
- It provides a technically sound approach with well-designed experiments that support the claims.
- The presentation is generally clear, with a comprehensive overview of the proposed method and its implications.

Weaknesses:
- The claims regarding pushing the standard for online recurrent learning are undermined by the limited scope of experiments, which primarily use synthetic data and simple tasks. A comparison with real-world data, such as small-scale language modeling, is necessary.
- The algorithm appears to be a variant of existing methods like e-prop, raising questions about its novelty.
- The paper lacks a thorough discussion of limitations, particularly regarding the performance on complex tasks and the implications of approximations used.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including experiments on real-world data to substantiate claims about long-range dependency learning. Additionally, clarifying the relationship between their algorithm and e-prop, as well as delineating their contributions from those of Orvieto et al. (2023), would enhance the paper's clarity. The authors should also provide detailed expressions for multi-layer networks to support discussions in the experiments section. Finally, addressing the limitations of their approach, particularly in terms of performance on longer sequences and the methodology for hyperparameter selection in an online context, would strengthen the overall contribution of the paper.