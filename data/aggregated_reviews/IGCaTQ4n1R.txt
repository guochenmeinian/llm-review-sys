ID: IGCaTQ4n1R
Title: OpenDlign: Open-World Point Cloud Understanding with Depth-Aligned Images
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 5, 7, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents OpenDlign, a framework for depth-based 3D understanding that aligns depth-image features through training on generated depth-aligned images, addressing the limitations of CAD-rendered images. The authors propose generating depth-aligned images using a contour-aware projection method from point cloud data and an off-the-shelf depth-controlled diffusion model. Experimental results indicate state-of-the-art performance in zero/few-shot 3D classification, zero-shot 3D object detection, and cross-modal retrieval tasks.

### Strengths and Weaknesses
Strengths:
- The innovative use of generated depth-image pairs enhances generalization and robustness in 3D learning.
- Comprehensive experiments demonstrate significant performance improvements across various tasks, supported by thorough ablation studies.
- The paper is well-written and provides sufficient methodological details, enhancing reproducibility.

Weaknesses:
- The consistency of generated textures across multiple views for a single object is questionable, potentially impacting performance.
- The motivations behind major design choices appear to be driven by evaluation benchmarks rather than a focus on effective training for 3D understanding from depth maps.
- The authors do not specify the diffusion model used, and the generation of a single set of multi-view images limits training diversity.

### Suggestions for Improvement
We recommend that the authors improve the discussion of existing works in the related works section to clarify the novelty of their method. Additionally, consider addressing the inconsistency of generated textures and the rationale for limiting the generation to a single set of multi-view images. It would also be beneficial to explore the use of real images and existing depth estimators to enhance the dataset's diversity and reduce computational overhead. Finally, we suggest revising the title and abstract to better reflect the focus on "point cloud understanding" rather than depth understanding.