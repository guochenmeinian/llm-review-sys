ID: 8abNCVJs2j
Title: S-STE: Continuous Pruning Function for Efficient 2:4 Sparse Pre-training
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 4, 4, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework to address the challenges of 2:4 sparse pre-training, particularly focusing on issues related to discontinuous pruning functions, such as incorrect descent direction, unpredictable descent extent, and sparse mask oscillation. The authors propose two modifications: 2:4-specific soft thresholding and fixed weight rescaling, which enhance pre-training efficiency and yield performance comparable to dense training. The paper also discusses a method for accelerating Transformer pre-training using 2:4 sparsity, demonstrating significant speedup in both inference and pre-training phases. The authors report end-to-end acceleration ratios, achieving 1.53x speedup for the feedforward layer and 1.32x for the network during pre-training on GPT-2 models with FP16 weights. Experimental results across machine translation, image classification, and generative tasks demonstrate the effectiveness of the proposed method, while the mathematical foundations, including the continuous projection of vectors, are rigorously addressed.

### Strengths and Weaknesses
Strengths:
1. The paper provides a clear and simple approach that is easy to understand and implement.
2. It offers a comprehensive analysis of the limitations of traditional pruning techniques and discusses important issues in N:M sparse pre-training.
3. The experiments validate the proposed method and demonstrate its effectiveness across various tasks, with detailed results showing real-world acceleration.
4. The mathematical derivations, particularly regarding continuous projection, are rigorously addressed, enhancing the theoretical foundation of the work.

Weaknesses:
1. The contributions are not sufficiently innovative, as many techniques discussed are already established in other contexts.
2. The discussions on the drawbacks of existing methods do not lead to a new algorithm, lacking a clear connection to the proposed approach.
3. The experimental results are limited, lacking important comparison methods and real training acceleration data.
4. The discussion on the acceleration effects is somewhat limited, requiring more comprehensive experimental validation against various methods.

### Suggestions for Improvement
We recommend that the authors improve the mathematical rigor in the discussion of formula (4) and provide a precise definition of continuous projection in Theorem 4.1. Additionally, it is crucial to experimentally demonstrate the performance improvement that results from adhering to the continuous projection property. Clarifying the dependency of \(\gamma\) on \(\beta\) in Table 1 and addressing the rationale behind simulating dense weights and fixed weight rescaling would enhance the paper's clarity. We suggest improving the experimental section by including additional comparison methods to strengthen the validity of their results and conducting more extensive experiments to empirically demonstrate the acceleration effects of their method compared to other existing techniques. Lastly, we recommend increasing the size of figures for better visibility and ensuring that all references include conference or journal names.