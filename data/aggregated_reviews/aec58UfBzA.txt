ID: aec58UfBzA
Title: RanPAC: Random Projections and Pre-trained Models for Continual Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 7, 7, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called RanPAC for continual learning that utilizes a frozen random projection layer with nonlinear activation to leverage pre-trained representations. The authors combine techniques from Parameter-Efficient Transfer Learning (PETL) and class prototypes, demonstrating strong performance in both class- and domain-incremental learning scenarios. The method is supported by theoretical analysis and extensive experiments across various datasets, showing superior accuracy compared to existing state-of-the-art techniques.

### Strengths and Weaknesses
Strengths:
1. The paper is well organized, with a clear introduction summarizing strategies for leveraging pre-training in continual learning.
2. The investigation into random projections of feature vectors from large pre-trained models is a novel contribution to continual learning research.
3. The experimental section is comprehensive, providing strong motivation and justification for the proposed approach.
4. The authors have included an ablation study and submitted code for transparency, enhancing the credibility of their findings.

Weaknesses:
1. The novelty of the proposed method is somewhat limited, as it appears to borrow heavily from existing PETL techniques, particularly in Phase 1, which constitutes a significant portion of the performance improvement.
2. The random projection with nonlinear activation may be equivalent to a randomly-initialized MLP layer, a common strategy in prompt-tuning, raising questions about its distinctiveness.
3. The parameter usage is high compared to other methods, with concerns about fairness in comparisons, especially against prompt-based approaches that require significantly fewer parameters.
4. Clarity issues exist in the presentation, particularly regarding the motivation for using the Gram matrix and the interpretation of figures and equations.

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of the paper by restructuring sections to enhance flow, particularly by moving the "Overview and Intuition" subsection to the beginning of the method section. Additionally, we suggest providing a more in-depth comparison of different MLP implementations to clarify the novelty and technical contributions of the proposed method. It would also be beneficial to address the discrepancies in parameter usage and provide a detailed computation time analysis for both training and inference. Finally, we encourage the authors to expand the limitations section to provide a more comprehensive discussion of the method's constraints and potential areas for future research.