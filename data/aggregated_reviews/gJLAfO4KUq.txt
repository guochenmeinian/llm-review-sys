ID: gJLAfO4KUq
Title: Pengi: An Audio Language Model for Audio Tasks
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 5, 7, 4, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Pengi, a novel audio language model that utilizes transfer learning to treat various audio tasks as text-generation tasks. The model processes audio and text inputs through dedicated encoders, generating free-form text outputs. Evaluated on 22 downstream tasks, Pengi achieved state-of-the-art performance in several areas. The authors propose a new learning framework that employs a single training procedure and a captioning objective function, leveraging audio task templates inspired by Instruction Tuning.

### Strengths and Weaknesses
Strengths:
- The proposed model is innovative, capable of handling multiple audio tasks without requiring additional fine-tuning or task-specific extensions.
- The framework is comprehensive, framing all audio tasks as text input to text output tasks, which simplifies the training process.
- Extensive evaluations across various audio domains demonstrate competitive performance, establishing a baseline for general-purpose audio language models.

Weaknesses:
- The evaluation appears limited and biased, lacking comparisons to strong baselines such as LAION CLAP, which raises concerns about the model's superiority.
- The model's performance is heavily reliant on the quality of the language model used, which is relatively small (GPT-2, 124M params), and its impact on downstream tasks is not thoroughly explored.
- There is insufficient ablation analysis of individual components, and the handling of variable-length text prompts is unclear.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including comparisons with stronger baselines, particularly LAION CLAP, to better justify Pengi's value. Additionally, we suggest conducting an ablation analysis of each component to clarify their contributions to the model's performance. The authors should also address the unclear handling of variable-length text prompts and provide more details on the evaluation pipeline, including error analysis. Finally, we encourage the authors to explore the potential of using a stronger language model to enhance performance on open-ended tasks.