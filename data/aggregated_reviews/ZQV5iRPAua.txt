ID: ZQV5iRPAua
Title: Evaluating Verifiability in Generative Search Engines
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a significant advancement in attribution evaluation for generative search engines by defining citation metrics and conducting comprehensive human evaluations. The authors analyze the performance of four search engines using citation recall and precision, revealing that only 51.5% of responses are fully supported by citations. Additionally, the paper explores the inverse correlation between fluency, perceived utility, and citation metrics, providing valuable insights into the trade-offs in GSE-generated answers.

### Strengths and Weaknesses
Strengths:
- The introduction of novel evaluation metrics tailored for assessing Generative Search Engines (GSEs) is essential for understanding their performance.
- The provision of human annotations enhances the research community's comprehension of GSE capabilities and limitations.
- The comprehensive analysis reveals critical insights into the low performance of GSEs on citation metrics and their relationship with fluency and perceived utility.

Weaknesses:
- The authors do not convincingly justify why the results on citation precision and recall are deemed unacceptable, which undermines the credibility of their claims.
- The definition of citation precision is problematic, as it evaluates citations individually rather than holistically, potentially inflating recall and precision scores with duplicate citations.
- Some conclusions drawn about fluency and perceived utility appear misleading and lack clarity, particularly regarding the relationship between paraphrasing and these metrics.

### Suggestions for Improvement
We recommend that the authors improve the justification for their claims regarding citation precision and recall metrics to enhance credibility. Additionally, we suggest refining the definitions of citation precision to account for duplicate citations and ensuring that a response only achieves citation recall of 1 if all aspects are covered. Furthermore, we encourage the authors to clarify their definitions of "fluent" and "cohesive" in the annotator guidelines and to provide numeric scores associated with qualitative Likert scales for fluency and perceived utility to avoid ambiguity.