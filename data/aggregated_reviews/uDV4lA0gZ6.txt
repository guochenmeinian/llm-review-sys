ID: uDV4lA0gZ6
Title: Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for Bayesian optimization (BO) that addresses input uncertainty through an MMD-based kernel and the Nystrom approximation for computational efficiency. The authors derive regret bounds that account for approximation errors and compare their algorithm against various baselines. The empirical results demonstrate the method's effectiveness in uncertain input scenarios. Additionally, the authors explore enhancements in uncertainty quantification, particularly around x=0.6, supported by existing literature. However, the figures in the second row of Figure 8 do not effectively illustrate how the proposed method benefits the optimization process, and the authors acknowledge the need for improvements in the experimental setup to better convey their motivation.

### Strengths and Weaknesses
Strengths:
1. The motivation for addressing BO with uncertain inputs is compelling and relevant.
2. The incorporation of approximation error into the regret bounds is useful and insightful.
3. The proposed method is sound, and the extensive empirical evaluation supports its effectiveness.
4. The authors demonstrate a clear understanding of the context and significance of their results.
5. There is a willingness to engage with reviewers and incorporate feedback into revisions.
6. The paper includes relevant and concrete examples of robust optimization applications.

Weaknesses:
1. The validity of the MMD-based kernel is not proven, and the conditions for the normalizing function $\eta$ to ensure it is a valid kernel are unclear.
2. The empirical evaluation lacks robustness; specifically, it should include an ablation study comparing the Nystrom approximation with the full MMDGP method, and the choice of baselines needs justification.
3. The writing contains numerous grammatical errors and unclear notations, which hinder comprehension.
4. The current presentation of Figure 8 does not adequately reflect the advancements in uncertainty quantification.
5. The discussion on the general applicability of the work to kernel families is insufficiently addressed.
6. The motivation behind the optimization problem setup remains unclear.

### Suggestions for Improvement
We recommend that the authors improve the theoretical foundation by providing a formal proof of the kernel's validity and clarifying the conditions for $\eta$. Additionally, the empirical evaluation should include an ablation study comparing the performance of MMDGP with and without the Nystrom approximation, and the authors should justify the selection of baselines. We also suggest that the authors improve the construction of Figure 8 to more clearly illustrate the benefits of their proposed input uncertainty treatment in the optimization process. Furthermore, we urge the authors to expand the discussion on the general applicability of their work to include various members of the kernel family and enhance the motivation for the optimization problem setup to provide clearer context for readers. Finally, we encourage the authors to enhance the clarity of the writing by correcting grammatical errors and refining notations for better readability.