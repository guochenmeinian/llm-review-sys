ID: loQCk0qruU
Title: Be Confident in What You Know: Bayesian Parameter Efficient Fine-Tuning of Vision Foundation Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a lightweight Bayesian Parameter Efficient Fine-Tuning (Bayesian-PEFT) framework aimed at improving the calibration performance of large transformer-based foundation models in few-shot learning settings. The authors identify significant mis-calibration issues, particularly under-confidence in predictions when fine-tuning data is limited. To address this, they propose a Bayesian approach that incorporates two components: a base rate adjustment to enhance prior beliefs from pre-trained knowledge and an evidential ensemble to manage uncertainty. The effectiveness of Bayesian-PEFT is supported by theoretical analysis and extensive experiments across multiple visual benchmarks.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and presents a clear storyline.
2. The proposed Bayesian-PEFT framework is simple yet effective, with theoretical analysis elucidating its underlying mechanisms.
3. Extensive experiments demonstrate the effectiveness of Bayesian-PEFT across various few-shot settings.

Weaknesses:
1. The title may be misleading; additional experiments on diverse foundation models, including large language models (LLMs) like LLaMA3, are necessary to validate the claims.
2. The paper lacks a thorough exploration of the under-confidence issue in relation to data size and the number of unfrozen parameters.
3. A schematic diagram of the proposed Bayesian-PEFT would enhance clarity.

### Suggestions for Improvement
We recommend that the authors improve the title to better reflect the scope of their work and include experiments on a wider range of foundation models, particularly LLMs. Additionally, a more in-depth discussion and empirical analysis regarding the relationship between data size and under-confidence in PEFT would be beneficial. We also suggest including a schematic diagram of the Bayesian-PEFT framework for better comprehension. Furthermore, addressing the numerical stability concerns in Eq. (6) and comparing the proposed method with simpler baselines, such as cosine classifiers or Laplace-Redux, would strengthen the paper. Lastly, incorporating typical OOD metrics in the OOD experiments would provide a clearer context for performance comparisons.