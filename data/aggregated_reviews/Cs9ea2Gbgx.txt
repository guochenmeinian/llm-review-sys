ID: Cs9ea2Gbgx
Title: List and Certificate Complexities in Replicable Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a mathematical exploration of replicability in machine learning, introducing two definitions: $L$ list-replicability, which requires an algorithm to output at most $L$ different answers with high probability, and $\ell$ certificate replicability, where the algorithm must produce a canonical output that is $\varepsilon$-accurate for most internal random strings. The authors establish replicable algorithms for the coin bias estimation problem and within the general framework of PAC learning, providing theoretical guarantees on these algorithms' properties. They demonstrate these definitions through algorithms for various statistical tasks, including estimating the bias of $d$ independent coins and binary classification in the PAC setting. The authors also establish upper bounds on sample complexities and prove optimality for certain parameters, while addressing limitations in their results.

### Strengths and Weaknesses
Strengths:  
- The work addresses a significant theoretical problem in machine learning, emphasizing the need for algorithms with provable replicability guarantees.  
- The proposed definitions of replicability are novel and contribute meaningfully to the literature.  
- The theoretical analysis appears sound, with reasoning steps explained in sufficient detail, even for those not specialized in learning theory.  
- The paper effectively connects its contributions to existing literature, demonstrating a clear understanding of related work.  
- The paper is generally well-written, with clear proof sketches that enhance comprehension.  

Weaknesses:  
- The results are somewhat limited to simpler problems, and the connection between different sections could be better unified.  
- Some inaccuracies and nuances in the exposition detract from clarity, particularly regarding the definitions and optimality claims.  
- The motivation for the complexity measures is not sufficiently articulated, leaving readers unclear on their significance.  
- The presentation could be improved; for instance, the abstract is overly lengthy, and the introduction includes a complete proof that makes it dense.  
- More insight into the motivation behind the introduced notions of replicability is needed, particularly regarding the advantages of certificate complexity over previous measures discussed in the CMY23 reference.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions of list-replicability and certificate replicability by placing them in Section 1 as standalone concepts. Additionally, we suggest that the authors provide more motivation for the complexity measures discussed, explaining how minimizing list and certificate complexity facilitates replicability. It would also be beneficial to address the inaccuracies noted, particularly regarding the optimality of sample complexity and the relationship between list replicability and other learning models. We recommend condensing the abstract and separating illustrative examples from the introduction to enhance clarity. Furthermore, we encourage the authors to clarify the implications of their findings on practical tasks and explore potential applications of the proposed replicability notions in distributed learning settings. Lastly, we suggest providing more context on the advantages of certificate complexity and including an example of its application in a standard machine learning problem, such as binary classification.