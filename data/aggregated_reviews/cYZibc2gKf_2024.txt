ID: cYZibc2gKf
Title: Abstract Reward Processes: Leveraging State Abstraction for Consistent Off-Policy Evaluation
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 7, 8, -1, -1
Original Confidences: 3, 2, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a new framework, STAR, for off-policy evaluation (OPE) that utilizes a chain with rewards (Markov Reward Process, MRP) to conduct evaluations. The framework addresses the bias introduced by the shift between behavior and evaluation policies by projecting continuous state space to a tabular space and introducing the abstract reward process (ARP). The authors prove that this projection preserves the return, ensuring soundness in their approach. The paper includes both theoretical validation and empirical comparisons against existing methods.

### Strengths and Weaknesses
Strengths:  
- The framework is technically sound and well-discussed, addressing significant questions regarding state abstraction and its consistency across varying parameters.  
- The introduction effectively outlines challenges and contributions, enhancing reader comprehension.  
- The empirical results demonstrate strong performance, with competitive comparisons to relevant baselines.  
- The presentation quality is high, with clear motivation for technical definitions and comprehensive reproducibility through provided source code.

Weaknesses:  
- The paper lacks theoretical results on variance reduction, which is a key motivation for the work, relying instead on experimental validation.  
- No finite sample analysis is provided, leaving a gap in understanding approximation error for datasets of limited size.  
- The method's reliance on explicit knowledge of the behavior policy is a significant limitation.  
- Minor issues include typos in equations and unclear definitions regarding the MSE for OPE and variance-bias measures.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework by including results on variance reduction to strengthen the motivation of the work. Additionally, conducting a finite sample analysis would enhance the robustness of the findings regarding approximation error. It would also be beneficial to clarify the assumptions regarding the behavior policy and address the minor typographical errors noted in the review. Finally, we suggest using the additional page to elaborate on the experimental evaluation details, including definitions of MSE and variance-bias measures, and to list Algorithm 1 in the main body for better accessibility.