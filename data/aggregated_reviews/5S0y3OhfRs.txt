ID: 5S0y3OhfRs
Title: OVT-B: A New Large-Scale Benchmark for Open-Vocabulary Multi-Object Tracking
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 8, 6
Original Confidences: 4, 3, 4

Aggregated Review:
### Key Points
This paper presents the Open-Vocabulary Tracking Benchmark (OVT-B), a large-scale benchmark for open-vocabulary multi-object tracking, featuring 1,048 object categories and 1,973 videos with extensive bounding box annotations. The authors propose OVTrack+, an enhanced baseline method that integrates motion features to improve tracking performance over its predecessor, OVTrack. The paper effectively demonstrates the benchmark's utility through comprehensive experiments and comparisons with existing state-of-the-art techniques, providing a solid foundation for future research in open-set object tracking.

### Strengths and Weaknesses
Strengths:
1. The introduction of OVT-B fills a significant gap in open-vocabulary multi-object tracking research, offering a robust platform for developing and evaluating new tracking methods.
2. The meticulous construction of the OVT-B dataset and the development of OVTrack+ reflect a thorough approach to dataset diversity, annotation accuracy, and integration of motion features.

Weaknesses:
1. The dataset requires significant computational resources for training and evaluation.
2. The average resolution of the dataset is lower than that of the existing OV-TAO-val dataset.
3. The documentation lacks a URL for data access, and the GitHub repository contains only a README without download links or data samples.
4. The paper does not provide sufficient implementation details regarding the baseline methods used in experiments.

### Suggestions for Improvement
We recommend that the authors improve the documentation by including a direct URL for dataset access and providing detailed implementation information for the baseline methods, including the detector used and its performance on the proposed dataset. Additionally, we suggest that the authors consider joint tracking and detection methods, such as MOTR, in their experiments. Furthermore, we encourage the authors to enhance the clarity of figures by using vector graphics or higher resolution images and organizing related figures more effectively. Lastly, we recommend that the authors provide insightful analysis on the current state of the open-set object tracking field based on the proposed dataset and clarify the definition of "open-set" in the context of 2D open-set detection.