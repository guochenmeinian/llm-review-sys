ID: 1Sn1dpNaP3
Title: Evaluating Parameter-Efficient Finetuning Approaches for Pre-trained Models on the Financial Domain
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on Parameter-Efficient Fine-Tuning (PEFT) methods, specifically Adapters and LoRA, applied to financial domain tasks using BERT-base, FinBERT, and FLANG-BERT across four datasets. The authors aim to evaluate the performance of PEFT methods compared to fully supervised fine-tuning, concluding that these methods can reduce fine-tuning time significantly while maintaining competitive performance. However, the motivation for studying PEFT in finance is weak, as existing literature already covers similar findings.

### Strengths and Weaknesses
Strengths:
- The paper extends the analysis of PEFT to the financial domain, demonstrating competitive performance relative to full fine-tuning.
- It provides a preliminary study indicating that PEFT methods can decrease fine-tuning time by approximately 50% for certain datasets.

Weaknesses:
- The novelty of the research is limited, as it does not introduce new datasets, algorithms, or significant insights beyond existing literature.
- The evaluation is restricted to only two PEFT methods and a narrow selection of classification tasks, lacking a broader exploration of other PEFT approaches and generative tasks.
- There are concerns regarding the reproducibility of results, with discrepancies noted between reported and reproduced values.

### Suggestions for Improvement
We recommend that the authors improve the motivation for their research questions by clearly articulating the unique contributions of their study. Additionally, expanding the analysis to include more PEFT methods, such as parallel adapters, IA^3, and prompt tuning, would enhance the depth of the evaluation. Exploring generative tasks and incorporating a wider variety of datasets could provide more comprehensive insights. Furthermore, addressing the reproducibility issues by clarifying the methodology and hyperparameter settings would strengthen the paper's credibility. Lastly, we suggest investigating the scenarios where one PEFT method outperforms another to provide valuable insights into their effectiveness.