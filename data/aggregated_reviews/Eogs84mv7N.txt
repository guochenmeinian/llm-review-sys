ID: Eogs84mv7N
Title: Biomedical Visual Instruction Tuning with Clinician Preference Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 7, 7, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents BioMed-VITAL, a data-centric framework designed to enhance the performance of multimodal foundation models in the biomedical domain by aligning instruction datasets with clinician preferences. The authors propose a method for generating and selecting clinician preference-aligned instruction data, resulting in significant improvements in model performance for open visual chat and medical visual question answering (VQA). The framework is supported by the release of 80K clinician preference-aligned datasets and instruction-tuned models, contributing valuable resources to the biomedical AI community.

### Strengths and Weaknesses
Strengths:
- The framework uniquely incorporates clinician preferences into data generation and selection, ensuring high relevance to medical professionals.
- Demonstrated substantial performance improvements, with an 18.5% relative increase in open visual chat and up to an 81.73% win rate in medical VQA.
- Comprehensive methodology detailing a three-stage process that is replicable and well-documented.
- Open-sourcing of datasets and models promotes further research and innovation in the field.

Weaknesses:
- The evaluation with GPT-4V may not be reliable, and the handling of annotator disagreement is not adequately addressed.
- Key details regarding dataset construction and evaluation metrics are primarily located in supplementary materials, making it difficult to assess the dataset's impact fully.
- The novelty of the proposed model may be perceived as minor, with significant contributions stemming from dataset collection rather than methodological innovation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the selection process described in the paper, particularly regarding the statement about selecting a subset S with complex captions. Additionally, please provide more details on how instructions and candidate responses are generated and clarify how disagreements between annotators are handled. It would be beneficial to justify the choice of loss function in equation (1) and explain the weight application in equation (3) and table (1). We suggest including limitations in the main paper rather than the appendix and addressing the potential biases in clinician-annotated data by ensuring diverse representation in the annotation process. Finally, we encourage the authors to expand the evaluation to include a wider range of multimodal language models and to integrate continuous benchmarking tools for community use.