ID: w79RtqIyoM
Title: Compositional Sculpting of Iterative Generative Processes
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 6, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 1, 5, 3, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for composing multiple iterative generative models, specifically GFlowNets and diffusion models, by utilizing classifier guidance to sample from a composition of pre-trained models. The authors propose a training algorithm for the classifier and validate their approach through empirical analyses on image datasets and molecular generation tasks. The method introduces novel compositionality operations that generalize previous work on energy-based models.

### Strengths and Weaknesses
Strengths:
- The method is generalizable across different GFlowNets and diffusion models.
- Empirical results validate the proposed approach, demonstrating its effectiveness in both toy and practical applications.
- The paper is well-written and presents a clear narrative, with appropriate illustrations and code provided.

Weaknesses:
- The experiments are limited to simpler tasks, such as colored MNIST, which may not adequately showcase the method's potential.
- Clarity is needed regarding specific points, such as the model dependencies mentioned in line 193 and the sampling scheme in line 209.
- There is insufficient discussion of related work, particularly regarding classifier guidance in diffusion models and the implications of the proposed method.
- The theoretical analysis lacks novelty, as it parallels existing insights in the literature without substantial new contributions.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by including more complex tasks and comparisons to existing methods in the classifier-guided diffusion literature. Additionally, we suggest clarifying the points of confusion regarding model dependencies and sampling schemes. A more thorough discussion of the relationship between GFlowNets and diffusion models, as well as the implications of classifier performance on the results, would strengthen the paper. Finally, addressing the computational costs associated with training classifiers compared to retraining generative models would provide valuable insights for readers.