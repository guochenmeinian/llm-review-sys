ID: LLuSjg59an
Title: Where does In-context Learning Happen in Large Language Models?
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 4, 3, 8, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates where in-context learning (ICL) occurs within large language models (LLMs), particularly in machine translation and code generation tasks. The authors propose a "layer-from context-masking" technique to identify the transition from task recognition to task execution during ICL. They find that models do not need to maintain attention over the entire context throughout all layers, identifying a "task recognition point" that varies among models. The authors characterize this process as a three-phase model and highlight potential computational savings of up to 45% by eliminating unnecessary context processing in later layers.

### Strengths and Weaknesses
Strengths:  
- The authors present a clear motivation and interesting research question, supported by well-defined and executed experiments.  
- The paper is well-written and engaging, with thorough experimental designs that include various masking strategies and analyses across multiple models.  
- The identification of a "task recognition point" and the three-phase process provide valuable insights into LLM functioning.

Weaknesses:  
- The practical utility of findings is questionable, particularly regarding the applicability of proposed improvements to inference efficiency, as critical layers differ across tasks and subtasks.  
- The experiments rely solely on multi-head attention, limiting the findings' applicability to models using other attention methods.  
- The layer-wise masking approach is problematic; conclusions may vary if layers are masked in reverse order.  
- The token masking strategies are not comprehensive, lacking discussion on individual examples' effects on ICL performance.  
- The paper does not adequately explore why different models exhibit varying behaviors in their "task recognition point" or address potential confounding factors.

### Suggestions for Improvement
We recommend that the authors improve the practical applicability of their findings by providing a more thorough analysis of how critical layers vary across tasks and subtasks. Additionally, we suggest expanding the scope of experiments to include models using different attention mechanisms, such as grouped-query attention. 

We also encourage the authors to refine their layer-wise masking strategy to substantiate the importance of the identified layers and to enhance the comprehensiveness of their token masking strategies. Furthermore, addressing the varying behaviors of models regarding the "task recognition point" and discussing potential confounding factors, such as tokenization schemes and prompt formats, would strengthen the paper's insights. Lastly, we recommend a systematic analysis of how model size affects the "task recognition point" and critical layers to reveal important trends.