ID: 2Cmdh5z6ph
Title: Discriminative Calibration: Check Bayesian Computation from Simulations and Flexible Classifier
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a classifier-based approach to measure miscalibration in Bayesian computation, particularly for Approximate Bayesian Computation (ABC) and Simulation-Based Inference (SBI) methods like neural posterior estimation. The authors propose a method that learns the test statistic from data, providing an interpretable divergence measure through a form of two-sample testing. They develop several classifier-based approaches utilizing "label mapping" and provide theoretical validation for these methods. Empirical results demonstrate improvements over standard simulation-based calibration methods, including experiments on cosmological data.

### Strengths and Weaknesses
Strengths:
- The paper clearly articulates the challenge of miscalibration and offers a statistically interpretable measure.
- It provides a comprehensive description of the proposed methods, including the innovative use of label mapping.
- The theoretical grounding for calibration measures is robust, with expected behaviors in large sample limits discussed.
- Implementation details and discussions on the legitimacy and power of tests are valuable.

Weaknesses:
- While the use of classifiers for two-sample testing is not novel, the authors introduce new methods based on label mapping, which may not be sufficiently differentiated from prior work.
- The empirical evaluation lacks complexity; more intricate experiments could better demonstrate the method's efficacy, particularly in controlled simulation settings like the SLCP problem.
- The paper does not adequately address challenges posed by imbalanced classes during classifier training, which could impact test performance.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including more complex and controlled examples, such as the SLCP problem from Papamakarios et al. This would provide clearer insights into the method's utility in SBI settings. Additionally, we suggest addressing the scaling of methods with respect to feature and parameter dimensions, as well as discussing the implications of nuisance parameters on diagnostic quality. Clarifying the relationship between the proposed methods and existing literature on simulation-based inference using classifiers would also strengthen the paper.