ID: I79q7wIRkS
Title: $\texttt{pfl-research}$: simulation framework for accelerating research in Private Federated Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 7, 8, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents `pfl-research`, a software suite designed for simulating federated learning (FL) training in TensorFlow and PyTorch. It details the framework's design and architecture, offering abstractions for authoring and an execution runtime that distributes workloads across hosts and GPUs. The authoring APIs enable testing of various learning algorithms alongside techniques like differential privacy. The authors provide three sets of benchmarking results: wall-clock time to reach specific accuracy thresholds, scaling behavior in distributed training, and comparisons of various FL algorithms across diverse datasets. Experimental results benchmark `pfl-research` against other federated learning software using the FLAIR benchmark, demonstrating that it is generally faster and exhibits good scaling properties.

### Strengths and Weaknesses
Strengths:  
- The paper effectively motivates the need for efficient software for federated learning, providing a valuable resource that alleviates the burden on researchers to build experimental infrastructure.  
- The framework shows significant speedups and sound design decisions, benefiting the federated learning research community.  
- It includes a thorough analysis of strong scaling properties and introduces a scheduling mechanism to enhance FL round speed, addressing the issue of stragglers.  
- The authors utilize interesting datasets beyond typical benchmarks, enhancing the framework's relevance for federated learning and large language models (LLMs).  
- Comprehensive system design and engineering details are provided, along with considerations for privacy in FL training.

Weaknesses:  
- The focus is primarily on software performance benchmarks rather than introducing new ML performance benchmarks or datasets, which may not align with the NeurIPS Datasets & Benchmarks track.  
- Some datasets lack detailed descriptions and usage instructions, particularly for Aya, SA, and OA.  
- The data-centric benchmarks do not yield clear insights into algorithmic performance or dataset statistics.  
- There are discrepancies with previously reported results regarding speed comparisons, and the use of outdated baselines raises concerns about the validity of claims.  
- Section 3.1 focuses excessively on API details rather than the rationale behind design decisions, which detracts from the paper's overall clarity.

### Suggestions for Improvement
We recommend that the authors improve the paper by focusing on introspective analysis of scaling measurements, particularly weak scaling alongside strong scaling. Additionally, providing stronger evidence for the claims regarding speed improvements and ensuring robust baseline comparisons would enhance the paper's credibility. We suggest improving the clarity of dataset descriptions, particularly for Aya, SA, and OA, by including details such as user statistics and partitioning methods. The authors should consider providing data-centric benchmarks that offer insights into algorithmic choices and dataset influences on performance. Furthermore, we encourage reducing the level of detail in Section 3.1, focusing more on the design decisions that differentiate `pfl-research` from other frameworks. Lastly, a clearer discussion of the influence of secure aggregation on simulation speed and a broader comparison with other frameworks would enhance the paper's relevance and impact.