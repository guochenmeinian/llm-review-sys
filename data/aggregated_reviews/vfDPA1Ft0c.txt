ID: vfDPA1Ft0c
Title: Best Arm Identification for Stochastic Rising Bandits
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 5, 6, 7, 6, -1, -1
Original Confidences: 3, 3, 4, 2, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the fixed-budget Best Arm Identification (BAI) problem within the context of Stochastic Rising Bandits (SRBs), where the expected reward of options increases with selection. The authors propose the R-UCBE and R-SR algorithms, demonstrating that these algorithms achieve small failure probabilities with sufficiently large time horizons. The paper includes a lower bound proof indicating that the R-SR algorithm is near-optimal, and empirical experiments validate the performance of the BAI algorithms.

### Strengths and Weaknesses
Strengths:
- The formulation of the rising bandit problem is clearly articulated in Section 2.
- The authors provide a thorough description of expected reward estimation in Section 3, which can inspire future algorithm designs beyond the rising bandit context.
- The experimental validation of BAI algorithms is well-executed.

Weaknesses:
- Theorem 6.1's lower bound on the time horizon \( T \) depends on \( \Delta_i(T) \), which appears to be a random variable influenced by the instance, horizon, and algorithm, raising concerns about its appropriateness in a lower bound context.
- Similar concerns arise regarding the upper bounds on failure probabilities for the R-UCBE and R-SR algorithms.
- The deterministic growth function \( \gamma \) may not reflect practical scenarios, as randomness in \( \gamma \) does not accumulate.
- The lower bound of Theorem 6.2 is of order \( \exp(-T/H) \), which contrasts with established lower bounds in stationary settings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the lower bound discussions by addressing the concerns regarding \( \Delta_i(T) \) and its variability across different algorithms. Additionally, we suggest revising the phrasing in Line 45 from "failing to represent" to "but failed to represent." It would also be beneficial to include a brief overview of related works and motivating examples in the main paper, rather than relegating them to Appendices A and B. Furthermore, we encourage the authors to explore the implications of specific reward models to derive more practically applicable results, potentially enhancing the relevance of their findings.