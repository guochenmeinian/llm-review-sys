ID: ujk0XrNTQZ
Title: Drago: Primal-Dual Coupled Variance Reduction for Faster Distributionally Robust Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 5, 7, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DRAGO, a variance-reduced optimization method for distributionally robust optimization (DRO). The authors propose a penalized DRO framework, viewing DRO as a min-max optimization problem with a finite-sum structure. The algorithm maintains explicit histories for individual loss functions, gradients, and dual parameters, which control variance in updates, leading to a fast linear convergence rate. DRAGO supports any non-negative penalty parameter and demonstrates strong empirical performance, particularly with tuned batch sizes. Additionally, the authors clarify their theoretical assumptions regarding the losses and the size of the uncertainty set, and they have conducted experiments showing DRAGO's performance in higher-dimensional problems with small batch sizes. The analysis reveals that while the batch size of $b=1$ performs competitively in specific scenarios, it generally yields inferior results compared to other variants and methods in terms of wall time.

### Strengths and Weaknesses
Strengths:
- Theoretical analysis indicates a fast linear convergence rate, with dependence on batch size and problem constants.
- The algorithm performs well in practice when batch sizes are optimized.
- It accommodates any non-negative penalization strength for dual parameters.
- The authors have effectively addressed reviewer comments, enhancing the clarity of theoretical assumptions and providing additional experimental results.
- The paper demonstrates a solid understanding of the impact of batch size on performance, particularly in varying dimensions.

Weaknesses:
- The paper's organization complicates understanding of the algorithmic components.
- Assumptions for convergence, such as Lipschitz continuity and smoothness, may be unrealistic.
- Experimental evaluation is limited to low-dimensional problems, raising concerns about scalability and memory requirements.
- The compactness assumptions may limit the general applicability of the method.
- The lack of a comprehensive ablation study on batch size could weaken the experimental validation of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the paper's organization by introducing Algorithm 1 first, followed by an explanation of the additional proximal terms needed for variance control. This would provide a clearer reference point for readers. Additionally, we suggest addressing the assumptions regarding Lipschitz conditions, as they may not hold in general cases. Expanding the experimental evaluation to include higher-dimensional datasets would also strengthen the paper, as would a more detailed comparison of the algorithm's performance in non-strongly convex settings. Furthermore, we recommend improving the discussion of batch size in the experimental section by including a larger-scale ablation study to further validate its impact on performance. Finally, incorporating a more detailed explanation of the theoretical assumptions would aid readers from diverse backgrounds in understanding the methodology. Clarifying the implications of the parameter $\nu$ and its relationship to the complexity bounds would enhance the overall clarity of the contribution.