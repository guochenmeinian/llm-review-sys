ID: HaSS8a3Oe7
Title: Can Language Models Understand Physical Concepts?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation benchmark, VEC, to assess language models' understanding of physical concepts, including visual and embodied attributes. The authors conducted a thorough analysis of various language models (LMs), revealing that scaling enhances comprehension of certain visual concepts, while embodied knowledge remains challenging. The study also proposes a knowledge distillation method to transfer embodied knowledge from vision-augmented models (VLMs) to text-only LMs, yielding significant performance improvements.

### Strengths and Weaknesses
Strengths:
- The introduction of the VEC benchmark fills a critical gap in evaluating LMs' grasp of physical attributes.
- The experiments are robust, providing clear insights into the performance of different LMs, including text-only and vision-augmented models.
- The paper is well-written and presents its findings in an accessible manner.

Weaknesses:
- The analysis lacks comparisons with recent LLMs like ChatGPT and Llama, which may exhibit richer physical commonsense knowledge and complex reasoning capabilities.
- The evaluation methods for CLIP differ significantly from those for BERT and OPT, raising questions about the comparability of results.
- The paper could benefit from a more detailed discussion on dataset development and the potential for creating a more complex embodied dataset.

### Suggestions for Improvement
We recommend that the authors improve the paper by including comparisons with recent state-of-the-art LLMs, such as ChatGPT and Llama, to enhance the relevance of their findings. Additionally, we suggest allocating more paragraphs to the dataset development process, emphasizing the core contributions of the paper. Finally, consider expanding the dataset to include more complex embodied interactions, rather than focusing solely on single attributes.