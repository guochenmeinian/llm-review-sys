ID: 67MTWzhEOn
Title: Revisit the Power of Vanilla Knowledge Distillation: from Small Scale to Large Scale
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 4, 6, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of vanilla knowledge distillation (KD) on large datasets, revealing its potential when combined with stronger data augmentation techniques. The authors identify a "small data pitfall," which leads to an underestimation of vanilla KD's effectiveness on larger datasets like ImageNet-1K. The extensive experiments conducted demonstrate that while vanilla KD can achieve state-of-the-art results, its performance is context-dependent, particularly influenced by model size and training strategies. Additionally, the authors propose innovative solutions that address existing gaps in the literature, contributing valuable insights to the field.

### Strengths and Weaknesses
Strengths:
1. The paper highlights the significant potential of vanilla KD, encouraging a broader perspective on knowledge distillation beyond small models or datasets.
2. Extensive experiments provide valuable insights and contribute to the state-of-the-art models.
3. The identification of the "small data pitfall" offers important implications for future research in knowledge distillation.
4. The authors exhibit a strong grasp of the topic, supported by comprehensive research and analysis. Their proposed solutions are both original and practical, enhancing the overall contribution of the paper.

Weaknesses:
1. The paper lacks clarity on the baseline performance of vanilla KD under stronger training recipes, where it generally performs worse.
2. There is insufficient discussion regarding the hyper-parameter $\alpha$ and temperature for KD, which are crucial for its performance.
3. The evaluation does not adequately address lightweight models like MobileNetv3, which are critical for deployment in resource-constrained environments.
4. Some areas lack depth in discussion, and certain methodologies could benefit from further clarification.
5. The paper could improve its engagement with recent literature to strengthen its arguments.
6. Some minor errors and missing details, such as typos and the training context for the teacher model, need correction.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the baseline performance under stronger training recipes, as this is essential for understanding the effectiveness of vanilla KD. Additionally, a thorough discussion on the hyper-parameter $\alpha$ and temperature for KD should be included to enhance the paper's depth. We also suggest conducting dedicated experiments on lightweight models like MobileNetv3 to address their importance in practical applications. Furthermore, we recommend improving the depth of discussion in the weaker areas and providing clearer explanations of the methodologies used. Finally, incorporating more recent literature will bolster the paper's arguments and relevance, while correcting typos and providing missing details will improve the overall quality and clarity of the paper.