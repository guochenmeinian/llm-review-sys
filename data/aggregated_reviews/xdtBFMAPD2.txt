ID: xdtBFMAPD2
Title: Explanation Shift: How Did Distribution Shift Impact the Model?
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 5, 7, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel concept called "explanation shift" for detecting shifts in data distributions by analyzing changes in attribution distributions from machine learning models. The authors argue that existing methods for detecting shifts are limited in their ability to identify changes in model behavior, and they propose that explanation shift offers more sensitive and interpretable indicators. The method is validated against other distribution shift detection methods using both synthetic and real datasets.

### Strengths and Weaknesses
Strengths:
- Leveraging changes in explanations to detect distribution shifts is a novel idea.
- The authors provide a comprehensive analysis linking explanation shift to various distribution shifts, aiding reader understanding.

Weaknesses:
- The overall presentation lacks clarity, with many key terminologies and notations poorly defined, such as $x^*$ in Equation (3) and the definitions of "sensitivity" and "accountability" used in experiments.
- The technical contribution is limited; the method for detecting explanation shift is simplistic and lacks theoretical justification compared to other methods.
- Key empirical studies are missing, particularly comparisons with baseline methods on real datasets, leaving unclear whether these methods can detect the same shifts.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by clearly defining all key terminologies and notations, particularly those mentioned in the weaknesses. Additionally, we suggest that the authors provide a theoretical justification for their method by comparing it against established techniques. It would also be beneficial to include empirical studies that evaluate the proposed method against baseline methods on real datasets to clarify its advantages or limitations. Finally, a thorough proofreading of the manuscript is necessary to correct inconsistencies in notation and improve overall readability.