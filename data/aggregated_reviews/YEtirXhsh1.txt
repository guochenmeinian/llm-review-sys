ID: YEtirXhsh1
Title: Towards Learning Group-Equivariant Features for Domain Adaptive 3D Detection
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework that employs a Gaussian Mixture Model (GMM) based grouping and exploration strategy to enhance domain adaptive 3D object detection. The authors propose clustering object descriptors from foreground points to ensure balanced learning and reduce false negatives through an explorative group update strategy. The framework is tested on datasets such as KITTI, NuScenes, and Waymo, demonstrating improvements in bridging inter-domain gaps compared to existing methods. The paper includes ablation studies assessing the impact of various components, including the effect of GMM-based grouping.

### Strengths and Weaknesses
Strengths:
- Originality in applying GMM-based grouping for domain adaptation in 3D object detection.
- Good performance relative to previous methods, supported by extensive ablation studies.
- Clear and understandable presentation, with well-executed figures.

Weaknesses:
- Insufficient implementation details, including missing hyper-parameters and model runtime, which hinders reproducibility.
- Limited variety in object categories tested, primarily focusing on cars, with a lack of qualitative result diversity.
- Some technical aspects, such as the updating mechanism of $\mu$ and the definition of "closed gap," are not clearly explained.

### Suggestions for Improvement
We recommend that the authors improve the clarity of implementation details by providing specific parameters and hyper-parameters, such as voxel size and model runtime, to facilitate reproducibility. Additionally, including a broader range of object categories, such as pedestrians and cyclists, in the experiments would enhance the generalizability of the findings. We also suggest summarizing the training process in an algorithm table for better clarity and addressing the technical questions regarding the updating of $\mu$ and the effects of hyper-parameters in practice.