ID: SO1aRpwVLk
Title: 4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 5, 6, 8, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework called 4Real for dynamic 3D scene generation using video diffusion models. The method begins by generating a reference video, from which a freeze-time video is created to reconstruct a canonical 3D scene. The framework employs video-SDS-based optimization to refine a dynamic 3DGS, enabling the generation of complex 4D scenes with multiple objects. The authors claim that their approach outperforms existing object-centric methods, providing empirical evidence of reduced computational costs through low-resolution pixel diffusion models.

### Strengths and Weaknesses
Strengths:
1. 4Real achieves photorealistic 180-degree 4D scenes, addressing limitations of previous object-centric models by relying solely on text-to-video diffusion models.
2. The framework introduces effective techniques for modeling complex scenes, including a heuristic loss for small motion regularization, which is both reasonable and novel.
3. The paper is well-written and presents detailed technical explanations, demonstrating significant improvements over prior methods in terms of speed and visual quality.

Weaknesses:
1. The context conditioning description lacks clarity, particularly regarding the dataset's characteristics and the importance of context embedding for reproducibility.
2. The evaluation omits comparisons with AYG, which is more relevant to the work than the compared methods, raising concerns about the validity of claims regarding visual quality.
3. Potential conflicts between the generated freeze-time video and the reference video are not addressed, and the canonical 3D generation appears to function as a standalone task without consideration of recent advancements in 3D scene generation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the context conditioning description and provide details on the dataset used, including its similarities to existing datasets like CO3D. Additionally, we suggest including comparisons with AYG in the evaluation to strengthen the claims about visual quality. The authors should also address the potential inconsistencies between the freeze-time video and the reference video, and consider discussing the applicability of recent 3D scene generation methods as alternatives in the canonical 3D generation stage. Lastly, we encourage the inclusion of failure cases in the results to provide a comprehensive understanding of the method's limitations.