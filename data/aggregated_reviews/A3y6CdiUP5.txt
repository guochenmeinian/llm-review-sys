ID: A3y6CdiUP5
Title: Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection
Conference: NeurIPS
Year: 2023
Number of Reviews: 1
Original Ratings: 7
Original Confidences: 3

Aggregated Review:
### Key Points
This paper presents an exploration of injecting Trojans into large language models (LLMs), addressing a significant and timely topic. The authors propose a comparison with existing work in the field, highlighting the relevance of their research.

### Strengths and Weaknesses
Strengths:  
- The topic is very interesting and relevant, focusing on the injection of Trojans into LLMs.  
- The paper includes a comparison with existing work, which adds value to the discussion.  

Weaknesses:  
- There is a lack of discussion regarding existing papers on backdoors in LLMs.  
- The technical contributions are unclear, as the techniques used, including the construction of instructions, seem similar to existing work, particularly Alpaca.  
- The paper does not provide comprehensive results for benign instructions, raising questions about the model's performance when the fine-tuned dataset includes both benign and malicious instructions, such as "Negatively describes Joe Biden" and "Describes Joe Biden."

### Suggestions for Improvement
We recommend that the authors improve the discussion of existing literature on backdoors in LLMs to provide context for their contributions. Additionally, clarifying the technical contributions and differentiating their techniques from those of Alpaca would strengthen the paper. Finally, we suggest including comprehensive results for benign instructions to assess the model's performance with conflicting data in the fine-tuned dataset.