ID: PAWQvrForJ
Title: Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 7, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a membership inference attack (MIA) for large language models (LLMs) that utilizes prompt calibration to assess variations in model behavior for neighboring inputs. The authors connect their framework to the neighborhood attack from Mattern et al. and demonstrate improved performance across multiple domains. The proposed self-prompt approach generates reference datasets, allowing for a probabilistic variation assessment based on memorization, which is compared against existing methods using the AUC metric. Additionally, the authors introduce a method that estimates features of partial second-order derivatives through score differences between target texts and their paraphrased versions, proposing a practical difficulty calibration (PDC) and probabilistic variation assessment (PVA) to enhance the accuracy of their method, SPV-MIA. The evaluation includes new metrics such as TPR@1%FPR and TPR@0.1%FPR, demonstrating that SPV-MIA outperforms existing baselines.

### Strengths and Weaknesses
Strengths:
- The paper addresses critical limitations in black-box privacy auditing, which is increasingly relevant as model access becomes restricted.
- Figures and diagrams are well-crafted and enhance the clarity of the writing.
- The assessment of robustness across different prompt sources contributes to understanding worst-case performance.
- The authors have incorporated detailed feedback from reviewers, enhancing the clarity and depth of the paper.
- The addition of new evaluation metrics highlights the performance of SPV-MIA in low false-positive rate scenarios.
- The ablation studies provide valuable insights into the contributions of the proposed components.

Weaknesses:
- The core concept is closely related to distillation-based stealing and a modified neighborhood attack, lacking significant novelty.
- There is an unclear relationship between "modest" paraphrasing and the corresponding perturbations in the embedding space, raising concerns about the assumptions made.
- The evaluation metrics are inadequate; reliance on AUC does not align with established standards for MIAs, which emphasize true-positive rates at low false-positive rates.
- The experimental settings lack clarity, particularly regarding the extraction of reference datasets and the necessity of shadow models.
- The methodology for estimating second-order derivatives in LLMs may still raise practical concerns.
- Some reviewers noted that the inclusion of more advanced baselines could strengthen the comparisons made in the paper.
- The writing quality is inconsistent, with grammatical errors and unclear statements hindering comprehension.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relationship between paraphrasing and perturbations in the embedding space, ensuring that assumptions are well-grounded. Additionally, we suggest revising the evaluation metrics to include the Full Log-scale ROC Curve and TPR at low FPRs, as these are standard in the field. It would be beneficial to provide clearer descriptions of the experimental settings, including the number of datasets extracted and the role of shadow models. We also recommend improving the clarity of the methodology regarding the estimation of second-order derivatives to address practical feasibility concerns. Consider including a thorough discussion on the limitations of using LDC-MIA as a baseline, as it may not be directly applicable to LLMs. Lastly, we urge the authors to enhance the writing quality by addressing grammatical issues and improving sentence structure for better readability, while also emphasizing the ablation study results in the main body of the paper to highlight the contributions of PDC and PVA more effectively.