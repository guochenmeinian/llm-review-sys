ID: Ih2yL7o2Gq
Title: Bayes beats Cross Validation: Efficient and Accurate Ridge Regression via Expectation Maximization
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 2, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for estimating the regularization parameter in ridge regression using a Bayesian framework, the Expectation-Maximization (EM) algorithm, and singular value decomposition (SVD) preprocessing. The authors argue that their approach is computationally more efficient than traditional leave-one-out cross-validation (LOOCV) and offers theoretical advantages, particularly in terms of minimax properties. Extensive experiments are included to compare the proposed method to LOOCV, demonstrating that the EM algorithm is faster while maintaining comparable predictive performance. However, there is skepticism regarding the novelty of the method and its claimed improvements over LOOCV, especially in finite sample scenarios.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and organized, making it accessible and easy to follow.  
- The proposed method shows a clear computational advantage over LOOCV, supported by extensive experimental results.  
- The use of SVD for preprocessing enhances computational efficiency.  
- The authors provide theoretical results, including a theorem establishing unimodality of the posterior for sufficiently large sample sizes.  

Weaknesses:  
- The theoretical justification for the proposed method's superiority over LOOCV is lacking; the authors do not sufficiently explain why the EM algorithm outperforms LOOCV.  
- The novelty of the method is questioned, as it combines commonly used techniques without clear evidence of significant innovation.  
- The theoretical advantages over LOOCV, particularly regarding minimaxity and convergence, are not universally accepted by the reviewers.  
- There is a need for a detailed comparison with gradient descent on the LOOCV objective, as both methods may share similar strengths and weaknesses regarding local optima.  
- Some important parameters, such as $k$ and $q$, are not well-defined, which affects clarity.  
- There is uncertainty about the number of iterations required for the EM algorithm compared to gradient descent on LOOCV.  

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for their method by explaining the underlying reasons for its performance advantages over LOOCV. Additionally, we suggest including a detailed empirical comparison to gradient descent on the LOOCV objective to address potential concerns about local minima and to clarify the theoretical and empirical aspects of efficiency. The authors should provide experiments demonstrating that their EM algorithm is significantly faster than LOOCV without sacrificing accuracy, ideally showing improvements of at least a couple of orders of magnitude. Furthermore, we encourage the authors to explore and clarify the theoretical implications of their results, particularly regarding the lemma on log-concavity and the minimax properties of their estimator in finite sample settings. Clarifying the definitions of parameters like $k$ and $q$ when they first appear in the text would enhance clarity. Finally, the authors should ensure all references to the appendix are clear and that any unexplained notation is defined.