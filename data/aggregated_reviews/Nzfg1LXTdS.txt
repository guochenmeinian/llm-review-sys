ID: Nzfg1LXTdS
Title: How Diffusion Models Learn to Factorize and Compose
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 8, 6, 5, 5, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the capabilities of diffusion models, specifically Denoising Diffusion Probabilistic Models (DDPMs), in learning factorized representations and achieving compositional generalization. The authors analyze training mechanisms, supporting the hypothesis that diffusion model architecture has an inductive bias towards factorized representations. Results indicate that to achieve out-of-distribution compositional generalization, the training set must contain a few compositional examples and present factors independently across their variability. The authors connect their findings to percolation theory, suggesting a need for correlated data to learn these representations.

### Strengths and Weaknesses
Strengths:
- The manuscript is well-written, with clear explanations and relevant methodology.
- The originality lies in linking factorization capability to percolation theory, supporting the need for correlation among independent features for faithful representation.
- The experiments provide a clear understanding of factorization and compositionality in diffusion models.

Weaknesses:
- The study is conducted in simplistic toy settings, limiting the generalizability of the findings to real-world applications.
- Some conclusions regarding compositional generalization have been previously addressed, providing limited new insights.
- Clarity in presentation and discussion of results could be improved, as some findings are difficult to parse.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by incorporating more complex and realistic datasets to validate the generalizability of their conclusions. Additionally, a broader evaluation of the model's performance across different conditions, such as varying input noise levels and compositionality patterns, would enhance the robustness of the findings. We suggest that the authors clarify the actionable insights derived from their experiments and tone down any overstated claims regarding the implications for future research. Furthermore, including a brief introduction to relevant concepts from geometry and topology would make the paper more accessible to a wider audience. Finally, we encourage the authors to address the minor issues noted in the reviews, such as typos and unclear terminology, to improve overall clarity.