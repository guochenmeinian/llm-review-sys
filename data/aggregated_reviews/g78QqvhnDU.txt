ID: g78QqvhnDU
Title: Prioritizing Samples in Reinforcement Learning with Reducible Loss
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel experience replay priority scheme based on reducible loss (ReLo), which estimates the difference in Q-loss between the online and target Q-networks. The authors demonstrate that ReLo improves upon both prioritized experience replay (PER) and uniform replay across various environments, including DM Control Suite, Mujoco tasks, and Atari. They argue that ReLo effectively mitigates forgetting and enhances performance in stochastic environments, as evidenced by experiments showing that ReLo consistently outperforms baselines in terms of TD error and sample efficiency. However, the experiments in the Atari environment are limited in duration, and the chosen benchmark tasks may not fully illustrate the benefits of ReLo in handling stochastic dynamics.

### Strengths and Weaknesses
Strengths:
- The simplicity and clarity of the proposed method make it easy to implement and adopt, requiring no significant changes to existing algorithms.
- The authors provide robust experimental results showing ReLo's superior performance and lower TD error across multiple tasks, indicating its potential for broader application in off-policy RL algorithms.
- The paper addresses significant concerns regarding statistical significance and clarity in reporting results, enhancing the overall quality of the work.

Weaknesses:
- The experiments, particularly in the Atari environment, are limited in duration (only 2M frames), making the results less reliable for drawing strong conclusions.
- The chosen benchmark tasks lack sufficient stochasticity to fully illustrate the benefits of ReLo, raising questions about its effectiveness in more complex environments.
- There is currently limited exploration of how ReLo prevents forgetting, with only one task explicitly demonstrating this capability.
- Clarity issues exist in the writing, particularly in the background and related work sections, and the definitions of key terms such as "learn-ability" and "reducible loss" need refinement.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by including a clear stochastic domain that highlights the differences between ReLo, PER, and uniform sampling. Adding noisy DMC results would address this gap. Additionally, we suggest that the authors clarify whether sticky-actions were enabled in the MinAtar and Atari experiments and provide justification for the choice of Atari games tested. 

To enhance clarity, we advise the authors to refine the background section and ensure that key terms are well-defined. We also recommend moving Section 4.6 earlier and including the additional forgetting experiment in this section, as it aids in understanding ReLo's core purposes. It would be beneficial to explicitly discuss "small scale forgetting" in DMC tasks, highlighting how it differs from the forgetting observed in toy experiments. Furthermore, we encourage the authors to include learning curves in the experimental results to better demonstrate sample efficiency and provide statistical significance tests for the results presented in figures and tables. Lastly, integrating the additional rebuttal results into the main body of the paper rather than an appendix could enhance its persuasiveness.