ID: 8ZRAHNT7E9
Title: LagrangeBench: A Lagrangian Fluid Mechanics Benchmarking Suite
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 8, 5, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LagrangeBench, the first benchmarking suite for Lagrangian particle problems, comprising seven new fluid dynamics datasets generated using the Smoothed Particle Hydrodynamics (SPH) method. The datasets encompass various physical scenarios, including solid wall interactions and free surfaces, and include both 2D and 3D cases. The authors propose a JAX-based API with recent training strategies and benchmark several established Graph Neural Networks (GNNs) to evaluate their performance on these datasets. The authors emphasize the purely Lagrangian nature of the datasets, which is crucial for learning surrogate dynamics, and introduce physically interpretable loss functions for chaotic systems. They have made their code publicly available and enhanced documentation, including tutorial notebooks.

### Strengths and Weaknesses
Strengths:
- The paper introduces a well-diversified set of datasets that cover different dynamics relevant to Lagrangian fluid mechanics, including turbulence and free surface flows.
- The research is thoroughly conducted, with claims supported by evidence, and the writing is clear and well-organized.
- The authors are transparent about the limitations of their dataset and methodology, which enhances the credibility of their work.
- Extensive documentation and tutorial notebooks enhance usability and accessibility of the framework.
- The introduction of physically interpretable losses is a significant advancement for learning in chaotic systems.

Weaknesses:
- The datasets may not fully represent the interests of the flow physics community, as only one case features turbulence, and many cases could be simulated using Eulerian methods.
- The current limitation on particle numbers restricts the simulation of resolved turbulence.
- The benchmarking and analysis of neural network performances lack depth, particularly in sections 3.5 and 4, which may disappoint readers expecting comprehensive discussions.
- The performance of certain GNN models, such as EGNN and PaiNN, is suboptimal, and the reasons for this could be more thoroughly discussed.
- Some technical aspects, such as the choice of baseline models and hyperparameter settings, are not sufficiently justified.
- The manuscript could benefit from a more standardized datasheet and a clearer maintenance plan for the datasets.

### Suggestions for Improvement
We recommend that the authors improve the dataset curation to include more cases relevant to Lagrangian physics, particularly those that exhibit turbulence. Additionally, a discussion or plan for future dataset expansions should be included in the appendix. 

We suggest enhancing the depth of analysis in sections 3.5 and 4 to provide more comprehensive explanations of neural network performance and unexpected results. We also recommend that the authors improve the discussion on the limitations of the current particle number in simulating turbulence and explore potential solutions.

We encourage the authors to clarify the choice of hyperparameters for baseline models and justify why certain configurations were fixed during their search. Furthermore, we suggest providing a more exhaustive analysis of the performance issues related to EGNN and PaiNN, including possible modifications to enhance their effectiveness.

We recommend including a public GitHub repository linked to the paper, along with a Jupyter notebook to facilitate user engagement with the datasets and models. Lastly, we advise the authors to elaborate on the training strategies mentioned, such as noise injection and the pushforward trick, and to explore their effects on model performance in future experiments. Incorporating a standardized datasheet in the appendix and clarifying the maintenance plan for the datasets would also strengthen the manuscript.