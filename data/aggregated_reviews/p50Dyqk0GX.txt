ID: p50Dyqk0GX
Title: Dual Risk Minimization: Towards Next-Level Robustness in Fine-tuning Zero-Shot Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 5, 6, 5, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a dual risk minimization (DRM) approach that integrates empirical risk minimization (ERM) and worst-case risk minimization (WCRM) to enhance the robustness of fine-tuning foundational models under distribution shifts. The authors utilize class descriptions generated by GPT-4 as prompts for model input and apply max-min normalization to classification probabilities. Experimental results indicate that this method significantly improves robustness in zero-shot fine-tuning and achieves state-of-the-art performance on datasets like ImageNet and iWildCam.

### Strengths and Weaknesses
Strengths:
- The authors propose a novel dual risk minimization approach supported by rigorous mathematical proofs and empirical results.
- The integration of LLM-generated class descriptions is innovative, allowing for practical implementation without human annotations.
- The method demonstrates significant performance improvements on challenging datasets, highlighting its effectiveness.

Weaknesses:
- The reliance on prompts generated by GPT-4 for classifier initialization may not yield optimal performance, and the authors do not explore the potential of directly fine-tuning the classifier.
- The sensitivity to the hyperparameter $\lambda$ is significant, with slight deviations leading to performance drops, and there is a lack of ablation studies across multiple datasets.
- The paper does not adequately analyze the reliability of the core features extracted from LLM-generated descriptions, nor does it address the implications of using different LLMs.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their method by conducting ablation studies on additional datasets beyond iWildCam to validate the performance of the hyperparameter $\lambda$. Additionally, the authors should provide a thorough analysis of the reliability of the core features obtained from LLM-generated descriptions and explore the impact of using different LLMs. Clarifying the computational efficiency and scalability of the proposed methods, as well as addressing the inter-class similarity in their approach, would enhance the paper's contributions.