ID: Yz3wBKoK0K
Title: Aggregate-and-Adapt Natural Language Prompts for Downstream Generalization of CLIP
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 5, 5, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method to enhance the downstream generalization of CLIP by distilling knowledge from LLM- or human-generated text prompts through a prompt generator that predicts prompt embeddings (AAPE) based on images. The approach aggregates multiple prompts to create an aggregated text feature, which guides the adaptive transformation of CLIP image features for various vision-language tasks, including few-shot classification, image captioning, and visual question answering (VQA). The method does not require LLM inference during testing, improving efficiency. Additionally, the authors analyze the modality gap in relation to generalization performance, asserting that a reduced modality gap does not necessarily correlate with improved generalization. They observe that after prompt tuning, the modality gap decreases in 7 out of 11 datasets while performance improves consistently. Strong image-text alignment, measured by cosine feature similarity through their image-to-text mapping function, is argued to contribute to good generalization. The multi-task learning objective, which incorporates a distillation loss, is highlighted as a factor that promotes generalization and mitigates overfitting, with implications that this approach can extend beyond classification tasks.

### Strengths and Weaknesses
Strengths:
- The method effectively aggregates and distills task-related knowledge from prompts, leading to improved downstream generalization of CLIP.
- The approach shows strong performance across multiple vision-language tasks, demonstrating its effectiveness and efficiency.
- The paper provides empirical evidence that challenges the correlation between modality gap reduction and generalization performance.
- Strong theoretical backing is presented regarding image-text alignment and multi-task learning's role in enhancing generalization.
- The evaluation of the modality gap across multiple datasets and tasks demonstrates a comprehensive approach.

Weaknesses:
- The analysis of the aggregated text prompt is insufficient, lacking evidence that the Input-Adapted Prompt Aggregator can mitigate the impact of noisy text.
- There is a lack of justification on how the adaptive transformation of CLIP image features overcomes the image-text modality gap, which remains a concern.
- The paper does not provide experimental results with varying distillation loss coefficient values, making it difficult to evaluate claims about parameter efficiency.
- The method exhibits overfitting on base classes, with lower performance on novel classes, indicating potential generalization issues.
- The authors do not claim to fully overcome the modality gap, which may leave questions about the completeness of their solution.
- The need for further verification of their hypothesis across more tasks and data distributions indicates potential limitations in the current findings.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the aggregated text prompt to provide evidence supporting the effectiveness of the Input-Adapted Prompt Aggregator in handling noisy text. Additionally, the authors should clarify how the proposed method addresses the image-text modality gap and provide experimental results with different distillation loss coefficient values to substantiate their claims. Including a comparison with more recent methods in prompt learning and addressing the overfitting issue on novel classes would enhance the comprehensiveness of the study. Furthermore, we recommend that the authors improve the clarity of their claims regarding the modality gap and its implications for generalization, ensuring that the distinction between correlation and causation is well articulated. Providing more empirical support in the Appendix for the relationship between multi-task loss and modality gap across various vision-language tasks would strengthen their argument. Lastly, expanding on the implications of their findings for future research could enhance the paper's impact.