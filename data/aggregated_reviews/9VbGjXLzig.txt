ID: 9VbGjXLzig
Title: No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 7, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the "zero-shot" performance of multimodal models such as CLIP and Stable-Diffusion, analyzing 34 models and 5 pretraining datasets. The findings indicate that these models require exponentially increasing amounts of data for linear performance improvements, highlighting the elusive nature of effective "zero-shot" generalization with large-scale training data. The authors explore the correlation between concept frequency in pretraining data and downstream task performance, revealing a log-linear relationship and reaffirming the long-tailed distribution of concepts in pretraining datasets. Additionally, the paper presents results for models pretrained on LAION-400M, detailing their performance across various datasets, including ImageNet and Let-It-Wag! The authors propose to address reviewer concerns by updating related work, discussing performance dips in high-frequency classes, and providing additional details on the Let-It-Wag dataset.

### Strengths and Weaknesses
Strengths:
- The paper presents a well-structured comparative analysis of model performance across various downstream tasks and the frequency of test concepts in pretraining datasets.
- It is clearly written and easy to follow, with extensive experimental evidence supporting its claims.
- The introduction of the "Let It Wag!" benchmark is a significant contribution for evaluating multimodal models on long-tail distributions.
- The inclusion of results for models pretrained on LAION-400M enhances the paper's comprehensiveness.
- Reviewers express satisfaction with the authors' responses and acknowledge the potential of the paper as a strong addition to NeurIPS 2024.

Weaknesses:
- The description of tables should be unified for clarity.
- The work lacks a discussion on correlations with prior research, limiting the assessment of its novelty.
- Insufficient qualitative visualizations and theoretical analysis are provided, despite the paper's accessible perspective.
- The authors' claim of a 'constant linear relationship' is questioned due to unexplained accuracy dips, and the novelty of establishing long-tailed distributions in pretraining datasets is overstated.
- Limited information on the dataset construction and the implications of findings for future research is presented.
- There is a need for further discussion on the limitations of foundational models like CLIP, particularly regarding their zero-shot recognition capabilities.
- The paper requires updates in specific sections to fully address reviewer feedback.

### Suggestions for Improvement
We recommend that the authors improve the clarity of table descriptions and unify them for better readability. Additionally, addressing correlations with prior work in the literature review will enhance the novelty assessment of this study. We suggest including qualitative visualizations and theoretical analyses to deepen the insights provided. The authors should clarify the motivation behind their work and discuss its implications for future research. Furthermore, we encourage the authors to validate their findings with tasks beyond classification and retrieval, and to provide more detailed explanations for Figures 24-27. Lastly, including dataset statistics and insights in the appendix will strengthen the paper's contribution. We also recommend that the authors incorporate feedback from the rebuttal by updating the related work, discussing the dips in high-frequency classes, and including detailed statistics for the 'Let It Wag' dataset.