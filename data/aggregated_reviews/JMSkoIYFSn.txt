ID: JMSkoIYFSn
Title: Improving Span Representation by Efficient Span-Level Attention
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on span-level attention mechanisms, proposing four attention patterns—inside-token, containment, adjacency, and all-token—to enhance span representations while reducing computational complexity. Experimental results demonstrate the effectiveness of these methods, particularly for nested Named Entity Recognition (NER) tasks.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- Extensive and solid experiments validate the proposed heuristics.
- The proposed attention patterns show superior performance over baselines, especially in nested NER tasks.

Weaknesses:
- The improvements in performance are marginal and lack novelty compared to existing methods.
- The practicality of combining the four attention patterns is unclear, as no unified solution is provided.
- The validation of the proposed methods is limited to BERT, lacking comparisons with other state-of-the-art models.
- There are inconsistencies in reported results, particularly in Table 3, and the writing contains typos and unclear descriptions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the differences between the "inside-token" and "all-token" patterns, as their equivalence could undermine the findings. Additionally, we suggest including experiments on other encoder backbones to demonstrate the generality of the proposed method. The authors should also explore the potential of overlapping and non-contiguous spans and provide a more detailed analysis of the effectiveness of the proposed attention patterns. Finally, addressing the typos and enhancing the overall writing quality will strengthen the paper.