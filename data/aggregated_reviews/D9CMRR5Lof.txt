ID: D9CMRR5Lof
Title: MGDD: A Meta Generator for Fast Dataset Distillation
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 8, 7, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to dataset distillation that leverages meta-learning to enhance efficiency by eliminating the need for backpropagation. The authors propose a method where synthetic labels are generated through an analytical least-squares solution in a feature space, and synthetic images are produced using a pre-learned meta generator. Theoretical evidence is provided to support the existence of a conditional generator that can transfer synthetic samples with an upper-bounded error. Experimental results indicate that this backprop-free dataset distillation method achieves comparable performance to state-of-the-art techniques while offering a 22Ã— acceleration and flexibility for cross-size generalization. Additionally, the authors analyze the performance of their method, focusing on the sensitivity of results to learning rates and performance upon convergence. They empirically set the learning rate to 0.001, noting that the default learning rate of 0.0003 was insufficient for satisfactory performance, while larger rates could lead to early saturation. The results demonstrate that their method consistently outperforms DSA and DM baselines and generally surpasses the FRePo baseline, particularly with smaller distilled datasets.

### Strengths and Weaknesses
Strengths:
- The paper effectively addresses a significant challenge in dataset distillation, presenting an elegant solution that achieves a remarkable speed improvement of 22 times while maintaining competitive performance levels.
- Robust theoretical analysis supports the proposed method, particularly through Theorem 1, which demonstrates the existence of a conditional generator with upper-bounded error.
- Comprehensive algorithmic workflows are provided for reproducibility, and the paper is well-written, making it easy to follow.
- Extensive supplementary material includes additional experimental results and visualizations, further validating the proposed method.
- The authors provide comprehensive results that address reviewer concerns, showcasing the effectiveness of their method against established baselines.
- The empirical analysis of learning rates adds depth to the understanding of performance dynamics.

Weaknesses:
- The claim of being backpropagation-free is contested, as the training of the meta-generator relies on backpropagation, which could mislead readers regarding the method's operational nature.
- The paper lacks clarity on the adaptation time and feed-forward time, which should be presented separately to illustrate the time costs involved.
- Limited evaluation across diverse datasets raises concerns about the generalizability of the method, as it primarily focuses on CIFAR-10 and CIFAR-100.
- The meta-training costs and the implications of using specific hardware for training are not adequately discussed.
- The paper may benefit from a title change to better reflect its contributions, as noted by reviewers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the adaptation and feed-forward time by presenting these metrics separately in the experimental results. Additionally, addressing the concerns regarding the backpropagation-free claim is essential; consider revising the title to reflect the pretraining nature of the method, such as "MGDD: A Meta Generator for Fast Dataset Distillation." Furthermore, we suggest expanding the evaluation to include more diverse datasets to strengthen the claims of adaptability. A clearer discussion of the meta-training costs and the hardware implications would enhance the paper's comprehensiveness. Finally, we recommend improving the clarity of the title to more accurately represent the contributions of the paper and consider supplementing the performance results upon convergence to provide a more complete picture of the method's effectiveness.