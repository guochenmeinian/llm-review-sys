ID: KRQADH68fG
Title: HuatuoGPT, Towards Taming Language Model to Be a Doctor
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HuatuoGPT, a large language model designed for medical consultation in Chinese, which integrates distilled data from ChatGPT and real-world data from doctors. The authors propose a reinforcement learning method utilizing mixed feedback to enhance the modelâ€™s performance. Evaluations demonstrate that HuatuoGPT outperforms existing open-source medical models and ChatGPT across various Chinese medical benchmarks, showcasing its effectiveness in both single-turn and multi-turn dialogues.

### Strengths and Weaknesses
Strengths:
- The model effectively combines supervised fine-tuning with reinforcement learning, demonstrating good performance in the medical domain.
- The use of both ChatGPT-distilled and real-world medical data is valuable for training.
- Comprehensive evaluation protocols, including both automated and human assessments, are employed.

Weaknesses:
- The definition of multi-turn conversation tasks is unclear, lacking specific input and output details.
- There is insufficient evidence to support claims regarding the necessity of GPT-distilled data for improving patient-friendliness.
- The paper appears to be more engineering-focused, with limited novelty and significant experimental validation.
- The test sets used for evaluation are small and may not be representative, raising concerns about the generalizability of results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the multi-turn conversation task definition by specifying the exact input and output for both patient-chatgpt and doctor-model. Additionally, we suggest providing experimental results to substantiate the claim that training with GPT-distilled data enhances patient-friendliness. It would be beneficial to clarify the data descriptions, including the exact amount and length of data used. We also encourage the authors to include a comparison between RLMF and RL in the ablation experiments to better illustrate the impact of the mixed-feedback approach. Finally, addressing the limited size and representativeness of the test sets could enhance the robustness of the findings.