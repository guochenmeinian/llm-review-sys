ID: dmD63sv0TZ
Title: Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 6, 5, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called Gradient-based Intervention Targeting (GIT) aimed at efficiently selecting intervention targets to enhance causal discovery from data. The authors claim that GIT is the first gradient-based intervention targeting method, utilizing gradient estimators from causal discovery frameworks to optimize the acquisition of interventional data. The effectiveness of GIT is validated through extensive empirical evaluations on both synthetic and real-world datasets, demonstrating superior performance, particularly in low-data scenarios.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant problem in causal discovery and presents a novel approach that is compatible with various frameworks.
- It is well-written and easy to understand, with extensive empirical evaluations showing that GIT outperforms other methods, especially in low-data conditions.
- The proposed method is technically sound, supported by thorough experimentation and comparisons with multiple baselines.

Weaknesses:
- The idea of using gradient information is not entirely novel, as the underlying principles have been explored in previous literature.
- There is a lack of clarity regarding why gradients from "imaginary" interventional data can approximate those from "real" interventional data; this warrants further exploration and clearer exposition in the text.
- The performance of GIT in continuous settings appears to be minimal compared to random baselines, raising questions about the choice of baselines and the method's robustness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the GIT method's underlying principles and provide a summary of gradient-based methods to make the paper more accessible. Additionally, the authors should address the surprising performance of random baselines and clarify the rationale behind their selection. We suggest including a hypothesis regarding the approximation of gradients from imaginary data and emphasizing this phenomenon as a potential direction for future work. Furthermore, the authors should consider evaluating the performance of \(\epsilon\)-greedy GIT as a safeguard against inaccuracies in gradient estimation. Lastly, a more detailed theoretical justification for the convergence of the method and the assumptions behind theorems presented would enhance the paper's rigor.