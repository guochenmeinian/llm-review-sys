ID: pR5g1bBqoV
Title: $\boldsymbol{\mu}\mathbf{P^2}$: Effective Sharpness Aware Minimization Requires Layerwise Perturbation Scaling
Conference: NeurIPS
Year: 2024
Number of Reviews: 27
Original Ratings: 4, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents $\mu P^2$, a novel parameterization that scales the perturbation radius of Sharpness-Aware Minimization (SAM) across different model widths. The authors analyze SAM's behavior in the infinite-width limit using tensor program theory, demonstrating that $\mu P^2$ facilitates hyperparameter transfer and enhances generalization performance through extensive experiments on various neural architectures. Additionally, the paper explores the spectral perspective in training neural networks, focusing on perturbation scaling and its implications for training time and stability. The authors argue that this perspective can simplify and generalize existing theories while addressing limitations in their tensor program (TP) theory. They also emphasize the importance of clearly stating assumptions and the need for further exploration of long training dynamics.

### Strengths and Weaknesses
Strengths:
- The paper introduces an original and significant idea, supported by convincing experimental results.
- It provides a rigorous theoretical analysis of SAM in the infinite-width limit, extending the understanding of SAM's dynamics in large networks.
- The authors demonstrate a commitment to clarifying the TP framework and its practical implications, engaging thoughtfully with reviewer feedback.

Weaknesses:
- The presentation is verbose and lacks clarity, making it difficult to grasp the main messages.
- There is an absence of figures to aid comprehension of the theoretical concepts.
- The writing assumes a high level of familiarity with tensor programming, which may alienate a broader audience.
- Certain definitions, particularly in the appendix, are incomplete, and the reliance on the appendix for key definitions limits accessibility.
- The method for setting the perturbation radius $\rho$ remains unclear after multiple readings.
- Concerns regarding the accuracy of statements related to batch sizes in SAM training exist, with discrepancies noted in the authors' claims.
- The authors' claims about rigor and uniqueness are inadequately justified, leading to confusion among reviewers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing by reducing verbosity and enhancing the organization of the content. Including more figures would help illustrate key concepts and theorems, and we suggest annotating each term in equations with intuitive meanings using color or underbraces to facilitate understanding. Additionally, we encourage the authors to provide a comprehensive table summarizing all notations used in the paper and ensure that all definitions are included in the main text rather than relegated to the appendix. To clarify the practical application of $\mu P^2$, we recommend providing a more intuitive explanation of the parameterization and a concrete example for setting the perturbation radius $\rho$. Furthermore, we suggest correcting inaccuracies regarding batch sizes in SAM training, specifically clarifying the relationship between perturbation and update batch sizes. Lastly, we encourage the authors to engage more deeply with the criticisms regarding the rigor of their assumptions and to consider integrating improvements to the core algorithms alongside their theoretical analysis to enhance the overall contribution of the paper.