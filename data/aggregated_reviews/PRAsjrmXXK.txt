ID: PRAsjrmXXK
Title: Group Robust Preference Optimization in Reward-free RLHF
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 4, 5, 7, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel preference optimization technique, Group Robust Preference Optimization (GRPO), which utilizes group distributional robust optimization to maximize worst-case group performance, thereby enhancing the robustness of large language models (LLMs). The authors provide theoretical insights into GRPO's convergence guarantees and closed-form solutions within the log-linear policy class. Empirical results on synthetic and real-world datasets demonstrate the effectiveness of the method.

### Strengths and Weaknesses
Strengths:
- This is the first study to apply the group DRO technique in the RLHF context.
- The paper includes several theoretical results that enhance understanding of the proposed method.
- GRPO is a general method applicable to various RLHF optimization techniques.

Weaknesses:
- The contribution lacks novelty as the group DRO technique is already well-established in various applications. The theoretical results, including Proposition 3.1 and Proposition 3.2, may not provide new insights.
- Stronger motivations and empirical results are needed to substantiate the necessity of the proposed method, particularly demonstrating group robustness issues in existing LLMs.
- The interpretation of results in Section 5.2 is insufficient, particularly regarding the unexpected performance improvements across all groups.
- The experiments are limited, and the evaluation metrics do not adequately reflect the model's performance across diverse groups.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contribution by providing a more thorough analysis of existing LLMs' group robustness issues, as suggested in lines 141-153. Additionally, a more detailed interpretation of the results in Section 5.2 is necessary to clarify the observed performance trends. It would also be beneficial to include a comparison of the zero-shot performances of Gemma-2B for each group and report the degree of bias in the pre-trained model. Furthermore, we suggest conducting experiments that fine-tune more layers or the entire neural network to strengthen the paper's contribution. Lastly, the authors should provide a comprehensive evaluation of the model's performance beyond reward model analysis, including alignment results across different groups.