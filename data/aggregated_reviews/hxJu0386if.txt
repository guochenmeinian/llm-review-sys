ID: hxJu0386if
Title: Focus on Query: Adversarial Mining Transformer for Few-Shot Segmentation
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 4, 2, 5, 5, 7, 7
Original Confidences: 5, 5, 4, 1, 3, 3

Aggregated Review:
### Key Points
This paper presents a query-centric Few-shot Segmentation (FSS) model called Adversarial Mining Transformer (AMFormer), which utilizes adversarial learning to generate segmentation results with minimal support guidance. The authors argue that traditional methods relying heavily on support samples are less effective. AMFormer achieves state-of-the-art (SOTA) performance on benchmark datasets like PASCAL-5 and COCO-20.

### Strengths and Weaknesses
Strengths:
1. The motivation for the query-centric approach is novel and compelling.
2. The paper is well-structured and easy to follow.
3. AMFormer demonstrates strong performance, surpassing existing methods on two datasets.

Weaknesses:
1. The baseline model appears overpowered, with results exceeding those of previous works without sufficient explanation. The authors should clarify the baseline model's performance and its components, particularly regarding the cycle-consistent transformer not introduced in the main sections.
2. The novelty of the proposed method is limited, as it closely resembles the self-support mechanism of SSP (ECCV2022).
3. The model's complexity may be high due to its architecture, and the implications of adversarial training on convergence and stability are unclear.
4. There is a lack of detailed analysis regarding the learned local proxies and their physical rationale.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the baseline model's performance and provide a detailed explanation of its components. Additionally, the authors should enhance the novelty of their work by distinguishing it more clearly from SSP. We suggest including a thorough ablation study on the impact of adversarial learning compared to direct supervised learning. Furthermore, the authors should clarify the task division among proxies and address model complexity concerns, particularly in relation to competing approaches. Lastly, we encourage the authors to investigate the generalizability of their method across different datasets, including those with small targets.