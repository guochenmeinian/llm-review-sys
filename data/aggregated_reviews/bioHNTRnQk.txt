ID: bioHNTRnQk
Title: Model Collapse Demystified: The Case of Regression
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 7, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the model collapse phenomenon in the context of Gaussian data and ridge regression, specifically examining how generative models trained on synthetic data can lead to increased test error. The authors argue that existing literature lacks a comprehensive theoretical framework and provide a mathematical framework that includes exact analytic formulae for test error as a function of synthetic data generations, regularization, and sample sizes. They highlight the implications of model collapse on scaling laws and label noise, revealing new phenomena such as changes in scaling laws and the potential for catastrophic collapse even in noiseless settings. The analysis is supported by empirical validation through experiments that extend findings beyond Gaussian data, demonstrating the applicability of their results in more complex settings. The authors propose adaptive regularization as a potential mitigation strategy.

### Strengths and Weaknesses
Strengths:
1. The paper offers a solid mathematical framework for analyzing model collapse, providing precise analytical formulae rather than just empirical observations.
2. It covers various scenarios, including different dimensionality regimes and noise levels, enhancing understanding of the phenomenon.
3. The work reveals new phenomena, such as changes in scaling laws and the potential for catastrophic collapse even in noiseless settings.
4. The analysis includes precise insights into scaling laws and the impact of label noise, which are critical for foundation models.
5. The authors have conducted additional experiments that extend their findings beyond Gaussian data, demonstrating the applicability of their results in more complex settings.

Weaknesses:
1. The analysis primarily focuses on linear regression, which may not capture the complexities of advanced machine learning models.
2. The theoretical framework relies on simplifying assumptions that may not hold in real-world scenarios.
3. While experiments are included, they primarily use synthetic datasets, limiting the applicability of findings to real-world contexts.
4. The paper claims to analyze kernel ridge regression, but none of the models described are in this setting, overselling its contributions.
5. The term "demystified" is considered ambiguous and does not accurately reflect the paper's focus; a more descriptive title is recommended.
6. The relationship between Gaussian results and non-Gaussian data generation remains unclear, raising concerns about the relevance of the findings to practical applications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the abstract by explicitly stating that the focus is on Gaussian data and specifying the regimes discussed. Additionally, the authors should address the limitations of their linear regression model in relation to more complex generative models and provide a more thorough exploration of alternative mitigation strategies beyond adaptive regularization. Furthermore, we suggest that the authors clarify the connection to self-distillation and ensure that claims regarding kernel regression analysis are accurately represented. We also recommend revising the title to something more descriptive, such as "Model Collapse Asymptotics in The Case of Gaussian Regression," and elucidating how their findings relate to non-Gaussian data generation, ensuring that the implications of their theoretical results for complex models are clearly articulated. Finally, enhancing the presentation, particularly in figures and notation, will improve readability and comprehension.