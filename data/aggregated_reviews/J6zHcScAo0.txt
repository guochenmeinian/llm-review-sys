ID: J6zHcScAo0
Title: Transcoders find interpretable LLM feature circuits
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the use of transcoders as a tool for mechanistic interpretability, replacing Sparse Autoencoders (SAEs). The authors propose that transcoders, which function as an encoder-decoder architecture, reconstruct the output of a Multi-Layer Perceptron (MLP) rather than its input, as SAEs do. The comparison between transcoders and SAEs is made in terms of sparsity, faithfulness, and human interpretability, demonstrating that transcoders perform comparably across various model sizes. The authors detail a methodology for weights-based circuit analysis through MLP sublayers, including attribution between features, identification of computational subgraphs, and the definition of de-embedding vectors. A case study on GPT2-small illustrates the application of transcoders in analyzing the "greater-than" circuit. Additionally, the paper discusses the limitations of MLP layers compared to attention layers and how transcoders facilitate input-invariant circuit analysis, addressing challenges posed by polysemantic neurons in MLPs.

### Strengths and Weaknesses
Strengths:
- The paper is the first to apply transcoders to large models, providing novel insights into their performance.
- The circuit analysis effectively disentangles input-invariant from input-dependent information, addressing a significant challenge in interpretability.
- The writing is clear and the methodology is well-structured, making the paper easy to follow.
- The new plots comparing MLP-out SAEs effectively address previous concerns, enhancing the paper's soundness.
- The proposed use of transcoders provides a clearer framework for circuit analysis, allowing for better decomposition of MLP layers.
- The authors demonstrate a solid understanding of the complexities involved in feature attributions and input interactions.

Weaknesses:
- The circuit analysis assumes transcoders perfectly emulate MLPs, which may not hold true due to the presence of multiple-head attention layers.
- The evaluation lacks depth; comparisons with SAEs focus primarily on sparsity and faithfulness, neglecting systematic interpretability experiments.
- The choice of baseline comparisons, particularly neuron-based analysis, is weak and could be improved with more appropriate alternatives.
- The interpretation of the input-dependent term in the context of feature activation may be less straightforward than suggested, as it depends on the activation state of the target feature.
- There are lingering concerns regarding the alignment of the paper's motivation with the authors' clarifications, which may affect clarity.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by conducting more systematic interpretability experiments, as this is crucial for validating the proposed method's effectiveness. Additionally, we suggest addressing the limitations of the circuit analysis by accounting for the effects of multiple-head attention layers in the MLP. Clarifying the rationale behind the choice of layer 15 for blind interpretability experiments and providing a more robust justification for the baseline comparisons would also strengthen the paper. We recommend improving the clarity of the motivation by ensuring that it aligns with the revisions made in response to feedback, specifically by editing the introduction to emphasize the limitations of SAEs in providing general input-output behavior of MLPs. Furthermore, we suggest reorganizing the sections to foreground circuit analysis, moving relevant discussions closer to the beginning of the paper. Including a brief discussion on why SAEs fail to provide input-invariant attributions through MLP sublayers in the new Section 3.2 would enhance understanding. Finally, we encourage the authors to explore ways to characterize interactions between input features more interpretably, as this could be a fruitful area for future research.