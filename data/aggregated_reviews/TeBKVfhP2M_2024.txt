ID: TeBKVfhP2M
Title: Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a formulation of hard prompt compression as a rate-distortion problem, providing an algorithm for hard prompt compression and another for estimating the RD curve (Algorithm 1). The authors propose a linear programming-based algorithm that produces compressed "hard" prompts suitable for black box models, supported by empirical results on synthetic datasets that illustrate the gap between existing methods and the theoretically optimal distortion-rate tradeoff.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and accessible, even for readers with limited knowledge of large language models (LLMs).
- It adopts a principled approach through information theory to investigate fundamental limits in prompt compression.
- The proposed distortion-rate formulation is simple, interpretable, and allows for query-adaptive solutions.
- Empirical results demonstrate improvements over existing methods, highlighting the potential of the proposed algorithm.

Weaknesses:
- The synthetic dataset is limited, featuring only binary support and a small number of queries, which may not reflect practical scenarios.
- The chosen distortion metrics (log-loss and 0/1 loss) do not clearly capture semantic distortion as intended, raising questions about their appropriateness.
- The motivation for query-aware compression is unclear, and the theoretical discussions focus on the query-agnostic setting, which may detract from the main content.

### Suggestions for Improvement
We recommend that the authors improve the empirical results by including experiments with real datasets, such as the NarrativeQA dataset, to enhance the practical relevance of their findings. Additionally, we suggest focusing more on the empirical results and clarifying why the RD formulation is suitable for this problem, as the machine learning community may not be familiar with information theory. We advise moving the entirety of section 3.3 to the appendix and replacing it with a discussion on how the proposed distortion functions can effectively measure semantic distortion. Finally, addressing the normalization of the rate without normalizing the distortion and providing comparisons of the efficiency of the proposed method with existing approaches would strengthen the paper.