ID: PjBEUTVzoe
Title: Implicit Bias of (Stochastic) Gradient Descent for Rank-1 Linear Neural Network
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the rank-1 linear network model, which consists of layers structured as rank-one matrices parameterized by two vectors. The authors argue that this model provides insights into the implicit biases of gradient descent (GD) and stochastic gradient descent (SGD) in training multilayer linear networks. The analysis reveals that the potential functions minimized during training depend on network depth, initialization, and stochasticity, suggesting that deeper networks mitigate the effects of initialization. Additionally, the authors analyze the relationship between rank-1 linear networks and standard linear networks, particularly focusing on implicit bias and dynamics under gradient flow. They propose that while single output linear networks do not adequately capture entry sparsity effects, a detailed comparison with multi-output cases will clarify these differences. The authors emphasize that their primary aim is to characterize the SGD solutions of rank-1 networks and analyze the implicit bias, particularly concerning initialization scale effects.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a novel model that contributes to understanding implicit biases in neural networks, particularly through rigorous analysis and clear presentation.  
- It provides explicit formulas for the dynamics of rank-1 linear networks, highlighting the relationship between initialization scale, depth, and implicit biases.  
- The authors provide a clear distinction between single output and multi-output cases, enhancing the understanding of sparsity effects.  
- The formulation in Proposition 1 effectively illustrates the similarities between rank-1 and standard linear networks.  
- The paper addresses a gap in the literature regarding the implicit bias of SGD for standard linear networks, offering a tractable model for rigorous analysis.

Weaknesses:  
- The focus on rank-1 linear networks may limit the relevance of findings, as they do not capture the sparsity effects observed in multi-output networks.  
- The assumption of balanced initialization is strong and may not reflect practical scenarios, raising questions about the generalizability of the results.  
- The distinction between gradient flow and GD/SGD is misleading, as the complexities of GD/SGD are not adequately addressed.  
- The characterization of the generality of the dynamics independence on width remains an open problem, requiring further rigorous analysis.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of the rank-1 linear network model, particularly regarding its inability to capture sparsity effects present in multi-output networks. Additionally, the authors should clarify the implications of their balanced initialization assumption and explore whether similar results hold under more general initializations. We also suggest that the authors replace references to GD and SGD with gradient flow and stochastic gradient flow where appropriate, and include relevant literature that discusses the differences between these processes. Furthermore, we recommend that the authors improve the clarity of the differences between single output and multi-output cases in the revision and further explore the generality of the independence of dynamics on width, as this remains an open area that could benefit from rigorous analysis.