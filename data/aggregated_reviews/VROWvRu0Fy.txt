ID: VROWvRu0Fy
Title: Towards Expansive and Adaptive Hard Negative Mining: Graph Contrastive Learning via Subspace Preserving
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method, GRAPE, for Graph Contrastive Learning (GCL) through Subspace Preserving. The authors propose a more comprehensive and adaptive approach to hard negative mining, which enhances node-level GCL by estimating negative hardness based on subspace theory. GRAPE captures hard negatives beyond message passing and adaptively scales the set of hard negatives. To address the impact of false negative samples, the authors devise two strategies. GRAPE shows superior performance across eight public graph datasets and includes a theoretical explanation of its properties and relationships with related methods.

### Strengths and Weaknesses
Strengths:  
- The authors provide comprehensive references that justify the motivation of the paper.  
- Solid technical details are presented.  
- The proposed method addresses issues of hard negative sampling in graph contrastive learning.  
- Extensive experiments evaluate performance, time cost, and hyperparameter influence, making GRAPE convincing.  
- Theoretical proofs and analysis of theorems and propositions are detailed.  
- The article has a coherent logical flow and aesthetically pleasing illustrations.  

Weaknesses:  
- The introduction contains excessive background information that should be relocated to the related work section, obscuring the core idea.  
- The last paragraph of the introduction is disconnected, with the proposed solution appearing abruptly.  
- Section 3.2 is poorly organized, lacking a clear structure.  
- Key parameters and definitions are inadequately introduced, making the paper hard to read.  
- Some definitions, such as Î¦_i, are blurred, leading to confusion regarding hard negatives and false negatives.  
- The initialization and selection of hard negative sets before training are unclear.  
- The method appears applicable only to homogeneous graphs, with limited exploration in heterogeneous graphs.

### Suggestions for Improvement
We recommend that the authors improve the organization of the introduction by moving excessive background content to the related work section and ensuring a smoother transition to the proposed solution. We suggest enhancing the clarity of Section 3.2 by providing a more structured presentation of the proposed method. Additionally, we encourage the authors to clearly define key parameters and concepts, particularly "subspace preserving," and to elaborate on the initialization and selection processes for hard negative sets. Lastly, we recommend addressing the applicability of the method to heterogeneous graphs and correcting any detailed errors, such as the spelling mistake in Figure 1.