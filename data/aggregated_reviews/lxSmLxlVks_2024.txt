ID: lxSmLxlVks
Title: Search for Efficient Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 6, 6, 4, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a neural architecture search method for Large Language Models (LLMs) that involves three steps: inheriting salient weights from the original model to create an initial sub-network, employing an evolutionary algorithm to identify the optimal sub-network, and reconstructing the original networkâ€™s output using calibration samples. The authors propose a technique based on mask mutation and candidate evaluation to enhance performance, demonstrating that the compressed LLM surpasses existing baselines.

### Strengths and Weaknesses
Strengths:
- The application of an evolutionary algorithm to the pruning problem effectively addresses the limitations of uniform sparsity across layers.
- The method avoids reliance on back-propagation, making it suitable for larger models with limited memory.
- The experimental results are comprehensive, showing significant improvements over baselines like SliceGPT and LLM-Pruner.
- The paper is well-organized and clearly written.

Weaknesses:
- The candidate evaluation relies solely on perplexity from WikiText2, which may bias the model towards language modeling tasks. Validation on other tasks, such as MMLU, is necessary.
- The performance on QA tasks is not presented; this should be included in the appendix if space is limited.
- The metric of tokens/s for speed is not rigorous; alternative metrics like MACs should be considered.
- The term "inheriting ratio" lacks a precise definition, and its distinction from "sparsity" should be clarified.
- The writing in Section 3.2 is confusing and requires improvement, particularly regarding the definitions and relationships of variables and masks.

### Suggestions for Improvement
We recommend that the authors improve the candidate evaluation by validating the pruned model on the MMLU dataset and including performance metrics for QA tasks in the appendix. Additionally, we suggest using more rigorous speed metrics, such as MACs, and providing clearer definitions for "inheriting ratio" and other terms. The authors should enhance the clarity of Section 3.2 by explicitly defining variables and addressing the inconsistencies in the use of masks across layers. Finally, we encourage a discussion on the trade-offs between the time spent on subnet searching versus post-training the compressed model.