ID: tZXaHWfsXB
Title: Transcending Scaling Laws with 0.1% Extra Compute
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called UL2R aimed at enhancing the scaling laws and performance of large language models, specifically PaLM. The authors propose continuing the training of an existing pretrained language model with a small number of additional steps using the UL2 training objective, which introduces diversity compared to causal language modeling and allows for bidirectional attention and infilling-style pretraining. Experiments demonstrate that UL2R significantly improves PaLM's performance on NLP tasks and scaling, enabling emergent abilities at smaller scales and offering new prompting capabilities.

### Strengths and Weaknesses
Strengths:
- UL2R substantially enhances PaLM's performance on downstream NLP tasks and scaling, achieving state-of-the-art results with minimal extra compute.
- The technique enables emergent abilities at smaller scales and introduces new prompting capabilities, such as bidirectional infilling.
- The paper is well-written, with thorough experiments and convincing results, and has practical applicability for increasing training efficiency.

Weaknesses:
- The evaluation is limited to a single model (PaLM), raising concerns about the generalizability of the gains.
- There is a lack of detailed analysis on why UL2R provides benefits and how different tasks interact.
- Some reviewers noted that the presentation of innovation points is vague and repetitive, complicating the understanding of the contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the innovation points in the introduction to avoid vagueness and repetition. Additionally, we suggest that the authors evaluate UL2R on multiple models to support the universality of the approach. It would also be beneficial to provide more detailed analysis on the interactions of different tasks and clarify the source of the extra compute mentioned in the title.