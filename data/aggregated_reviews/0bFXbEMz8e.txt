ID: 0bFXbEMz8e
Title: FlowLLM: Flow Matching for Material Generation with Large Language Models as Base Distributions
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a hybrid approach, FlowLLM, that combines a large language model (LLM) and a Riemannian flow matching (RFM) model to enhance the generation of crystal structures. The authors first fine-tune an LLM to generate crystal structures and then apply an RFM model to improve the stability of these structures. The motivation is to address the limitations of LLMs in handling real-valued parameters, which are essential in crystal representations. Experimental results indicate that the introduction of the RFM model significantly enhances the stability of the generated crystals.

### Strengths and Weaknesses
Strengths:
1. Clear motivation for using a post-processing model to refine LLM-generated structures, addressing the challenges LLMs face with real values.
2. Demonstrated improved stability of generated crystals when using the RFM model compared to models without such refinement.

Weaknesses:
1. Lack of comparison with other established second-stage refinement strategies, such as machine learning force fields (MLFFs) like M3GNet and CHGNet, which could refine generated structures without the need for a separate RFM model.
2. Limited contribution due to the existence of well-established methods addressing similar issues, making the introduction of the RFM model appear somewhat redundant.
3. Significant drop in stability rates when duplicates are removed, suggesting that many generated crystals are similar or identical.
4. Inability to generate materials with specific properties, indicating a need for further discussion on the model's limitations compared to other computationally cheaper models like DiffCSP.

### Suggestions for Improvement
We recommend that the authors improve the discussion of alternative refinement strategies, particularly MLFFs, to provide a more comprehensive context for their approach. Additionally, addressing the significant drop in stability rates when duplicates are removed would clarify the uniqueness of generated crystals. We suggest including a comparison of generation times between RFM with a simple base distribution and FlowLLM without prior sampling. Furthermore, expanding the evaluation beyond the MP-20 dataset to include other datasets would strengthen the validation of the proposed method. Lastly, we encourage the authors to report the rejection rate of invalid samples generated by the LLM to enhance the transparency of their results.