ID: CUFRaWY7y5
Title: Log Probabilities Are a Reliable Estimate of Semantic Plausibility in Base and Instruction-Tuned Language Models
Conference: EMNLP/2024/Workshop/BlackBoxNLP
Year: 2024
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 4, 4, 3

Aggregated Review:
### Key Points
This paper presents a comparison of log-probability estimates and meta-linguistic judgments elicited via prompting, focusing on base versus instruction-tuned models across several semantic plausibility datasets. The authors find that log-probability performance is significantly above chance and consistent across model types, while meta-linguistic judgments show less power and consistency. A second experiment examines log-probability judgments in context, revealing subtle results and comparisons to human judgments. Overall, the paper is well-written, with thorough experiments that contribute valuable insights to the field.

### Strengths and Weaknesses
Strengths:
- Detailed comparison of log-probability judgments versus explicit plausibility judgments across multiple datasets.
- Instruction-tuning does not enhance performance on plausibility datasets.
- The contextual plausibility analysis includes three explicit metrics and insightful comparisons of word- versus sentence-level effects.

Weaknesses:
- Some results could benefit from increased quantification.
- Prompting data for Experiment 2 would be better placed in the main text rather than supplementary information.

### Suggestions for Improvement
We recommend that the authors improve the quantification of certain results to enhance clarity. Additionally, consider including the prompting data for Experiment 2 in the main text. For Figure 2, can the role of active vs passive be made into a statistical test, such as a regression of accuracy against model, base vs instruct, active vs passive, and interactions? In Section 4.4, Result 2, sign-posting to Figure 3 would aid comprehension, and reporting a statistical test or correlation to quantify the "remarkable match" would strengthen the analysis. Lastly, a more explicit discussion of how this paper's findings relate to those of Hu and Levy (2023) in the introduction could benefit readers unfamiliar with the prior work.