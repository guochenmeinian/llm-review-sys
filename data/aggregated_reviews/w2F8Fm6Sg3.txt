ID: w2F8Fm6Sg3
Title: Balanced Training for Sparse GANs
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 4, 8, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for dynamic sparse training (DST) in GANs, introducing a metric called balance ratio (BR) to assess the balance between the generator and discriminator. The authors propose a balanced dynamic sparse training strategy (ADAPT) to optimize this balance, aiming for an effective trade-off between performance and computational cost. The methodology is well-structured, detailing the implementation of ADAPT and the significance of BR in improving training efficiency.

### Strengths and Weaknesses
Strengths:
1. The motivation for balancing sparse GAN training is well-articulated.
2. The balance ratio is an interesting and potentially useful metric for understanding generator and discriminator sparsity.
3. The paper is well-written, with a clear logical flow and comprehensive methodology.

Weaknesses:
1. The experimental baselines (SNGAN, BigGAN) are outdated, and the use of small datasets limits the demonstration of the proposed method's value; larger datasets should be considered.
2. The motivation for applying DST to GANs lacks novelty, and improvements over existing methods like STU-GAN appear trivial.
3. The fluctuations in normalized training FLOPs raise questions about the consistency of the balance ratio with stable GAN training.
4. The rationale for the two variants of ADAPT is unclear, and the authors should clarify their expected performance differences.

### Suggestions for Improvement
We recommend that the authors improve their experimental design by incorporating larger datasets, such as FFHQ, to better showcase the effectiveness of their method. Additionally, a literature review discussing the advantages of the balance ratio over previous metrics would strengthen the paper. Clarifying the rationale behind the ADAPT variants and addressing the fluctuations in FLOPs would also enhance the paper's rigor. Finally, including experiments with structured sparsity could validate the proposed method's effectiveness in real-device scenarios.