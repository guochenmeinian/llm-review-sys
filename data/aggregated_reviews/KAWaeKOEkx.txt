ID: KAWaeKOEkx
Title: DropCompute: simple and more robust  distributed synchronous training via compute variance reduction
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DropCompute, a method designed to mitigate load imbalance across workers in distributed training by dropping micro-batches of a mini-batch when execution time exceeds a threshold. The authors provide theoretical convergence proofs and conduct experiments on models like ResNet-50 and BERT to demonstrate the method's effectiveness in improving training speed without significantly impacting accuracy.

### Strengths and Weaknesses
Strengths:
- The paper addresses the important issue of load imbalance in distributed deep learning, which is somewhat understudied.
- DropCompute is straightforward to implement, particularly in decentralized systems, and integrates well with micro-batching.
- The theoretical analysis of convergence and performance is thorough and clear, supported by experiments on representative networks.

Weaknesses:
- The convergence proof relies on several additional assumptions, making it unclear why the new algorithm performs better.
- The performance gains are marginal, with theoretical improvements of at most 20% and practical gains around 5%, raising questions about the method's robustness.
- The method's applicability to modern LLM training is limited due to the requirement for large mini-batch sizes and the potential for dropped samples to be lost permanently.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation and significance of load imbalance in practice by providing concrete examples of use cases where it is a significant issue. Additionally, we suggest including comparisons with other techniques for handling stragglers, such as asynchronous methods. To enhance the robustness of DropCompute, consider implementing a mechanism to reuse dropped samples in future iterations rather than discarding them entirely. Lastly, we encourage the authors to conduct experiments that validate the method's effectiveness without relying on artificially induced delays, as well as to clarify the implications of the drop rate on model accuracy.