ID: eQrkAPcGRF
Title: Math2Sym: A System for Solving Elementary Problems via Large Language Models and Symbolic Solvers
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 7, 7, 4, 7
Original Confidences: 3, 3, 4, 4

Aggregated Review:
### Key Points
This paper presents Math2Sym, a methodology that integrates LLMs with symbolic solvers to convert math word problems (MWPs) into symbolic forms for effective problem-solving. The authors introduce the EMSF dataset, which contains a variety of math problems, demonstrating that Math2Sym significantly enhances performance in elementary-level math tasks compared to other LLM-based approaches.

### Strengths and Weaknesses
Strengths:
- Math2Sym effectively combines LLMs' language comprehension with the precision of symbolic solvers, leading to accurate and interpretable solutions.
- The EMSF dataset is a valuable resource for fine-tuning LLMs, although some details about its size are missing.
- The paper provides strong empirical results, showing significant improvements over baselines like GPT-3.5 and competitive performance with GPT-4-mini in few-shot learning tasks.

Weaknesses:
- The evaluation is limited to elementary-level problems, restricting generalizability to more complex tasks, with no experiments on advanced topics like calculus.
- The system's robustness in real-world applications is not addressed, as it is evaluated on synthetic datasets.
- The methodology lacks innovation, as the task of converting elementary problems is seen as one of data curation rather than significant mathematical reasoning advancement.

### Suggestions for Improvement
We recommend that the authors improve their analysis by providing more detailed evaluations beyond a single score table. Additionally, conducting experiments with larger models, such as Mixtral 8x7B or Llama-70B, would help assess the EMSF dataset's impact. The authors should also clarify how their approach handles integer-only solutions, particularly in cases where the symbolic execution yields floating-point results. Finally, we suggest including evaluations on larger open-source LLMs and addressing the clarity of presented scores and comparisons in tables.