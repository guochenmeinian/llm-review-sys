ID: s1FjXzJ0jy
Title: Focused Transformer: Contrastive Training for Context Scaling
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 8, 8, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an improved training approach for memory-augmented Transformers, specifically addressing the distraction issue in memory-augmented models where the attention mechanism focuses on irrelevant contexts during long sequences. The authors propose cross-batch training, inspired by contrastive learning, to expose the attention mechanism to both relevant and irrelevant documents. The Focused Transformer (FoT) modifies the attention layer to Memory Attention, allowing the model to learn long context without local attention constraints. Experimental results validate the effectiveness of the proposed methods in shaping the key-value space and extending model context size.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and well-structured.
- The identified distraction issue is compelling and contributes significantly to the long-context research community.
- The proposed method is straightforward and demonstrates significant effectiveness in identifying relevant information.
- The cross-batch training method is effective and incurs relatively low training costs.

Weaknesses:
- Increased computational costs for the memory-augmented attention layer, with complexity rising from $O(bn^2)$ to $O(b d n^2)$, may introduce significant overhead.
- The main text does not clearly introduce FoT and cross-batch training, leading to confusion until the appendix is consulted.
- The inference stage's reliance on kNN may affect the model's upper performance limit.
- Limited novelty in contributions, as negative sampling and memorizing transformers have been previously proposed.
- The clarity and presentation of the paper could be improved, with important details often relegated to the appendix.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the main text by providing a more explicit introduction to FoT and cross-batch training. Additionally, the authors should address the potential computational overhead associated with increased complexity in the memory-augmented attention layer. It would be beneficial to discuss the differences between their cross-batch technique and existing methods, such as those proposed by Zhong et al. (2022). Furthermore, we encourage the authors to clarify the definitions of "external memory" and the distinctions between positive and negative contexts in their training objective. Lastly, including comparisons with other schemes for increasing context length, such as Parallel Context Window, would strengthen the paper's contributions.