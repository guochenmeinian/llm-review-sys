ID: 6Kg26g1quR
Title: ROIDICE: Offline Return on Investment Maximization for Efficient Decision Making
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel offline algorithm, ROIDICE, for optimizing Return on Investment (ROI) in reinforcement learning (RL) by formulating the problem as linear-fractional programming, which is then transformed into linear programming. The authors derive a new regularization term to address distribution shifts in offline learning and demonstrate that ROIDICE achieves a better trade-off between return and accumulated cost across various datasets.

### Strengths and Weaknesses
Strengths:
- The optimization of ROI is a novel and relevant approach that balances action costs and policy rewards.
- The mathematical soundness of the proposed method is well elucidated, and the experimental results show superiority over existing methods.
- The derivation of the algorithm is clearly described, contributing substantively to the state-of-the-art.

Weaknesses:
- The paper lacks a robust argument for the advantages of ROI optimization beyond its business applications, which diminishes its impact.
- Important baselines are missing, particularly comparisons with other offline constrained RL methods and approaches that integrate action costs as negative rewards.
- The implementation details for continuous domains are unclear, and the paper does not provide guarantees on cost compared to existing methods like CoptiDICE.
- There is no theoretical analysis of sample complexity, which is critical for understanding the algorithm's performance in large state-action spaces.

### Suggestions for Improvement
We recommend that the authors improve the rationale for why ROI is a suitable objective for RL beyond its business context. Additionally, the authors should include comparisons with relevant baselines that incorporate action costs as negative rewards. Clarifying the implementation of the proposed method in continuous domains, possibly with pseudocode, would enhance understanding. Providing a theoretical analysis of sample complexity and addressing the computational resources required for large datasets would also strengthen the paper.