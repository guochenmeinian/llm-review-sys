ID: bTidcHIK2t
Title: Sample-Efficient and Safe Deep Reinforcement Learning via Reset Deep Ensemble Agents
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 7, 5, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel reset-based method that leverages deep ensemble learning to mitigate the primacy bias in deep reinforcement learning (RL). The authors propose a technique that constructs N-ensemble agents and resets each agent sequentially to prevent performance collapses while improving sample efficiency. The method is evaluated across various environments, including safety-critical scenarios, demonstrating its effectiveness and potential for real-world applications.

### Strengths and Weaknesses
Strengths:
- The paper addresses the significant issue of primacy bias in deep RL, proposing a valuable method to mitigate its effects.
- The innovative use of deep ensemble learning enhances performance and prevents performance collapses.
- The analysis of the proposed method is comprehensive, providing clarity on its operations and effectiveness.

Weaknesses:
- The paper lacks extensive validation of the proposed method in safety-critical tasks, having conducted only one experiment on a safe RL benchmark.
- The presentation could be improved, with some irrelevant preliminaries and inconsistencies in Algorithm 1 that detract from clarity.
- There is insufficient discussion on related works addressing overfitting in deep Q-learning, and the paper does not adequately explore the computational implications of using ensembles.

### Suggestions for Improvement
We recommend that the authors improve the discussion on related works that address overfitting in deep Q-learning to provide better context for their contribution. Additionally, we suggest conducting more ablation studies to explore the trade-offs in ensemble resetting versus prior methods, including varying the number of ensemble agents and the frequency of resets. Addressing the computational costs associated with using additional agents and clarifying the selection mechanism in the context of the oldest Q-function would also enhance the paper's rigor. Finally, we encourage the authors to expand their experimental validation on safe RL benchmarks to strengthen their claims regarding the method's applicability in safety-critical scenarios.