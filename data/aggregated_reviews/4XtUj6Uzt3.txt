ID: 4XtUj6Uzt3
Title: Training Bayesian Neural Networks with Sparse Subspace Variational Inference
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 4, 4, 4

Aggregated Review:
### Key Points
This paper presents a method for sparsifying Bayesian neural networks (BNNs) to reduce computational costs during training and inference. The authors propose five criteria for adding and removing connections to achieve a predefined sparsity level. The work extends previous research by Evci on sparsity in deterministic neural networks, yet the authors do not explicitly clarify this relationship early in the paper. The authors briefly mention their approach's alignment with Evci's methods but should provide a clearer distinction between their algorithm and the RigL algorithm for transparency.

### Strengths and Weaknesses
Strengths:  
- Thorough literature review and solid motivation for the research.  
- Comprehensive experimentation with various selection criteria and robustness to weight initialization.  
- The ability to set a desired sparsity level in advance is a notable advantage.

Weaknesses:  
- Major issues with method explanations, theoretical setup, and result discussions.  
- Redundant theoretical background in Section 2; a concise reference to classical BNN formulation would suffice.  
- Confusing notation and unclear constraints in Section 3, particularly regarding the sparsification of variance versus mean tensors, and the role of gamma in Equation 3.  
- Lack of clarity on how the model guarantees the desired sparsity level and inconsistencies in the loss function formulation.  
- Insufficient detail in Section 4 regarding metrics and comparisons, particularly in Table 2 and Figure 2, leading to potential misinterpretations of results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by explicitly stating the relationship to Evci's work early in the paper. Additionally, please clarify the distinction between the proposed algorithm and RigL. In Section 2, consider condensing the theoretical background and moving detailed formulations to the appendix if necessary. In Section 3, clarify the notation regarding the subspace dimension and the role of gamma in Equation 3. Explain how the constrained minimization guarantees the desired sparsity level and ensure consistency in terminology throughout the paper. In Section 4, provide detailed explanations of the metrics used, particularly in Table 2, and include standard deviations for better comparison. Finally, consider expanding the dataset variety in future experiments to validate the method's applicability beyond CIFAR datasets.