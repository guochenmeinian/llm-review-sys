ID: ux826WlJtt
Title: DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource Event Extraction
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the DemoSG model for low-resource event extraction (EE), utilizing demonstration-based learning and schema-guided generation to enhance performance in resource-scarce contexts. The methodology involves a pre-trained encoder-decoder language model (BART) that incorporates demonstrations of event types and schema-based prompts. The evaluation is conducted on ACE05-EN and ACE05-EN+ datasets, showing competitive or improved performance compared to state-of-the-art systems across various settings.

### Strengths and Weaknesses
Strengths:
- The paper demonstrates solid performance improvements in low-resource scenarios, particularly in argument extraction.
- The structure is well-organized, enhancing comprehensibility, and the reproducibility is facilitated by adequate methodological descriptions and supplemental code.

Weaknesses:
- The lack of experiments on diverse datasets limits the generalizability of findings.
- Concerns regarding computational complexity and input length may hinder practical applicability.
- The ablation study lacks rigor, and the model's weaknesses are not adequately analyzed, leaving questions about performance in special cases and error analysis.

### Suggestions for Improvement
We recommend that the authors improve clarity in Figure 1 by specifying whether graph information is part of the input and elucidating the selection process for demonstration examples. Additionally, we suggest providing more details on the training and inference times, including metrics like GMACs or GFLOPs, to enhance the analysis. The authors should also consider conducting a thorough error analysis to identify model weaknesses and address the impact of demonstration selection on performance. Finally, we encourage the authors to explore future directions for event extraction beyond the ACE2005 dataset, considering the opportunities presented by large language models.