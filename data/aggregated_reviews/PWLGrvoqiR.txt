ID: PWLGrvoqiR
Title: RaLEs: a Benchmark for Radiology Language Evaluations
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 5, 6, 8, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RaLEs, a new benchmark for evaluating natural language understanding (NLU) and natural language generation (NLG) in the radiology domain. The benchmark comprises six datasets and four tasks, including named entity recognition (NER), relationship extraction (RE), procedure classification, and summarization. The authors curate one novel dataset and benchmark 16 pre-trained models across various domains, highlighting the need for a dedicated radiology benchmark due to the limitations of existing models trained on private datasets. Additionally, the paper analyzes radiology report generation and procedure selection models, focusing on performance metrics and dataset characteristics. The authors propose improvements in procedure selection, noting that performance varies with label availability and class representation. They emphasize the importance of context-aware models for report summarization and address potential data leakage issues in dataset splits. Furthermore, the authors introduce a small de-identified dataset of approximately 150 patients and the MIMIC III procedure selection dataset, aiming to provide new data and tools for benchmarking across various radiology settings.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and provides a comprehensive overview of the radiology NLP landscape.
- It introduces a new dataset alongside five existing ones, facilitating a more robust evaluation of NLP models.
- The evaluation of multiple models across diverse tasks yields valuable insights into the performance of domain-specific versus general-purpose models.
- The authors provide detailed error analyses and performance metrics across various models and datasets.
- The paper is well-structured, with clear explanations of dataset characteristics and model performance.
- The introduction of a patient-stratified setup and improved figure quality is commendable.

Weaknesses:
- The authors do not provide sufficient details on data curation and quality control for the newly created dataset.
- There is a lack of granularity in dataset statistics, such as anatomical and modality breakdowns, which could inform future evaluations.
- The figures, particularly Figure 2, are of low quality and lack clarity, making it difficult to interpret results.
- The documentation and reproducibility of the codebase are initially sparse, which may hinder user engagement.
- There are inconsistencies in the code, and the citation style disrupts the text flow.
- The rationale for model performance discrepancies is not fully explored.
- The amount of new data generated is minimal and comes with significant access restrictions.
- The framework's utility as a true benchmark is limited due to restricted access to datasets.

### Suggestions for Improvement
We recommend that the authors improve the documentation by providing clearer instructions on compiling the benchmark dataset and including a tutorial for testing models across different datasets. Additionally, we suggest that the authors elaborate on the dataset statistics, including anatomical and modality distributions, to enhance understanding of the benchmark's limitations. It would also be beneficial to include baseline evaluations of models without fine-tuning to assess the impact of pre-training on performance. Furthermore, we recommend improving the clarity of dataset split methodologies to prevent potential data leakage and providing more comprehensive documentation, including clear instructions for compiling the benchmark dataset and tutorials for model testing across different datasets. Addressing class imbalance more explicitly during pre-processing and model evaluation would enhance the robustness of their findings. Lastly, we advise the authors to ensure consistency in code references and citation formatting throughout the manuscript and to further benchmark against existing models, particularly in the context of the claims made about the lack of benchmarks in radiology.