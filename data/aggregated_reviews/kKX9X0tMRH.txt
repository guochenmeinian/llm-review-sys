ID: kKX9X0tMRH
Title: DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DisCo, a method for distilling large language models into multiple small models using co-training for semi-supervised learning. The authors propose different distillation objectives and data transformations to create a committee of student models, employing consistency training loss among students. The empirical results indicate that the student models outperform baseline models of similar size.

### Strengths and Weaknesses
Strengths:  
- The authors provide comprehensive evaluation results, demonstrating robust performance on classification tasks.  
- The innovative approach to knowledge distillation in low-labeled data scenarios is promising and offers valuable insights for researchers.  
- The extensive analysis and ablations included in the paper will be beneficial for future research in knowledge distillation.

Weaknesses:  
- The necessity of the two-student approach is unclear, raising concerns about model selection due to variance among student models.  
- Some technical choices, such as using MSE for measuring probability distribution differences, are not ideal; KL divergence or cross-entropy would be more appropriate.  
- The datasets used may not accurately represent "low resource" scenarios, and some claims, particularly regarding performance, may be overstated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the explanation regarding “SOFT FORM” and “HARD FORM.” Additionally, we suggest providing a more accessible link to the code and data, as the current statement is insufficient. To enhance model selection, consider providing validation set performance for each student model. Furthermore, we advise re-evaluating the use of MSE in favor of KL divergence or cross-entropy for measuring distribution differences. Lastly, it would be beneficial to clarify the datasets' relevance to low-resource scenarios and to moderate the strength of claims made in the paper.