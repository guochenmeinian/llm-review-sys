ID: rG1M3kOVba
Title: FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 5, 5, 6, 5, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents FLuID, a framework designed to mitigate the straggler problem in Federated Learning (FL) environments characterized by heterogeneous devices. The authors propose Invariant Dropout, which identifies neurons that exhibit minimal updates over several training rounds and excludes them from training for specific clients. This method aims to optimize the training process by dynamically adjusting the model sent to clients based on their performance capabilities. The experimental evaluation demonstrates that FLuID can achieve speedups of up to 18% and accuracy improvements of up to 1.4 percentage points over the state-of-the-art Ordered Dropout.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant issue in FL, particularly in cross-device scenarios where clients may have varying performance capabilities.
- The proposed Invariant Dropout technique effectively reduces training time for stragglers while maintaining or improving accuracy.
- The experimental methodology is well-executed, with evaluations conducted on multiple models and datasets.

Weaknesses:
- The experiments primarily involve only five devices, raising concerns about the scalability of the results in larger client scenarios.
- The paper lacks clarity on how client sampling during training rounds may impact the dropout scheme's performance.
- Some design decisions and evaluation setups are inadequately explained, leading to potential misunderstandings regarding the framework's implementation and effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the scalability of their experiments by including evaluations with 50 to 100 clients, and ideally scaling up to 1000-3000 clients to better reflect practical FL environments. Additionally, the authors should clarify how client sampling in each training round affects the invariant dropout scheme's performance. Enhancing the presentation by addressing repeated sentences and adding labels to figures, such as in Fig. 5, would also improve clarity. Lastly, further elaboration on the design decisions, particularly regarding T_target and th, is necessary to strengthen the paper's technical rigor.