ID: QAT5suGpNL
Title: Breaking the Language Barrier: Improving Cross-Lingual Reasoning with Structured Self-Attention
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of multilingual language models (MLMs) regarding their cross-lingual reasoning abilities, particularly in code-switched contexts. The authors propose an attention mechanism called cross-lingual aware self-attention, which incorporates techniques like cross-lingual queries and structured attention dropout to enhance reasoning capabilities. The experimental evaluation on RuleTaker and LeapOfThought benchmarks demonstrates the effectiveness of the proposed method, revealing that while MLMs perform well in monolingual settings, their performance declines in code-switched scenarios.

### Strengths and Weaknesses
Strengths:
- The paper breaks new ground by exploring the reasoning capabilities of MLMs in multilingual settings, particularly through code-switching.
- It is well-motivated, addressing gaps in previous research on logical reasoning and cross-lingual transfer.
- The proposed attention mechanism shows promise in improving performance in code-switched reasoning tasks.
- Thorough experimental evaluations provide empirical evidence supporting the proposed method.

Weaknesses:
- The dual focus on evaluating reasoning abilities and proposing a method can lead to confusion due to compressed content and lack of clarity.
- The research question regarding the transfer of logical reasoning abilities is not clearly addressed, with insufficient empirical analysis.
- The experimental results lack comparison with existing works, making it difficult to assess reliability and reproducibility.
- Clarity on the contributions of each module in the proposed attention mechanism is lacking, and the presentation is non-standard, hindering understanding.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the research question and ensure that the contributions of each module in the cross-lingual aware self-attention mechanism are explicitly analyzed. It would be beneficial to include a formal evaluation protocol with references to existing works to enhance the reliability of the results. Additionally, we suggest that the authors provide more details on the experimental setup and clarify the motivations behind using specific methods, such as BitFit for fine-tuning. Finally, addressing the limitations of using machine-translated datasets and considering comparisons with existing methods like adapters would strengthen the paper.