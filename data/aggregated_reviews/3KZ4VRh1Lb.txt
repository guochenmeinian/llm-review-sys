ID: 3KZ4VRh1Lb
Title: CausalBench: A Comprehensive Benchmark for Evaluating Causal Reasoning Capabilities of Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 6, 8
Original Confidences: 3, 4, 3

Aggregated Review:
### Key Points
This paper presents CausalBench, a comprehensive benchmark for evaluating causal reasoning in LLMs across multiple domains, including text, math, and code. The benchmark addresses limitations of existing datasets by expanding causal reasoning measurement to four dimensions: cause-to-effect, effect-to-cause, and interventions. The authors propose a rigorous dataset creation process that includes manual generation, LLM scaling, and human quality control, ensuring accuracy and reliability. Empirical evaluations of state-of-the-art LLMs provide insights into their strengths and limitations in causal reasoning tasks.

### Strengths and Weaknesses
Strengths:
- Comprehensive Benchmark Coverage: CausalBench encompasses a wide range of causal reasoning tasks, making it a well-rounded tool for assessing LLMs.
- Robust Dataset Creation and Quality Control: The three-step dataset creation process enhances accuracy and diversity.
- Thorough Evaluation Across Models: Empirical evaluations of several LLMs enhance practical relevance and benchmark utility.

Weaknesses:
- Length Exceeds Workshop Limit: The paper's detail makes it exceed the 4-page limit, necessitating condensation of certain sections.
- Limitations of Benchmark: The yes/no ground truth may not effectively measure understanding of causal models, and the benchmark design does not guard against data leakage.
- Insufficient Evaluation Analysis: Performance relations among the four dimensions are understudied, and the evaluation metrics may not be comprehensive.
- Preference Leakage Risk: Bootstrapping via GPT-4 Turbo introduces potential inductive bias, contaminating both experiments and the benchmark.

### Suggestions for Improvement
We recommend that the authors improve the paper's conciseness to meet the workshop's page limit by condensing sections on dataset creation and evaluation results, potentially moving some content to an appendix. Additionally, consider leveraging explanations in the ground truth to enhance the evaluation process, such as using multi-choice question formats. Address the performance discrepancies observed in intervention questions and clarify how to handle scenarios where both the GPT-4 Turbo and the Causal Inference Engine provide incorrect answers. To mitigate bias from the bootstrapping process, we suggest implementing measures to assess potential contamination in evaluations. Finally, we encourage the authors to revisit the benchmark's name due to existing literature and to enhance clarity in results presentation by sorting or highlighting key findings in tables.