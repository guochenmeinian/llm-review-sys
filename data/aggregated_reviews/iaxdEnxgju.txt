ID: iaxdEnxgju
Title: FaLA: Fast Linear Adaptation for Replacing Backbone Models on Edge Devices
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on language model backbone replacement for personalized downstream tasks in a non-stationary on-device scenario, proposing a lightweight tuning method that achieves over 1000x computation reduction in FLOPs for back-propagation while significantly outperforming popular transfer learning methods. The authors introduce Fast Linear Adaption (FaLA) to efficiently personalize models after backbone replacement, emphasizing the importance of separating cloud and edge components for efficiency and privacy in machine learning applications.

### Strengths and Weaknesses
Strengths:
- The topic is important and relevant, addressing real-world implications of model personalization.
- The proposed method is lightweight and shows significant performance improvements over existing methods.
- Extensive experiments and an ablation study validate the approach across various models and datasets.

Weaknesses:
- The writing and presentation quality are inadequate for acceptance; the problem definition lacks clarity and mathematical rigor.
- The paper does not provide sufficient experimental details, such as memory usage and actual running times, which are crucial for on-device applications.
- There is a lack of related work on federated learning, which is pertinent to the study's context.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the problem definition by providing a strict mathematical formulation, including a summary of the training flow and a more detailed description of Figure 1. Additionally, it is essential to report memory usage for each method and include actual running times alongside FLOPs to enhance the experimental evidence. The authors should also incorporate related work on parameter-efficient personalized federated learning and consider adding a baseline from the suggested references. Furthermore, revising the wording to clarify the applicability of the method to generative tasks would strengthen the paper.