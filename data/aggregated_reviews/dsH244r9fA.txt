ID: dsH244r9fA
Title: Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 7, 5, 6, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for off-policy evaluation (OPE) that incorporates human annotations for counterfactual state-action pairs, aiming to improve performance using offline data. The authors propose a new importance sampling scheme that adjusts for the biases introduced by these annotations. They introduce a unique task that combines offline data with counterfactual annotations, which could enhance confidence in new policies in high-stakes domains like healthcare and autonomous driving. Theoretical analyses are provided to discuss the bias and variance of the proposed estimator, and experiments demonstrate its superiority over traditional methods in both synthetic bandit problems and healthcare simulations. The authors acknowledge the challenges of obtaining counterfactual annotations, including cost and potential inaccuracies, but assert that these are manageable compared to the risks of online evaluations. Their contributions extend beyond the C-PDIS estimator to include practical recommendations for its application.

### Strengths and Weaknesses
Strengths:
1. The proposed method addresses a timely and relevant issue in OPE by integrating human annotations, which is a direction worth exploring.
2. The authors propose a unique task that combines offline data with counterfactual annotations, potentially benefiting high-stakes RL applications.
3. The theoretical discussion is thorough, particularly regarding bias and variance, and the paper is well-written and easy to follow.
4. The authors provide a comprehensive theoretical framework and empirical evidence supporting their methods.
5. The discussion on the implications of annotation quality and variability is insightful and relevant.

Weaknesses:
1. The method and baselines are perceived as naive, lacking comparisons with standard OPE methods like direct and doubly robust approaches.
2. Concerns remain regarding the reliability of counterfactual annotations, particularly their potential bias and noise.
3. The reliance on expert annotations is costly and subjective, raising concerns about practical implementation and the generalizability of results to real-world scenarios.
4. The paper lacks real-world validation, which is crucial for assessing the practical impact of the proposed methods.
5. The challenges associated with applying existing weighting methods, such as DICE, to their framework are not fully resolved.
6. The paper does not adequately address the distribution shift between the behavior and evaluation policies, nor does it explore the implications of varying human annotation quality.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the introduction by discussing the concept of "annotation" earlier to facilitate reader understanding. Additionally, the authors should provide a more detailed comparison with standard OPE baselines to better contextualize the benefits of their approach. It would also be beneficial to conduct real-world experiments to validate the proposed method and address the practical challenges of obtaining human annotations. We encourage the authors to improve the robustness of their approach by conducting empirical studies to measure the impact of various factors on annotation quality, including the timing of annotations and the inherent stochasticity of state-actions. Furthermore, we suggest exploring the trade-offs between annotation quality and utility, particularly in selecting which counterfactual annotations to query under fixed budgets. Finally, we suggest that the authors explore the implications of distribution shifts more thoroughly and clarify how to set the weights for interaction data and human annotations to optimize estimation error reduction, as well as incorporating synthetic experiments to systematically investigate the effects of annotator variance on estimator performance.