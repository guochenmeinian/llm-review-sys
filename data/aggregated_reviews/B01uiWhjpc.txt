ID: B01uiWhjpc
Title: Parameterizing Context: Unleashing the Power of Parameter-Efficient Fine-Tuning and In-Context Tuning for Continual Table Semantic Parsing
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 5, 5, 7, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach that combines parameter-efficient fine-tuning (PEFT) and in-context tuning (ICT) to train a continual table semantic parser. The authors propose a teacher-student framework and develop two task streams from WikiSQL and Spider benchmarks. Experimental results demonstrate that their method outperforms existing competitors, achieving state-of-the-art performance across multiple metrics.

### Strengths and Weaknesses
Strengths:
- The integration of ICT and PEFT within a continual framework is novel and effective for the proposed task.
- The framework and method are clearly described and well-structured.
- The paper addresses significant challenges in table semantic parsing, particularly overfitting and catastrophic forgetting.

Weaknesses:
- The authors do not clarify whether the Problem Formulation in Section 2.2 has been previously studied.
- The motivation for the continual table semantic parsing problem is weak, lacking empirical evidence for claims regarding performance drops after training on new tasks.
- The assertion of "invisibility of past demonstrations" in ICT is unrealistic, as few examples can be sourced from public databases or synthetic content.
- Additional baselines should be included, such as training on a reasonable size of public text2sql data to demonstrate the accuracy gap and rationale for continual training.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the novelty of the Problem Formulation in Section 2.2 and provide a stronger motivation for the continual table semantic parsing problem. Additionally, empirical evidence supporting claims about performance drops should be included. The authors should reconsider the "invisibility of past demonstrations" claim, as it appears unrealistic, and include more diverse examples from public databases. Furthermore, we suggest adding additional baselines to strengthen the evaluation of the proposed method.