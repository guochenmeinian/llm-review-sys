ID: vMpmabFTFw
Title: Learning to Compose Representations of Different Encoder Layers towards Improving Compositional Generalization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called "COMPOSITION" aimed at enhancing compositional generalization in transformer models by disentangling the information passed between the encoder and decoder. The authors introduce a composed layer that integrates outputs from various encoder layers, allowing the decoder to utilize a richer representation of both semantic and syntactic information. The effectiveness of "COMPOSITION" is empirically demonstrated through experiments on two challenging tasks: CoGnition (machine translation) and CFQ (semantic parsing), achieving state-of-the-art results on CoGnition. The paper is well-structured, providing detailed analyses and case studies to support its claims.

### Strengths and Weaknesses
Strengths:
- The proposed method is simple, requiring minimal modifications to existing transformer architectures, and is broadly applicable to various seq2seq models.
- The paper is well-motivated, clearly articulating the significance of addressing the entanglement of semantic and syntactic information.
- Experimental results indicate strong performance improvements, particularly in machine translation tasks, supported by comprehensive analyses.

Weaknesses:
- The novelty of the approach is limited, with significant overlap with prior work not adequately addressed.
- The paper lacks analysis of the additional computational costs associated with the composed layer compared to vanilla transformer models.
- Some claims are insufficiently justified, particularly regarding the general applicability of the method and the effectiveness of combining semantic and syntactic information.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the discussion regarding the entanglement of source keys and values, particularly in relation to the PCA visualizations presented. Additionally, please provide a thorough analysis of the extra computational costs, training time, and inference time introduced by the composed layer. It would also be beneficial to include results demonstrating the effectiveness of "COMPOSITION" on larger pre-trained models, such as T5 or BART. Furthermore, we suggest addressing the methodological details regarding the training approach used in the experiments and clarifying the differences between $H_i^{SA}$ and $H_i^{FF}$. Lastly, updating the "Related Works" section with recent publications on compositional generalization would enhance the paper's context and relevance.