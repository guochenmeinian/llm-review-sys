ID: ODeHH5FBwx
Title: M2C: Towards Automatic Multimodal Manga Complement
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel task, Multimodal Manga Complement (M2C), aimed at enhancing manga comprehension by integrating visual and textual features to address missing comic text content. The authors propose a new dataset, the M2C benchmark, in both English and French, and establish a baseline method called FVP-M2. The methodology incorporates local and global visual features during feature encoding and employs visual prompt generation for improved dialogue complementation. The experiments conducted demonstrate the effectiveness of the proposed approach.

### Strengths and Weaknesses
Strengths:
- The task of multimodal manga complementation is engaging and valuable, addressing significant challenges in understanding comics.
- The proposed dataset serves as a promising resource for future research in this domain.
- The model's architecture logically integrates visual features and prompts, utilizing co-attention for sentence completion.

Weaknesses:
- The loss function's treatment of English and French languages lacks clarity, as it appears to consider both languages while training on only one.
- The role of Manga Chain-of-Thought (CoT) is insufficiently explained, leading to confusion regarding its integration within the model.
- The multimodal baselines do not include state-of-the-art methods like CoCa and BLIP-2, which limits the evaluation's robustness.
- There is a lack of statistical evidence supporting the real-world applicability of the proposed solution.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the loss function by explicitly detailing how both English and French languages are utilized during training. Additionally, a comprehensive explanation of the Manga Chain-of-Thought (CoT) and its role within the FVP-M2 model should be provided to enhance understanding. To strengthen the evaluation, we suggest including comparisons with current state-of-the-art methods such as CoCa and BLIP-2. Furthermore, we encourage the authors to present statistical evidence regarding the prevalence of missing content in comics to substantiate the practical relevance of their work. An ablation study focusing on the necessity of the image modality and the fine-grained visual prompt generation would also be beneficial for understanding the model's components.