ID: S75ccNdOYG
Title: Posterior Sampling for Competitive RL: Function Approximation and Partial Observation
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into posterior sampling algorithms for competitive reinforcement learning (RL) within zero-sum Markov games (MGs) using general function approximations. The authors propose model-based self-play and adversarial posterior sampling methods to learn Nash equilibrium in partially observable states, providing low regret bounds applicable to various tractable zero-sum MG classes. Additionally, the paper discusses complexity measures for function approximation and theoretical results indicating sublinear convergence.

### Strengths and Weaknesses
Strengths:
1. The paper addresses an important problem and is well-structured, written in clear mathematical language.
2. It contributes significantly to the Multiagent+PORL community by proposing a generic posterior sampling algorithm with rigorous analyses.
3. The introduction of two new generalized eluder coefficients as complexity measures is noteworthy, and the technical contributions appear solid.

Weaknesses:
1. The theoretical results may be challenging for non-specialists to assess, particularly regarding their practical implications for solving zero-sum Markov games.
2. The algorithms seem abstract, making it difficult to calculate the indicated distributions in experimental tasks; an example would strengthen the paper.
3. The existence of a Nash equilibrium in games with a fixed number of steps $H$ is questionable, and clarification or references are needed.
4. The supplementary material is excessively long, potentially obscuring important details.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical results to enhance their practical relevance, possibly by including examples that demonstrate the feasibility of the proposed algorithms. Additionally, we suggest combining Algorithms 1 and 2 for conciseness and adding a comparative discussion between self-play and adversarial results to clarify their implications. Furthermore, addressing the Nash equilibrium assumption in finite-step games and revising the supplementary material for brevity would be beneficial. Lastly, including intuitive explanations of technical details in the main text could help readers better follow the contributions.