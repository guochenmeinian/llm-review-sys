ID: 3iDxHRQfVy
Title: Had enough of experts? Elicitation and evaluation of Bayesian priors from large language models
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 4, 8
Original Confidences: 3, 4

Aggregated Review:
### Key Points
This paper investigates the use of LLMs as experts to elicit prior distributions for Bayesian models. The authors address an important problem in prior elicitation, particularly relevant for small datasets, and explore the potential utility of LLMs in this context. The methodology includes a chain-of-thought based approach, although the exact prompt methodology lacks clarity. The study presents a systematic comparison of LLMs with human experts and evaluates the elicited priors against actual expert-elicited ones.

### Strengths and Weaknesses
Strengths:
- The paper presents an innovative approach to leveraging LLMs for eliciting expert-informed priors, addressing a significant gap in statistical analysis.
- The methodology is well-constructed, featuring a clear framework for eliciting priors and a systematic comparison with human experts.
- The results are clearly presented, and the comparison with historical weather data effectively tests the LLMs' predictive capabilities.

Weaknesses:
- The prompt methodology is unclear, particularly regarding the chain-of-thought prompting and the relationship between the expert prompt initialization and task specification modules.
- There is insufficient discussion on potential biases introduced by LLM training data and their impact on generalizability.
- Some claims, such as the novelty of direct elicitation from domain expert LLMs, are inaccurate, and there are inconsistencies in terminology and citations throughout the manuscript.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the prompt methodology, particularly by explicitly detailing the chain-of-thought prompting and the relationship between the modules mentioned. Additionally, a more thorough discussion on the biases introduced by LLM training data and their implications for generalizability should be included. We also suggest adding analyses that explore the impact of different model architectures or training datasets on the elicited priors. Finally, we encourage the authors to refine the manuscript for clarity and conciseness, addressing the identified inconsistencies and citation needs throughout the text.