ID: 2LctgfN6Ty
Title: Distributional Preference Alignment of LLMs via Optimal Transport
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 8, 6, 4, -1, -1, -1, -1
Original Confidences: 3, 3, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel technique for preference alignment called Alignment via Optimal Transport (AOT), which operates under both paired and unpaired alignment settings. The authors introduce a new perspective on preference alignment through stochastic dominance, ensuring that the reward distribution of positive samples is stochastically dominant over negative samples. They explain DPO as a specific case of this approach and demonstrate that convex relaxation corresponds to minimizing a one-dimensional optimal transport problem. The SInkhorn-Knopp algorithm is employed to enhance the differentiability of the objective. Experimental results indicate that AOT achieves state-of-the-art performance in the 7B family of models, evaluated using Open LLM Benchmarks and AlpacaEval.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- It proposes an original approach based on stochastic dominance and optimal transport, with a novel connection to optimal transport theory.
- The method is applicable to both unpaired and paired alignment settings.
- Extensive experiments are conducted across various datasets, demonstrating competitive performance against existing methods.
- AOT achieves state-of-the-art results on the AlpacaEval benchmark.

Weaknesses:
- The evaluation utilizes Llama3-70B-Instruct instead of GPT4, which is not standard, although it is claimed to yield similar results.
- There is insufficient motivation for the practical desirability of the FSD condition over the conditional dominance condition of DPO.
- Empirical results do not show clear improvements over prior approaches, raising questions about AOT's advantages in specific settings.
- The theoretical insights provided do not clearly indicate when AOT is likely to excel.

### Suggestions for Improvement
We recommend that the authors improve the motivation for the FSD condition, clarifying its practical advantages over DPO's conditional dominance. Additionally, we suggest providing a more thorough comparison with KTO, particularly regarding the uniqueness of solutions satisfying the FSD condition and how ties are broken. It would be beneficial to explore the impact of hyperparameters used in the Sinkhorn algorithm on results. Furthermore, we encourage the authors to investigate why the unpaired setting yields better results than the paired setting and to address the empirical performance discrepancies observed in their experiments. Lastly, we suggest testing smaller values of $\beta$ in their experiments to understand its effect on performance better.