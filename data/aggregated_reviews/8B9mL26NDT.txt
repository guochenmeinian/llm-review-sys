ID: 8B9mL26NDT
Title: On the Impact of Cross-Domain Data on German Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a German dataset comprising texts from five domains and describes the development of German language models trained on this dataset. The authors propose three models based on the DeBERTa architecture: $GeBERTa_{large}^Q$, trained with quality-focused data; $GeBERTa_{large}^{V}$, trained on variety-enhanced data; and $GeBERTa_{xlarge}^{V}$, a larger model also trained on variety-enhanced data. The study claims that training on cross-domain datasets improves performance compared to models trained solely on web crawl data.

### Strengths and Weaknesses
Strengths:  
- The paper provides a valuable German dataset and pre-trained models that are not available in previous studies, which will aid future research.  
- The models achieve strong results on various NLP tasks and demonstrate the utility of cross-domain datasets for model training.  
- The authors plan to release their code, contributing to the community.

Weaknesses:  
- The contribution is perceived as limited due to the incremental nature of the work and the lack of novelty in the findings regarding cross-domain training.  
- Some claims, such as the significant reduction in computational resources through diverse datasets, are considered misleading and lack sufficient support.  
- The improvements in performance may not always be statistically significant, raising questions about the robustness of the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding the novelty and significance of their findings. Specifically, they should provide more robust statistical analysis to support claims of performance improvements and the impact of dataset diversity on computational efficiency. Additionally, addressing the concerns about the limited contributions beyond the cross-domain corpus would strengthen the paper's overall impact.