ID: bRyduWAAVT
Title: Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 7, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to self-attention mechanisms in transformers through the introduction of Primal-Attention, which utilizes asymmetric Kernel Singular Value Decomposition (KSVD) for low-rank approximation of the attention matrix. The authors formulate a primal-dual representation of self-attention aimed at maximizing projection variances in attention outputs, demonstrating that Primal-Attention can achieve a zero-value objective. Empirical results indicate that Primal-Attention performs effectively while maintaining low time complexity.

### Strengths and Weaknesses
Strengths:
1. The formulation of self-attention as an optimization problem based on KSVD is innovative and has potential for significant impact.
2. The derivation of self-attention from KSVD is both elegant and correct.
3. The paper is well-written, making it accessible and enjoyable to read.

Weaknesses:
1. The clarity of methodology exposition is insufficient, particularly regarding the abrupt introduction of primal optimization in self-attention with KSVD.
2. The omission of a citation for the Johnson–Lindenstrauss lemma in Algorithm 1 is inappropriate.
3. The experimental results show limited improvements compared to baseline methods, with only the LRA benchmark providing efficiency analyses.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology, particularly by elaborating on the connection between KSVD and self-attention, especially regarding the primal optimization function in Equation 6. Additionally, we suggest including a citation for the Johnson–Lindenstrauss lemma in Algorithm 1. Conducting more large-scale experiments, such as on full ImageNet or WikiText-103, would enhance the empirical analysis. Furthermore, providing a detailed explanation of the significance of the left singular vectors in the projection scores and conducting efficiency analyses across more benchmarks would strengthen the paper's contributions.