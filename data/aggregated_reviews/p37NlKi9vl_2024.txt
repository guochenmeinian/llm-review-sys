ID: p37NlKi9vl
Title: Exact, Tractable Gauss-Newton Optimization in Deep Reversible Architectures Reveal Poor Generalization
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of Gauss-Newton (GN) optimization in deep neural networks, specifically focusing on its computational efficiency and generalization compared to stochastic gradient descent (SGD). The authors propose a neural architecture utilizing reversible networks with linear bottleneck layers, allowing for analytical derivation of updates via the Moore-Penrose pseudo-inverse. Their empirical analysis indicates that while GN achieves rapid convergence in full batch settings, it tends to overfit in stochastic settings, leading to poorer performance on datasets like MNIST and CIFAR-10 compared to SGD and Adam. 

### Strengths and Weaknesses
Strengths:  
The paper provides a clear and concise presentation of analytically computable GN updates for reversible neural networks. The theoretical framework for efficient Jacobian pseudoinverses is well articulated, and the experiments yield intriguing preliminary results that pave the way for future research into GN training.

Weaknesses:  
The claim of introducing exact updates for a realistic application of neural networks is questionable, as the constructed model's realism is unclear. The drastic use of inverse bottlenecks raises concerns about the transferability of results to more conventional architectures. Additionally, the experiments lack depth, particularly regarding batch size dependence and performance comparisons in settings where SGD is effective.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the impact of the inverse bottleneck layer and the number of random weights on the results. It would be beneficial to elaborate on the implications of using cross-entropy loss in the experimental section, especially concerning the additional Hessian term. We also suggest expanding the experiments to include a more comprehensive exploration of batch size effects and the performance of GN training in scenarios where SGD performs comparably to Adam. Finally, incorporating intuitions about the form of the pseudoinverse of the Jacobian into the main text would enhance understanding.