ID: fx6aSBMu6z
Title: HC-GAE: The Hierarchical Cluster-based Graph Auto-Encoder for Graph Representation Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 3, 4, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel Hierarchical Cluster-based Graph Auto-encoder (HC-GAE) aimed at improving unsupervised graph representation learning. The authors propose that HC-GAE mitigates the over-smoothing problem by employing hard node assignment for encoding and soft node assignment for decoding, facilitating hierarchical compression and expansion of graphs. The method claims to enhance performance across node and graph classification tasks.

### Strengths and Weaknesses
Strengths:
1. The motivation for the research is clear and easily comprehensible.
2. The proposed method appears simple yet effective.
3. The paper is well-structured and easy to follow, with promising numerical results.

Weaknesses:
1. The novelty of the work is limited, as both the motivations and the method are not sufficiently original, being combinations of existing techniques (e.g., DiffPool, VGAE).
2. The authors should conduct experiments on link prediction tasks, as this is crucial for validating the method's multi-task capabilities.
3. The loss function's reliance on multiple KL-divergence computations may lead to significant time costs; an analysis of time and space complexity is encouraged.
4. The baseline comparisons are outdated; the authors should include more recent methods, such as GraphMAE2 and various contrastive learning approaches.
5. Experimental results on additional datasets are needed to strengthen the findings.
6. There is a lack of sensitivity testing on hyper-parameters.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the paper by clearly distinguishing HC-GAE from existing models, particularly in terms of qualitative and quantitative comparisons. Additionally, the authors should validate their claims regarding the reduction of over-smoothing through empirical results that assess performance against varying numbers of layers. It is crucial to release the code at the review stage to ensure reproducibility. Furthermore, we suggest refining the writing in Section 3 for clarity and addressing minor grammatical issues. Finally, the authors should provide a more thorough analysis of the proposed loss function and its implications for node classification.