ID: iVYInarGXg
Title: On the Identifiability and Interpretability of Gaussian Process Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 29
Original Ratings: 5, 3, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive examination of additive mixtures of Matern kernels in Gaussian processes (GPs), focusing on their identifiability, interpretability, and performance in both finite sample and asymptotic scenarios. The authors demonstrate that for single-output cases, a mixture of Matern kernels is equivalent to the least smooth component (Theorem 2). They leverage results from Bachoc et al. (2022) to analyze microergodic parameters in separable multi-output kernels and discuss the implications of smoothness in mixture kernels. Theoretical findings are supported by experiments on simulated and real-world datasets, including the Mauna Loa dataset, revealing that including kernels with varying smoothness does not necessarily enhance prediction accuracy, particularly in fixed-domain analyses.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents significant contributions to GP modeling, particularly in addressing additive mixtures of kernels.
- The main results, including Theorem 2 and discussions on identifiability, are intriguing and may have important implications for GP practitioners.
- The authors provide a comprehensive analysis of kernel performance across various training sample sizes, with empirical results that align with theoretical expectations.
- Additional experiments conducted to address reviewer concerns enhance the rigor of the work.

Weaknesses:
- The proof of Theorem 1 lacks support for the claim regarding the smoothness of the kernel being determined by the least smooth component, failing to demonstrate that the kernel is not $(d + 1)$-times MSD.
- The proof of Theorem 2 contains potential inaccuracies regarding the asymptotic behavior of function ratios.
- Corollary 1's conclusion that there is no advantage in including additional components is disputed, as it overlooks practical significance in finite data predictions.
- The focus on fixed-domain settings may overlook complexities relevant to increasing-domain scenarios, particularly in time series analysis.
- The notion of equivalence used in the paper may be too weak for practical machine learning contexts, and concerns regarding parameter identifiability in finite samples remain inadequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the proof of Theorem 1 by demonstrating that the kernel is not $(d + 1)$-times MSD and clarify the proof of Theorem 2 to ensure its robustness. Additionally, we suggest reconsidering the conclusion of Corollary 1, providing a more nuanced discussion of the implications of including additional components in mixtures. We encourage the authors to explicitly acknowledge the contributions of Bachoc et al. (2022) in the main body of the paper and to improve the clarity of their stance on the practical implications of their theoretical results. Furthermore, we recommend conducting further experiments on datasets where a Matern mixture kernel is conventionally a good choice, such as time series with both noisy short-length-scale and smooth long-length-scale components. Lastly, enhancing the discussion on the identifiability of parameters and the implications of distinguishing between equivalent processes would improve the manuscript's clarity and accessibility.