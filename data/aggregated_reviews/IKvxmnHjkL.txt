ID: IKvxmnHjkL
Title: Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a KL privacy bound for training fully connected networks, analyzing how network width, depth, and initialization affect privacy guarantees. The authors show that the KL privacy bound can improve with increased depth, particularly under specific parameter initializations like LeCun's. The analysis employs KL divergence as a privacy metric and connects network structure to privacy bounds.

### Strengths and Weaknesses
Strengths:
1. The investigation addresses a significant problem regarding the effects of overparameterization on privacy in deep learning models.
2. The results are coherent and align well with the research topic, providing valuable insights into the privacy-utility trade-off.
3. The paper is well-written, facilitating comprehension and a smooth flow of ideas.

Weaknesses:
1. The paper lacks numerical experiments to validate the claims, particularly regarding the tightness of the proposed privacy bounds.
2. There are several notational inconsistencies and typos that hinder clarity, such as the use of the same notation for different parameters and unclear definitions.
3. Some assumptions, like the scaling of input samples, appear unrealistic, and the analysis primarily focuses on ReLU networks, raising questions about generalizability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of notations and definitions, particularly in Table 1 and the captions of figures. It would be beneficial to include numerical experiments to validate the theoretical findings and demonstrate the evolution of empirical KL privacy leakage with varying depths and widths. Additionally, we suggest addressing the limitations of the analysis tools used, particularly regarding the assumptions made about initialization and scaling. Finally, providing a more intuitive explanation for the significance of the initialization factors could enhance the paper's impact.