ID: P5hYS77k10
Title: Quantifying the redundancy between prosody and text
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper quantifies prosodic information using large language models (LLMs) and estimates mutual information (MI) to assess prosodic features such as prominence, energy, duration, pause, and pitch. The authors propose that MI increases between prosody and text when considering preceding context, highlighting the significance of prosodic features. The study extracts prosodic features from a spoken English corpus and presents a pipeline for comparing mutual information between prosodic and word embedding features, contributing to the understanding of how prosodic information relates to text.

### Strengths and Weaknesses
Strengths:  
- The paper connects state-of-the-art NLP research to model prosody and presents a clear and novel methodology applicable to various languages.  
- It is well-written, with limitations cogently discussed, and offers a reproducible framework for future studies on prosody and other linguistic features.  
- The mathematical modeling is clear, and the study is thorough, providing strong support for its claims.

Weaknesses:  
- Further justification for using LLMs is necessary, particularly regarding their interpretability.  
- The study is limited to the English language and one domain (audiobooks), which restricts the generalizability of the results.  
- A simpler baseline for comparison could strengthen the paper, and the use of "F0" instead of "pitch" is recommended for clarity.

### Suggestions for Improvement
We recommend that the authors improve the justification for using LLMs, addressing concerns about their interpretability. Additionally, consider expanding the study to include multiple languages and domains to enhance the generalizability of the findings. We suggest providing a simpler baseline for comparison and using "F0" in place of "pitch" for precision. Lastly, please address the questions raised regarding tokenization, model fine-tuning, and the treatment of pauses and punctuation in the analysis.