ID: YWsPN0EMZr
Title: Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 5, 7, 8, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 1, 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a connection between the convex formulation of gated ReLU networks and multiple kernel learning (MKL), highlighting that the finite-width ReLU network is superior to the neural tangent kernel (NTK) limit. The authors propose an iterative reweighted method to solve the MKL problem, aiming to achieve optimal kernel performance. The theoretical analysis of predictive error is also provided. Additionally, the study focuses on the optimization of regularized neural networks through a novel kernel perspective, specifically emphasizing a convex MKL approach. The authors clarify that the NTK is a kernel method approximating conventional neural network training, derived from the infinite width limit, while their MKL approach does not require this assumption and demonstrates equivalence to conventional neural networks on the training set. Empirical results indicate that the convex MKL formulation outperforms the NTK formulation across multiple datasets.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a novel framework that enhances understanding of neural network optimization beyond the infinite width setting.  
- It offers a comprehensive overview of various perspectives on ReLU networks, including convex programs and group lasso, which could facilitate further research.  
- The theoretical results are sound, and the numerical experiments effectively illustrate the proposed concepts.  
- The paper provides a fresh perspective on neural network optimization by introducing a kernel characterization that does not rely on the infinite width assumption, with empirical results showing superior test performance compared to the NTK formulation.

Weaknesses:  
- The contribution may be limited as many results for ReLU networks are already established, raising questions about the novelty of the findings.  
- The paper lacks explicit comparisons with prior works on convex formulations of ReLU networks, which could clarify its advancements.  
- There is insufficient discussion on the generalization error and the implications of the proposed methods, particularly regarding the uniqueness of solutions outside the training set.  
- The paper lacks a comprehensive theoretical analysis and extensive experiments regarding the generalization properties of the proposed approach.  
- There is ambiguity in the experimental setup, particularly regarding the comparison between the NTK and the authors' kernel characterization.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the differences between their work and prior studies on convex formulations of ReLU networks. A more explicit discussion on how the gated ReLU networks differ from traditional ReLU networks, particularly in terms of optimization uniqueness, would strengthen the paper. Additionally, we suggest including comparisons with trained networks in the experiments to provide a fair assessment of performance against the optimal MKL kernel. Furthermore, we recommend improving the theoretical framework by providing a more detailed analysis of the generalization properties of their approach. Clarifying the experimental setup to facilitate a more conclusive comparison between the NTK and the proposed kernel characterization is also essential. Consulting the framework introduced in [2] may provide valuable insights for designing new experiments that compare trained neural networks with various widths to Gaussian processes using the NNGP kernel. Finally, addressing the potential limitations of extending the convex formulation to other activation functions would enhance the robustness of the findings.