ID: 7wJhlDMNH7
Title: Can We Edit Multimodal Large Language Models?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MMEdit, a benchmark for evaluating the editing of multimodal large language models (LLMs). The authors propose innovative metrics for assessing reliability, generality, and locality in editing tasks, specifically focusing on Image Captioning and Visual Question-Answering (VQA). Comprehensive experiments are conducted to analyze the impact of various editing techniques on two backbone multimodal LLMs, revealing that while editing is feasible, its effectiveness remains limited.

### Strengths and Weaknesses
Strengths:
- The introduction of a novel task for editing multimodal LLMs extends existing methodologies for single-modal LLMs.
- A detailed description of the MMEdit benchmark and its evaluation metrics can facilitate future research.
- The manuscript effectively highlights the need for editing in multimodal LLMs and presents valuable insights for the NLP community.

Weaknesses:
- The dataset construction section lacks sufficient detail for understanding data creation.
- The insights from the editing techniques and observations are not clearly articulated.
- Missing analysis on performance degradation after editing multiple instances and a lack of implementation details hinder reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the dataset construction section by providing more detailed explanations. Additionally, including an analysis of performance degradation after editing multiple instances would enhance the manuscript. An appendix with implementation details, including wall clock time and GPU consumption, would aid reproducibility. Furthermore, a clearer discussion of the editing techniques and their comparative performance, particularly regarding the locate-then-edit approach, would provide deeper insights. Lastly, addressing the clarity of results in tables and figures, as well as correcting any inconsistencies in the equations, would improve the overall presentation.