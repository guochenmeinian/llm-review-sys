ID: 4VWnC5unAV
Title: The High Line: Exact Risk and Learning Rate Curves of Stochastic Adaptive Learning Rate Algorithms
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 7, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a continuation of research on deterministic limits of stochastic gradient descent (SGD), focusing on deriving solutions to a coupled integro-differential equation in complex space. The authors propose a novel approach for least squares that incorporates adaptive step sizes and non-identity covariance in datasets, addressing the limitations of fixed step sizes in realistic scenarios. The study explores the differences between various learning rate schedules, including Line Search, Polyak step-size, and AdaGrad Norm, revealing phase transitions in learning dynamics based on covariance structures. Additionally, the paper provides an analytical analysis of the dynamics of a class of optimization algorithms with adaptive learning rates applied to linear models with Gaussian data, establishing convergence of the optimization trajectory to a deterministic ODE as the dimension \(d\) approaches infinity. The theoretical results are validated through numerical experiments on synthetic data, and the authors effectively characterize the dynamics of risk and learning rate for the AdaGrad-Norm algorithm across various covariance matrix spectra.

### Strengths and Weaknesses
Strengths:
- The motivation is clearly articulated, engaging the reader's interest.
- Proposition 4.3 demonstrates that starting from a null initialization can still yield a non-zero learning rate under certain conditions, which is a significant insight.
- The overarching goal of predicting SGD behavior is crucial for understanding model dynamics.
- The techniques employed are of independent interest and contribute to a systematic exploration of SGD dynamics.
- The convergence of Polyak's step-size to the inverse of the average eigenvalue validates its broader applicability beyond convex optimization.
- The main result regarding convergence to a deterministic continuous-time ODE is valid for general loss functions, providing a precise characterization of the optimization trajectory.
- The authors effectively characterize the dynamics of risk and learning rate for the AdaGrad-Norm algorithm across various covariance matrix spectra.

Weaknesses:
- The paper raises several questions that need addressing, particularly regarding the practical implications of certain assumptions.
- Proposition 4.4's assumption about scaling at each site lacks clarity on its realistic applicability.
- The authors claim to have included all necessary assumptions, yet a clearer statement of limitations would enhance the paper's credibility.
- Some equations may be missing terms that could affect their symmetry and dynamics.
- The scaling assumptions may not align with existing literature, raising concerns about their validity and implications.
- The manuscript suffers from poor writing, with excessive focus on the introduction and setup, leaving insufficient space for results and discussion.
- Figures 2, 3, and 4 are not referenced in the text, complicating their interpretation.
- The paper does not clearly delineate which results are new versus those derived from previous work, particularly in relation to [14].
- Results for non-quadratic losses are underexplored, with relevant findings relegated to appendices, making it difficult to extract significant conclusions.

### Suggestions for Improvement
We recommend that the authors improve clarity by explicitly stating the limitations of their work, including where the theory may not extend naturally. Additionally, we suggest revising Proposition 4.4 to clarify the realistic scenarios in which the scaling assumption applies. The authors should also ensure that all equations are complete and accurately reflect the dynamics being studied. Furthermore, we encourage a more concise presentation of the general framework, possibly relegating some details to the Appendix to enhance readability. We recommend that the authors improve the paper's structure by reducing the length of the introduction and notation sections, allowing for a more comprehensive presentation of results and discussions. It would be beneficial to reference all figures in the text to enhance clarity. Additionally, we suggest that the authors explicitly outline the distinctions between their results and those in [14], particularly regarding Theorem 2.1 and assumptions 1-6. To strengthen the analysis, we encourage the authors to include more detailed results for non-quadratic losses in the main body of the paper rather than relegating them to appendices.