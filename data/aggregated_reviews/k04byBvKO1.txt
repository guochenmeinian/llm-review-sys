ID: k04byBvKO1
Title: NAT4AT: Using Non-Autoregressive Translation Makes Autoregressive Translation Faster and Better
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents NAT4AT, a neural machine translation model that integrates the strengths of Autoregressive Translation (AT) and Non-Autoregressive Translation (NAT) to enhance translation speed without compromising quality. The authors analyze the feasibility of this combination through experiments and detail the NAT4AT method, confirming its advantages via comparative experiments with related work and varying parameter settings.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written, easy to follow, and presents a sound, effective method.  
- Comprehensive experimental design and thorough analysis at the sentence level demonstrate the generality and effectiveness of NAT4AT on major WMT benchmarks.  
- Extensive experimental results validate the method's superiority over strong baselines, with useful ablation studies.

Weaknesses:  
- The references are not standardized, and some key references are missing, impacting reproducibility.  
- The method lacks a strong theoretical foundation, and further sensitivity analysis regarding hyperparameters like the threshold and window width is needed.  
- There is insufficient discussion on the comparison with Speculative Decoding and the impact of multilingual Large Language Models (LLMs).

### Suggestions for Improvement
We recommend that the authors improve the standardization of references throughout the paper and ensure all datasets used are properly cited. Additionally, please clarify the relationship between AT and NAT models in the Introduction and Related Work sections, and include comparisons with pure AT models in the experimental results. We suggest incorporating absolute speed metrics alongside the Speedup metric in Tables 2 and 6, and exploring additional evaluation metrics beyond BLEU. Furthermore, we encourage the authors to include a discussion of LLMs in the Experiment section and to integrate the results of Table 6 into Table 2. Lastly, a deeper complexity analysis and justification for the choice of hyperparameters would enhance the theoretical foundation of the work.