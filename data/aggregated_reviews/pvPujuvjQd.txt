ID: pvPujuvjQd
Title: Most Neural Networks Are Almost Learnable
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 4, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a polynomial-time approximation scheme (PTAS) for learning random Xavier networks of constant depth up to a fixed additive error $\epsilon$. The authors assert that a sufficiently wide neural network, randomly sampled according to Xavier initialization, can be learned in polynomial time with respect to the network size. They achieve this by approximating activation functions with low-degree Hermite polynomials, allowing for the use of standard SGD on these networks. The results indicate a significant milestone in learning theory for deep networks, particularly as they extend learnability results beyond shallow architectures.

### Strengths and Weaknesses
Strengths:
- The work addresses a fundamental problem in learning deeper-than-two-layers neural networks, which is highly relevant to the effectiveness of deep learning.
- The presentation is generally clear, and the results are technically impressive, particularly the distribution-free nature of the PTAS and the practical applicability of SGD.

Weaknesses:
- The paper's technical complexity may hinder comprehension, with a need for more intuitive explanations and a clearer presentation of proofs.
- The validity of the results is contingent upon the network being sufficiently wide, which raises concerns about practical applicability.
- The mathematical formulation lacks sufficient explanation, and comparisons to prior work on approximation methods are not clearly articulated.

### Suggestions for Improvement
We recommend that the authors improve the presentation by providing more intuitive explanations and moving detailed proofs to the appendix. Additionally, it would be beneficial to include a clear comparison to previous approximation attempts in the literature to highlight the novelty of their approach. The authors should also address the limitations regarding the minimum width of the network in the abstract and introduction, and consider conducting experiments to validate their theoretical results. Lastly, clarifying the implications of the width requirement and exploring ways to relax this assumption would enhance the paper's impact.