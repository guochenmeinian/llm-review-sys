ID: 51HQpkQy3t
Title: DiTFastAttn: Attention Compression for Diffusion Transformer Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 25
Original Ratings: 5, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel post-training model compression method aimed at accelerating Diffusion Transformers (DiTs) for image and video generation tasks. The authors identify spatial, temporal, and conditional redundancies in attention blocks and propose techniques to address these inefficiencies, enabling more efficient inference with minimal quality loss. The methods are evaluated across multiple DiT models, demonstrating significant reductions in computational cost and latency while maintaining generation quality. Additionally, the authors provide a comprehensive evaluation of the DiT model, comparing it with ADM and LDM, and highlighting differences in diffusion timesteps, evaluation software, and cfg_scale settings. Results reveal significant reductions in attention FLOPs while maintaining satisfactory generation quality, and the authors explore the impact of CFG dropping in final steps and the applicability of HiDiffusion techniques.

### Strengths and Weaknesses
Strengths:
- The proposed methods are interesting, effective, and well-justified empirically.
- The identification and analysis of redundancies in attention mechanisms are thorough and well-founded.
- Extensive experiments provide strong evidence of the methods' effectiveness and generalizability.
- The authors conducted thorough ablation studies and provided detailed results that align with the DiT paper.
- They effectively demonstrated the method's efficiency in reducing attention FLOPs without significant degradation in performance.
- The inclusion of additional evaluation metrics, such as LPIPS, enhances the robustness of their findings.

Weaknesses:
- The evaluation is limited by the use of only 5K images for generation quality assessment, whereas the standard is 50K images.
- There is a lack of clarity regarding the adopted compression strategy in each layer and timestep; a visual illustration of the compression plan would be beneficial.
- The evaluation of the method's components and design choices is insufficient, particularly regarding ablations and the compression plan algorithm.
- The paper does not adequately compare related methods, such as flashattention and KV cache, which weakens its contribution.
- The reliance on a low number of calibration images raises concerns about the generality of FID and IS as evaluation metrics.
- The computational overhead associated with certain metrics, such as LPIPS, limits the ability to provide comprehensive results within the rebuttal period.
- The presentation of results, particularly in Figure 5, is hard to parse and could be improved for clarity.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by using a larger sample size, ideally 50K images, to assess generation quality more robustly. Additionally, we suggest providing a clear illustration of the compression strategy employed at each layer and timestep. The authors should include more detailed comparisons with existing acceleration methods like flashattention and KV cache to strengthen their contribution. We also advise conducting ablation studies to evaluate the impact of different components of the proposed method. Furthermore, we recommend improving the number of calibration images used in their experiments to enhance the generalizability of their results. Including the LPIPS results and further qualitative examples in the revised version for completeness would be beneficial. It would also be advantageous to explore the implications of CFG dropping more thoroughly and consider automating the search for optimal CFG dropping points. Lastly, addressing the concerns regarding the use of FLOPs as a primary metric by focusing on overall inference latency would strengthen the paper's conclusions. Revising Figure 5 for better readability and clarity, possibly by separating metrics into distinct graphs and using dual axes where appropriate, would enhance the presentation.