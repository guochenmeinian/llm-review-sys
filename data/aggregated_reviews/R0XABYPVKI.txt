ID: R0XABYPVKI
Title: Knowledge Corpus Error in Question Answering
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper examines the phenomenon of "Knowledge Corpus Error," which occurs when the knowledge corpus for retrieval excludes relevant content. The authors propose that generated passages can outperform retrieved ones in question answering (QA) tasks due to this error. They benchmark the concept by paraphrasing golden passages across four QA tasks and evaluating two large language models (LLMs), demonstrating significant improvements in exact match (EM) and accuracy in three out of four tasks.

### Strengths and Weaknesses
Strengths:
- The concept of "Knowledge Corpus Error" is novel and provides insights into current language model errors, potentially guiding future research.
- Empirical evidence shows that knowledge corpus errors exist in some open-domain QA settings, with clear mathematical formulations.
- The experiments are well-detailed for reproducibility, and the writing style is clear and focused.

Weaknesses:
- The main experiment lacks clarity on how well LLM-generated paraphrases represent external knowledge, failing to validate their effectiveness.
- The contribution is limited, as the results merely confirm known phenomena without deeper analysis or discussion on the differences between paraphrased and gold documents.
- The experiments are conducted only once without varying hyperparameters, and the reasons for chosen hyperparameters are not provided.
- There is insufficient analysis of the reasons behind accuracy degradation in specific tasks, and unsupported claims weaken the overall argument.

### Suggestions for Improvement
We recommend that the authors improve the validation of generated paraphrases to demonstrate their distinction from corpus knowledge. Additionally, a more detailed analysis of the reasons behind knowledge corpus errors should be included, particularly how paraphrased documents differ from gold documents. The authors should also clarify the rationale for hyperparameter choices and conduct multiple experiments to strengthen their findings. Finally, we suggest providing examples for specific tasks like QASC and exploring the consistency of the phenomenon across various LMs, including those not instruction-tuned or trained with reinforcement learning from human feedback (RLHF).