ID: a1wf2N967T
Title: Graph-based Unsupervised Disentangled Representation Learning via Multimodal Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework that integrates a β-VAE with multimodal large language models (MLLMs) within a bidirectional weighted graph structure, DisGraph, to enhance disentangled representation learning. The authors propose a β-VAE module for extracting initial factors and utilize MLLMs to uncover latent correlations and update weighted edges. Experimental results indicate that the proposed framework achieves superior performance in disentanglement and reconstruction across various datasets.

### Strengths and Weaknesses
Strengths:
1. The integration of β-VAE with MLLMs within a graph-based framework is a novel approach that addresses significant gaps in existing methods.
2. The paper is well-motivated, with clear illustrations of the limitations of existing works and the advantages of the proposed framework.
3. The methodology is rigorous, with thorough evaluations demonstrating the model's effectiveness.

Weaknesses:
1. The paper lacks significant innovation at the neural network and algorithmic levels, with insufficient justification for the optimality of the proposed combination compared to other models.
2. Technical details, such as the derivation of the objective function and the training mechanism for the graph learner, are inadequately explained.
3. The reliance on MLLMs introduces potential biases, and the paper does not sufficiently address the implications of this dependence.
4. The writing and organization could be clearer, with some technical terms used without adequate definitions.

### Suggestions for Improvement
We recommend that the authors improve the justification for the optimality of their framework by providing a comparative analysis with other non-trivial combinations, including diffusion models, and conducting empirical evaluations to support their claims. Additionally, we suggest that the authors clarify the derivation of their objective function, including the assumptions made, and provide a more detailed explanation of the training process for the graph learner. It would also be beneficial to enhance the clarity of the writing and organization throughout the paper, ensuring that all technical terms are well-defined. Finally, addressing the potential biases introduced by the reliance on MLLMs would strengthen the paper's contributions.