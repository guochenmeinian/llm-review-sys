ID: iNUKoLU8xb
Title: Your contrastive learning problem is secretly a distribution alignment problem
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 4, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel perspective on contrastive learning (CL) by framing it as a distribution alignment problem using entropic optimal transport (OT). The authors propose the Generalized Contrastive Alignment (GCA) framework, which connects various self-supervised learning (SSL) paradigms to optimal transport. They leverage unbalanced optimal transport theory to introduce an unbalanced CL loss that addresses outliers. The paper establishes theoretical insights and empirical evidence demonstrating that GCA improves generalization and robustness in representation learning. Empirical evaluations on CIFAR-10 and CIFAR-100 datasets, as well as the introduction of new loss functions and multi-step variants for existing CL methods, show how these can enhance performance in both clean and noisy settings.

### Strengths and Weaknesses
Strengths:
- The paper offers a fresh view on contrastive learning by linking it to optimal transport, providing a solid theoretical foundation for understanding and improving CL methods.
- The rigorous theoretical analysis supports the convergence of proposed methods, and the GCA framework allows for customization of representation spaces.
- The methodology is sound, grounded in optimal transport theory and its entropic regularization.
- The writing is clear and well-structured, indicating a thorough literature review.
- The empirical validation across multiple datasets demonstrates the effectiveness of GCA in image classification and domain generalization tasks.

Weaknesses:
- The specific criteria for convergence are not clearly defined, raising concerns about the algorithm's efficiency in large-scale applications.
- There is significant overlap in ideas and content with Shi et al. (2023), particularly regarding Algorithm 1 and the proposed loss function, raising questions about the novelty of the contribution.
- The empirical evaluation lacks depth, as it does not include Shi et al. (2023) as a baseline and fails to assess performance on larger datasets like ImageNet.
- The computational burden of using proximal operators for T steps and auto-differentiation for parameter Î¸ is not sufficiently addressed.
- The authors do not clarify how they tuned the entropic regularization parameter $\varepsilon$, which is crucial for the stability of the transport plan. A visualization of the transport plan quality, akin to Figure 3 in Shi et al. (2023), is recommended.
- The experiments lack a detailed runtime analysis, and the computational resources necessary for implementing the algorithm are not clearly outlined.
- The paper's structure could be improved for accessibility, as some reviewers found it challenging to follow.

### Suggestions for Improvement
We recommend that the authors improve the clarity of convergence criteria and provide a detailed runtime analysis to address computational efficiency concerns. Additionally, we suggest including a discussion on the computational overhead introduced by the GCA framework, particularly for multi-step variants. To enhance the novelty of their contribution, the authors should clearly distinguish their work from Shi et al. (2023), particularly in the context of Algorithm 1 and the proposed loss function. Including Shi et al. (2023) as a baseline in the empirical evaluation and expanding the analysis to larger datasets like ImageNet would enhance the robustness of their findings. Furthermore, we recommend that the authors provide a detailed explanation of how they tuned the entropic regularization parameter $\varepsilon$ and include visualizations of the transport plan quality. Finally, restructuring the paper to enhance clarity and accessibility for a broader audience is essential. A dedicated "Limitations" section discussing potential computational overhead and hyperparameter tuning challenges would strengthen the paper's comprehensiveness.