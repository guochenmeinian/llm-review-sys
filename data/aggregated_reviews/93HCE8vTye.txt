ID: 93HCE8vTye
Title: Transformers need glasses! Information over-squashing in language tasks
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of decoder-only Transformers, focusing on their limitations in information propagation, specifically identifying "representational collapse" and "over-squashing." These phenomena lead to significant information loss, particularly in tasks requiring precise token handling, such as counting and copying. The authors combine theoretical analysis with empirical evidence from contemporary language models, including Gemini 1.5 and Gemma 7B, to substantiate their claims.

### Strengths and Weaknesses
Strengths:
- The paper provides a robust theoretical framework for understanding the limitations of decoder-only Transformers, with well-formulated concepts of representational collapse and over-squashing.
- The clarity of narrative is maintained alongside strong theoretical analysis, and the presentation quality is excellent.
- Empirical evidence supports the theoretical claims, demonstrating real-world relevance, particularly regarding the impact of low-precision floating-point formats.
- Comprehensive details about the experiments are provided.

Weaknesses:
- The experiments focus on specific artificial tasks, lacking analysis of real-world texts; including statistics on token representation differences in standard corpora would be beneficial.
- The authors propose only one artificial solution for sequence splitting, which may not be practical.
- The theoretical analysis relies on simplifying assumptions, such as treating attention weights as independent of input, which could be explored more thoroughly.

### Suggestions for Improvement
We recommend that the authors improve the empirical analysis by including statistics on token representation differences in standard text corpora to investigate representational collapse in natural texts. Additionally, we suggest exploring more practical solutions to address representational collapse in real-world scenarios, rather than relying solely on the proposed artificial solution. It would also be beneficial to examine the impact of different positional embeddings on the observed phenomena, as well as to clarify how the theoretical results might generalize to non-causal language modeling. Lastly, addressing minor presentation issues, such as the font size in figures, would enhance readability.