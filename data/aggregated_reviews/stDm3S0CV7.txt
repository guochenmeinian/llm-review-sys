ID: stDm3S0CV7
Title: The Simplicity Bias in Multi-Task RNNs: Shared Attractors, Reuse of Dynamics, and Geometric Representation
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 6, 4, 7, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies how recurrent neural networks (RNNs) form shared attractors and reuse dynamics in a multitask setting, revealing a "simplicity bias" where RNNs avoid creating new attractors unless necessary. The authors explore how task similarities can be translated into representation similarities and relate their findings to continual learning and modularity. The study introduces a systematic framework to minimize interference from input and output correlations, demonstrating that RNNs develop attractors sequentially during training. The research suggests potential applications, such as using the geometry of shared attractors to infer unknown tasks.

### Strengths and Weaknesses
Strengths:  
- The paper poses an interesting scientific question regarding attractor formation in RNNs and presents a novel investigation of the simplicity bias.  
- The experiments are well-controlled, and the results are clear and convincing.  
- The logical progression from single to multiple tasks aligns with real-world conditions, and the standardized input/output structure enhances the validity of the findings.  
- The insights about simplicity bias are novel and warrant further exploration.  

Weaknesses:  
- The scope is limited to toy synthetic datasets, raising questions about the scalability of conclusions to more complex datasets or architectures like transformers.  
- The paper lacks a clear outline of limitations, contributions, and relevant literature, which would help position the work within the broader context.  
- Figures are poorly annotated, making them difficult to interpret, particularly Figures 3 and 5.  
- The impact of the simplicity bias on practical applications and improvements to RNNs is insufficiently discussed.  

### Suggestions for Improvement
We recommend that the authors improve the clarity and accessibility of the paper by enhancing figure annotations and ensuring all figures are properly referenced in the text. Additionally, we suggest providing a clearer outline of the paper's limitations and contributions, including a discussion on the computational costs associated with the proposed approach. To strengthen the biological plausibility of the conclusions, we encourage the authors to explore connections to known neurophysiological phenomena. Finally, addressing the scalability of the findings to more complex tasks and architectures would significantly enhance the paper's impact.