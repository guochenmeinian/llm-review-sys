ID: nfC1OA6NeE
Title: SDEs for Adaptive Methods: The Role of Noise
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a derivation of stochastic differential equations (SDEs) for adaptive gradient methods, specifically SignSGD, AdamW, and RMSpropW. The authors analyze the role of gradient noise and highlight significant differences between SignSGD and SGD. They generalize the SDE analysis for AdamW and RMSpropW, revealing key properties of weight decay. The integration of derived SDEs with Euler-Maruyama demonstrates that the SDEs accurately track the behavior of their respective optimizers across various modern neural networks.

### Strengths and Weaknesses
Strengths:
- The theoretical results are novel, marking the first SDE analysis for SignSGD with quantitatively accurate descriptions.
- The analysis reports interesting novel properties regarding gradient noise and convergence.
- The proofs appear complete and reasonable.
- The experiments validate the theory, showing that SDEs align well with empirical results across different optimizers and models.

Weaknesses:
- The reported theoretical results do not directly lead to theory-inspired improvements, raising questions about the work's significance.
- Some important references are missing, such as [1], which analyzes Adam using SDEs. A review of recent papers on weight decay properties would be beneficial.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including significant references, particularly [1], to enhance the theoretical context. Additionally, we encourage the authors to clarify how L2 regularization and decoupled weight decay behave differently in their results. Addressing the questions regarding the notation of \(W_t\), the extension of Lemma 3.13 to convex settings, and the learning rate used in experiments would strengthen the paper. Finally, providing an intuitive explanation for the asymptotic expected loss of SignSGD and the loss spike phenomenon would enhance the reader's understanding.