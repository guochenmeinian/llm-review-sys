ID: SVBR6xBaMl
Title: Language Models Meet World Models: Embodied Experiences Enhance Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 5, 3, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to enhance the embodied reasoning capabilities of pre-trained language models by incorporating data and tasks from an embodied environment. The authors propose a fine-tuning strategy that combines Elastic Weight Consolidation (EWC) and Low-Rank Adaptation (LoRA) to achieve stable fine-tuning. The experiments demonstrate that the proposed method can effectively fine-tune models like GPT-J, outperforming larger models such as ChatGPT in certain scenarios. The paper also introduces a comprehensive reasoning task set based on collected data, evaluating various aspects of reasoning performance. Furthermore, the authors evaluate the EWC method, demonstrating its superiority over direct fine-tuning in out-of-domain tasks, and clarify that their evaluation tasks, such as plan generation and Housework QA, are intentionally designed to differ from training tasks to assess the model's generality. They emphasize that EWC helps maintain the generality of language model (LM) capabilities, enhancing performance on these out-of-domain tasks. The authors argue that their approach to developing a general-purpose model enriched with embodied experiences is distinct from prior works that focus on task-specific models or modules.

### Strengths and Weaknesses
Strengths:
- The authors design two data collection methods in the embodied environment, addressing both goal-specific and general cases.
- A comprehensive reasoning task set is developed to evaluate reasoning performance across different dimensions, including plan generation and object tracking.
- The EWC-LoRA parameter updating strategy is efficient in terms of time and memory, showing promising results in fine-tuning.
- The authors provide a clear rationale for the choice of out-of-domain evaluation tasks, reinforcing the generalization capabilities of their model.
- The distinction between their approach and previous studies on embodied agents is well-articulated, highlighting the novelty of their work.

Weaknesses:
- The paper lacks experiments justifying the necessity of EWC, as its purpose overlaps with that of LoRA. The performance of EWC-LoRA is lower than LoRA in most cases, except for a minor improvement in perplexity.
- There is limited diversity in the evaluated models, with only two GPT-based LLMs used, and no comparison with other methods for enhancing LMs with embodied knowledge.
- The evaluation benchmarks are limited, primarily focusing on bAbI, while other embodied benchmarks could provide a more realistic assessment.
- The clarity of the experimental results could be improved, particularly in presenting data in figures.
- The comparison to prior works could be more explicitly detailed to strengthen the argument for the uniqueness of their approach.
- Some evaluation settings may require further clarification to ensure comprehensibility for readers unfamiliar with the context.

### Suggestions for Improvement
We recommend that the authors improve the justification for including EWC by conducting experiments that explore its additional benefits. It would be beneficial to ensure that the answers in the negation Housework QA are genuinely irrelevant and to clarify how confusion terms are introduced in the evaluation. We suggest expanding the evaluation to include a broader range of recent LLMs and additional embodied benchmarks, such as ALFWorld, to provide a clearer comparison with existing methods. Additionally, we recommend enhancing the clarity of their comparison to prior works on embodied agents by providing more explicit details on how their approach differs. Lastly, enhancing the clarity of experimental results, including adding numerical data to figures, would strengthen the presentation of findings and ensure that all readers can fully grasp the context and significance of their findings.