ID: ViFTWelHVZ
Title: Efficient Activation Function Optimization through Surrogate Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 8, 7, 3, 4, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for optimizing activation functions in neural networks, aiming to enhance performance in machine learning tasks. The authors introduce benchmark datasets created by training convolutional, residual, and vision transformer architectures with systematically generated activation functions. They develop a surrogate-based optimization method, AQuaSurF, which utilizes the Fisher information matrix's spectrum and activation function output distribution to predict performance. The method demonstrates significant improvements in accuracy on CIFAR-100 and ImageNet tasks.

### Strengths and Weaknesses
Strengths:
1. The paper introduces an innovative approach to enhancing activation functions, surpassing existing techniques in efficiency and effectiveness.
2. It is exceptionally well-written, with thorough experiments and clear presentation.
3. The benchmark datasets provide a valuable foundation for future research on activation function properties and their impact on performance.

Weaknesses:
1. The motivation for using "Activation Function Outputs" as a feature is not clearly explained.
2. The generalizability of the discovered activation functions is limited, as they vary across tasks and networks, suggesting potential overfitting.
3. The complexity of the search space raises concerns about interpretability and the potential for overfitting to specific tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation and definition of "Activation Function Outputs" in Section 3. Additionally, to strengthen the generalizability of the results, the authors should extend the list of baseline activations in Table 1 to include widely used functions such as GELU and LeakyReLU. Furthermore, we suggest that the authors apply their method to new network architectures and more challenging datasets to better evaluate its effectiveness. Lastly, refining the structure of the paper to enhance readability and comparing the method's efficiency against existing approaches in both benchmark search and new tasks would significantly improve the manuscript.