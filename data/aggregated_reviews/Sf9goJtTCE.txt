ID: Sf9goJtTCE
Title: Sampling from Gaussian Process Posteriors using Stochastic Gradient Descent
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 8, 9, 7, 8, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for sampling from Gaussian process (GP) posteriors using stochastic gradient descent (SGD), which avoids the cubic complexity of traditional methods. The authors reformulate the problem as a quadratic optimization task, enabling efficient gradient approximation through random kernel features. The theoretical framework demonstrates that the algorithm provides reliable variance estimates across various data densities and outperforms conjugate gradients (CG) and sparse variational Gaussian processes (SVGP) in specific scenarios. The paper includes comprehensive experiments that validate the method's effectiveness in large-scale settings.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with clear derivations and informative figures that enhance understanding.
- The use of SGD for atypical objectives is innovative, supported by thorough theoretical and empirical analyses.
- Limitations of the approach are transparently discussed, particularly regarding approximation quality in different data density regions.
- Diverse experiments across GP regression and Bayesian optimization (BO) illustrate the method's applicability.

Weaknesses:
- The omission of subset of data (SoD) methods in both the related work and experimental sections is a significant oversight, as these methods are competitive in similar contexts.
- Comparisons with SVGP are deemed unfair due to inadequate tuning of competing methods and a limited number of inducing points.
- The analysis lacks a formal characterization of SGD's convergence in extrapolation regions, where slow convergence may pose issues in practical applications.

### Suggestions for Improvement
We recommend that the authors improve the discussion of SoD methods in the related work and include empirical comparisons with these methods in the experimental section. Additionally, we suggest conducting a parameter study of SVGP with a stronger optimization algorithm to provide a fairer baseline comparison. It would also be beneficial to formally characterize the convergence behavior of SGD in different regions and address the implications of slow convergence in extrapolation regions. Lastly, clarifying the significance of the fixed hyperparameter setting and its relevance to broader applications would strengthen the paper's impact.