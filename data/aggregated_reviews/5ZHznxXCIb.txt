ID: 5ZHznxXCIb
Title: Context-faithful Prompting for Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a detailed analysis of instruction-based and opinion-based formulation prompts aimed at contextual question answering. The authors propose simple prompts that can be utilized across various language models, demonstrating that these prompts encourage models to leverage provided context rather than relying solely on training data. Additionally, the paper addresses the faithfulness issue of Large Language Models (LLMs) in context-specific NLP tasks, proposing two methods—opinion-based prompts and counterfactual demonstrations—that enhance contextual faithfulness.

### Strengths and Weaknesses
Strengths:  
- The proposed methods are simple and can be easily formulated, allowing models to rely more on context without extensive retraining, making them cost-effective.  
- The paper investigates a critical issue of LLMs, focusing on faithfulness and demonstrating promising performance across three NLP tasks.  
- The motivations and explanations provided in the paper are clear and well-articulated.  

Weaknesses:  
- The work lacks sufficient baseline comparisons, particularly with other publicly available models like LLaMa or T5, which could validate the effectiveness of the prompting strategies.  
- There is a need for more ablation studies on the in-context learning demonstration number, as existing literature suggests that a large number of demonstrations may not yield optimal results.  
- The evaluation is limited to three datasets, and a broader assessment across diverse NLP tasks would enhance the generalizability of the proposed methods.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their findings by incorporating additional baselines, including models such as ChatGPT or Alpaca, to validate the effectiveness of the proposed tailored prompt methods. Furthermore, we suggest conducting more ablation studies on the in-context learning demonstration number to explore its impact on performance, as indicated by existing research. Lastly, expanding the evaluation to include a wider variety of NLP tasks would strengthen the generalizability of the conclusions drawn from the study.