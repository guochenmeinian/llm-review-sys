ID: 46Jr4sgTWa
Title: Recurrent neural networks: vanishing and exploding gradients are not the end of the story
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 5, 5, 7, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the vanishing and exploding gradient phenomenon in recurrent neural networks (RNNs), addressing whether resolving these issues guarantees well-behaved loss landscapes. The authors reveal that the element-wise recurrence design pattern and careful parameterizations are crucial in mitigating hidden state scaling issues. They introduce the "curse of memory," which persists even when gradients are controlled. The paper examines various recurrent architectures, including LSTMs and state-space models (SSMs), and discusses optimization improvements. Additionally, the authors focus on the implications of LRU (Least Recently Used) architectures in neural networks, proposing that while the empirical results are primarily LRU-centric, their findings also apply to other architectures. The reviewers express concerns about the organization of the contributions and the clarity of assumptions made regarding simplified model versions.

### Strengths and Weaknesses
Strengths:
- The analysis of the curse of memory is novel and connects theoretical insights with empirical validation, providing valuable contributions to the machine learning community.
- The paper is well-structured and presents clear explanations of how current architectures address the discussed issues.
- The authors have engaged actively with reviewer feedback, leading to improved clarity and organization.
- The work is recognized as valuable to the community, with some reviewers increasing their scores based on the authors' responses.

Weaknesses:
- The assumption of wide-sense stationarity is strong and lacks justification, particularly regarding its applicability to datasets like Cifar10.
- The argument surrounding input normalization is vague and does not adequately address the scaling issues.
- The discussion on adaptive learning rates lacks direct training curves, which are necessary to illustrate the advantages of adaptive optimizers.
- The paper is perceived as overly focused on LRU architectures, which may limit its broader applicability.
- The assumptions regarding the wide-sense stationary property and its necessity for deriving key results are questioned, with suggestions that it may not be essential.
- The presentation suffers from numerous typos and unclear sections, which detract from the overall clarity and coherence of the paper.

### Suggestions for Improvement
We recommend that the authors improve the justification for the assumption of wide-sense stationarity, acknowledging it as a limitation and exploring its implications. Additionally, we suggest clarifying the argument on input normalization and providing a more rigorous discussion of its effects on scaling. To enhance the section on adaptive learning rates, we recommend including training curves that compare results using SGD and adaptive methods like AdamW. Furthermore, we advise the authors to thoroughly proofread the manuscript to correct typos and improve clarity, particularly in sections that are currently confusing. We also recommend that the authors improve the organization of the paper by integrating the contribution statement from the rebuttal into the manuscript. Additionally, we suggest that the authors either make the paper more LRU-centric by emphasizing LRU throughout or reduce its LRU focus by discussing alternative normalizations and reparameterizations, as well as including a wider range of architectures in the experiments. Lastly, we encourage the authors to provide a more in-depth justification for the assumptions related to simplified model versions and clarify how their contributions differ from existing LRU literature.