ID: qBAED3u1XZ
Title: VLATTACK: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 5, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VLATTACK, a new adversarial attack framework designed to evaluate the robustness of large vision-language models (VLMs). The authors propose a two-step adversarial attack approach: first, attacking each modality (image and text) independently, followed by a cross-search attack strategy to iteratively query the fine-tuned model for perturbation updates. The framework is tested on multiple pre-trained models fine-tuned for various downstream vision tasks.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written, with a clear presentation of the proposed framework.
2. VLATTACK combines both white-box gradient and black-box query attacks, presenting an interesting and practical implementation.
3. The evaluation includes multiple pre-trained models, with the BSA component outperforming state-of-the-art image-space adversarial baselines.

Weaknesses:
1. The black-box setting is questionable, as the first stage of attack utilizes white-box access to widely-used foundations like ViLT, potentially sharing mutual information with the victim model. More elaboration on transferability is needed.
2. Section 4.2 claims to operate under a black-box setting, yet it describes a process for updating perturbations that requires gradient access. Clarity on how gradients are obtained is necessary.
3. In Section 4.1, the use of clean images instead of generated adversaries for text optimization may hinder the success rate of adversarial textual optimization. The authors should clarify the rationale behind avoiding unnecessary modifications.
4. The effectiveness of the attack relies on a sufficient number of queries, and an evaluation of query costs (e.g., time of inference/API call) should be included.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the black-box setting by elaborating on the source of transferability and how gradients are obtained during the perturbation update process. Additionally, consider using generated adversaries for text optimization to potentially enhance the success rate of adversarial textual optimization. It would also be beneficial to evaluate the query costs associated with generating effective adversaries and discuss the attack's effectiveness relative to the number of queries. Finally, a systematic validation of the semantic similarity between generated adversarial examples and original inputs, as well as an analysis of perturbation degrees, should be included to strengthen the evaluation.