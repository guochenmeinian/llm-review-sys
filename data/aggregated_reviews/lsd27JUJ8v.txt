ID: lsd27JUJ8v
Title: Exploiting the Replay Memory Before Exploring the Environment: Enhancing Reinforcement Learning Through Empirical MDP Iteration
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 7, 5, 7, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Empirical MDP Iteration (EMIT) framework, which enhances online reinforcement learning by regularizing algorithms with a sequence of empirical MDPs derived from replay memory data. EMIT focuses on in-sample bootstrapping to ensure stable and unique convergence of Q-functions, leading to monotonic policy improvement. The authors demonstrate that EMIT can be integrated with existing RL algorithms like DQN and TD3, significantly reducing estimation errors and improving performance across various benchmarks, including Atari and MuJoCo.

### Strengths and Weaknesses
Strengths:
- Comprehensive experimental evaluation on both discrete (Atari) and continuous (MuJoCo) environments, validating EMIT's effectiveness.
- Improved stability and performance of existing RL algorithms, with clear reductions in estimation errors and notable policy improvements.
- The paper is generally well-written and easy to follow, with good analysis and numerical results.

Weaknesses:
- Limited novelty in core ideas, as similar concepts have been explored in offline and distributional reinforcement learning. The authors should clarify how EMIT advances beyond existing methods like Implicit Q-Learning and In-Sample Actor-Critic.
- Scalability concerns due to the need to maintain and update two Q-functions, which may double computational and memory requirements.
- Lack of in-depth analysis regarding hyperparameter sensitivity, particularly for the regularization parameter and exploration bonus.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by explicitly delineating how EMIT differs from existing methods like Implicit Q-Learning and In-Sample Actor-Critic. Additionally, a more detailed discussion on the scalability of EMIT in complex environments is necessary. The authors should also provide insights on the sensitivity of EMIT to its hyperparameters, particularly the regularization parameter $\alpha$ and exploration bonus $\delta(s, a)$. Furthermore, enhancing the explanation of how updates occur in continuous state spaces and improving the readability of figures, especially Figure 2(a) and Figure 4, would benefit the paper. Lastly, including a comparison with basic regularization techniques and advanced exploration methods would provide valuable context for the reader.