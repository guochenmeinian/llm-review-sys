ID: DLYkSal8oC
Title: BATON: Enhancing Batch-wise Inference Efficiency for Large Language Models via Dynamic Re-batching
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents BATON, a batch-wise LLM inference scheme that dynamically adjusts processing batches to achieve near-zero idle computations without additional resource consumption. The authors claim that BATON improves throughput by up to 1.75 times compared to the state-of-the-art solution, Orca. The major contributions include a tensor shaping and embedding strategy for seamless query-level batch inference, a tensor alignment policy based on P&D decoupling, and experiments demonstrating significant throughput improvements with representative LLMs.

### Strengths and Weaknesses
Strengths:
1. The proposed BATON framework is novel and efficiently addresses idle computations in batch-wise inference.
2. The paper is well-written and organized, facilitating reader comprehension.
3. BATON is designed to be a generic solution applicable to existing LLMs using the KV-Cache policy.

Weaknesses:
1. The experimental validation is limited, focusing only on one baseline (Orca) from 2022, which does not sufficiently demonstrate BATON's advancement.
2. The related work section lacks coverage of recent literature, particularly from 2023 and 2024.
3. There is insufficient evaluation of BATON's performance across different LLMs, and the impact on resource utilization and other metrics like inference precision is not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the experimental validation by including comparisons with more recent state-of-the-art solutions, such as vLLM, Distserve, and Splitwise. Additionally, the related work section should be updated to reflect the latest developments in batching inference for LLMs. To enhance the comprehensiveness of the study, we suggest evaluating BATON's performance on a wider range of LLMs and providing detailed analyses of resource utilization, including CPU and CUDA memory metrics. Furthermore, the authors should clarify the implementation details of BATON and explore its applicability in large-scale distributed systems, addressing unique challenges such as network latency and load balancing.