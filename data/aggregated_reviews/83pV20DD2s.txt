ID: 83pV20DD2s
Title: Learning from Pattern Completion: Self-supervised Controllable Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a self-supervised controllable generation (SCG) framework that utilizes a modular autoencoder with an equivariant constraint to extract distinct visual patterns from images. The authors propose a two-stage training process: the first stage focuses on learning modular representations, while the second stage employs these representations for conditioning a ControlNet. The authors claim that SCG demonstrates impressive zero-shot capabilities for generating various types of images, achieving competitive results compared to ControlNet in terms of SSIM, PSNR, and CLIP scores.

### Strengths and Weaknesses
Strengths:
- The SCG framework is a fully unsupervised method that does not rely on supervised tools, allowing for flexible conditioning inputs.
- The modular autoencoder effectively learns functional specializations, enhancing controllable image generation.
- The authors provide their code, facilitating further research and validation.

Weaknesses:
- The paper suffers from clarity issues, with confusing sections and numerous typographical errors that hinder understanding.
- The evaluation is limited to ControlNet, raising concerns about the fairness of comparisons, particularly since both training and evaluation use the MS-COCO dataset.
- The experimental validation lacks depth, failing to address variability in conditioning and the performance of SCG against other methods, such as Principal Component Analysis (PCA).

### Suggestions for Improvement
We recommend that the authors improve the clarity and rigor of the manuscript by addressing typographical errors and refining confusing sections, particularly in the introduction and experimental validation. Additionally, we suggest conducting further experiments to explore the impact of using different conditioning representations beyond the Canny edge detector and to clarify the contribution of the symmetry and equivariance loss terms. It would also be beneficial to include ablation studies on the number of modules and types of equivariant constraints to better understand their effects. Finally, we encourage the authors to discuss the limitations of their approach more comprehensively, particularly regarding the lack of semantic diversity in generated images.