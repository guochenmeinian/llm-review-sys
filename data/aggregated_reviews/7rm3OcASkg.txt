ID: 7rm3OcASkg
Title: DP-Mix: Mixup-based Data Augmentation for Differentially Private Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 5, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two data augmentation techniques, MIXUP-SELFAUG and MIXUP-DIFFUSION, aimed at enhancing differentially private learning. The authors explore the limitations of traditional multi-sample data augmentation methods like mixup, which do not perform well under differential privacy constraints. MIXUP-SELFAUG applies mixup to self-augmented data, resulting in smoother training, while MIXUP-DIFFUSION incorporates synthetic data from a pre-trained diffusion model. The authors also discuss the effectiveness of the mixup technique in machine learning, particularly regarding label mixing, proposing that mixup should blend both labels and data to enhance generalization. The paper addresses concerns about performance discrepancies between their results and previous works, particularly regarding the SelfAug method, and clarifies the meaning of "Baseline" in their tables. The proposed techniques achieve state-of-the-art classification performance across various datasets.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to read.
- It effectively employs DP-SGD for differentially private learning.
- The proposed techniques enhance the performance of mixup-based data augmentation.
- The authors provide a clear rationale for the necessity of label mixing in mixup, emphasizing its role in improving generalization.
- They conduct additional experiments comparing various augmentation techniques, demonstrating that mixup-SelfAug outperforms traditional methods.
- The authors engage with recent literature and provide empirical results supporting their claims.
- The rebuttal effectively addresses reviewers' concerns, leading to increased scores from multiple reviewers.

Weaknesses:
- The absence of a theoretical analysis for MIXUP-SELFAUG is notable; a theoretical guarantee for differential privacy would strengthen the paper.
- The rationale behind the performance improvement of MIXUP-SELFAUG, which does not mix labels, is not adequately addressed.
- The justification for using a pre-trained diffusion model is insufficient, raising concerns about potential knowledge distillation effects and its implications for differential privacy.
- The experimental details lack clarity, making reproducibility difficult, and the comparison with prior work is not sufficiently rigorous.
- There is a lack of clarity regarding the term "Baseline" in Table 1, which could confuse readers.
- The reported performance of SelfAug differs from previous works, raising questions about the validity of comparisons.
- Some reviewers express skepticism about the theoretical underpinnings of the mixup method without label mixing.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of MIXUP-SELFAUG to provide a formal guarantee for differential privacy. Additionally, the authors should clarify the effectiveness of their approach in the context of label mixing, potentially through intuition or mathematical formulation. It would be beneficial to justify the incorporation of the diffusion model more thoroughly, addressing concerns about its training and implications for differential privacy. Furthermore, we encourage the authors to enhance the experimental section by providing detailed descriptions of the augmentations used, computing resources, and runtime comparisons with prior work to facilitate reproducibility. We also recommend improving the clarity of the term "Baseline" in Table 1 to enhance reader understanding. Lastly, the authors should further investigate and clarify the performance discrepancies between their results and those of previous works, particularly regarding SelfAug, and provide a more robust theoretical explanation for the effectiveness of mixing the same label, as this remains a point of contention among reviewers. Addressing minor grammatical and typesetting issues would also improve the overall clarity of the paper.