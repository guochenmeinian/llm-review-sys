ID: RRUVZygUtr
Title: Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 3, 8, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on distribution shifts in dynamic graph data, focusing on out-of-distribution generalization in the spectral domain. The authors propose a model called Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts, which utilizes Fourier transforms to analyze ego-graph trajectory spectrums. The model aims to filter graph dynamics through a disentangled spectrum mask and encourages reliance on invariant patterns for generalization. Experimental results demonstrate the model's effectiveness in node classification and link prediction tasks. Additionally, the authors introduce a dynamic GNN that processes sequences of graph snapshots, contrasting with baseline models that utilize static GNNs. They argue that their comparisons are valid, as dynamic graph baselines are included, and assert that spectral invariant learning contributes to performance gains, as demonstrated through ablation studies.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a significant and novel problem regarding distribution shifts in dynamic graphs, particularly in the spectral domain.
2. The theoretical analyses and intuitive examples provided strengthen the motivations for the proposed method.
3. The authors provide a clear rationale for the validity of their model comparisons, including the inclusion of dynamic graph baselines.
4. The spectral component of the model is noted as innovative and inspiring, contributing to the novelty of the work.
5. Experimental results validate the model's superiority in handling distribution shifts.

Weaknesses:
1. The novelty of the proposed model is limited, as it closely resembles existing frameworks, particularly the NeurIPS22 work DIDA.
2. Concerns remain regarding the model's ability to learn complex invariant frequency patterns from limited data, as datasets contain only a dozen snapshots.
3. The presentation lacks clarity in certain sections, particularly in the model description and the calculation flow in Figure 2.
4. The empirical evaluation is insufficient, with a lack of comparisons to common baselines that do not involve temporal modeling, such as GCN and GraphSAGE.
5. The theoretical results are perceived as trivial, and reproducibility is questioned due to insufficient detail in the methodology.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the model section, particularly in Figure 2, by highlighting the order of each module. Additionally, we suggest including F1 scores as metrics for node classification comparisons. The authors should clarify the notations used in equations, specifically '||' in Eq. 4 and $\odot$ in Eq. 5. Furthermore, we encourage a more thorough discussion on the assumptions regarding the disentanglement of invariant and variant patterns, including their applicability to real-world data. We also recommend that the authors improve the clarity of how the model learns invariant frequency patterns, providing more concrete examples and analyses to strengthen their claims. Lastly, we advise that the authors expand their empirical comparisons to include more relevant baselines, particularly those that do not utilize temporal information, and ensure all relevant citations are properly acknowledged to reinforce the originality of their work.