ID: wWT51dSyBj
Title: Gradient-based Gradual Pruning for Language-Specific Multilingual Neural Machine Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a gradient-based gradual pruning approach for multilingual machine translation (MNMT) aimed at mitigating parameter interference and enhancing translation quality. The authors propose extracting language-pair specific subnetworks and analyze the correlation between sub-network overlap and language similarity. The study includes comparisons of different sub-network extraction techniques and demonstrates the effectiveness of gradual gradient pruning through experiments on IWSLT and WMT datasets.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- It provides a thorough analysis of the impact of various pruning methods on model performance.
- The reported improvements in BLEU scores are promising.

Weaknesses:
- The novelty of the work is limited, as it closely resembles previous research (Lin et al., 2021).
- The proposed method's complexity may hinder its applicability in future research.
- The method is not sufficiently compared to other existing approaches, and the results for individual language pairs are not detailed.

### Suggestions for Improvement
We recommend that the authors improve the comparison of their method against other relevant techniques, such as LSL-NAS + dense pre-training, X-MOD, and Adapter-based approaches, to better contextualize their contributions. Additionally, please include detailed results for every language pair on IWSLT and WMT, rather than just average scores. We suggest incorporating more advanced evaluation metrics, such as chrF and COMET scores, to complement BLEU. Furthermore, we encourage the authors to analyze the implications of their approach on zero-shot performance and provide a thorough assessment of disk space requirements and inference speed across different methods. Lastly, consider adding a bilingual model baseline to evaluate the effectiveness of the proposed method in addressing parameter interference.