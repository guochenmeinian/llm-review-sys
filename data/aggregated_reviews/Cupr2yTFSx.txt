ID: Cupr2yTFSx
Title: Online POMDP Planning with Anytime Deterministic Guarantees
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 5, 6, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 1, 3, 3, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents theoretical bounds for POMDP planning algorithms and derives a new planning algorithm applicable to POMDPs with discrete action and observation spaces, where the transition and observation distributions are known. The authors compute the difference in value between a simplified POMDP and the original problem for a given policy, extending this derivation to the optimal policy under limited observations. The theoretical results improve existing tree-based POMDP planners, evaluated on benchmark problems.

### Strengths and Weaknesses
Strengths:  
- The authors leverage the explicit transition model in a tree-based POMDP solver, deriving bounds using a simplified POMDP model that ignores non-planning tree observations.  
- The theoretical bounds are easier to use than previous probabilistic bounds in AR-DESPOT, and the algorithm shows competitive performance against state-of-the-art methods.  
- The main approach is clear and well-motivated.  

Weaknesses:  
- The soundness and intuitive justification for the simplified model choice need further motivation.  
- Notation can be confusing and requires clarification; for example, Equation (6) lacks normalization of distributions.  
- The probabilistic nature of the algorithm's performance could be misleading, as the bounds are deterministic.  
- The complexity discussion contains inaccuracies, particularly regarding the Bellman update's time complexity.  
- The evaluation on the Tiger problem, which has minimal state and observation spaces, raises questions about the method's applicability to more complex scenarios.  

### Suggestions for Improvement
We recommend that the authors improve the motivation for the choice of the simplified model and clarify the probabilistic aspects of the algorithm's performance. Additionally, the authors should address the inaccuracies in the complexity discussion and provide a more comprehensive evaluation on larger POMDP problems. Clarifying the notation and ensuring consistency throughout the paper is essential, particularly regarding reward functions and the definitions of terms used in equations. Finally, we suggest adding more detailed explanations for the assumptions regarding limited state and observation sets, possibly including examples in the appendix to enhance understanding.