ID: 87AXdbkRyd
Title: Self-supervised Transformation Learning for Equivariant Representations
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Self-supervised Transformation Learning (STL) aimed at learning equivariant representations without relying on transformation labels. The authors propose using pairs of data with the same transformation to derive transformation representations, allowing for the application of complex augmentation schemes like AugMix. The method's effectiveness is demonstrated through various classification and object detection tasks, showing competitive performance compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and well-structured, making it easy to follow.
- The proposed approach is intuitive and efficient, applying invariant self-supervised learning principles to equivariant representation learning.
- The evaluation pipeline is thorough, covering semantic classification across multiple datasets and object detection, with limited sensitivity to hyperparameters.
- The method's performance is convincingly shown to be competitive across a range of tasks.

Weaknesses:
- The evaluation lacks key baselines, specifically SIE and SEN, which should be included to analyze the differences in supervision levels.
- Experimental results are computed for a single seed, which may limit robustness, particularly in object detection where performance gaps are narrow.
- The paper does not report performance on the pretraining dataset, which is crucial for understanding model behavior in and out of domain.
- The complexity of the approach may lead to computational demands that are not adequately discussed.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including comparisons with SIE and SEN to address the supervision level differences. Additionally, conducting experiments with multiple seeds would enhance the robustness of the results, particularly in object detection tasks. It would be beneficial to report performance on the pretraining dataset to provide a clearer understanding of model behavior. Furthermore, we suggest a more in-depth analysis of the learned transformation representations to clarify their relationships in latent space. Finally, addressing the computational costs associated with the method would strengthen the paper.