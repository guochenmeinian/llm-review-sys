ID: 29WbraPk8U
Title: Sharpness-Aware Minimization Leads to Low-Rank Features
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on Sharpness-Aware Minimization (SAM) and its impact on the numerical rank of features in neural networks. The authors conclude that (1) SAM reduces feature rank during training, with greater reductions for larger neighborhood sizes (rho), (2) intermediate rho values enhance generalizability in K-NN, (3) in certain models, rank reduction is linked to ReLU unit inactivity, and (4) using a bottleneck layer to reduce rank does not replicate SAM's generalization benefits. The empirical evidence supports these claims across various tasks and architectures.

### Strengths and Weaknesses
Strengths:
- The main claim regarding rank reduction is well-supported by extensive empirical evidence.
- The findings enhance understanding of SAM and the flat-minima phenomenon, potentially leading to faster training and inference through integration with compression methods.
- The paper is well-organized and adequately covers relevant background and related work.

Weaknesses:
- W1: Representations are extracted from different network points, complicating the interpretation of results, particularly regarding neural collapse and low-rank behavior.
- W2: Neuron activity is evaluated on a limited set of architectures, leaving unclear whether low-rank phenomena are due to ReLU inactivity across all models.
- W3: The text fails to distinguish between monotonic and U-shaped patterns in results, leading to confusion about the correlation between rank and generalizability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their experimental design by ensuring consistent representation extraction points across architectures and including relevant plots in the revision. Additionally, the authors should evaluate neuron activity across a broader range of architectures to clarify the relationship with low-rank phenomena. It is crucial to edit the text and figure captions to clearly differentiate between monotonic and U-shaped patterns to avoid misleading implications about the correlation between rank and generalizability. Furthermore, addressing minor comments, such as providing citations for the bottleneck layer and clarifying the teacher-student setup, would enhance the paper's rigor.