ID: gFf0a0ZxJM
Title: OpenAGI: When LLM Meets Domain Experts
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 6, 8, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the OpenAGI platform, a framework designed for benchmarking large language models (LLMs) in open-domain model synthesis (OMS), which involves creating plans to utilize multiple expert models for complex tasks. The platform includes benchmark tasks with ground-truth outputs for automated evaluation and open-ended tasks requiring human evaluation. The authors propose a Reinforcement Learning from Task Feedback (RLTF) method to enhance model performance. They benchmark ChatGPT, Vicuna-7B, T5-Flan, GPT-4, Claude-2, and Llama-2-13b across various setups, revealing that ChatGPT excels in zero-shot and few-shot scenarios, while T5-Flan outperforms ChatGPT after fine-tuning. The authors emphasize the importance of establishing benchmarks for LLMs to facilitate standardized measurement of capabilities and guide future research directions.

### Strengths and Weaknesses
**Strengths:**  
- The OpenAGI platform addresses a significant gap in benchmarks for open-domain model synthesis, making it relevant and necessary. 
- The framework supports both benchmark and open-ended tasks, promoting user interaction and feedback.
- The paper is generally clear and easy to read, particularly in its graphical representations.
- The variety of tasks and the innovative approach to task creation through data augmentation techniques are commendable.
- The authors have expanded their related work section to include recent LLM agent studies, enhancing the paper's relevance.
- The implementation of the RLTF mechanism shows promise in improving LLM performance through trial-and-error learning.

**Weaknesses:**  
- The task set lacks naturalness, as many tasks do not reflect typical user requests for general-purpose AI models.
- The evaluation set lacks diversity and quantity, raising concerns about the robustness of the findings.
- Benchmarking is limited to a few models; additional models, including closed-source and open-source variants, would enhance the study.
- Some reviewers noted that the complexity of tasks may not accurately reflect human-like problem-solving capabilities.
- There is no analysis of the successes and failures of the models, limiting understanding of their performance.
- The benchmark lacks clarity in implementation details, such as the inclusion of expert models in prompts and the parsing of output plans.
- There was initial skepticism regarding the project's contribution to AGI, with calls for a more thorough exploration of related works.

### Suggestions for Improvement
We recommend that the authors improve the naturalness of the task set by ensuring tasks are more reflective of user needs and clearly indicate which expert models should be employed. Additionally, enhancing the diversity and quantity of the evaluation sets would better gauge the models' capabilities. Benchmarking more models, including Claude and GPT-4, would provide a broader perspective on performance. A thorough analysis of model performance, including qualitative assessments of successes and failures, should be included to guide future research. We suggest clarifying the rationale behind the complexity of benchmark tasks and how they relate to real-world applications. Enhanced documentation in the GitHub repository is necessary to clarify implementation details and facilitate reproducibility. Lastly, we recommend expanding the discussion of limitations, particularly regarding the implications of the RLTF method and the potential for distribution shifts, while maintaining ongoing community engagement to ensure the framework remains relevant and effective in advancing research in this field.