ID: Cf2c9Pk9yF
Title: ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification
Conference: NeurIPS
Year: 2023
Number of Reviews: 27
Original Ratings: 7, 5, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the ImageNet-Hard dataset, which includes images that models struggle to classify despite various zoom-in transformations. The authors propose that tuning zoom-in augmentations significantly enhances performance on ImageNet-related test sets and improves the state-of-the-art test-time augmentation method, MEMO. The study also identifies a center bias in ImageNet-A and ObjectNet, suggesting that models may implicitly zoom into salient regions of images. Furthermore, the authors explore the impact of zooming on image classification performance across various datasets, including ImageNet-A, ObjectNet, ImageNet-R, and ImageNet-Sketch, indicating that different datasets exhibit distinct trends regarding zoom levels. The findings show that upscaling images and applying center-cropping can significantly enhance accuracy. The authors also provide evidence supporting the hypothesis that image classifiers can achieve high accuracy by zooming into the most discriminative regions of an image, demonstrating that even older classifiers like AlexNet can reach nearly 90% accuracy on ImageNet using this strategy. Additionally, the paper emphasizes the importance of transparency in dataset construction, particularly regarding the challenges posed by ImageNet-Hard.

### Strengths and Weaknesses
Strengths:
- The analysis of shape bias offers a novel perspective, diverging from previous studies focused on texture and color biases.
- The proposed ImageNet-Hard benchmark is expected to be a significant challenge for future research.
- Extensive experimental details on zoom-in transformations are provided, facilitating reproducibility.
- The paper provides compelling evidence for the effectiveness of zooming in improving classification accuracy, supported by detailed experimental results.
- It introduces a novel method for dataset construction that does not rely on bounding box information, allowing for easier evaluation of positional bias.
- The authors clarify and revise key sections of the manuscript based on reviewer feedback, enhancing overall clarity.

Weaknesses:
- The hypothesis regarding models zooming into the most discriminative regions lacks empirical validation and should be either substantiated or removed.
- The causal relationship between zooming and improved performance remains ambiguous, with alternative hypotheses not fully disentangled.
- The discussion on the implications of zooming as an inductive bias lacks depth, particularly regarding its applicability across different contexts.
- The terminology of test-time augmentation (TTA) may confuse readers familiar with test-time adaptation.
- The "random" baseline in Table 1 is inadequately defined, leading to ambiguity.
- Questions arise regarding the construction of ImageNet-Hard, particularly concerning the rationale for differing group sizes and the criteria for class overlap.
- Some reviewers noted a lack of clarity in certain explanations, particularly regarding center bias, which is not considered novel.
- The discrete zoom-crop strategy may miss optimal zoom configurations, potentially affecting the accuracy of classifications.

### Suggestions for Improvement
We recommend that the authors improve the abstract by clarifying the claim about models zooming into discriminative regions, providing empirical evidence or reconsidering its inclusion. Additionally, we suggest refining the terminology around TTA to avoid confusion with test-time adaptation. The authors should specify the "random" baseline in Table 1 and address the construction questions regarding ImageNet-Hard, particularly the reasoning behind group sizes and overlap criteria. Furthermore, it would be beneficial to report the percentage change in accuracy associated with different zoom configurations and to include error bars for the performance improvements of MEMO + RRC to enhance the credibility of the findings. We also recommend improving the clarity of the causal claims regarding zooming and classification performance by providing more robust evidence to support their hypothesis. A more thorough discussion on the limitations of zooming as an inductive bias, including examples of scenarios where it may not be ideal, would strengthen the paper. Lastly, we suggest clarifying the implications of center bias findings to better articulate their relevance to the community and exploring methods that learn optimal zoom configurations to provide a competitive benchmark for future research.