ID: dkpmfIydrF
Title: Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AdvUnlearn, a robust unlearning framework that integrates adversarial training into diffusion models to enhance concept erasure. The authors propose optimizing the text encoder rather than the UNet to achieve a balance between effective concept erasure and high image generation quality. Extensive experiments demonstrate AdvUnlearn's robustness across various unlearning scenarios, including nudity, style, and object unlearning.

### Strengths and Weaknesses
Strengths:
1. AdvUnlearn significantly improves the robustness of diffusion models against adversarial prompt attacks, effectively preventing the generation of undesired content.
2. The method maintains high image generation quality by focusing on the text encoder and utilizing utility-retaining regularization.
3. Its plug-and-play usability allows the robust text encoder to be shared across different diffusion models, enhancing practical implementation.

Weaknesses:
1. The evaluation primarily focuses on the UnlearnDiff attack, lacking sufficient assessment against other baseline attacks such as CCE and RAB, which raises questions about the robustness claims.
2. The paper does not provide quantitative erasing results for various unlearning scenarios, limiting the understanding of the trade-offs between robustness and erasing performance.
3. The introduction of C_{retrain} is inadequately explained, and the choice of prompt subsets for evaluation lacks clarity.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by incorporating a wider range of baseline attacks, specifically CLIP-based adversarial attack methods [1] and [2], to validate the robustness claims of AdvUnlearn. Additionally, providing more details on how the text encoder is optimized using Eq.(5) would clarify the methodology. The authors should also present quantitative erasing results across all unlearning scenarios to assess the performance comprehensively. Finally, a more thorough discussion on the implications of using only the text encoder for robustification, including potential vulnerabilities to embedding-based attacks, would strengthen the paper.