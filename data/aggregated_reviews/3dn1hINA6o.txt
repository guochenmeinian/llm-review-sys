ID: 3dn1hINA6o
Title: The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the "edge-of-reach problem" in offline model-based reinforcement learning (RL), where existing methods fail even with accurate dynamics models due to truncated rollouts. The authors propose Reach-Aware Value Learning (RAVL) to address this issue, demonstrating its effectiveness through empirical results on D4RL and V-D4RL benchmarks. They provide a thorough investigation, including theoretical analysis and practical experiments, to support their claims.

### Strengths and Weaknesses
Strengths:
- The paper identifies an important and previously overlooked issue in offline model-based RL.
- It includes well-organized content with a clear abstract, introduction, and conclusion.
- The authors provide open-source code and detailed hyperparameter settings, facilitating reproducibility.
- Comprehensive experiments validate the "edge-of-reach" hypothesis and demonstrate RAVL's effectiveness.

Weaknesses:
- The methodology contribution is limited, as RAVL does not show clear superiority over existing state-of-the-art methods.
- The paper's formatting, including excessive use of italics and numerous subsections, detracts from readability.
- The initialization of the Q-function is not adequately discussed, which is crucial for understanding potential overestimation bias.

### Suggestions for Improvement
We recommend that the authors improve the paper's formatting by removing all italics and reducing the number of subsections, particularly in Section 6, by replacing \subsection with \paragraph. Additionally, we suggest clarifying the initialization of the Q-function, as this is vital for understanding its impact on overestimation bias. Addressing the unclear performance of RAVL compared to existing methods on more complex benchmarks would strengthen the contribution. Furthermore, reducing future references throughout the text would enhance clarity for readers.