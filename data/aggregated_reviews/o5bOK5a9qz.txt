ID: o5bOK5a9qz
Title: DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DemaFormer, a novel architecture for temporal language grounding that utilizes an energy-based model to learn moment-query distributions. The authors propose a Transformer-based architecture that incorporates an exponential moving average with a learnable damping factor to effectively encode moment-query inputs. The study reports state-of-the-art results on four public datasets, addressing the limitations of naive attention mechanisms in capturing relationships between video moments and textual queries.

### Strengths and Weaknesses
Strengths:  
- The introduction of an energy-based model (EBM) provides a novel perspective on modeling moment-query relationships.  
- The DEMA module appears to be innovative, and the experiments are well-conducted, yielding strong motivations and results.  
- The paper addresses reviewer concerns during the rebuttal period, enhancing clarity on performance comparisons.

Weaknesses:  
- The motivation for introducing EBM is unclear, particularly regarding its utility in modeling moment-query distributions.  
- The paper lacks a thorough discussion of related works on EBM and does not adequately justify the use of adjacent video features.  
- Performance metrics are not state-of-the-art compared to existing literature, and some methodological details, such as parameter analysis, are missing.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation for using EBM and explicitly explain its advantages in modeling moment-query representations. Additionally, we suggest including a discussion of related EBM works in Section 2 and justifying the inclusion of adjacent video features. To strengthen the paper, the authors should provide a detailed comparison with existing methods, particularly regarding performance metrics, and include missing methodological details such as how to obtain \(\lambda_{NLL}\). Lastly, addressing minor typographical errors will enhance the overall presentation.