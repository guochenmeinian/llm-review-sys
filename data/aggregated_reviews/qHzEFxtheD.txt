ID: qHzEFxtheD
Title: Sketching Algorithms for Sparse Dictionary Learning: PTAS and Turnstile Streaming
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 6, 7, 5, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on sparse dictionary learning and k-means clustering, utilizing sketching techniques. It provides lower bounds in the streaming setting, with a notable result being a lower bound for k-means derived from the communication complexity of the multiparty set-intersection problem. Additionally, it establishes a new upper bound under input sensitivity assumptions in the random order model. The paper also introduces approximation schemes for both problems, leveraging dimensionality reduction, and discusses space complexity in the turnstile model, employing sketching tools for matrix optimization.

### Strengths and Weaknesses
Strengths:
- The paper addresses two significant problems in machine learning.
- It introduces several innovative ideas from sketching, demonstrating a strong grasp of the relevant literature.
- The writing is clear and the presentation is accessible.

Weaknesses:
- Several Theorems lack precise running time statements, raising concerns about the practicality of the algorithms.
- The section on the turnstile model is misleadingly labeled as "space complexity" without clearly stating both space and time complexities.
- The organization of results is confusing, with discrepancies between the introduction and subsequent sections.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the running time statements for all Theorems to enhance the practical applicability of the algorithms. It would be beneficial to clearly delineate both space and time complexities in the turnstile model section. Additionally, we suggest restructuring the paper to provide a clear summary of results in the introduction, aligning the order of presentation with the main sections to facilitate easier navigation for readers. Consider incorporating key results from the appendix into the main paper to avoid obscuring important contributions.