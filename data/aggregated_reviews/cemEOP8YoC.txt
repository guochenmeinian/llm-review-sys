ID: cemEOP8YoC
Title: IBA: Towards Irreversible Backdoor Attacks in Federated Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 4, 4, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a two-phased backdoor injection framework, called IBA, for Federated Learning systems. IBA incorporates an adaptive trigger generation mechanism and a gradual implantation process to insert stealthy backdoors into the global model. It enhances attack efficiency and durability through selective poisoning of specific model parameters. Evaluation on multiple datasets shows IBA achieves high success rates and outperforms existing backdoor injection methods, even against various defense techniques.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, addressing a crucial problem in federated learning.
- The experimental setup is thorough, considering various datasets, settings, and defenses.
- The two-stage approach is unique and powerful, with clear mathematical formulations.

Weaknesses:
- The primary contribution lacks distinctiveness compared to existing works, such as Alam et al. and Zhang et al., which also utilize adversarial examples and selective parameter poisoning.
- IBA is not evaluated against important state-of-the-art defenses like FLAME and SparseFed, nor compared with other significant attacks like 3DFed.
- The paper does not adequately address the specificity of adversarial triggers or the computational overhead of trigger generation.
- There are inconsistencies in reported accuracy and results, particularly in Table 1 and Table 2, which require clarification.
- The evaluation could benefit from more practical benchmark datasets and a discussion on the significance of using FedProx and FedNova.

### Suggestions for Improvement
We recommend that the authors improve the distinctiveness of their contribution by elaborating on how IBA differs from existing methods. Additionally, the authors should include evaluations against state-of-the-art defenses and attacks, such as FLAME, SparseFed, and Neurotoxin. A more comprehensive explanation of the trigger generation process and its computational overhead is necessary. The authors should clarify discrepancies in accuracy reporting and ensure consistency in the representation of results. Including evaluations on more complex datasets and discussing the significance of different federated learning models would enhance the paper's depth. Finally, addressing the clarity of figures and tables, as well as providing a detailed explanation of specific results, will improve overall comprehension.