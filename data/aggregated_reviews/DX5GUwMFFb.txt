ID: DX5GUwMFFb
Title: Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel incremental reinforcement learning algorithm called Action Value Gradient (AVG), designed for resource-scarce environments. The authors observe that traditional algorithms like PPO, SAC, and TD3 struggle under constraints such as limited replay buffer sizes or single sample updates. AVG employs techniques such as feature normalization, state normalization, and return scaling to enhance learning stability and performance. The authors demonstrate the effectiveness of AVG through extensive experiments across various tasks, including simulations and real-world robotic applications, achieving superior results compared to baseline methods.

### Strengths and Weaknesses
Strengths:
1. The authors address a significant challenge in reinforcement learning within resource-constrained setups.
2. The diversity of tasks evaluated, including 30 seeds per task in OpenAI Gym, adds robustness to the findings.
3. The problem is clearly articulated, and the paper is well-structured.
4. The analysis of normalization effects on off-policy actor-critic performance is timely and relevant.

Weaknesses:
1. The introduction of AVG could be more concise, as the extensive discussion on orthogonal initialization and other techniques may mislead readers about their novelty in off-policy actor-critic methods.
2. The authors should clarify whether AVG is essentially SAC without Clipped Double Q-learning, emphasizing its unique contributions.
3. Comparisons with other normalization techniques commonly used in off-policy RL would enhance the contextual understanding of AVG's performance.
4. The algorithm's sensitivity to hyper-parameters and sample efficiency raises concerns about its robustness.

### Suggestions for Improvement
We recommend that the authors improve the introduction of AVG by succinctly stating its differences from SAC, particularly regarding the absence of Clipped Double Q-learning and the addition of normalization techniques. Additionally, we suggest conducting comparisons with other incremental learning algorithms and normalization methods to provide a clearer context for AVG's performance. An ablation study exploring varying buffer sizes and batch sizes would also strengthen the manuscript. Finally, addressing the sensitivity of AVG to hyper-parameters and discussing the implications of not using target networks in more detail would enhance the paper's clarity and depth.