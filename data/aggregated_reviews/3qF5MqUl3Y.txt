ID: 3qF5MqUl3Y
Title: R2H: Building Multimodal Navigation Helpers that Respond to Help Requests
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Respond to Help (R2H) benchmark, designed to evaluate multi-modal conversational navigation helper agents. The benchmark includes two tasks: Respond to Dialog History (RDH), which assesses the agent's ability to generate informative responses from dialog histories, and Respond during Interaction (RdI), which evaluates response effectiveness during cooperative tasks. The authors propose two construction approaches for the helper agent: fine-tuning a task-oriented multi-modal model named SeeRee and utilizing a multi-modal large language model in a zero-shot manner. The research incorporates both automatic benchmarking and human evaluations.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and clearly presents its contributions.
2. The R2H benchmark is a significant addition to the field, promoting the development of navigation helpers that can effectively respond to help requests.
3. The introduction of the Conditional Optimized Sparse attention mask and the Parse by Step approach enhances training and performance.

Weaknesses:
1. The motivation for the R2H benchmark is insufficiently convincing, as it primarily evaluates the helper agent without considering the collaborative dynamics with task performers.
2. The experimental evaluation is limited and lacks comprehensive comparisons with existing methods, raising concerns about the reliability of the proposed solutions.
3. More detailed analysis is needed regarding the effectiveness of the Conditional Optimized Sparse attention mask and the implications of high language similarity to human responses.

### Suggestions for Improvement
We recommend that the authors improve the justification for the R2H benchmark by addressing the collaborative aspects between helper and performer agents. Additionally, we suggest enhancing the experimental evaluation by including comparisons with a broader range of existing methods to validate the proposed approaches. More in-depth analysis of the Conditional Optimized Sparse attention mask is necessary, specifically regarding what the learnable conditional mask captures and its performance on the test set. Lastly, we encourage the authors to investigate the impact of different task performers on the results within the same dataset.