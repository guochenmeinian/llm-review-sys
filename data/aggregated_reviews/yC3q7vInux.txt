ID: yC3q7vInux
Title: Siamese Masked Autoencoders
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 6, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Siamese Masked Autoencoders (SiamMAE) for learning visual correspondence from video data. The authors propose a method that samples pairs of video frames, applying asymmetric masking to the future frame while leaving the past frame unmasked. This approach encourages the model to focus on object motion rather than low-level details. The authors demonstrate that SiamMAE outperforms existing methods in various downstream tasks, including video object segmentation and pose propagation, and provide extensive ablation studies to justify their architectural choices.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a simple yet effective method for video self-supervised learning.
- The motivation for using masked autoencoders for object-based correspondence is clear, and the architecture design choices are well-explained.
- The method achieves impressive results, outperforming previous state-of-the-art approaches on several tasks.

Weaknesses:
- The approach appears incremental over existing Masked Autoencoders (MAE) and may not significantly advance the field despite its effectiveness.
- The reliance on temporal smoothness may limit applicability to in-the-wild videos with abrupt scene changes, and a discussion on handling such cases is lacking.
- The paper does not evaluate the method on video classification tasks, which could further validate its effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of the method, particularly regarding its performance on videos with sharp scene changes. Additionally, it would be beneficial to include experiments on video classification tasks, such as UCF101 and HMDB51, to demonstrate the method's versatility. Clarifying the rationale behind not using temporal positional encoding and providing more details on the k-NN propagation process would enhance the paper's clarity. Finally, we suggest exploring the impact of varying the mask ratio for grid masking and comparing the results with FrameMAE for a more comprehensive evaluation.