ID: k2VHhq2LH9
Title: Reasoning about Ambiguous Definite Descriptions
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel dataset and benchmark aimed at evaluating how well systems can resolve ambiguity in language through reasoning, specifically focusing on ambiguous definite descriptions. The authors generate the dataset, RADD-Wikidata-5-EN, using five property-pairs from Wikidata, and conduct experiments with large language models (LLMs) to assess their performance in distinguishing between de dicto and de re interpretations in a temporal context. The findings indicate that LLMs struggle with this task, particularly with de re interpretations, although chain-of-thought prompting offers some improvement.

### Strengths and Weaknesses
Strengths:  
- The paper addresses an interesting and underexplored problem of ambiguity resolution through reasoning.  
- It introduces a new semi-automatically generated benchmark dataset based on real-world knowledge.  
- The careful prompt design isolates reasoning ability, and the experiments are well-executed across major recent dialogue models.  
- The paper is clear, well-written, and the code and data are publicly available, facilitating reproducibility.  

Weaknesses:  
- The analysis could be strengthened by including a human performance baseline and exploring the types of reasoning failures that occur.  
- The dataset is limited to only five property-pairs, which may affect the generalizability of the results.  
- The experiments do not consider fine-tuning the models on the task, which could provide insights into model performance.

### Suggestions for Improvement
We recommend that the authors improve the analysis by including a human performance baseline to provide context for the model results. Additionally, a finer-grained breakdown of reasoning failures should be included to enhance insights. We suggest expanding the discussion with concrete future work proposals based on the insights gained from the experiments. Furthermore, considering the inclusion of F1 scores for each class in addition to accuracy could succinctly illustrate model performance differences between de re and de dicto instances.