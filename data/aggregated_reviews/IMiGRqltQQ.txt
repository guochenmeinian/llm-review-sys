ID: IMiGRqltQQ
Title: “Why Not Looking backward?” A Robust Two-Step Method to Automatically Terminate Bayesian Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 4, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new termination criterion for Bayesian optimization (BO) that involves a two-step process: first, it checks for convergence within a convex region, and then it evaluates whether the estimated local regret falls below a predefined threshold. The authors provide a theoretical analysis supporting their approach and demonstrate its effectiveness through empirical comparisons against existing methods. Additionally, the paper discusses transformation methods used in surrogate modeling, specifically addressing concerns about handling negative values in objective functions. The authors clarify that their approach focuses on enhancing surrogate model accuracy rather than transforming negative values into a positive space. They also highlight the importance of termination criteria in BO and assert that the baseline algorithms used for comparison are appropriate. Furthermore, the authors utilize a parameter $\tau=10$ in their experiments, which they will clarify in the final version, and provide evidence of the flexibility of $\tau$ values through sensitivity analysis.

### Strengths and Weaknesses
Strengths:
- The proposed algorithm is novel and introduces a unique approach to testing for convexity and assessing local regret.
- The theoretical justification for the termination criteria is well-articulated, and the empirical results are comprehensive, covering various benchmarks and tasks.
- The authors provide a clear rationale for their use of transformation methods, emphasizing their focus on surrogate model accuracy.
- The discussion on termination criteria in BO is insightful and highlights an important yet understudied area.
- The sensitivity analysis on the parameter $\tau$ demonstrates the robustness of their approach.

Weaknesses:
- The paper's exposition lacks clarity in several areas, particularly in the proofs of Theorems 1 and 2, which could benefit from more detailed explanations.
- The naive baseline for comparison appears overly simplistic, as it does not adequately explore a diverse set of termination thresholds, leading to suboptimal performance in some cases.
- The assumption of convexity for the variance function during local regret estimation may not hold in practice, raising concerns about the robustness of the proposed method.
- The manuscript lacks clarity regarding the specific value of $\tau$ used in experiments, which could lead to confusion.
- The distinction between hyperparameters $\omega$ and $\sigma_\epsilon$ and their roles could be more explicitly stated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the exposition, particularly in the proofs and the intuition behind key choices, to enhance reader comprehension. Additionally, we suggest exploring a broader range of termination thresholds for the naive baseline to provide a more robust comparison. It would also be beneficial to address the implications of the convexity assumption on the variance function and discuss how model misspecification might affect the proposed algorithm. Furthermore, we recommend that the authors explicitly state the value of $\tau$ used in their experiments and provide a more detailed explanation of the roles of the hyperparameters $\omega$ and $\sigma_\epsilon$ to enhance reader understanding. Finally, including a pseudo-code for the stopping criterion would aid in understanding the implementation details.