ID: 1PXPP9Gzgc
Title: BERTwich: Extending BERTâ€™s Capabilities to Model Dialectal and Noisy Text
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to enhance the robustness of a Pretrained Language Model, specifically BERT, against naturally occurring noise and linguistic variations. The authors propose a "sandwich" architecture by adding layers to the beginning and end of BERT, continuing pretraining on synthetically noised data, and fine-tuning on similarly noised data. The findings indicate improvements in handling English typos and a Bavarian dialect of German, while performance in Swiss German only benefits from fine-tuning on noisy data. The paper also analyzes token-level alignment across models and its correlation with classifiers trained at different layers.

### Strengths and Weaknesses
Strengths:
- The exploration of synthetic noise during pretraining and fine-tuning is an intriguing question that may yield insights into model robustness.
- The analysis of dialectal performance provides valuable insights into the effects of linguistic variation.
- The paper is well-structured, with comprehensive ablation studies that clarify the impact of each component.

Weaknesses:
- The novelty primarily lies in the architectural change, lacking comparisons to established modifications like Adapters, Prefix-Tuning, or LoRA, which raises questions about the significance of the results.
- The synthetic noise approach is limited, primarily addressing typos and low-edit distance dialectal changes, while neglecting broader linguistic variations.
- Key training details are omitted, complicating reproducibility, and the evaluation does not include other BERT variants or datasets.

### Suggestions for Improvement
We recommend that the authors improve the comparison with other widely used architecture modifications such as Adapters, Prefix-Tuning, or LoRA to substantiate the architectural novelty. Additionally, incorporating synthetic noise grounded in actual typo patterns and linguistic variations would strengthen the model's robustness. It is crucial to include results on standard tasks alongside respective baselines and quantify the additional computational overhead associated with the architectural modifications. Finally, exploring the performance of BERTwich on other BERT variants and datasets would enhance the paper's contributions.