ID: RQQGbBqvbL
Title: Collaborative Refining for Learning from Inaccurate Labels
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 5, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a collaborative refining approach for learning from inaccurate labels provided by multiple annotators, focusing on binary classification. The authors propose strategies based on annotator agreement to filter noise and enhance data quality, utilizing a comparative strategy for conflicting labels and an aggregating method for consistent annotations. The framework refines unreliable datasets into more reliable ones, employing submodels corresponding to the number of annotators. Extensive experiments demonstrate the effectiveness of the proposed methods across various datasets.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clear, making it easy to follow.
- The proposed method is theoretically motivated and of high practical interest, addressing the important issue of data refinement in the context of noisy labels.
- Comprehensive empirical evaluations show significant improvements in learning performance despite label inaccuracies.

Weaknesses:
- The problem is not well articulated, with a lack of literature review and reliance on synthetic datasets rather than real-world data.
- The objectives of the proposed method are unclear, particularly regarding the distinction between obtaining labels for unlabeled samples and learning a classifier.
- The novelty of the robust union selection method is limited, as it closely resembles existing techniques, and comparisons with relevant methods are insufficient.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the problem formulation and explicitly define the objectives of their approach. It would be beneficial to incorporate probabilistic labels as seen in programmatic weak supervision and to utilize datasets from the WRENCH library for more robust evaluations. Additionally, we suggest including more comparative methods, such as Dawid Skene and Iterative Weighted Majority Voting, to enrich the analysis. Clarifying the relationship between the proposed method and unreliable partial label learning would also enhance the paper's contribution. Lastly, addressing the limitations regarding binary classification and the assumptions about annotator agreement in the introduction would strengthen the overall presentation.