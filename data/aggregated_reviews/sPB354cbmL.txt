ID: sPB354cbmL
Title: Improved Training of Deep Text Clustering
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called Deep Clustering optimization from Generalized Labeled and Unlabeled data (DCGLU) aimed at improving deep text clustering by addressing noise in generalized labels derived from various sources. The authors propose a binary classification framework inspired by positive and unlabeled learning to categorize samples into high and low confidence groups. The contributions include introducing generalized supervision and labels, proposing a general optimization method applicable to existing deep clustering techniques, and demonstrating improved clustering performance through experiments on two text datasets.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant challenge in deep text clustering, relevant for various NLP applications.
- It proposes a novel method that integrates well with existing deep clustering models without requiring additional parameters.
- The theoretical analysis is clear, connecting the method to empirical risk minimization.
- Extensive experiments validate the effectiveness and robustness of the proposed method.

Weaknesses:
- The concept of generalized supervision and labels lacks clear definition, potentially causing confusion.
- The paper does not compare the proposed method with other noise reduction techniques, limiting evaluation comprehensiveness.
- Absence of ablation studies or sensitivity analyses affects interpretability and reproducibility.
- Limitations regarding scalability and efficiency are not discussed.
- The novelty may be insufficient for the NLP community, as the method builds on established techniques without significant innovation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions for generalized supervision and generalized labels to avoid reader confusion. Additionally, the authors should compare their method against existing noise reduction techniques like instance weighting to enhance evaluation fairness. Conducting ablation studies would help clarify the contributions of each component of the proposed method. Furthermore, discussing potential limitations, such as scalability and efficiency, would provide a more balanced view of the method's applicability. Lastly, we suggest making the paper more self-contained by including vital algorithm details in the main body rather than the appendix to enhance clarity and accessibility.