ID: dhFHO90INk
Title: Implicitly Guided Design with PropEn: Match your Data to Follow the Gradient
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel deep learning framework for design optimization, referred to as "PropEn," which implicitly optimizes a seed input design to enhance a specific property \( g(\cdot) \) until convergence. The authors provide a theoretical derivation demonstrating that the learned neural network function \( f^*(\cdot) \) approximates the gradient direction of the property function, despite not being explicitly trained for it. The framework is evaluated across three diverse design tasks, including airfoil and protein optimization, showing superior performance compared to existing methods. The authors express a willingness to enhance their manuscript by including additional results and clarifications, particularly regarding comparisons to established methods in the literature.

### Strengths and Weaknesses
Strengths:
- The proposed method is well-motivated and presents a novel approach to design optimization without adversarial classification.
- Experiments on both toy and realistic tasks indicate that the method outperforms state-of-the-art models.
- The theoretical insights regarding the optimal function \( f^*(\cdot) \) and its relation to the gradient of \( g(\cdot) \) are valuable contributions to the field.
- The authors have provided a thorough response to reviewer comments, demonstrating engagement with the feedback and a willingness to incorporate additional experiments and discussions.

Weaknesses:
- There is insufficient analysis regarding the selection and sensitivity of parameters \( \Delta_x \) and \( \Delta_y \), which are critical to the matching step.
- The method currently addresses only a single property \( g(\cdot) \), raising concerns about its applicability to real-world scenarios where multiple properties must be optimized simultaneously.
- A rigorous evaluation of the seed design's quality and its impact on convergence speed and accuracy is lacking.
- The paper lacks comparisons to strong baselines, which limits the assessment of the proposed algorithm's performance.
- The clarity of terminology, such as the name "Cross xx2xx," could be improved for better understanding.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the parameters \( \Delta_x \) and \( \Delta_y \), including their selection methodology and sensitivity in various design contexts. Additionally, the authors should explore extending the method to accommodate multiple properties, thereby enhancing its practical applicability. A thorough examination of the "goodness" of the seed design and its influence on convergence should also be included. Furthermore, we suggest benchmarking PropEn against established Bayesian optimization methods to demonstrate its effectiveness in low-data regimes. We also recommend improving clarity by using a more descriptive name than "Cross xx2xx" and including brief details about the models in the appendix. Finally, conducting comparisons with established results in the literature and including an experiment evaluating PropEn in Bayesian Optimization on standard test functions like Branin-Currin or DTLZ would provide a more grounded impression of the algorithm's performance.