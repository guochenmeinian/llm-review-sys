ID: BTRcVP7ZJn
Title: Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 5, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on pruning for the matrix sensing problem, specifically focusing on the over-parameterized symmetric matrix sensing problem. The authors demonstrate that an approximate solution derived from a smoothed version of the group-Lasso problem can be pruned to achieve small error values using a greedy pruning algorithm. Furthermore, they show that fine-tuning the pruned solution can further reduce reconstruction error. The authors also explore an algorithm framework based on overparameterized Burer-Monteiro matrix factorization, incorporating a group lasso penalty and a gradient descent-type approach intertwined with iterative pruning.

### Strengths and Weaknesses
Strengths:
- The results indicating that fine-tuning a local minimum can enhance its quality are intriguing, especially in light of recent observations in deep learning.
- Theorem 3's lack of requirement for global optimality is noteworthy.
- The paper is well-structured and clearly written, providing valuable insights into the efficiency of overparameterized Burer-Monteiro factorization.
- The theoretical justification for alternating pruning and fine-tuning steps in deep neural network training is compelling.

Weaknesses:
- Theorem 1's reliance on the continuous time version of gradient descent (gradient flow) may limit its practical applicability.
- The penalty discussed in Section 4 appears somewhat arbitrary, necessitating better motivation for its choice and its practical usage.
- The proof of Theorem 1 may contain errors, particularly regarding the treatment of the quantity \( r(t) = U^T U_\star \) as a vector instead of a matrix.
- Many results focus on a population setting where the ground truth is known, lacking a clearer focus on more realistic scenarios.

### Suggestions for Improvement
We recommend that the authors improve the motivation for the choice of the penalty discussed in Section 4 and clarify its practical applications. Additionally, addressing the potential errors in the proof of Theorem 1 is crucial. It would also be beneficial to enhance the manuscript's focus on the empirical loss minimization setting, which is more realistic. Furthermore, we suggest that the authors clarify the assumptions of Theorems 4 and 5, particularly regarding the use of the RIP, and provide details on the number of oracle calls required for the perturbed gradient descent scheme to achieve the desired precision.