ID: 7tMgzSvopH
Title: Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multilingual dataset for sentiment analysis, comprising 79 datasets across 27 languages, and proposes a benchmark for evaluating sentiment classification models. The authors emphasize the performance variations between low-resource and high-resource languages and highlight the importance of high-quality annotations and the methodology behind inter-annotator agreement (IAA). The dataset is publicly accessible and aims to facilitate research in resource-lean languages, serving as a significant contribution to the NLP community. Additionally, the authors provide extended experimental results that detail classification outcomes by language and domain, identifying challenging cases for multilingual sentiment models.

### Strengths and Weaknesses
Strengths:
- The dataset serves as a valuable resource for sentiment analysis, particularly for low-resource languages.
- The authors provide detailed annotation guidelines and a clear explanation of the IAA metrics.
- The paper includes a comprehensive list of datasets with links, enhancing reproducibility and usability.
- The authors address various technical aspects, such as transfer learning and hyperparameter selection, with clarity.
- The supplementary materials are enhanced with extensive experimental results and examples, improving the resource's utility.

Weaknesses:
- The writing lacks clarity in several sections, making it difficult to follow the arguments.
- Many statements are unsubstantiated by citations, and comparisons with existing datasets are insufficient.
- The relationship to prior work is inadequately discussed, lacking comparisons with existing datasets.
- The complete benchmarking results are not fully integrated into the main draft, with some relegated to the appendix and external sources.
- The discussion lacks sufficient error analysis, failing to highlight specific languages or datasets that present challenges for sentiment classification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing by providing more examples and justifications for their choices, particularly in sections discussing quality criteria and dataset selection. The authors should enhance the discussion on the dataset selection process, including the inter-annotator agreement analysis, and provide details on the annotators' backgrounds and guidelines. We suggest including a comparative table of their dataset against existing ones in terms of size and diversity, along with a summary of key metrics. Additionally, we encourage the authors to integrate more benchmarking results into the main draft and conduct a thorough error analysis, specifically addressing which sentiment classes are inadequately captured and identifying challenging languages or datasets. This could better demonstrate the dataset's utility and the limitations of various models.