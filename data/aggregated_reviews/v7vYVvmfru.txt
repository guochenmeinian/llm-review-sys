ID: v7vYVvmfru
Title: An Accelerated Algorithm for Stochastic Bilevel Optimization under Unbounded Smoothness
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm, AccBO, for stochastic bilevel optimization problems where the upper-level function is nonconvex with unbounded smoothness, and the lower-level function is strongly convex. The authors achieve a near-optimal oracle complexity of $\tilde{O}(\epsilon^{-3})$ for finding an $\epsilon$-stationary point. The algorithm employs normalized stochastic gradient descent for the upper-level variable and stochastic Nesterov accelerated gradient descent for the lower-level variable. Experimental results demonstrate that AccBO significantly outperforms baseline algorithms in tasks such as deep AUC maximization and data hypercleaning.

### Strengths and Weaknesses
Strengths:
1. The paper introduces AccBO, which advances the state-of-the-art complexity for stochastic bilevel optimization under the unbounded smoothness assumption.
2. The algorithm's design effectively combines techniques from variance reduction and Nesterov acceleration, leading to substantial improvements in oracle complexity.
3. Empirical results validate the theoretical claims, showing significant performance gains over existing methods.

Weaknesses:
1. Assumption 4.2 is problematic as it is based on algorithm-generated variables rather than the problem setting, raising concerns about its verifiability.
2. The algorithm's convergence analysis for Option I is limited to a one-dimensional quadratic lower-level problem, reducing its general applicability.
3. The implementation of the algorithm has errors that may invalidate experimental results, and the documentation for reproducibility is insufficient.

### Suggestions for Improvement
We recommend that the authors improve the clarity and verifiability of Assumption 4.2 by relating it to the problem setting, potentially bounding $\Delta_t$ in terms of $\|x_{t+1}-x_t\|$. Additionally, we suggest providing clearer definitions and justifications for the algorithm's components, particularly regarding the averaging step in Algorithm 2 and the learning rate choices. Addressing the code errors and enhancing the documentation for reproducibility will also strengthen the paper's contributions.