ID: qOSFiJdVkZ
Title: Continual learning with the neural tangent ensemble
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 5, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 2, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical framework for mitigating catastrophic forgetting in neural networks by interpreting a single network as an ensemble of fixed classifiers through the Neural Tangent Ensemble (NTE) approach. The authors derive a Bayesian posterior updating rule that parallels a scaled and projected version of stochastic gradient descent (SGD) without momentum. The work emphasizes the implications of network width on performance and the relationship between SGD and posterior updates, providing insights into continual learning.

### Strengths and Weaknesses
Strengths:  
- Theoretical rigor and detailed analysis, including proofs and derivations.  
- Insightful connection between the NTE posterior update rule and SGD, which could influence optimization strategies.  
- Clear presentation and logical flow of ideas.  
- Empirical results support theoretical claims, particularly regarding the effects of momentum and network width on continual learning.

Weaknesses:  
- Limited empirical validation, primarily using the Permuted MNIST dataset; broader experimentation on complex datasets is needed.  
- The claim that Bayesian ensembles of fixed functions are natural continual learners is not convincingly demonstrated for finite networks.  
- Insufficient exploration of the sensitivity of the NTE update rule to data ordering and lack of experiments on the independence of experts.  
- Section 5.4 appears misplaced and could be condensed to enhance clarity.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation by including additional experiments on more complex datasets to strengthen the claims. It would be beneficial to compare the proposed method with other optimization-based continual learning techniques to validate its effectiveness. We suggest addressing the potential fragility of the discussion regarding expert independence and suboptimality by evaluating the impact of these factors. Additionally, clarifying the implications of the NTE framework in the context of finite networks and providing a more detailed explanation of the continual learning task setup in the experiments would enhance understanding. Lastly, we encourage the authors to consider codifying their proofs using a proof assistant like Lean4 or Coq to bolster the theoretical robustness of their work.