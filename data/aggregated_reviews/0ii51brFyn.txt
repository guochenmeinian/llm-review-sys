ID: 0ii51brFyn
Title: Enhanced Simultaneous Machine Translation with Word-level Policies
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a shift from subword-level to word-level policies in Simultaneous Machine Translation (SiMT), claiming that this transition enhances translation quality by leveraging complete words rather than fragments. The authors propose integrating language models (LMs) to further improve SiMT performance, asserting that word-level policies facilitate fair evaluations independent of subword algorithms. The results indicate consistent improvements in translation quality across various latency settings.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel method for integrating LMs into SiMT systems, enhancing translation quality across all latency levels.
- The proposed word-level policy effectively addresses vocabulary mismatches between the LM and the SiMT model.
- The results demonstrate significant improvements in translation quality.

Weaknesses:
- The main claim regarding the superiority of word-level policies is contested, as prior research has already utilized word-level adaptive policies.
- The presentation of results is cluttered, making it difficult to discern the contributions of different factors.
- The proposed method may not be applicable to languages without clear word delimiters, such as Chinese, and lacks speed experiments to address low-latency demands.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their results presentation by reducing the number of plots in figures and focusing on the most significant findings. Specifically, they should discard less impactful plots and clearly delineate the contributions of word-level policies versus LMs. Additionally, the authors should explore potential solutions for applying their method to languages lacking spaces, such as Chinese, and include speed experiments to assess the feasibility of integrating large LMs under low-latency conditions. Lastly, we suggest providing more detailed definitions of "chunk attention" to clarify its implementation.