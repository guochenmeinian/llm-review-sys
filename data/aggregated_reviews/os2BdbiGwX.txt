ID: os2BdbiGwX
Title: Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 5, 6, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a mutual learning approach to enhance Bayesian Neural Networks (BNNs) by diversifying both parameter and feature distributions. The authors approximate the posterior of BNNs using Variational Inference with a Gaussian distribution and introduce a distance estimate between parameter and feature distributions into the objective function. Empirical results demonstrate that the proposed method outperforms existing mutual learning techniques and vanilla BNNs in accuracy, negative log likelihood loss, and expected calibration error. An ablation study is included to assess the contribution of each component.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- The idea of increasing diversity in parameter and feature distributions to enhance performance is innovative.
- Comprehensive experiments and detailed ablation studies validate the effectiveness of the proposed method.

Weaknesses:
- The baseline comparisons are limited to BNN models trained with or without mutual learning, lacking deterministic model results which could provide a clearer performance context.
- The discussion on the computational cost of stochastic gradient MCMC is insufficient, as it may not be as prohibitive as suggested.
- The absence of provided code may hinder reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the experimental validation by including deterministic model results to better contextualize the performance of the proposed method. Additionally, we suggest discussing the sensitivity of the model to hyperparameters such as α and β, as well as providing a clearer explanation of the temperature parameter's role. Including standard deviations in the results would enhance the significance of the findings. Lastly, clarifying the observed increase in KL divergence in the supplementary material would strengthen the paper's arguments.