ID: mwIZW97PVQ
Title: Deep Unlearn: Benchmarking Machine Unlearning
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 3, 4, 6, 8, -1, -1, -1
Original Confidences: 5, 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive benchmarking of machine unlearning (MU) methods, evaluating 18 techniques across 5 datasets and 2 model architectures, specifically ResNet18 and TinyViT. The authors emphasize the importance of proper baselines and hyperparameter tuning, demonstrating that methods like Masked Small Gradients and Convolution Transpose consistently outperform others in terms of accuracy and efficiency. The evaluation includes membership inference attacks and per-sample unlearning likelihood ratio attacks, contributing significant insights into the effectiveness of various MU approaches.

### Strengths and Weaknesses
Strengths:
- The paper conducts an extensive evaluation of 18 state-of-the-art MU methods, involving over 100,000 models.
- It provides a principled empirical study, highlighting the strengths and weaknesses of various approaches.
- The findings are relevant to the broader research community, particularly regarding data privacy and model trustworthiness.

Weaknesses:
- The experimental setting is limited in realism, focusing on simplistic image classification tasks and small model architectures, which may not reflect practical applications.
- The paper does not evaluate robustness across different unlearning budgets, limiting the comprehensiveness of the benchmarking.
- There is a lack of clarity regarding certain evaluation criteria and missing values in the results tables.

### Suggestions for Improvement
We recommend that the authors improve the justification for the paperâ€™s checklist and clarify the conditions for the ranking evaluations in Section 4. Additionally, the authors should consider including a comparison table for the state-of-the-art MU methods and expand the discussion on the limitations of their study, particularly regarding generalization to other models and datasets. Evaluating pretrained foundation models and exploring tasks beyond image classification, such as natural language processing, would enhance the relevance of the findings. Furthermore, addressing the inconsistencies in citations and clarifying the meaning of symbols used in figures would improve the overall clarity of the paper.