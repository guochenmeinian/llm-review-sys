ID: 8ApMP162qy
Title: DONUT-hole: DONUT Sparsification by Harnessing Knowledge and Optimizing Learning Efficiency
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 4, 4, 4

Aggregated Review:
### Key Points
This paper presents a study on compressing large dense vision-language models, specifically focusing on the DONUT model for efficient deployment. The authors propose a new architecture, DONUT-small, which utilizes pruning and knowledge distillation techniques, achieving a compression ratio of approximately 50% while maintaining competitive accuracy. The prune-then-distill approach effectively mitigates performance degradation from pruning. CKA similarity analysis offers insights into representation changes across training methods, and the results demonstrate applicability to real-world document understanding tasks.

### Strengths and Weaknesses
Strengths:
* The paper addresses a significant industrial application of model compression.
* The structure is clear, and the writing is well-organized and easy to follow.
* The results show interesting potential for real-world applications.

Weaknesses:
* The techniques employed, such as distillation and pruning, are standard and lack novel contributions.
* The comparison between models trained on different datasets raises validity concerns.
* Conclusions regarding efficiency are weakly supported due to insufficient metrics and analysis.
* More ablation studies and efficiency metrics like inference time and memory usage are needed.
* Testing on additional architectures could enhance the generality of the findings.

### Suggestions for Improvement
We recommend that the authors improve the methodology by including more ablation studies to isolate the impact of specific design choices. Additionally, incorporating efficiency metrics such as inference time and memory usage would help quantify real-world gains. A deeper analysis of the trade-offs between accuracy, compression rate, and efficiency is necessary. Finally, testing on other architectures beyond DONUT could better showcase the generality of the proposed methods.