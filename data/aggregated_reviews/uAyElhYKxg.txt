ID: uAyElhYKxg
Title: (S)GD over Diagonal Linear Networks: Implicit bias, Large Stepsizes and Edge of Stability
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 7, 6, 5, 6, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the implicit biases of gradient descent (GD) and stochastic gradient descent (SGD) on two-layer diagonal linear networks, particularly with large step sizes. It demonstrates that SGD converges to a limit influenced by the trajectory and effective initialization, while GD and SGD behave similarly under small step sizes. The findings elucidate how SGD can achieve sparse solutions with larger step sizes, contributing to understanding the generation gap between GD and SGD.

### Strengths and Weaknesses
Strengths:  
- The paper provides a significant contribution by analyzing the implicit bias of SGD across various step sizes, revealing that larger batch sizes can enhance SGD performance before divergence.  
- The motivation and theoretical framework are robust, successfully proving previous empirical observations regarding SGD's ability to recover sparse signals.  
- The writing is clear, and the presentation effectively discusses the impact of stochasticity and step size on convergence.

Weaknesses:  
- The two-layer diagonal linear network model may be overly simplistic, necessitating further evidence for the applicability of findings in more complex settings.  
- Some technical definitions, such as effective initialization, are challenging to follow, and the presentation could benefit from simpler examples to clarify key results.  
- The stability of the learning rate and its practical implications for sparse recovery remain unclear, particularly regarding the optimal choice of learning rates and their computational efficiency compared to GD.

### Suggestions for Improvement
We recommend that the authors improve the clarity of technical definitions, particularly effective initialization, by incorporating simpler examples or visual aids. Additionally, addressing the robustness of the observed phenomena across different learning rates and providing explicit convergence rates would enhance the paper's contributions. It would also be beneficial to discuss the practical implications of SGD with large learning rates compared to GD with small initialization, including computational costs and efficiency.