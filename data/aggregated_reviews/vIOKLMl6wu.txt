ID: vIOKLMl6wu
Title: LOVA3: Learning to Visual Question Answering, Asking and Assessment
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LOVA3, a framework designed to enhance Multimodal Large Language Models (MLLMs) by integrating visual question answering (VQA), question generation (GenQA), and evaluation of question-answer pairs (EvalQA). The authors introduce EvalQABench, a benchmark with 64,000 training samples aimed at evaluating VQA data quality. The framework utilizes the LLaVA-1.5 model and incorporates datasets like VQAv2 and GQA. Experimental results across ten multimodal benchmarks demonstrate significant improvements in model performance, underscoring the benefits of comprehensive questioning and evaluation capabilities. However, the evaluation currently emphasizes VQA, which may not adequately reflect the method's utility in interactive contexts, raising questions about the relationship between improved capabilities and task relevance.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated, arguing that training MLLMs to ask questions and evaluate answers can enhance visual understanding.
- The introduction of EvalQABench is a valuable contribution, providing a rigorous method for testing and improving MLLMs.
- The experimental section is detailed and extensive, showcasing robust improvements against various SOTA models.
- The paper addresses a relevant area in multimodal foundation model training, and the authors have clarified several points, leading to an improved evaluation score.

Weaknesses:
- The evaluation is limited to smaller models, which may not reflect the technique's utility for larger models, as model size can confound results.
- The contribution appears incremental, as prior work has already explored question answering and generation, with the addition of the assessment task being less novel.
- The synthesized data provides limited performance improvement while incurring significant computational overhead, raising questions about the necessity of the specific data augmentation approach.
- The evaluation settings are not aligned with the proposed method's potential applications, limiting its impact.
- The assertion that entirely different methods are required for task adaptation is questionable, given the method's aim to enhance general capabilities.

### Suggestions for Improvement
We recommend that the authors improve the scope of their evaluation to include embodied environments and interactive tasks, which would better demonstrate the practical applications of the asking and assessing capabilities. Additionally, we suggest that the authors clarify the claims regarding the automatic generation of EvalQABench, ensuring that the methodology is accurately represented. It would also be beneficial to provide a more in-depth analysis of potential data biases and strategies for mitigation, as well as to include the generated dataset for review to validate the work. Furthermore, the authors should clarify why entirely different methods are necessary for task adaptation if the goal is to enhance the general capabilities of foundation models, as this could strengthen the paper's argument.