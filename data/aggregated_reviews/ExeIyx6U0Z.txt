ID: ExeIyx6U0Z
Title: LLaNA: Large Language and NeRF Assistant
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 6, 5, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel pipeline that allows Large Language Models (LLMs) to interact with trained object-centric NeRF models by utilizing a pretrained meta-network. This network ingests NeRF MLP weights and outputs a low-dimensional feature vector, which is then transformed into the language model's input space via a projection layer. The authors train their method on a dataset of 40K NeRF models, extended with 240K paired text descriptions, demonstrating favorable performance compared to prior works focused solely on images or point clouds.

### Strengths and Weaknesses
Strengths:
1. The interaction between NeRFs and LLMs is a novel and intriguing approach, potentially enabling new applications in robotics and AR.
2. The experiments yield promising results, with the proposed method outperforming chosen baselines.
3. The dataset and code will be released upon acceptance, facilitating future research and comparisons.
4. The paper is well-written, making complex concepts accessible.

Weaknesses:
1. The experimental comparisons appear unfair as baselines were not fine-tuned on the proposed dataset, leading to a domain gap that undermines the argument for NeRFs as an input modality for LLMs.
2. The authors should consider rendering multiple images from varying viewpoints to provide a more balanced comparison with 2D-LLMs, rather than relying on single-view baselines.
3. Clarification is needed regarding whether the test set contains only unseen object classes.
4. The definition of a ‘hard’ view in Table 5 is unclear, making the table confusing.
5. The claim that Ballerini et al. are the first to utilize NeRFs as an input modality is inaccurate; it should be revised to specify the context of language tasks.

### Suggestions for Improvement
We recommend that the authors improve the fairness of their experimental comparisons by fine-tuning the baselines on the proposed dataset. Additionally, consider conducting experiments that showcase the generalization ability of the proposed method by training a limited number of NeRF models on Objaverse or ModelNet40 without fine-tuning. We also suggest evaluating the 2D baselines with multiple views to enhance the comparison. Furthermore, please clarify the training dataset split and provide a more detailed statistical analysis of the dataset, including sizes and diversities of questions. Lastly, ensure that claims regarding model safety are supported by experiments or remove unsupported assertions.