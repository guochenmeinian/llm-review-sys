ID: K7p2SnqFoN
Title: Toward Human Readable Prompt Tuning: Kubrick’s The Shining is a good movie, and a good prompt too?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 8
Original Ratings: -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for automatic prompt tuning called FLUENTPROMPT, which aims to generate human-readable prompts that are both effective and fluent. The authors replace stochastic gradient descent with Langevin dynamics and introduce a fluency constraint during prompt training. They conduct experiments on three datasets—Amazon Polarity, SST-2, and AGNEWS—and analyze the characteristics that contribute to prompt efficacy, revealing insights such as the importance of topic relevance and calibration of output labels. Additionally, the paper explores the mechanics of prompt tuning, focusing on human readability and efficiency, and introduces an unsupervised approach for generating prompts without labeled data.

### Strengths and Weaknesses
Strengths:
- The proposed FLUENTPROMPT method enhances interpretability and effectiveness in prompt design.
- The analysis of factors contributing to prompt success is thorough and insightful, emphasizing topic relevance and calibration for label bias.
- The manuscript includes extensive experiments, demonstrating the method's performance against baselines and consistently outperforming them.
- The exploration of prompt attributes is expected to inspire future research in the field.

Weaknesses:
- The rationale for substituting SGD with Langevin dynamics is insufficiently motivated.
- The analysis of effective prompts lacks robustness, particularly in quantitative assessments.
- The experimental setup is limited to a single model (GPT-2), which raises questions about the generalizability of the findings.
- The 'human-readable' prompts sometimes lack clarity, raising concerns about their interpretability.
- The focus on classification problems may limit generalizability to other NLP tasks.
- The paper primarily compares FLUENTPROMPT with a few baselines, prompting requests for broader comparisons, including continuous soft prompt tuning and other discrete techniques.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the method description, particularly regarding Langevin dynamics, to ensure that readers without prior knowledge can follow along. Additionally, enhancing the clarity of the 'human-readable' prompts would improve interpretability. We suggest reorganizing section 3.2 to clarify the distinction between supervised and unsupervised models and to include more details about the data and loss used. To strengthen the analysis, the authors should include a broader range of baseline comparisons, including continuous soft prompt tuning, and conduct an ablation study to elucidate the contributions of different components of their method. Finally, considering additional models beyond GPT-2 in the experiments would enhance the generalizability of the findings, and addressing reproducibility concerns by providing code and data would significantly enhance the paper's credibility.