ID: GnAfyR8AhC
Title: Towards Calibrated Robust Fine-Tuning of Vision-Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 4, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for robust fine-tuning of Vision Language Models (VLMs) to enhance out-of-distribution (OOD) accuracy and calibration. The authors propose a constrained multimodal contrastive loss framework, incorporating a singular-based constraint term, self-distillation, and exponential moving average (EMA). Theoretical insights reveal that OOD calibration and classification errors can be bounded by in-distribution (ID) calibration errors and the smallest singular value of the ID input covariance matrix. The authors also demonstrate that their method, CaRot, is effective not only for VLMs but also for vision-only models, showcasing its versatility. Extensive experiments on synthesized data and ImageNet benchmarks validate the effectiveness of CaRot in improving OOD performance and reliability, with significant improvements in expected calibration error (ECE) that are not merely a byproduct of accuracy enhancements.

### Strengths and Weaknesses
Strengths:
- The investigation into the calibration of CLIP after fine-tuning addresses an under-explored yet significant problem.
- The theoretical analysis linking the smallest singular value of image representation to OOD robustness is insightful.
- The authors effectively address reviewer concerns, enhancing the manuscript's clarity and quality.
- The theoretical contributions are broad and applicable beyond VLMs, showcasing the versatility of the proposed method.
- The paper is well-organized, with clear presentation and extensive empirical validation demonstrating the method's effectiveness.

Weaknesses:
- The overall framework lacks strong motivation; the relevance of self-distillation and EMA to the primary theoretical analysis is unclear.
- The connection to Vision-Language Models is weak, raising questions about the specificity of the proposed soft constraint term to CLIP.
- Result analysis is insufficient, particularly the absence of average confidence level reporting, which may mislead interpretations of calibration improvements.
- There is a potential misunderstanding regarding the relationship between accuracy and calibration, which the authors attempt to clarify but may still require further elaboration.
- Some notations are misleading, and certain assumptions, such as the one-dimensional function hypothesis, are unrealistic.
- The orthogonal constraint may be too strong, potentially compromising ID accuracy while not sufficiently addressing feature diversity.
- The presentation of results could benefit from clearer differentiation between the contributions of CaRot and other methods.

### Suggestions for Improvement
We recommend that the authors improve the motivation for incorporating self-distillation and EMA, clarifying their relevance to the theoretical framework. Additionally, strengthening the connection between the proposed method and Vision-Language Models, particularly CLIP, would enhance the paper's impact. The authors should also provide average confidence level metrics in their results to clarify the calibration improvements. Addressing misleading notations and unrealistic assumptions in the theoretical analysis is essential. Furthermore, we suggest improving the clarity of the relationship between accuracy and calibration in the manuscript, possibly by providing additional examples or a more detailed discussion of the counter-examples referenced. Finally, refining the presentation of empirical results to distinctly highlight the contributions of CaRot compared to other methods would enhance reader comprehension.