ID: VzmpXQAn6E
Title: Exposing Attention Glitches with Flip-Flop Language Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 9, 8, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Flip-Flop Language Modeling (FFLM) to investigate closed-domain hallucinations in Large Language Models (LLMs). FFLM is a synthetic benchmark that simulates a single bit of memory with operations: read (r), ignore (i), and write (w). The authors find that Transformers exhibit long-tail reasoning errors, termed "attention glitches," which are difficult to mitigate, while LSTMs perform the task perfectly with significantly less training. The authors also explore attention sharpening's effects on training Transformers, noting that while it can help, it may also introduce new errors.

### Strengths and Weaknesses
Strengths:
- The paper provides significant insights into Transformer architecture and its reasoning errors.
- It identifies that Transformers struggle with long-range dependencies, while LSTMs excel in this simple task.
- The comprehensive experiments cover various training and architectural adjustments, leading to a useful analysis of LLM limitations.
- The writing is clear, and the figures are well-explained.

Weaknesses:
- The study is limited to flip-flop languages, which may restrict the generalizability of findings.
- Some implications and connections to existing literature are not sufficiently highlighted.
- The paper lacks a clear solution to the identified errors and does not explore the proposed hypotheses in depth.
- Experimental settings, such as dataset construction and tokenizer choice, are not clearly defined.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the implications of their findings in relation to existing literature, particularly Ji et al., 2023. Additionally, it would be beneficial to provide clearer experimental settings and a more detailed exploration of the proposed hypotheses in Section 4.1. We suggest including quantitative results in Section 5.3 and clarifying the notation used, such as introducing $\Delta$ in probability discussions. Finally, we encourage the authors to explore the potential of extending the task to more values and to consider alternative regularization techniques for attention sharpening.