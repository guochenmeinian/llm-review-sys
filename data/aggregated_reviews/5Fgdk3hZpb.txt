ID: 5Fgdk3hZpb
Title: Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 5, 6, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new framework for dataset condensation called Squeeze, Recover, and Relabel (SRe2L), which decouples the optimization of model training and synthetic data generation. The authors propose a three-step approach: training a model on the original dataset, generating synthetic data from a pretrained model, and relabeling this data using a crop-level scheme. Extensive experiments demonstrate significant performance improvements over state-of-the-art methods, particularly on large-scale datasets like ImageNet.

### Strengths and Weaknesses
Strengths:
1. The writing is clear and easy to understand.
2. The framework shows generalizability across varying dataset scales, input resolutions, and network architectures.
3. The experimental methodology is robust, with well-designed ablations that justify architectural choices and provide insights into the relabeling process.
4. The empirical comparisons with state-of-the-art methods demonstrate strong performance and reduced computational and memory requirements.

Weaknesses:
1. The claims regarding "Cross-Architecture Generalization" are unclear, particularly the performance disparity observed in different architectures.
2. There is a perceived lack of strong technical novelty, as the method appears to combine existing techniques without substantial innovation.
3. Limited theoretical analysis is provided, which could enhance understanding of the proposed method's principles and limitations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the "Cross-Architecture Generalization" section to address the observed performance gaps across different architectures. Additionally, we suggest incorporating a more comprehensive theoretical analysis, including error bounds or performance guarantees, to strengthen the paper's contributions. Finally, we encourage the authors to conduct further ablation studies, particularly regarding the impact of varying dataset sizes and the number of classes, to provide deeper insights into the method's applicability.