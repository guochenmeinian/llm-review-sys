ID: AF32GbuupC
Title: Fast Graph Sharpness-Aware Minimization for Enhancing and Accelerating Few-Shot Node Classification
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 5, 8, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an innovative approach to enhance the generalization capabilities of Graph Neural Networks (GNNs) in few-shot node classification tasks through a novel algorithm, Fast Graph Sharpness-Aware Minimization (FGSAM). The authors incorporate sharpness-aware minimization (SAM) techniques into GNN training while reducing computational overhead by utilizing multilayer perceptrons (MLPs). The method demonstrates superior performance on few-shot learning tasks and offers significant computational advantages, particularly in training time reduction.

### Strengths and Weaknesses
Strengths:  
1. The integration of SAM with GNNs, removing message passing during training and reintroducing it during inference, is a compelling approach to improve generalization in few-shot node classification.  
2. The proposed method can be easily integrated into existing GNN-based few-shot node classification models, as verified by experiments.  
3. The experiments effectively highlight the method's capability to reduce computational costs in GNN training.  
4. The paper is well-organized, clearly explaining complex ideas, and includes helpful visualizations and analyses.

Weaknesses:  
1. The innovation is not significantly distinct from related works on applying SAM to few-shot tasks, such as Sharp-MAML and performing SAM every $k$ steps.  
2. The experiments primarily compare with small-sample node classification models and variants of SAM from 2022, lacking comparisons with the latest baseline methods in few-shot node classification.  
3. Some experiments are missing, and the proof needs careful verification.  
4. Certain sections could benefit from style improvements for clarity.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for PeerMLP and clarify the proof of theorem 4.1, particularly regarding the assumption about node degrees. Additionally, the authors should provide standard deviations for the results in Table 4 and discuss the significance of the improvements. A comparison with state-of-the-art FSNC methods, such as COSMIC and COLA, should be included in both theoretical analysis and experiments. Furthermore, the authors should explain why adding 25% noisy edges yields better results than adding only 15% in Figure 4. Lastly, we suggest refining the presentation of certain paragraphs and ensuring that the best results in Table 8 are correctly highlighted.