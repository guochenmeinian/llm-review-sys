ID: 4iMpwAlza1
Title: Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 7, 6, 7, 8, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents EMBROID, a method aimed at enhancing language model predictions through prompt patching by leveraging neighboring instances retrieved via embedding models like BERT or RoBERTa. The authors propose a combination of original predictions and neighborhood predictions, integrating them through a majority voting mechanism. Theoretical analyses support the method's efficacy, and empirical results demonstrate consistent improvements in binary classification tasks, outperforming existing techniques such as Chain-of-thought prompting.

### Strengths and Weaknesses
Strengths:
- The method is simple and empirically validated, showing strong performance.
- The theoretical analysis effectively supports the empirical findings.
- The combination of Chain-of-thought and prompt patching yields significant improvements.
- Extensive empirical evaluations and robust ablation studies are included.

Weaknesses:
- The novelty is moderate, as the approach builds on existing weak supervision methods and faces limited technical challenges.
- Evaluation is restricted to binary classification tasks, with no exploration of multi-class or generative tasks.
- The method relies on multiple embedding models, and the paper lacks a discussion on self-consistency methods.
- The writing in Sec. 6.2 could be clearer regarding the differences between Embroid-1 and Embroid-3 compared to baselines.

### Suggestions for Improvement
We recommend that the authors improve the clarity in Sec. 6.2 to better delineate how Embroid-1 and Embroid-3 differ from the baselines in terms of prompt usage. Additionally, expanding the evaluation to include multi-class classification and generative tasks would enhance the paper's applicability. Providing a theoretical analysis of the smoothness of embedding functions and discussing selection strategies for embeddings would also strengthen the contribution. Finally, evaluating EMBROID on state-of-the-art models like GPT-4 could provide insights into the robustness of the method across different architectures.