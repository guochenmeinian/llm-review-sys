ID: oYs7h2dE2e
Title: CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for adapting black-box large language models (LMs) to new tasks and domains by integrating a smaller white-box LM, which is fine-tuned on a domain-specific corpus. The adaptation is achieved through a combination function that merges the probability distributions of both models, evaluated across various domains and tasks, including machine translation. The method demonstrates significant performance improvements while requiring less computational resources than full fine-tuning.

### Strengths and Weaknesses
Strengths:
- The proposed method allows interpretable adaptation of black-box LMs without needing access to their internal weights or activations.
- It explores various combination functions, providing a comprehensive overview of adaptation techniques.
- The paper is well-written, reproducible, and offers practical value in leveraging large LMs.

Weaknesses:
- The method is only evaluated in English, limiting its applicability to other languages.
- It assumes that the black-box and white-box LMs share the same tokenizer, which may not be true for all models.
- There is a lack of theoretical analysis regarding the combination functions, such as their convergence or stability.
- Reproducibility is hindered due to insufficient details and reliance on data not available outside the authors' institution.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their method by evaluating its performance on non-English languages. Additionally, providing a theoretical analysis of the combination functions would strengthen the paper. Addressing the tokenizer assumption and enhancing the reproducibility by including more detailed methodologies would also be beneficial.