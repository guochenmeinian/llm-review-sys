ID: mKuH13Oq3x
Title: Adaptive Gating in Mixture-of-Experts based Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an adaptive gating mechanism in a Mixture of Experts (MoE) training scheme, allowing tokens to be processed by a flexible number of experts based on expert probability distribution. The authors propose using top-2 experts only when the difference between the highest and second-highest expert probabilities is below a threshold, which optimizes computational efficiency while maintaining performance. The results indicate improved training efficiency without compromising inference performance, particularly with the integration of curriculum learning.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and clear, with a solid experimental setup across multiple NLP tasks.  
- The proposed adaptive gating mechanism is intuitive and shows promise in improving training time and inference performance.  
- The analysis and ablation study provide valuable insights into model design choices.  

Weaknesses:  
- The requirement for a sophisticated curriculum learning schedule complicates the training pipeline, making it less plug-and-play.  
- The novelty of the approach is questionable, as it appears to combine existing techniques without sufficient original contribution.  
- Performance improvements over baselines are relatively small, and the paper lacks sufficient experimental depth and variety in baseline models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of expert initialization, specifically how parameters are split among experts. Additionally, we suggest providing more detailed explanations of findings in Section 3.1 and Figure 1. The authors should consider restructuring Sections 2 and 4 for better clarity and organization. Finally, we encourage the inclusion of more baseline models to evaluate training time, FLOPs, and inference performance comprehensively.