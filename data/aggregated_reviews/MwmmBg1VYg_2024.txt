ID: MwmmBg1VYg
Title: Why are Visually-Grounded Language Models Bad at Image Classification?
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 3, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the underperformance of Vision-Language Models (VLMs) in image classification tasks compared to models like CLIP. The authors explore various hypotheses, including training objectives and data-related issues, concluding that the lack of sufficient class labels in instruction-tuning data is a primary factor. They propose incorporating classification-specific datasets, such as ImageNet, into VLM training to enhance performance, demonstrating an 11.8% improvement on the ImageWikiQA dataset after fine-tuning.

### Strengths and Weaknesses
Strengths:
- The paper provides a thorough analysis of the performance issues of current VLMs in image classification, filling a significant research gap.
- It introduces a new dataset, ImageWikiQA, to evaluate VLMs' question-answering capabilities related to fine-grained image classes.
- The structured approach and empirical studies effectively support the authors' conclusions about the importance of data in improving VLM performance.

Weaknesses:
- The novelty of the contributions is limited, primarily focusing on fine-tuning existing models rather than introducing fundamentally new methodologies.
- The paper lacks sufficient analysis of proprietary VLMs and does not clarify the evaluation protocols used in various sections.
- Some experimental results, such as the performance of fine-tuned models compared to zero-shot models, raise questions about the effectiveness of the proposed solutions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their conclusions regarding the reasons for VLM underperformance, specifically distinguishing between data limitations and decoding issues. Additionally, addressing the evaluation protocols more explicitly throughout the paper would enhance understanding. The authors should also provide a more detailed analysis of proprietary VLMs and clarify the discrepancies in performance metrics presented in various tables. Finally, exploring the impact of multiple textual labels for ImageNet classes on VLM accuracy could strengthen the findings.