ID: 2DtxPCL3T5
Title: Learning to Compress Prompts with Gist Tokens
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "gisting," a method for compressing instruction prompts into smaller sets of compressed context, utilizing special '<GIST>' tokens. The authors propose that this approach saves context window space and computational resources. The experimental results indicate that gisting can effectively represent instruction prompts with only marginal performance loss, achieving a 26x compression rate and a 40% reduction in FLOPs.

### Strengths and Weaknesses
Strengths:
1. The idea of compressing instruction prompts is novel and has potential real-world applications.
2. The implementation is straightforward, requiring only minor modifications to attention masks.
3. The experiments are well-designed, utilizing comprehensive evaluations through ROUGE-L, ChatGPT, and human assessments, with consistent results.
4. The paper is clearly written, making it easy to follow.

Weaknesses:
1. The evaluation scheme raises concerns about the reliability of human evaluators, as low agreement scores suggest variability in assessments.
2. The method's 4% wall-clock time reduction is not significant, especially given the accuracy trade-offs.
3. The compression is lossy, and the fixed number of gist tokens does not allow for dynamic adjustments based on task requirements.
4. Some descriptions in the paper may mislead readers regarding the nature of gist tokens and activations.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the working mechanisms of gist tokens, particularly in relation to unseen prompts and the number of gist tokens. Addressing the evaluation concerns by providing results on datasets with objective answers would enhance the credibility of the findings. Additionally, clarifying the distinction between gist tokens and activations in the paper is essential to avoid confusion. Lastly, we encourage the authors to explore further optimizations to achieve more significant speedup beyond the current 4%.