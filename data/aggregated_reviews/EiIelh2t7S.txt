ID: EiIelh2t7S
Title: Base of RoPE Bounds Context Length
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 8, 6, 6, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the role of Rotary Position Embedding (RoPE) in Large Language Models (LLMs), focusing on the relationship between RoPE's base and the model's long-context ability. The authors derive a theoretical and empirical lower bound for the RoPE base necessary for maintaining long-context capabilities, validated through experiments with models like Llama2-7B and Baichuan2-7B. The study reveals that smaller RoPE bases may lead to superficial long-context abilities, achieving low perplexity but failing to retrieve relevant information in extended contexts. The paper establishes two desiderata for attention mechanisms in language modeling and introduces a novel property of long-term decay.

### Strengths and Weaknesses
Strengths:
1. The research topic is theoretically important and practically relevant, addressing the under-explored issue of long-context capabilities.
2. The relationship between the RoPE base and long-context ability is insightful, with the proposed lower bound being novel and significant.
3. The experiments are comprehensive and robust, supporting the paper's claims effectively.
4. The presentation is clear and well-organized, making the content accessible.
5. The findings are inspiring for the development of long-context LLMs.

Weaknesses:
1. The motivation section lacks clarity and organization, presenting previous empirical observations without deep discussion.
2. The empirical verification of the desideratum regarding similar tokens receiving more attention is necessary, as it is fundamental to Theorem 1.
3. The caption of Figure 6 should include a detailed derivation of the dotted line value.
4. The authors should consider releasing the trained models to benefit future research.
5. Clarification is needed regarding the interpretation of negative and positive values in equations (8) and (9).
6. Section 5.3 should be regarded as a conjecture rather than a definitive explanation.
7. Minor inconsistencies in title casing and terminology should be addressed.

### Suggestions for Improvement
We recommend that the authors improve the motivation section by providing a clearer and more organized discussion of previous empirical observations. Additionally, we suggest that the authors extensively test the model using benchmarks such as RULER and provide more empirical observations on the relationship between the RoPE base and model performance on these benchmarks. Furthermore, it would be beneficial to clarify the derivation in Figure 6 and address the interpretation of attention values in equations (8) and (9). Lastly, we encourage the authors to release their trained models to facilitate further research in this area.