ID: hcOq2buakM
Title: BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 7, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an assessment framework called BetterBench to evaluate the quality of AI benchmarks, introducing a comprehensive approach that includes a checklist of 40 criteria based on expert interviews and literature surveys. The authors apply this framework to 40 benchmarks, revealing significant quality differences and issues, particularly in the reporting of statistical significance. The paper aims to guide developers in improving benchmark quality and offers a living repository for continuous updates.

### Strengths and Weaknesses
Strengths:
- The proposed assessment rubric is a potentially valuable tool for the ML community, promoting adherence to best practices.
- The authors consulted a diverse group of stakeholders, including policymakers and model users, ensuring a broad perspective on benchmark usability.
- The methodology is well-explained, supported by effective figures.

Weaknesses:
- The paper lacks a discussion on dataset documentation practices and data curation considerations, which are crucial for benchmark usability.
- The approach conflates benchmarks with their associated leaderboards and does not clearly differentiate between benchmark datasets and reported results, leading to confusion.
- Some criteria may set unrealistically high standards, limiting practical utility.

### Suggestions for Improvement
We recommend that the authors improve the discussion on dataset documentation practices and data curation considerations, incorporating principles such as the FAIR guidelines and persistent identifiers for datasets. Additionally, we suggest providing more details on the stakeholder feedback process to clarify how it informed the benchmark lifecycle model. Expanding the criteria related to data documentation in Section 4.3 is also essential, as this area is critical for benchmark assessment. Finally, we encourage the authors to clarify the distinction between benchmark creation and deployment/usage/reporting to enhance the framework's clarity and utility.