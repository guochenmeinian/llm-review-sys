ID: XX73vFMemG
Title: Co-training and Co-distillation for Quality Improvement and Compression of Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework called Co-Training and Co-Distillation (CTCD) for knowledge distillation, which aims to improve model performance while compressing large pre-trained language models (PLMs). The authors propose that co-training a large teacher model and a small student model facilitates bidirectional knowledge transfer, leading to enhanced performance. Experimental results on the GLUE benchmark indicate that the distilled small model outperforms the original larger model by 1.66, and findings suggest that distilling knowledge from the smaller model to the larger model improves the latter's performance, which in turn benefits the smaller model.

### Strengths and Weaknesses
Strengths:
- The paper addresses a critical limitation of existing knowledge distillation methods by proposing a bi-directional approach that allows mutual knowledge transfer.
- Extensive ablation studies validate the effectiveness of CTCD, demonstrating its potential to outperform traditional one-way distillation methods.
- The framework is well-written and presents a novel idea that could benefit the community.

Weaknesses:
- Concerns exist regarding the source of performance improvements, particularly whether they stem from the distillation process or hyperparameter settings.
- The selection of teacher and student models appears unconventional, with suggestions for more standard configurations.
- Performance gains are perceived as marginal compared to other established distillation techniques, and evaluations are limited to BERT models.

### Suggestions for Improvement
We recommend that the authors improve the empirical analysis by providing additional ablation studies or theoretical insights to clarify that performance gains are due to the distillation approach rather than hyperparameter settings. Additionally, we suggest exploring more conventional configurations for the teacher and student models, such as using a 12-layer BERT as the teacher. Furthermore, including implementation details for the fine-tuning phase and conducting experiments to assess the framework's effectiveness during a fine-tuning-only stage would enhance the paper's robustness. Lastly, testing on a broader range of PLMs beyond BERT would strengthen the findings.