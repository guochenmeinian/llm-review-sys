ID: aSfLEeRn4L
Title: Self-Comparison for Dataset-Level Membership Inference in Large (Vision-)Language Model
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for Dataset-level Membership Inference (MIA) in Large Language Models (LLMs) through a novel mechanism called Self-Comparison. The authors propose that by analyzing distribution shifts after paraphrasing validation datasets, Self-Comparison Membership Inference (SMI) can effectively determine if a dataset was used in training. The method achieves high F1 scores across various LLMs and datasets, demonstrating its effectiveness compared to existing MIA techniques.

### Strengths and Weaknesses
Strengths:
1. SMI is a simple and efficient method that does not require access to ground-truth member or non-member data from identical distributions.
2. The evaluation is comprehensive, covering various datasets and models, and includes comparisons against both pre-trained and fine-tuned models.
3. The methodology is well-explained, with clear experimental backing and a detailed introduction to preliminary studies.

Weaknesses:
1. The paper lacks formal justification for the assumption that paraphrasing enhances distribution shifts for non-member data, relying solely on empirical analysis.
2. Insufficient details regarding experimental settings and implementation environment, including GPU specifications and runtime duration.
3. The relationship between dataset-level MIA and sample-level MIA is not formally discussed, leading to potential confusion.

### Suggestions for Improvement
We recommend that the authors improve the formal justification for the assumption regarding paraphrasing and its impact on distribution shifts by providing proofs or solid theoretical backing. Additionally, please clarify the experimental settings in Section 3.3 to avoid confusion and include more dataset-level MIA baselines in the comparison experiments to validate the proposed method's performance. It would also be beneficial to elaborate on the relationship between dataset-level MIA and sample-level MIA to enhance clarity. Finally, consider addressing the writing style to maintain objectivity and provide a clearer definition of protected data.