ID: ZQMlfNijY5
Title: Normalizing flow neural networks by JKO scheme
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 8, 7, 7, 7, -1, -1, -1
Original Confidences: 3, 3, 2, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to train continuous normalizing flows and score-based diffusion models by leveraging the Wasserstein gradient flow theory and the JKO iterative scheme. The authors propose to approximate the density evolution of the variance-preserving forward dynamics of a diffusion model using deterministic transport maps, leading to invertible dynamics that can map the stationary distribution back to the data distribution. The training method differs significantly from traditional score-based diffusion models, as the score is trained indirectly through a deterministic algorithm.

### Strengths and Weaknesses
Strengths:  
- The work is original and introduces a new training method for score-based models, supported by robust theoretical foundations.  
- The paper is well-explained, despite its mathematical complexity, which adds value.  
- The experimental section is comprehensive, showing good results on low-dimensional datasets and convincing outcomes on images, although further exploration in generative computer vision is acknowledged as necessary.  
- The method is computationally efficient and flexible, utilizing neural ODEs with small step sizes for invertibility.  

Weaknesses:  
- The image generation experiments could be expanded, particularly by including larger datasets like CelebA HD and ImageNet.  
- The evaluation metrics should include FID scores, as they are standard in image generation literature and more reliable than BPD scores.  
- The exposition would benefit from a more detailed connection between the JKO approach and standard score-matching diffusion theory to aid readers from the diffusion literature.  
- The proposed method struggles with generating high-quality image samples, and the variety of experiments could be richer, particularly in multidimensional probabilistic regression and latent space applications.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including larger datasets such as CelebA HD and ImageNet to enhance the scope of image generation results. Additionally, we suggest complementing the evaluation metrics with FID scores for a more reliable assessment of image quality. It would also be beneficial to extend the exposition connecting the JKO approach to standard score-matching diffusion theory to better inform readers from the diffusion literature. Lastly, we encourage the authors to explore the performance of their method on larger datasets and in various applications, such as inverse problems and probabilistic regression scenarios.