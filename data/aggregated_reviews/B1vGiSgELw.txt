ID: B1vGiSgELw
Title: Matryoshka Query Transformer for Large Vision-Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Matryoshka Query Transformer (MQT), which integrates Matryoshka information packing to enhance flexibility in visual tokens for multimodal vision language models like LLaVA. The authors demonstrate that reduced tokens can lower the quadratic complexity of language models while maintaining accuracy across various benchmarks. The paper includes extensive experiments and analyses, showcasing significant efficiency benefits.

### Strengths and Weaknesses
Strengths:
1. The paper is exceptionally well-written and easy to follow.
2. The integration of Matryoshka and Query transformer concepts, despite their prior existence, yields commendable benefits.
3. The modeling is clear, with well-detailed mechanisms.
4. Extensive experiments and analyses are presented.
5. Visualizations are well-executed, including TFLOPs measurements.
6. The analysis extends beyond benchmark numbers.

Weaknesses:
1. Questions remain regarding TFLOPs measurement and its components.
2. The impact of selecting a fixed number of tokens for MQT versus obtaining a dynamic number needs clarification.
3. The relationship between sampling and joint training requires further exploration.
4. The performance discrepancy in log-based spacing versus fine granularity is surprising and warrants explanation.
5. Clarification is needed on the meaning of "one random token" in visualizations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of TFLOPs measurement by detailing its components, including the vision encoder and LLM processing. Additionally, the authors should investigate and report the performance implications of selecting a dynamic number of tokens versus a fixed number. A deeper analysis of sampling versus joint training would also enhance the paper. Furthermore, addressing the surprising performance gap in log-based spacing compared to fine granularity is essential. Lastly, we suggest providing a clearer definition of "one random token" in the visualizations to avoid confusion.