ID: 4xDxVQHsbZ
Title: NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 7, 6, 6, -1, -1, -1
Original Confidences: 4, 4, 5, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents NoMAD-Attention, which utilizes SIMD registers in CPUs to accelerate LLM inference by replacing multiply-add operations with a lookup-table based alternative for the attention mechanism. The authors motivate the application well for Transformer inference on CPUs, showcasing significant speedup while maintaining performance. The paper also explores product quantization (PQ) to enhance memory access efficiency during deployment on GPUs.

### Strengths and Weaknesses
Strengths:
- Addresses an important problem of efficiency on CPUs, an understudied platform.
- Demonstrates significant speedup compared to standard attention mechanisms.
- The evaluation includes both upstream and downstream benchmarks, with clear motivation and improved quality from previous versions.

Weaknesses:
- ML benchmarks appear weak; evaluation on harder tasks like MT-Bench is recommended.
- Additional evaluation on recent open models (LLaMa-2, LLaMa-3, Gemma, QWEN) would enhance the paper's credibility.
- The clarity of certain technical aspects, such as the definition of d_sub and the implications of product quantization, is lacking.
- The performance degradation observed with larger sub-vector dimensions suggests limitations in the algorithm's applicability.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including harder benchmarks and more recent open models to strengthen their claims. Clarifying the definition of d_sub and its implications for product quantization is essential. Additionally, providing data on GPU latency for comparison and addressing the performance issues with larger sub-vector dimensions would enhance the paper's impact.