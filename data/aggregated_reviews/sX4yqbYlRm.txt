ID: sX4yqbYlRm
Title: Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on structured analogical reasoning in large language models (LLMs), moving beyond traditional word analogies. The authors propose a benchmark called Scientific Analogical Reasoning (ScAR), which includes 400 analogies across 13 domains and 1600 concept mappings, supplemented with background knowledge. The evaluation reveals that while LLMs perform well on simpler word analogy tasks, they often fail to grasp deeper structural connections, highlighting a gap between LLMs and human cognition. The main contributions are the ScAR benchmark and an analysis of LLMs' analogical reasoning capabilities.

### Strengths and Weaknesses
Strengths:
- The paper introduces a high-quality benchmark dataset that encompasses a diverse range of analogies, enriched with structural details and background knowledge.
- It provides a preliminary study that critiques the inadequacies of existing word analogy tasks, encouraging a more holistic approach to analogical reasoning in the community.
- The analysis of benchmark results is thorough, covering various prompt engineering aspects and exploring earlier representation-based approaches.

Weaknesses:
- The paper does not propose new modeling techniques for applying LLMs to analogy tasks, resulting in limited variation in benchmark results.
- There is insufficient exploration of the impact of pre-training data on LLMs' analogical reasoning capabilities.
- The linkage to human cognition is ambiguous, and methodological issues weaken the overall argument.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the use of Structure Mapping Theory (SMT) in the context of analogical reasoning, ensuring a clear distinction between its broad and narrow applications. Additionally, we suggest elaborating on the rationale for using the E-KAR dataset, particularly its cultural implications, and addressing the extensive use of GPT-4 in dataset creation. Providing more details about annotators and their tasks in the Ethics Statement would enhance transparency. Furthermore, we encourage the authors to clarify the embedding technique used in Section 2 and to explore the impact of LLM pre-training data on analogical reasoning more rigorously. Lastly, consistent y-axis bounds in figures and the inclusion of error bars would facilitate better comparisons in the results.