ID: motImXq3B1
Title: P$^2$C$^2$Net: PDE-Preserved Coarse Correction Network for efficient prediction of spatiotemporal dynamics
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the P^2C^2Net framework designed for efficiently solving and simulating partial differential equations (PDEs) on coarse grids using minimal training data. The architecture includes a trainable PDE block, a neural network block, a symmetric convolutional filter for estimating spatial derivatives, and a Poisson block for pressure field computation. The authors demonstrate the framework's effectiveness through extensive experiments on classical benchmark problems, achieving superior accuracy compared to existing baselines.

### Strengths and Weaknesses
Strengths:
- The P^2C^2Net framework integrates physics knowledge, enhancing interpretability and generalizability with limited data.
- The architecture shows impressive accuracy gains over baselines, particularly in long-term predictions and out-of-distribution tests.

Weaknesses:
- The necessity and interconnections among the various components of the architecture are not clearly justified, raising questions about the model's complexity.
- The training efficiency and size of the proposed model compared to baselines remain unclear, making it difficult to ascertain the source of its superior performance.
- The data generation process lacks clarity, particularly regarding the initial conditions used for training and testing datasets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the connections among the components of the P^2C^2Net framework and justify the inclusion of each module. Additionally, providing a direct comparison of model sizes and training times with baseline models would clarify the efficiency of the proposed architecture. It is crucial to elaborate on the data generation process, including the initial conditions for training samples, to support claims of generalization. Finally, we suggest conducting ablation studies to verify the contributions of individual components, such as the necessity of the Poisson block and the differences between the correction block and the neural network block.