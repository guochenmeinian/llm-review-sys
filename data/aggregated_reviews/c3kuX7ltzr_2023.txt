ID: c3kuX7ltzr
Title: FORB: A Flat Object Retrieval Benchmark for Universal Image Embedding
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 6, 6, 6, 9, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Flat Object Retrieval Benchmark (FORB), a dataset designed for benchmarking visual search methods on flat images featuring diverse patterns. It includes eight types of flat objects and introduces a new evaluation metric, termed t-mAP, to assess image embedding quality. The authors conduct a comprehensive analysis of various baseline methods, demonstrating that some generalize well to this new dataset. They clarify the potential overlap of FORB with existing datasets, particularly LAION-438M and LAION-5B, and provide an estimation of the duplication rates, asserting that this overlap has minimal impact on performance evaluations. The writing is clear, and the experiments are well-executed.

### Strengths and Weaknesses
Strengths:
- The introduced dataset is a valuable asset to the community.
- The proposed metric t-mAP is logically sound.
- The extensive study of baseline methods is thorough and beneficial.
- The authors provide a thorough analysis of potential dataset overlaps and their implications for performance evaluations.
- The inclusion of a new comparative table enhances the understanding of FORB in relation to other datasets.
- The paper is well-written and organized, with additional analysis and clarifications addressing reviewer concerns.

Weaknesses:
- The comparison of methods trained on different datasets raises concerns about fairness and the validity of conclusions drawn from their performance on FORB.
- The comparison among different feature extractors is deemed unfair due to variations in training datasets and model architectures.
- The definition of "heterogeneity" is unclear and requires clarification for readers unfamiliar with the term.
- Details regarding data collection, annotation consistency, and data splits are insufficient.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the term "heterogeneity" to enhance reader understanding. Additionally, we suggest that the authors add a discussion in the limitations section regarding the fairness of comparisons among feature extractors trained on different datasets, as this is a significant concern raised by reviewers. It would also be beneficial to provide a comparative table of existing datasets for image retrieval to contextualize FORB's significance. The authors should augment Table 2 with more detailed information about the training datasets, including the number of training images, to address concerns regarding the fairness of method comparisons. Furthermore, the authors should clarify the criteria used for determining the difficulty levels of images and provide a clear definition of distractor images, possibly with examples. Lastly, we suggest that the authors consider including their own baseline method in future work to strengthen the evaluation framework.