ID: yXpfrLMIr2
Title: Binarized Diffusion Model for Image Super-Resolution
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 8, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents BI-DiffSR, a novel binarized diffusion model aimed at enhancing the efficiency of image super-resolution (SR). The authors propose a UNet architecture optimized for binarization, incorporating consistent-pixel downsampling/upsampling and channel-shuffle fusion to resolve dimension mismatch and fusion challenges. Additionally, they introduce timestep-aware redistribution (TaR) and a timestep-aware activation function (TaA) to adapt to varying activation distributions across timesteps. The model achieves state-of-the-art performance in super-resolution while significantly reducing memory and computational costs.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, organized, and easy to understand.
- It introduces a novel 1-bit UNet for accurate binarized diffusion models, featuring innovative modules such as CP-Down, CP-Up, and channel shuffle fusion.
- The proposed methods demonstrate significant performance improvements over existing binarization techniques, achieving results comparable to full-precision models.
- Comprehensive experiments validate the effectiveness of the proposed model, including real inference time evaluations on edge hardware.

Weaknesses:
- The basic BI-Conv block lacks novelty, resembling the binarized module in ReActNet.
- The rationale behind TaR's different parameters for various timesteps is unclear, particularly in relation to the normal time embedding.
- The choice of SR3 as a baseline is questioned, with suggestions to consider ResShift and SinSR for more relevant comparisons.
- The paper does not adequately address the computational demands of self-attention and MLP modules in diffusion models, raising concerns about the method's extensibility.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing and presentation, particularly in the grammar and description of fundamental concepts. Additionally, we suggest providing a more detailed breakdown of the computational efficiency of each network component before and after binarization to better illustrate the advantages of the proposed method. Further discussion on the proposed challenges, including visual and quantitative analyses, would enhance the depth of the paper. Lastly, we encourage the authors to compare their work with recent binarization methods and discuss the differences to provide a more comprehensive evaluation.