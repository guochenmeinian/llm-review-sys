ID: siiVduxdRz
Title: Condensing Multilingual Knowledge with Lightweight Language-Specific Modules
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for efficient multilingual models, introducing Language Specific Matrix Synthesis (LMS) as a parameter-efficient alternative to language adapters. LMS decomposes the weight matrix into two low-rank matrices, which can be language-specific or language-pair specific. The authors also propose Fuse Distillation (FD) to distill knowledge from language-specific modules into a shared module, enhancing efficiency. Experimental results indicate that language-pair specific LMS outperforms language-specific LMS, and LMS-FD-Shared surpasses Naive NMT, suggesting that fused distillation acts as a regularizer during multilingual training.

### Strengths and Weaknesses
Strengths:
- The proposed techniques, LMS and FD, are innovative and show promising results in improving efficiency and performance in multilingual tasks.
- The paper is well-written, making it accessible and potentially easy to replicate, with thorough analyses supporting its claims.

Weaknesses:
- The training details for the different model variants are insufficiently documented, which could hinder reproducibility.
- The authors do not provide comparisons with relevant baseline methods, which undermines the claims of significant performance improvements.
- There is a lack of clarity regarding the computation of inference parameters and the distinction between language-specific and multilingual inference.

### Suggestions for Improvement
We recommend that the authors improve the documentation of training details for the various model configurations to enhance reproducibility. Clarifying how inference parameters are computed, particularly in distinguishing between language-specific and multilingual scenarios, would also be beneficial. Additionally, we suggest that the authors include comparisons with other parameter-efficient methods, such as language-specific or language-pair specific adapter layers, to strengthen their claims. Reporting total FLOPs for training and inference would provide a more comprehensive view of efficiency. Lastly, we encourage the authors to address the minor typographical errors and ensure all relevant results are presented clearly in the paper.