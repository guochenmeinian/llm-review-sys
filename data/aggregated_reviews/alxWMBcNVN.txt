ID: alxWMBcNVN
Title: Personalized Distillation: Empowering Open-Sourced LLMs with Adaptive Learning for Code Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a personalized distillation method that enables knowledge transfer from closed-source large language models (LLMs) to smaller open-source LLMs. The student model first attempts to solve tasks, after which the teacher model provides adaptive refinements. The authors claim that their method significantly outperforms traditional distillation approaches using only one-third of the data. However, the focus is primarily on code generation, raising questions about the method's generalizability to other tasks.

### Strengths and Weaknesses
Strengths:
- The proposed method is innovative and demonstrates solid experimental results in the code generation domain.
- The writing is clear, and the experimental design is comprehensive, effectively validating the motivation for personalized learning.

Weaknesses:
- The novelty of the approach is questioned, as training with feedback is common in various research fields, including reinforcement learning.
- The focus on basic programming problems limits the scope, and the lack of publicly available training data hinders reproducibility.
- Some experimental results lack clarity, particularly regarding comparisons and improvements over existing methods.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their method by evaluating it on more complex programming problems beyond basic tasks. Additionally, clarifying the novelty of their approach in relation to existing feedback-based training methods would strengthen the paper. We also suggest providing more detailed experimental data to support claims, particularly regarding the overlap of training and test data, and ensuring that all results are accurately represented and explained. Finally, addressing the questions regarding the multi-step inference process and the training data acquisition would enhance the paper's rigor.