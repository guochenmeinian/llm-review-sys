ID: iykao97YXf
Title: Reinforcement Learning with LTL and $\omega$-Regular Objectives via Optimality-Preserving Translation to Average Rewards
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 7, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical framework for learning $\omega$-regular properties in unknown MDPs through average reward optimization using an "optimality preserving" approach. The authors propose a method that allows for multi-chain MDPs by formalizing properties through a deterministic Rabin automaton and constructing an auxiliary model (a product MDP) that integrates the automaton's state space and transitions into the original MDP. The resulting reward machine, which can be constructed on the fly, enables optimization through reinforcement learning (RL). The authors demonstrate that optimizing the discounted return objective reduces to optimizing the average reward objective, leading to convergence guarantees via a sequential algorithm that calls any PAC-MDP algorithm for discounted return.

### Strengths and Weaknesses
Strengths:  
- The authors address previously open questions regarding the learning of $\omega$-regular properties through limit average reward objectives in an optimality-preserving manner, which is not feasible with Markovian rewards.  
- The paper is well-written, with effective proof sketches that enhance reader comprehension.  
- It provides original theoretical tools for RL with $\omega$-regular objectives by linking them to limit-average rewards, thus allowing the application of existing methodologies to a broader context.

Weaknesses:  
- The paper may be challenging for readers unfamiliar with the concepts, particularly regarding the practical implications of detecting minimal end components (ECs) and the use of graph algorithms for this purpose.  
- There is a lack of empirical evaluation, leaving questions about the practical applicability of the proposed approach.  
- Some sections, such as the relationship between the choice of automata and the results, require further clarification.

### Suggestions for Improvement
We recommend that the authors improve the accessibility of the paper by providing more context on the practical aspects of detecting minimal ECs and discussing the feasibility of their approach in real-world scenarios. Additionally, we suggest including a discussion on how to implement Algorithm 1 in practice, as well as addressing the potential costs associated with running the transformations. Clarifying the relationship between the results and prior work would also strengthen the paper. Furthermore, we recommend explicitly stating the relevance of the discounted return objective to the average return in the main text, as this connection is crucial for reader understanding. Lastly, addressing minor typos and ensuring consistency in notation would enhance the overall quality of the manuscript.