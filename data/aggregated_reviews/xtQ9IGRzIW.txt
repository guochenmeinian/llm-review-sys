ID: xtQ9IGRzIW
Title: Faster Discrete Convex Function Minimization with Predictions: The M-Convex Case
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 5, 6, 7, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 1, 3, 4, 3, 4, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new method to accelerate M-convex function minimization using past predictions, a technique known as warm-start. The authors propose a framework that applies to Laminar, Box, and Nested classes of problems, improving time bounds through warm-starting. The theoretical performance guarantees are promising, with running times of O(n*eta) for Laminar and Nested, and O(n+log(n)*eta) for Box, where eta represents the L1-error of the prediction.

### Strengths and Weaknesses
Strengths:  
- The paper is well-structured and clearly written, with promising experimental results that validate the theoretical claims. The algorithms for rounding and steepest descent in the laminar case are particularly noteworthy.
  
Weaknesses:  
- The contributions are somewhat unclear due to the paper's theoretical density and broad scope. Focusing solely on the Laminar problem could enhance clarity. The framework appears to be a re-adaptation of existing work, limiting its originality. The experimental section lacks depth, with simplistic tests on synthetic data and insufficient comparison with methods lacking warm-starts.

### Suggestions for Improvement
We recommend that the authors improve clarity by concentrating on the Laminar problem and possibly including extensions in an appendix. Additionally, we suggest conducting more comprehensive experiments, including synthetic data comparisons and real-world applications, to substantiate the superiority of the proposed method. It would also be beneficial to provide a clearer explanation of the learning predictions methodology and to address the assumptions regarding the accuracy of predictions more thoroughly.