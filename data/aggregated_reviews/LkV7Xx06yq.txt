ID: LkV7Xx06yq
Title: MEGClass: Extremely Weakly Supervised Text Classification via Mutually-Enhancing Text Granularities
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel weakly supervised text classification method, MEGClass, which utilizes various granularities—words, sentences, and documents—to enhance classification performance. The authors propose an iterative feedback mechanism to construct pseudo-labeled training data, demonstrating the method's effectiveness through experiments on multiple benchmark datasets, achieving state-of-the-art results. The integration of multi-granularity information and the introduction of a regularized contrastive loss for updating contextualized document representations are key contributions.

### Strengths and Weaknesses
Strengths:
- The method is innovative and intuitively understandable, effectively utilizing multi-granularity information.
- The paper provides thorough qualitative and quantitative evaluations, showcasing significant performance improvements over existing weak supervision approaches.
- The writing is clear and the analysis of datasets, performance metrics, and ablation studies is well-justified.

Weaknesses:
- The paper lacks quantitative comparisons with chatGPT/LLMs in a zero-shot setting and presents results only on bert-base-uncased, necessitating studies with various LLMs for broader insights.
- The motivation behind the research is not adequately articulated, particularly in the toy example presented, leading to potential confusion.
- The schematic representation of the model is unclear and lacks proper academic standards, with some components appearing unreferenced.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the model's schematic representation and ensure all components are properly referenced. Additionally, the authors should provide a more compelling and intuitive example in Section 1 to better articulate the motivation behind their research. It would also be beneficial to include quantitative results comparing their method with chatGPT/LLMs and to extend the experimental evaluation to fine-grained datasets like DBpedia and 20News. Finally, we suggest conducting ablation studies to clarify the contributions of various components to the overall performance and addressing the discrepancies in experimental results.