ID: gt6prlTEGL
Title: HARDMATH: A Benchmark Dataset for Challenging Problems in Applied Mathematics
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 8, 6, 8
Original Confidences: 4, 4, 5

Aggregated Review:
### Key Points
This paper presents HARDMATH, a benchmark dataset aimed at evaluating Large Language Models (LLMs) on advanced applied mathematics problems that necessitate analytical approximation techniques. The dataset addresses a gap in existing benchmarks by focusing on complex problems requiring mathematical reasoning, computational tools, and subjective judgment. The authors evaluate several LLMs, including both open-source and closed-source models, revealing significant limitations in their performance on these advanced problems.

### Strengths and Weaknesses
Strengths:
- The dataset introduces a novel focus on advanced applied mathematics, an area previously underrepresented in LLM evaluation.
- The algorithmic generation of problems and solutions enhances scalability compared to manually curated datasets.
- Comprehensive evaluation of multiple LLMs highlights their strengths and weaknesses in solving complex mathematical problems.
- The dataset has the potential to drive advancements in LLM capabilities in applied mathematics.

Weaknesses:
- Some sections, particularly those detailing technical aspects, may be challenging for readers lacking a strong background in applied mathematics or LLM evaluation.
- The dataset primarily targets a specific technique (dominant balance in asymptotic analysis), neglecting other important topics in applied mathematics such as complex numerical methods and optimization problems.
- The dataset's size is limited, containing only 40 math-word problems, which raises concerns about its scalability.

### Suggestions for Improvement
We recommend that the authors improve the dataset's name to better reflect its focus and features. Additionally, we suggest modifying the claims regarding the dataset's applicability to applied mathematics, as it does not provide a holistic evaluation of LLM capabilities in this field. To enhance the dataset's credibility, the authors should consider expanding the number of problems and including a broader range of mathematical techniques. Furthermore, we encourage the authors to explore more sophisticated prompting techniques, such as Show Your Work, in their evaluations. Lastly, it would be beneficial to include performance results from OpenAI's o1 model on the proposed dataset when the API becomes publicly available.