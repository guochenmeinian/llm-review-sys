ID: Vtqymej1tA
Title: Multiplication-Free Transformer Training via Piecewise Affine Operations
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to training deep networks, specifically Transformers, entirely without multiplication by utilizing a piecewise affine approximation through bit additions of floating-point representations. The authors claim that this method allows for a full multiplication-free training process, covering all phases of training while maintaining minimal accuracy degradation. The results indicate potential savings in area and power requirements.

### Strengths and Weaknesses
Strengths:
- The concept of multiplication-free training is intriguing and potentially significant.
- The approximation method is concise and shows minimal accuracy loss compared to traditional methods.
- The paper introduces a new scheme applicable to various deep learning architectures.

Weaknesses:
- The claim of being the first to achieve multiplication-free training is overstated, as other networks exist that do not use multiplications.
- The paper lacks detailed latency and efficiency metrics, which are crucial given the motivation to reduce training costs.
- The presentation of how piecewise affine multiplication (PAM) reduces to bit-addition is unclear.
- The performance comparison with existing multiplication-free networks and reduced-precision methods is insufficient.
- The paper does not adequately address the limitations of modern hardware support for multiplication, which may render the proposed method less efficient in practice.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 2.2 regarding the reduction of PAM to bit-addition. Additionally, please provide a thorough comparison of latency between bit-addition and multiplication, and clarify the compatibility of the proposed method with low-precision formats. It would be beneficial to report results with all layers replaced with PAM and include model architecture details in the machine translation results. We also suggest that the authors address the limitations of their method in the context of existing hardware support and provide concrete examples of hardware architectures that could benefit from their approach. Lastly, a more comprehensive analysis of memory costs and comparisons with reduced-precision training methods would strengthen the paper's contributions.