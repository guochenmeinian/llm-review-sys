ID: m6dRQJw280
Title: Equivariant Adaptation of Large Pretrained Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 6, 3, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for integrating equivariance into pre-trained models using canonicalization functions that normalize inputs to a common orientation. The authors introduce a canonicalization prior regularization to address challenges in training these functions, demonstrating improved equivariant properties through experiments in image and point-cloud domains. The approach aims to enhance robustness against group actions in various downstream tasks.

### Strengths and Weaknesses
Strengths:
1. The paper is clearly written and effectively motivates the challenges associated with using canonicalization for pre-trained models.
2. It addresses an important problem of efficient equivariance in neural networks, particularly relevant for large models.
3. The proposed technique is simple and effective, showing robustness across multiple finetuning datasets.

Weaknesses:
1. The experimental setup is flawed as it provides the model with information about test set augmentation during training, which is inappropriate.
2. The method relies heavily on prior work, limiting its methodological novelty.
3. The experiments primarily demonstrate invariance rather than equivariance, which raises questions about the robustness of the canonicalization function to distribution shifts.
4. Fine-tuning the entire model, including the canonicalization function, may distort pretrained features, making it vulnerable to out-of-distribution data.

### Suggestions for Improvement
We recommend that the authors improve the experimental setup by either using a uniform continuous prior over group elements or performing data augmentation with discrete group actions for baseline models. Additionally, consider conducting experiments with frozen pre-trained backbones and trainable classification/segmentation modules to justify the fine-tuning approach. It would also be beneficial to clarify the overall loss function and the specific structure of the canonicalization module. Finally, we suggest including experiments that evaluate equivariance in its general form and exploring transformations beyond rotations to strengthen the paper.