ID: jSKtxmxc0M
Title: VideoGUI: A Benchmark for GUI Automation from Instructional Videos
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VideoGUI, a multi-modal benchmark designed to evaluate GUI assistants learning from instructional videos. The benchmark includes 86 complex tasks and 463 subtasks, derived from high-quality web instructional videos, and introduces three different query types: text, image, and video. The authors propose a novel hierarchical evaluation process encompassing high-level planning, mid-level planning, and atomic action execution, revealing performance gaps in visual conditioned planning.

### Strengths and Weaknesses
Strengths:
- The paper effectively addresses the reliance on instructional videos for mastering complex skills and introduces three query types for the first time.
- The writing, illustration, and experimental design are commendable, providing clarity and systematic evaluation across different models.
- The hierarchical evaluation process is a significant contribution, aiding in identifying detailed failure causes.

Weaknesses:
- The benchmark's ability to handle alternative task execution methods is unclear, raising concerns about its robustness.
- The evaluation metrics for high and mid-level planning lack clarity, particularly regarding the scale definitions and percentage evaluations.
- There is no provided code or comprehensive task list for reproducibility, which is crucial for benchmarks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how the benchmark accounts for alternative execution methods, such as keyboard shortcuts. Additionally, the authors should provide a detailed explanation of the evaluation metrics for high and mid-level planning, including the definitions of the 0 to 5 scale and the percentage evaluations. Furthermore, including the evaluation results of Gemini-1.5-Pro could enhance the paper. Lastly, to bolster reproducibility, the authors should publish the project code and a full list of tasks.