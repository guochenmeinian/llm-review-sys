ID: mxMvWwyBWe
Title: Crafting Interpretable Embeddings for Language Neuroscience by Asking LLMs Questions
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 5, 3, 5, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents QA-embedding, a method for generating interpretable text embeddings through prompting large language models (LLMs) with yes/no questions. The authors propose that QA-embedding addresses the limitations of traditional embedding methods, such as bag-of-words and BM-25, by providing interpretable representations that enhance reliability in scientific applications. The process involves collecting evaluation questions via GPT-4, transforming responses into binary vectors, and fitting these representations for downstream tasks using ridge regression. The evaluation demonstrates QA-embedding's effectiveness in fMRI interpretation, information retrieval, and text clustering, although it faces challenges regarding computational cost and LLM accuracy.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a critical issue in generating interpretable embeddings, which is significant for both research and practical applications.
2. QA-embedding is an intuitive method that effectively captures the semantic features of text.
3. The approach shows promising results in fMRI interpretation, potentially inspiring further studies on LLMs and interpretability.

Weaknesses:
1. Certain implementation details, particularly in lines 149-165, need clearer descriptions, including PCA, inverse PCA, and the sampling process.
2. The reliance on manually crafted prompts for question generation raises concerns about the robustness and consistency of QA-embedding, as evidenced by varying numbers of questions generated.
3. The computational intensity of the method, requiring numerous LLM prompts, may hinder its applicability across diverse tasks, and its performance in information retrieval is notably inferior to BM25.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the implementation procedures, particularly regarding PCA and the sampling process. Additionally, conducting a thorough analysis of the time efficiency of QA-embedding, including comparisons before and after distilling and against other embedding methods, would clarify its limitations. We also suggest testing the classification accuracy of the distilled RoBERTa model on various tasks to demonstrate the effectiveness of the distillation technique. Finally, performing error analysis on downstream tasks could enhance the competitiveness of QA-embedding by tracing model prediction errors.