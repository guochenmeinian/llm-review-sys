ID: bFoQXD7Uls
Title: VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VeLoRA, a novel algorithm for memory-efficient training of large-scale language models by compressing intermediate activations into a fixed one-dimensional subspace. VeLoRA divides tokens into smaller sub-tokens, projecting them during the forward pass and reconstructing them during the backward pass, leading to reduced memory usage. Experimental results show that VeLoRA achieves competitive performance across various benchmarks, complementing existing parameter-efficient fine-tuning (PEFT) methods while significantly lowering memory requirements.

### Strengths and Weaknesses
Strengths:  
1. VeLoRA offers a clear and effective method for enhancing memory efficiency in training large language models.  
2. The paper includes extensive experiments across multiple benchmarks, demonstrating the broad applicability of VeLoRA.  
3. The method is compatible with existing PEFT approaches, making it practical for integration into current workflows.  

Weaknesses:  
1. The number of parameters used by VeLoRA is not specified, particularly in Tables 1 and 2.  
2. Performance degradation on certain datasets, such as Caltech101, Resisc45, and Clevr-Count, is not adequately explained.  
3. The experiments on visual tasks are insufficient, as they only cover the Parameter Tuning category, neglecting Prompt Tuning and Adapter Tuning.  
4. The training duration before and after introducing VeLoRA is not presented, despite being mentioned in the Limitations section.  
5. Improvements shown in Tables 1 and 2 are not significant, necessitating a significance analysis.  
6. The theoretical analysis of the rank-1 mapping method is limited, and its effectiveness needs clearer justification.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the parameter specifications in Tables 1 and 2. Additionally, the authors should provide explanations for the performance drops on specific datasets and expand the experimental scope to include Prompt Tuning and Adapter Tuning. It is also essential to present the training duration results and conduct a significance analysis of the improvements shown in the tables. Finally, we encourage the authors to enhance the theoretical analysis of the rank-1 mapping method to clarify its effectiveness.