ID: waDF0oACu2
Title: Collaboratively Learning Linear Models with Structured Missing Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 6, 4, 5, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 2, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on collaboratively estimating least squares for multiple agents, each with access to different feature subsets. The authors propose a distributed algorithm, *COLLAB*, which operates through local training, aggregation, and distribution steps, aiming to minimize communication costs while achieving optimal estimators without sharing labeled data. The algorithm is theoretically shown to be near-asymptotic local minimax optimal and is validated through experiments on both real and synthetic datasets.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a significant problem in distributed learning with heterogeneous data sources, providing robust theoretical guarantees and demonstrating strong theoretical results.  
- The focus on minimizing communication resources is an interesting and relevant angle, and the experimental results support the effectiveness of the proposed method.  
- The theoretical insights into the *COLLAB* algorithm, particularly the results in Theorems 4.1 and 4.2, are novel and surprising.

Weaknesses:  
- The assumptions regarding agents having the same linear model and sufficient data to estimate $\Sigma$ are restrictive and need further justification.  
- Some sections, particularly 3.1 and 3.2, are convoluted and lack clarity, making it difficult to follow the derivations.  
- Evaluations are limited, primarily focusing on linear models and Gaussian features, which restricts the modeling capability. More empirical evaluations against other methods are needed, especially in non-linear cases.  
- The paper lacks a discussion on the limitations and potential societal impacts of the work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Sections 3.1 and 3.2 by adhering to consistent notation, such as using bold **`X`** for matrices and bold **`x`** for vectors. Additionally, we suggest providing more empirical evaluations to showcase the performance of *COLLAB* against other methods, particularly in non-linear scenarios. Addressing the impact of inaccurate estimates of $\hat{\Sigma}_i$ in the algorithm's performance would also enhance the paper. Furthermore, a discussion on the challenges of the problem setting and a breakdown of key proof ideas would help readers appreciate the contributions more fully. Lastly, we encourage the authors to elaborate on the potential generalizations of their work to heterogeneous feature-observation federated learning settings.