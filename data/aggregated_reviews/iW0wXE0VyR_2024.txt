ID: iW0wXE0VyR
Title: Induced Model Matching: Restricted Models Help Train Full-Featured Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for training models that utilize all features by aligning their marginal distribution with that of a restricted model, which uses only a subset of features. The authors relate this approach to knowledge distillation and noising, proposing an approximate objective for marginalization over the larger model. They provide a sampling-based computation method and evaluate their approach on logistic regression and language modeling tasks. Additionally, the paper introduces a computationally efficient implementation of the proposed *no-sampling IMM*, contrasting it with the *sampled IMM* discussed in the original paper. The authors successfully applied this approach to a logistic regression example, addressing high variance through SGD with momentum, gradient clipping, and learning-rate decay schedules. They also tackled the challenge of maintaining an induced model by updating it every $n$ iterations, discovering that using a stale model for the correction factor mitigated instability. The results indicate that *no-sampling IMM* outperforms *sampled IMM* in terms of noising and is significantly faster than the latter while remaining comparable to the baseline.

### Strengths and Weaknesses
Strengths:
- The connection of the proposed method to existing work in noising is insightful and offers a fresh perspective.
- Highlighting the relationship to knowledge distillation adds value.
- The implementation of *no-sampling IMM* demonstrates a clear advancement in computational efficiency.
- The authors effectively address technical challenges, such as high variance and model staleness, leading to improved stability in the optimization process.
- The availability of code for the method is beneficial.
- Empirical results show that *no-sampling IMM* performs well, particularly as dataset size increases, and the paper is generally well-written, clearly explaining the proposed method.

Weaknesses:
- Empirical results are based on older models (RNNs, BERT), and the inclusion of modern models like decoder-only transformers would enhance the relevance of the findings.
- The computational overhead of the method is significant and not sufficiently addressed, raising concerns about scalability to larger datasets and models.
- The RNN experiment incurs a substantial overhead factor due to gradient accumulation, which may worsen with the need to replace context for each summand in the IMM objective.
- The auxiliary data structure for context lookup could require excessive space, complicating implementation for large datasets.
- The paper does not yet include large-scale applications, which may limit its perceived applicability in current machine learning contexts.
- The authors acknowledge that they have not fully resolved all computational challenges associated with IMM.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by simplifying the notation and providing a graphical illustration of the IMM principle. Additionally, addressing the computational and memory complexity of the method in terms of sequences and sequence length would be beneficial. The authors should also provide a FLOPs-corrected comparison between the IMM method and vanilla training, and discuss scenarios where the IMM approach may not be advantageous. Furthermore, we suggest that the authors improve the visualization of the toy example by displaying the Bayes restricted model alongside the model-in-training and its induced model to illustrate the alignment encouraged by IMM. We encourage the authors to provide a more detailed discussion of potential large-scale applications of IMM, as this could enhance the paper's relevance and impact in the broader machine learning community. Finally, we urge the authors to ensure that the time and memory complexity details are clearly itemized in the paper to reinforce their commitment to computational efficiency.