ID: RtTNbJthjV
Title: The Karp Dataset
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 8, 6, 7, 6, 6
Original Confidences: 4, 4, 3, 3, 4

Aggregated Review:
### Key Points
This paper presents the Karp dataset, a structured collection of NP-completeness reductions designed to assess the reasoning abilities of language models (LLMs). The dataset spans a range of difficulties, enabling robust evaluations and addressing a critical gap in benchmarking reasoning tasks. The authors propose that the dataset can differentiate between state-of-the-art models and demonstrate the impact of fine-tuning on reasoning performance, although the reliance on human evaluation limits its current utility.

### Strengths and Weaknesses
Strengths:
- The dataset is innovative and high-quality, filling a gap in existing math datasets by focusing on NP-completeness reductions.
- It provides a benchmark for evaluating LLMs on complex tasks, showing potential improvements through fine-tuning.
- The rigorous experimental design validates the dataset's effectiveness.

Weaknesses:
- The dataset's reliance on human evaluation restricts scalability and limits the number of problems tested.
- The evaluation metrics and rating system (0, 1, 2) are unclear, and the validation dataset is relatively small, which may impact robustness.
- The focus on NP-completeness may limit applicability for general mathematical reasoning evaluation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation metrics and consider adopting a percentage-based rating system. Additionally, providing a more detailed description of the dataset, including the total number of samples and proof lengths, would enhance understanding. To address scalability concerns, we suggest exploring automated verification strategies, such as comparing the LLM-as-a-judge approach. Furthermore, including evaluations on more mathematical or downstream tasks and providing mixed dataset training recipes would strengthen the paper's contributions.