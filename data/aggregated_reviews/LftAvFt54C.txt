ID: LftAvFt54C
Title: Hybrid Policy Optimization from Imperfect Demonstrations
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 7, 5, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HYPO, a reinforcement learning (RL) algorithm designed to learn in sparse reward environments using imperfect demonstrations. The method incorporates a hybrid discriminator, an offline guider policy trained through behavior cloning, and an online agent that interacts with the environment. The offline guider utilizes both offline data from a sub-optimal expert and online data from the learning agent to enhance exploration and learning efficiency. Experimental results indicate that HYPO outperforms several baselines, particularly in Mujoco environments.

### Strengths and Weaknesses
Strengths:
1. The motivation for using imperfect demonstrations is clearly articulated, and the training approach for the guider policy is innovative.
2. The idea of adaptive target behavior cloning to dynamically train the offline policy is novel and creative.
3. The experimental results demonstrate significant improvements over various baselines, showcasing the robustness of the proposed method.

Weaknesses:
1. The experimental results do not sufficiently demonstrate the superiority of the proposed method; additional ablation studies are needed for a comprehensive analysis.
2. The comparison is limited to Mujoco experiments; expanding evaluations to other environments would provide a more thorough assessment.
3. The inclusion of the offline guider in the discriminator lacks clarity, and the necessity of the offline policy remains ambiguous.
4. There are inconsistencies in notation and terminology, which may confuse readers regarding the roles of the guider, agent, and demonstrator policies.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the exposition, particularly in sections discussing the discriminator and the offline policy's role. Conducting additional ablation studies to analyze the impact of the offline guider on performance would be beneficial. We also suggest including a sensitivity analysis for hyperparameters such as $\alpha$, $\eta$, and $C$, along with a discussion on the decay methodology employed. Furthermore, expanding the experimental comparisons to include other baselines, such as GAIL-PPO and DAPG, would substantiate the need for an offline guider. Lastly, proofreading the manuscript for clarity and consistency in notation is essential to enhance the overall presentation.