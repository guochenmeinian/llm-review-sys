ID: lLztVBaBVU
Title: PDP: Parameter-free Differentiable Pruning is All You Need
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 7, 5, 7, 5, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 2, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel pruning algorithm named parameter-free differentiable pruning (PDP), which generates soft pruning masks using a dynamic function of weights during training. PDP is designed to enhance gradient propagation while minimizing accuracy loss across various tasks in vision and NLP. The authors propose that PDP can generalize well to random, structured, and channel pruning, achieving superior results compared to existing methods.

### Strengths and Weaknesses
Strengths:
1. The PDP approach is innovative and effectively generalizes across different pruning types.
2. The analysis in Section 3.1 provides valuable insights into the method's performance.
3. The proposed method is efficient, requiring no additional parameters and yielding faster results with less pruning accuracy loss.

Weaknesses:
1. The approach of adjusting network weights to generate soft masks may seem counterintuitive, as it diverges from traditional methods that learn masks directly.
2. The paper lacks a theoretical proof to substantiate why PDP is more effective than existing non-parameter-free approaches.
3. The effectiveness of PDP on larger models, such as vision transformers and LLMs, remains uncertain.
4. The description of the PDP algorithm in Section 3.2 is ambiguous, particularly regarding the calculation of the threshold t.
5. There is a notable absence of experiments on Transformer architectures beyond BERT, and the sparsity ratio selections in the experiments are limited.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the PDP algorithm description, particularly in Section 3.2, and provide a theoretical justification for the method's effectiveness. Additionally, we suggest conducting more experiments on larger models, such as ViTs and LLMs, and including a broader range of sparsity ratios to better assess the proposed method's impact. Lastly, we encourage the authors to reorganize the paper for better flow, possibly by moving Section 3.2 before 3.1 to enhance reader comprehension.