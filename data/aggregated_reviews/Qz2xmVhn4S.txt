ID: Qz2xmVhn4S
Title: Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 7, 8, 8, -1, -1
Original Confidences: 2, 3, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents Spider2-V, a new multimodal benchmark for evaluating AI agents in data science and engineering tasks, comprising 494 real-world tasks across 20 professional software applications. The authors propose a setup that includes both GUI and CLI interfaces, with a controllable environment for executing tasks. The paper details the evaluation metrics and configurations for each task, benchmarking various state-of-the-art models, including GPT-4o, to highlight the challenges faced by current models.

### Strengths and Weaknesses
Strengths:
- The benchmark is well-motivated and addresses the need for specifically-designed evaluations in a crowded LLM agent space.
- The tasks are thoughtfully designed, utilizing real-world tools and environments, which enhances the relevance of the benchmark.
- The paper is well-written, with clear communication of the task construction and evaluation processes.

Weaknesses:
- The evaluation lacks confidence bounds or error bars, making it difficult to assess model performance rigorously.
- The structure of the paper is odd, with the "related work" section placed at the end rather than the beginning.
- The novelty claims are not explicitly detailed, leading to confusion regarding the contributions beyond the introduction of new tasks.

### Suggestions for Improvement
We recommend that the authors improve the rigor of the evaluation by including confidence bounds or error bars in the results. Additionally, we suggest restructuring the paper to place the "related work" section at the beginning for better flow. Clarifying the novelty claims and explicitly listing the contributions in the introduction would enhance understanding. Furthermore, including at least one baseline model in each evaluation table could facilitate comparisons with existing benchmarks. Lastly, addressing the complexity of the setup and providing clearer documentation on the runtime and compute complexity would aid in reproducibility.