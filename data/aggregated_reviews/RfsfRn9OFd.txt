ID: RfsfRn9OFd
Title: EEG2Video: Towards Decoding Dynamic Visual Perception from EEG Signals
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 4, 4, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an EEG decoding model aimed at reconstructing video stimuli from EEG signals, utilizing a dataset collected from twenty participants. The dataset is annotated with various features, including dominant color and human presence in video frames. The authors propose a novel framework, EEG2Video, which serves as a baseline for future research in this area. They emphasize the challenges of mapping EEG to unseen categories and acknowledge the limitations of their evaluation metrics, particularly in the context of zero-shot transfer learning. The authors argue that while their model can generate videos across categories, the current technology lacks a robust alignment between EEG representations and video generation models. They highlight the need for a comprehensive dataset to support future multimodal alignment research.

### Strengths and Weaknesses
Strengths:  
- The manuscript addresses a significant and growing problem in brain-computer interfaces, contributing a valuable dataset that serves as the first batch of paired data between EEG and video, which is crucial for future research in multimodal alignment.  
- The source code is provided, facilitating the reproduction of results.  
- The reconstructed videos are visually appealing, demonstrating the potential of EEG decoding.  
- The authors acknowledge the limitations of their approach and the challenges inherent in the task, demonstrating a clear understanding of the current state of technology.

Weaknesses:  
- The decoding power of the EEG signals is questionable, with reported accuracies often near or below chance levels, raising concerns about statistical significance due to the absence of tests like the Student t-test or Wilcoxon Signed-Rank test.  
- The dataset exhibits categorical leakage, as concepts are shared between training and test sets, potentially skewing results.  
- The choices for classification categories appear arbitrary, lacking a clear rationale.  
- The evaluation metrics used may not adequately reflect the model's performance, especially in zero-shot scenarios, potentially leading to misleading comparisons.  
- The reliance on semantic metrics may allow simpler models to perform well without truly capturing the complexity of the task.  
- The method's innovation is limited, primarily reusing existing modules without sufficient comparative analysis against other video generation techniques.

### Suggestions for Improvement
We recommend that the authors improve the statistical analysis by conducting appropriate tests to validate the significance of results above chance levels. Additionally, clarifying the rationale behind the chosen classification categories would enhance the manuscript's depth. We suggest including an ablation study to evaluate the impact of global and local data streams on performance. Furthermore, we recommend improving the clarity of their evaluation metrics by including a baseline that relies solely on high-level class information to assess the performance of their model against simpler alternatives. It is also essential to report that the reconstructions in Figures 5, 8-12 were manually selected based on their semantic alignment with the ground truth, as this information is critical for interpreting the results accurately. Finally, we suggest providing a more detailed comparison of their method with existing video generation approaches and highlighting the originality of their proposed method more clearly in Section 4.