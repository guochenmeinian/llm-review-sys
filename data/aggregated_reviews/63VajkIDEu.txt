ID: 63VajkIDEu
Title: Worst-Case Offline Reinforcement Learning with Arbitrary Data Support
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to offline reinforcement learning (RL) without data support assumptions. The authors propose a new worst-case policy value setting and utilize a regularized Lagrangian framework to analyze sample complexity. They demonstrate that an upper bound of $O(\epsilon^{-2}(1 - \gamma)^{-4}\ln(1/\delta))$ can be achieved without relying on the concentrability condition, thereby improving upon previous bounds.

### Strengths and Weaknesses
Strengths:  
- The connection between the truncated environment and the regularized Lagrangian problem is innovative and beneficial.  
- The authors significantly enhance the sample complexity to $O(1/\epsilon^2)$ for learning an $\epsilon$-optimal policy.  
- The paper introduces a new framework that simplifies traditional offline RL evaluations by eliminating the need for pessimism or behavioral cloning hyperparameters.  
- The theoretical proof supporting the upper bound is robust.  

Weaknesses:  
- The paper is overly technical and linear, lacking a clear summary of the main ideas at the beginning, which would aid in understanding its position relative to existing work.  
- The proposed truncated environment may discard useful information between similar state-action pairs, potentially limiting the derived bounds.  
- The notation is often too similar, making it difficult for readers to distinguish between terms.  
- The algorithm's reliance on knowledge of the behavior policy may pose practical challenges.  
- There is insufficient comparison with relevant literature, and some explanations of superscripts and terms are lacking clarity.

### Suggestions for Improvement
We recommend that the authors improve the paper's presentation by including a clearer summary of the main ideas at the beginning to facilitate understanding. Additionally, consider revising the notation to ensure greater distinction between terms. Addressing the potential limitations of the truncated environment by discussing the information that may be lost could enhance the robustness of the findings. Furthermore, clarifying the algorithm's dependence on the behavior policy and providing a more comprehensive comparison with relevant works would strengthen the paper.