ID: KjNEzWRIqn
Title: Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 5, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for synthetically generating a dataset of agent trajectories for website navigation using annotated programs, specifically Python code interleaved with comments. The authors propose utilizing GPT-4 to create plausible trajectories from two data sources: wikiHow tutorials and ClueWeb HTML snapshots. The resultant dataset, when used to fine-tune CodeLlama-instruct-7b, demonstrates improved performance on benchmarks such as Mind2Web, MiniWoB++, and WebArena. The approach aims to address the high costs and scalability issues associated with human annotation in dataset generation.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated and clearly written in sections.
- It creatively combines two distinct data sources, enhancing the dataset's robustness.
- The approach makes strides toward developing competent LLM agents.
- Synthetic trajectories show improved benchmark performance.

Weaknesses:
- The reliance on GPT-4 for synthetic data generation raises questions about its applicability for state-of-the-art (SOTA) advancements.
- Comparisons with human-annotated trajectories in Section 6.2 are unequal, as Mind2Web tasks are simpler than those in Synatra; a more equitable comparison is needed.
- Section 6.3 lacks depth and clarity.
- The paper does not adequately address the potential overfitting to unique IDs in training demonstrations or the implications of using viewports in the context of the benchmarks.

### Suggestions for Improvement
We recommend that the authors improve the comparison with human annotations by collecting a smaller set of human annotations that align more closely with the task distribution of Synatra. Additionally, we suggest including a broader range of baselines in the evaluation, particularly those that are fine-tuned with direct demonstrations. Clarifying the fine-tuning process and hyper-parameters used for the LLM would enhance the methodological transparency. Furthermore, addressing the potential overfitting issue related to unique IDs in training demonstrations and providing error estimates for performance figures would strengthen the paper's findings. Lastly, we encourage the authors to discuss the robustness of LLM agents and the security risks associated with deploying them.