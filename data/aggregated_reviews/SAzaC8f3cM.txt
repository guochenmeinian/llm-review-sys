ID: SAzaC8f3cM
Title: Towards Self-Interpretable Graph-Level Anomaly Detection
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SIGNET, a self-explainable method for anomalous graph detection, inspired by the multi-view information bottleneck (MIB) and dual hypergraph transformation (DHT). The authors propose three key designs during model training: 1) Dual hypergraph-based view construction, 2) Bottleneck subgraph extraction, and 3) Cross-view maximization. The method aims to maximize mutual information between graph rationales from different views to detect anomalies and provide explanations. The authors claim their work is a pioneering attempt to introduce explainability into graph-level anomaly detection (GLAD), although recent literature suggests that this may not be entirely accurate. They evaluate their model on 10 TU datasets, employing AD-AUC as a performance metric, which they clarify is equivalent to AUC, and also introduce explainability metrics NX-AUC and EX-AUC. The authors acknowledge the need for a more comprehensive review of related works and comparisons with other explainable techniques.

### Strengths and Weaknesses
Strengths:
- The problem addressed is significant, focusing on understanding the subgraphs that contribute to graph anomalies.
- The paper clearly outlines the challenges of self-interpretable graph-level anomaly detection and why existing GNN explainers are inadequate.
- The authors effectively clarify their contributions and the unique challenges of GLAD.
- They provide a rationale for their choice of datasets and metrics, addressing potential concerns about fairness in comparisons.
- The structure of the paper is coherent and easy to follow, and the commitment to expanding discussions on stability and robustness in their results is commendable.

Weaknesses:
- The motivations for using MIB and DHT in the context of graph-level anomaly detection are not sufficiently articulated, lacking theoretical grounding.
- The claim of being the "first attempt" at explainable GLAD is undermined by existing literature.
- The experimental dataset is small-scale, and a time complexity analysis is absent, which is crucial for understanding scalability.
- The evaluation metrics and dataset discrepancies raise questions about the robustness of the findings.
- Limited comparisons with other explainable techniques may weaken the perceived novelty of the approach.
- There is a need for more comprehensive related work on anomaly detection and explainability mechanisms.

### Suggestions for Improvement
We recommend that the authors improve the theoretical discussion surrounding the motivations for employing MIB and DHT, providing clearer intuition on their relevance to graph-level anomaly detection. Additionally, consider expanding the dataset used in experiments and including a time complexity analysis to address scalability concerns. We suggest enhancing the related works section to better illustrate how their work differs from existing explainable anomaly detection methods. It would be beneficial to provide a detailed explanation of the differences between AD-AUC and AUC, as well as to include AUC results using the same settings as the baselines. We encourage the authors to consider comparing their model with other explainable techniques, such as influence functions and Shapley value-based methods, in future research. Furthermore, enhancing the qualitative analysis of explanations, particularly in Figure 3, and providing more detailed captions would improve clarity. Lastly, discussing hyperparameter selection for baselines and potential limitations in more depth would be beneficial.