ID: pMCRGmB7Rv
Title: BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of LLMs in biology protocol planning, focusing on an automated evaluation method. The authors introduce a dataset named BioPort, which includes published biology protocols and machine-generated pseudocode verified by experts. They propose three quantitative tasks to assess LLM performance in protocol planning and validate model-generated protocols through real-world laboratory experiments.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important application of LLMs in supporting experimental scientists with protocol generation.
- The authors construct a novel dataset that combines natural language descriptions with pseudocode.
- The introduction of three quantitative tasks facilitates automatic evaluation of LLM performance.
- Real-world validation of machine-generated protocols demonstrates the practical applicability of LLMs.

Weaknesses:
- The writing and organization require improvement, with issues such as typos and missing table references.
- The evaluation on machine-generated data may limit the dataset's coverage and generalizability, particularly since GPT-4 was used to generate the pseudocode.
- Evaluating GPT-4 on a dataset it helped create may lead to an overestimation of its performance.
- The dataset's size is relatively small, consisting of only 70 protocols, which may affect robustness.

### Suggestions for Improvement
We recommend that the authors improve the writing and organization of the paper to enhance clarity and coherence. Addressing typos and ensuring proper table references will strengthen the presentation. Additionally, we suggest that the authors consider evaluating models outside the OpenAI family to enhance replicability and mitigate potential biases from using GPT-4 for both dataset creation and evaluation. Expanding the dataset beyond 70 protocols could also improve the generalizability of the findings.