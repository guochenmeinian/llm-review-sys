ID: OIJ3VXDy6s
Title: RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 5, 5, 7, 7, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a model-based reinforcement learning (RL) method that learns task-relevant latent-space dynamics by maximizing the mutual information between latent states and rewards while minimizing it between latent states and observations. The authors introduce a support constraint regularization technique to adapt to distribution shifts at test time. The method is evaluated using modified versions of the DeepMind Control Suite and Maniskill with video backgrounds.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant problem in deploying model-based RL in real-world robotics.
- The logic flow is coherent, and the presentation is clear, making it accessible despite its complexity.
- Empirical results demonstrate the effectiveness of the proposed method.

Weaknesses:
- The positioning against prior work is weak, as relevant visual model-based RL methods are not adequately cited or compared.
- The training curves lack clarity regarding convergence, and numerous minor corrections and typos are present.
- The method is not task-agnostic, and the implications of this limitation are not sufficiently discussed.

### Suggestions for Improvement
We recommend that the authors improve the positioning of their work by including and comparing it with relevant methods such as Denoised MDPs, TD-MPC, and ALM in both the related work and experiments sections. Additionally, the authors should clarify the training curves to indicate convergence points more clearly. We also suggest addressing the numerous minor corrections and typos throughout the paper to enhance readability. Finally, a discussion of the limitations of the method, including its non-task-agnostic nature, would strengthen the paper's credibility.