ID: 9D0fELXbrg
Title: Scale-teaching: Robust Multi-scale Training for Time Series Classification with Noisy Labels
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 4, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a deep learning paradigm called scale-teaching to address noisy labels in time series classification. It introduces a fine-to-coarse cross-scale fusion mechanism that trains multiple deep neural networks (DNNs) simultaneously, utilizing complementary information from different scales to select small-loss samples as clean labels. Additionally, the authors propose a multi-scale embedding graph learning method via label propagation to correct labels from unselected large-loss samples. Extensive experiments demonstrate the method's superior performance across various datasets.

### Strengths and Weaknesses
Strengths:
1. The research problem is both interesting and important.
2. The proposed idea is simple yet effective.
3. The paper is well written and organized.
4. Extensive experiments yield good results.

Weaknesses:
1. Some equations and symbols, such as the operator || in Eq. (2), lack clarity.
2. The problem definition in Label Correction for Eq. (5) is difficult to understand.
3. The complexity of the proposed method and its procedure, including Algorithm 1, is not clearly articulated.
4. The flowchart in Figure 2 could better illustrate the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the equations and symbols, particularly the operator || in Eq. (2) and the problem definition in Eq. (5). Additionally, the authors should simplify the explanation of the proposed method's procedure, including Algorithm 1, and consider adding a flowchart to Figure 2 for better visualization. To enhance the novelty of the work, we suggest conducting more ablation studies to evaluate individual components and exploring alternative label correction techniques. Furthermore, we encourage the authors to assess the performance of the scale-teaching framework on datasets without label noise to provide a more comprehensive analysis.