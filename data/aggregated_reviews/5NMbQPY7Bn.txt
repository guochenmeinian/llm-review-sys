ID: 5NMbQPY7Bn
Title: TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 8, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Text-Only Pre-Alignment (TOPA), a framework designed to extend large language models (LLMs) for video understanding without relying on real video data. TOPA generates "Textual Videos" (Tideo) using LLMs to simulate video data and employs the CLIP model for feature extraction, achieving notable results across various benchmarks. The effectiveness of TOPA is validated through extensive experiments, including zero-shot evaluations and ablation studies. The authors also analyze the performance of TOPA in relation to the Ego4D and EgoSchema datasets, asserting that their use of "scenarios" from Ego4D does not lead to significant information leakage, as demonstrated by their ablation study results. They argue that the common "scenarios" in Ego4D metadata are well-represented in the Tideos dataset, suggesting that excluding Tideo-Ego4D does not substantially affect performance on EgoSchema. The authors intend to clarify their position on the zero-shot setting in the final version of the paper.

### Strengths and Weaknesses
Strengths:
- The introduction of the TOPA framework is innovative, reducing reliance on video data and human annotations.
- The TextVid dataset and the concept of Tideo effectively bridge the gap between textual and real video representations.
- Comprehensive evaluations across multiple benchmarks demonstrate the robustness of TOPA.
- The paper provides a thorough comparison with existing approaches, serving as a valuable reference for future research.
- The authors have addressed some concerns regarding performance against VideoAgent.
- The ablation study provides quantitative results that support the claim of minimal impact from Ego4D scenarios on performance.

Weaknesses:
- A more detailed comparison between Tideo and real video is needed, particularly regarding the diversity of domains in the TextVid dataset.
- The key-frame level representation of Tideo may not adequately capture the temporal dynamics of real videos.
- Experiment results on MVBench should be integrated into Section 4.1.2 for clarity on TOPA's strengths and limitations.
- There is ongoing concern about the inclusion of EgoSchema scenarios in the training of TOPA, which may affect the zero-shot claim.
- The argument regarding the commonality of scenarios may not fully alleviate concerns about potential information leakage.

### Suggestions for Improvement
We recommend that the authors improve the comparison between Tideo and real video by analyzing the diversity of domains in the TextVid dataset. Additionally, the authors should consider enhancing the representation of Tideo to better capture temporal dynamics. Incorporating the MVBench results into Section 4.1.2 will also provide a clearer understanding of TOPA's performance. Furthermore, we suggest that the authors improve clarity by demonstrating TOPA's performance when trained on Ego4D excluding EgoSchema scenarios to support their zero-shot claim. Explicitly stating the exact data used in their final version will enhance transparency regarding their methodology. Lastly, clarifying the training process and stages, particularly in the "Video-LLM alignment" subsection, will enhance the paper's presentation.