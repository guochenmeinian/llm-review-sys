ID: D0Mp7ILZME
Title: SuperTweetEval: A Challenging, Unified and Heterogeneous Benchmark for Social Media NLP Research
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark called SuperTweetEval, which consists of multiple social media datasets, primarily from Twitter, and includes 10 classification and 2 generation tasks. The authors propose this benchmark to provide a test bed for NLP models focused on social media. The contributions include the unification of 12 datasets and the results of experiments conducted on various models, establishing baselines and insights for future research.

### Strengths and Weaknesses
Strengths:
- The paper builds upon the existing TweetEval benchmark, extending it by incorporating new datasets and generation tasks.
- It unifies multiple datasets, providing a comprehensive benchmark that addresses gaps in social NLP research.
- The taxonomy of task clusters is well-organized, aiding researchers in focusing on specific task types.

Weaknesses:
- The evaluation lacks stronger existing models, primarily using smaller models, which raises concerns about the benchmark's ability to challenge current NLP models.
- There is insufficient clarity on how the new tasks were selected and their relevance to the broader social media NLP research community.
- The dataset is limited in size and lacks non-English data, which may restrict its applicability.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including stronger existing models, such as ChatGPT or GPT-4, to better demonstrate the benchmark's challenges. Additionally, we suggest providing a clearer explanation of how the tasks and datasets were selected, ensuring their relevance to social media NLP researchers. Including an oracle or proxy for the tasks would also help establish upper bounds on task difficulty. Finally, we encourage the authors to expand the dataset size and include more evaluation metrics to enhance the benchmark's robustness.