ID: 3PjCt4kmRx
Title: From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 6, 4, 6, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PIX2ACT, a model designed to interact with graphical user interfaces (GUIs) using pixel-level visual representations and generic low-level actions, bypassing the need for structured data sources like HTML or DOM trees. The authors demonstrate that their approach can outperform human crowdworkers on the MiniWob++ benchmark for GUI-based instruction-following tasks. The model employs a pre-trained system, PIX2STRUCT, and utilizes a tree search for policy enhancement, achieving competitive performance without relying on DOM-related information. Additionally, the authors provide a comparative analysis of PIX2ACT with related works, specifically Seq2Act and Spotlight, clarifying that Seq2Act is not pixel-based and primarily uses text-based representations of Android UI, while PIX2ACT focuses on pixel-based input. They also differentiate Spotlight, which does not involve an interactive agent but benchmarks command grounding tasks without environmental interaction, and plan to expand their discussion of these works in the revised paper.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach to automated GUI interactions using pixel-based inputs, which is a significant advancement in the field.
- The experiments are well-structured, providing valuable insights into the effectiveness of the proposed methods and the benefits of pre-training.
- The authors successfully adapt the WebShop benchmark for pixel-based observations, establishing a foundational performance standard.
- The authors provide a clear distinction between PIX2ACT and Seq2Act, highlighting the pixel-based approach of their model.
- They acknowledge the limitations of existing datasets and the need for more realistic environments for training GUI-based instruction-following agents.

Weaknesses:
- The evaluation procedure is insufficiently detailed, making it difficult to understand the methodology and results.
- There is a lack of statistical significance analysis to support claims of performance superiority over existing methods.
- The discussion on ethical implications is limited, failing to address potential misuse of AI systems in GUI interactions.
- The novelty of the proposed method compared to existing pixel-based agents is not clearly articulated, leaving readers unclear about its advantages.
- The authors' assertion that Spotlight does not present an interactive agent has been questioned, as it has demonstrated command grounding capabilities.
- The paper lacks detailed experimental information, which reviewers have requested for clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation procedure by providing a more detailed description of the methodology used. Additionally, we suggest conducting a statistical significance analysis to substantiate claims of performance superiority over existing systems. The authors should enhance the discussion section to better articulate the contributions of their work and address ethical concerns related to the potential misuse of AI in GUI interactions. Furthermore, we encourage the authors to clarify the novelty of their approach compared to existing pixel-based agents and provide a more comprehensive analysis of the tree search methodology, including its motivations and benefits. We also recommend that the authors improve their discussion of the relationships and differences between PIX2ACT, Seq2Act, and Spotlight, particularly addressing the interactive nature of agents. Lastly, please incorporate more experimental details in the revised paper, highlighting the fundamental differences between the command grounding dataset and the MiniWob++ and WebShop environments, including a discussion on which task is more challenging and why.