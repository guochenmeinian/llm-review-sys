ID: v7JgI9dny2
Title: Simpler neural networks prefer subregular languages
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the learning capabilities of LSTMs regarding subregular versus regular languages, demonstrating that LSTMs exhibit a bias towards simplicity when trained on subregular languages. The authors empirically analyze LSTM performance across various language types, including minimal description and human-aligned subregular languages, concluding that LSTMs learn these better than regular languages. The work contributes to understanding the relationship between human language and LSTM training.

### Strengths and Weaknesses
Strengths:  
- The exploration of bias towards simplicity in LSTM training is original and insightful.  
- The paper is well-written, providing clear explanations of complex ideas and results.  
- It offers significant insights into formal language theory and its connection to neural networks.

Weaknesses:  
- There is insufficient pedagogical coverage of subregular languages, which may alienate readers unfamiliar with the topic.  
- The connection between LSTM performance on subregular languages and sparsity is not adequately clarified.  
- The LSTM model's architecture is relatively small, raising questions about the scalability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the pedagogical coverage of subregular languages in Section 2 to accommodate readers with varying levels of familiarity. Additionally, we suggest clarifying the relationship between LSTM biases towards subregular languages and sparsity, particularly addressing the rationale behind using the L0 norm and its connection to network simplicity. Furthermore, the authors should explore the implications of scaling the network architecture and increasing the sample size beyond 2,000 words from the pFSA to enhance the robustness of their analysis. Lastly, we advise providing more precise statements regarding the results related to strictly piecewise and strictly local languages in comparison to regular languages.