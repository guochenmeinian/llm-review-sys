ID: RFgv7cfMUy
Title: On-the-Fly Adapting Code Summarization on Trainable Cost-Effective Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 5, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AdaCom, an approach designed to enhance code comment generation through on-the-fly model adaptation based on influence metrics. The authors demonstrate AdaCom's effectiveness across seven deep comment generators and four public datasets, reporting a performance increase of 0.19% in absolute terms for CSN-python, with notable improvements in other languages, particularly JavaScript. However, the choice of CodeGen as a baseline has been criticized due to its poor performance on the dataset. The approach incurs significant runtime overhead due to the on-the-fly retraining process, averaging 5.54 seconds for large models to generate a comment, which raises concerns about its practicality in real-world applications.

### Strengths and Weaknesses
Strengths:
- The proposed AdaCom is technically sound, yielding convincing performance gains across multiple datasets and programming languages.
- The paper reports nontrivial performance increases on most languages and provides a detailed analysis of timing and BLEU scores, demonstrating the potential benefits of AdaCom.
- The paper is well-structured and easy to read, and the authors have engaged with reviewers to provide clarifications that positively influenced some scores.

Weaknesses:
- The method introduces considerable runtime overhead, which is unacceptable for real-world applications requiring sub-second latency.
- The use of CodeGen as a baseline is questioned due to its inadequate performance, and the work lacks comparisons with state-of-the-art code language models, limiting its applicability to larger models that are widely deployed.
- The empirical influence estimation lacks a clear theoretical or empirical connection to the defined metrics, and concerns remain about the practicality and efficiency of the fine-tuning process, especially in the context of rapidly evolving large language models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper, particularly in explaining the minimum edit distance metric and the connection between empirical and practical influence estimation. Additionally, we suggest including comparisons with retrieval-augmented generation techniques to clarify how AdaCom differs and where it excels. The authors should provide a comprehensive evaluation of AdaCom's performance on larger language models and address the significant runtime overhead associated with the on-the-fly retraining process. We also recommend improving the justification for the inclusion of the "empirical influence" metric, ensuring it is clearly related to the work's objectives or consider omitting it entirely. Furthermore, providing a comprehensive table detailing completion times across various models, including CodeT5 and AdaCom, along with hardware properties and median token generation statistics, would enhance the clarity of the performance comparisons. Finally, we encourage the authors to address the concerns regarding the cost-effectiveness and generalizability of the fine-tuning approach, particularly in relation to larger models.