ID: HPvIf4w5Dd
Title: Finding good policies in average-reward Markov Decision Processes without prior knowledge
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 4, 4, 8, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on learning near-optimal policies in average reward Markov Decision Processes (MDPs) without prior knowledge of complexity parameters. The authors demonstrate that estimating the optimal bias span is complex, while introducing the Diameter Free Exploration (DFE) algorithm, which operates without this knowledge and achieves near-optimal sample complexity. They also establish that achieving polynomial sample complexity in the online setting is infeasible and propose an online algorithm with a sample complexity of \(O(SAD^2/\epsilon^2)\). Additionally, a data-dependent stopping rule is introduced, although its effectiveness is not fully analyzed.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a fundamental problem in reinforcement learning and makes significant contributions, particularly with the introduction of the DFE algorithm and the insights regarding the complexity of estimating the optimal bias span.
- The findings regarding the infeasibility of achieving polynomial sample complexity in the online setting are novel and set important theoretical boundaries.
- The presentation is generally clear, and the literature review is thorough.

Weaknesses:  
- The contributions are perceived as incremental, with concerns that the proposed solutions lack novelty, as they heavily rely on existing algorithms and techniques.
- The claim regarding the necessity of knowing \(H\) in previous works is misleading, as only an upper bound is required.
- The algorithm's design appears to be a straightforward combination of existing methods, diminishing its perceived innovation.
- The stopping rule's analysis is deferred to future work, which limits its contribution in the current context.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contributions by exploring more original approaches rather than relying on existing algorithms. Clarifying the necessity of knowing \(H\) in previous works would strengthen the paper's claims. Additionally, we suggest that the authors provide a more thorough analysis of the data-dependent stopping rule within the current paper to enhance its contribution. Finally, addressing the assumption of a unique optimal policy and providing a more careful definition of the optimal bias span would improve the clarity and applicability of the results.