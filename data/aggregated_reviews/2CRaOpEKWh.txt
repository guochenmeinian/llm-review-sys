ID: 2CRaOpEKWh
Title: Characterizing the Optimal $0-1$ Loss for Multi-class Classification with a Test-time Attacker
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 8, 6, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a generalization of lower bounds on adversarially robust error from binary to multi-class classification. The authors reformulate the adversarial learning problem as a linear program based on hypergraph hyperedges, addressing computational challenges through truncation techniques. The work aims to establish both lower and upper bounds for optimal loss under adversarial perturbations, providing empirical evaluations on datasets like MNIST and CIFAR-10.

### Strengths and Weaknesses
Strengths:
- The paper is well presented and easy to read, effectively conveying complex theoretical concepts.
- The formalization and assumptions are clearly articulated, avoiding unnecessary complications.
- The theory developed appears correct, and the experimental evaluations are sensible and understandable.
- The approach of using linear programming is innovative and contributes significantly to the field.

Weaknesses:
- Some aspects of the formalization and experimental design are not entirely novel, as they are derived from previous work on binary classification.
- Certain mathematical claims and definitions lack clarity, making it difficult for readers without a strong background to fully grasp the contributions.
- The computational applicability of the proposed methods remains limited, as evidenced by the experiments conducted on only three-class problems.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing a brief introduction to central mathematical concepts and definitions that may not be familiar to all readers. Specifically, consider including a proof outline of Lemma 1 in the main text to clarify its validity. Additionally, we suggest that the authors address the concerns regarding the correctness of Lemma 1 and the definitions used throughout the paper. Including evaluations on both training and test sets would also enhance the understanding of the model's performance relative to the established bounds. Finally, discussing the implications of the limitations regarding the scalability of the algorithm and how to bridge the gap between current robust classifiers and the optimal classifier would significantly strengthen the paper's impact.