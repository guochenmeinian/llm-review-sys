ID: PJhjkSFlbG
Title: Dynamics Generalisation in Reinforcement Learning via Adaptive Context-Aware Policies
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 8, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to learning adaptable policies in contextual reinforcement learning (RL) by utilizing "adapter" modules whose parameters are determined by a hypernetwork focused on context variables. The authors analyze the performance of context-unaware policies in contextual Markov Decision Processes (CMDPs) and empirically evaluate their decision adapter architecture across multiple environments, demonstrating its potential for generalization. Additionally, the paper introduces a novel architecture for context generalization in neural networks, specifically addressing the limitations of cGate in extrapolating from training contexts. The authors clarify that cGate is not a hypernetwork but can be implemented by one, highlighting differences in inductive biases and training dynamics. They demonstrate that their model exhibits superior generalization and robustness to distractor variables compared to cGate, while also exploring the implications of using hypernetworks and adapter layers in various domains.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and easy to follow, with effective illustrations that enhance understanding.  
- It addresses the critical issue of generalization in RL, combining existing ideas innovatively.  
- The discussion of limitations and thorough experimental evaluation supports the findings, making the work significant for the RL community.  
- The distinction between extrapolating and interpolating generalization is an important contribution.  
- The authors provide empirical evidence showing that their model outperforms cGate in terms of generalization and robustness.  
- The incorporation of hypernetworks and adapter layers is well-grounded in existing literature from NLP and Computer Vision.  
- The authors are responsive to reviewer feedback, conducting additional experiments to address concerns.

Weaknesses:  
- The title and abstract are overly general and do not adequately reflect the specific contributions of the paper.  
- The novelty is somewhat limited due to the existence of similar architectures like cGate, which is not sufficiently discussed in relation to the proposed method.  
- Some theoretical results and definitions are unclear or only referenced in the appendix, making them difficult to understand.  
- The empirical results do not consistently demonstrate the superiority of the proposed method over simpler approaches like naive concatenation.  
- The connection between the theoretical and experimental sections remains unclear, particularly regarding the relevance of Theorem 4.5 and the quantification of the $\alpha^{\pi}(s)$ ratio.  
- The use of binary distractor variables (0's and 1's) may not accurately reflect real-world scenarios, raising concerns about the experimental design.

### Suggestions for Improvement
We recommend that the authors improve the title and abstract to better reflect the specific contributions of the paper. Clarifying the relationship between the proposed method and cGate, including a discussion of why the hypernetwork is advantageous beyond its nonlinearity, would strengthen the paper. We suggest providing clearer definitions and explanations for theoretical results in the main text rather than relegating them to the appendix. Additionally, including architectural ablations and comparing the proposed method against alternatives that do not involve hypernetworks would enhance the empirical evaluation. We also recommend improving the clarity of the connection between the theoretical and experimental parts of the paper, particularly by addressing how Theorem 4.5 relates to the experiments and considering the quantification of the $\alpha^{\pi}(s)$ ratio. Furthermore, we suggest exploring the use of Gaussian distributions for distractor variables during both training and testing, as this may provide a more realistic assessment of model performance. Finally, we encourage the authors to enhance the visual presentation of graphs, ensuring distinct color schemes and improving the aesthetics of figures for better readability.