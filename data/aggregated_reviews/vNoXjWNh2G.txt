ID: vNoXjWNh2G
Title: LLM Internal States Reveal Hallucination Risk Faced With a Query
Conference: EMNLP/2024/Workshop/BlackBoxNLP
Year: 2024
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 3, 3, 3

Aggregated Review:
### Key Points
This paper investigates the use of internal LLM states to predict hallucination risks, proposing that generative models can evaluate their reliability in generating answers to queries, thereby reducing hallucinations. The authors employ an MLP classifier to estimate hallucination risk, testing their method across various tasks. They conclude that LLM internal states can indicate whether a query was part of the training data and the likelihood of hallucination.

### Strengths and Weaknesses
Strengths:  
- The approach allows for straightforward assessment of internal states to detect hallucination risks, which is beneficial for real-world applications.  
- The method demonstrates robustness and reliability compared to baseline approaches, with convincing results across diverse tasks.  
- The experiments contribute to understanding LLM mechanisms and include interesting visualizations, particularly regarding inference time efficiency.

Weaknesses:  
- Some methodological decisions appear ad-hoc and require further clarification, particularly regarding the selection of internal states in Section 3.3 and the rationale for using last token states in Section 3.4.  
- The documentation lacks detail, making it unclear how hallucinations are measured and how seen/unseen examples are distinguished in tasks like translation.  
- The paper overclaims regarding conclusions, and the relevance to existing literature is insufficiently addressed.  
- The use of analogies with human cognition and self-awareness is excessive and detracts from the technical focus on model calibration and uncertainty estimation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the selection process for internal states used in the classifier and provide a more detailed explanation of the preliminary study in Section 3.3. Additionally, we suggest discussing the reliability of the annotation methods and the heuristics applied in data construction. Clarifying the visualizations in Figure 2 and the interpretation of results, particularly regarding seen/unseen queries, would enhance understanding. We also advise against drawing parallels with human cognition, focusing instead on practical aspects of model calibration and uncertainty estimation. Finally, addressing the lack of references to relevant literature would strengthen the paper's contributions.