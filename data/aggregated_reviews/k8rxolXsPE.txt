ID: k8rxolXsPE
Title: SuperDialseg: A Large-scale Dataset for Supervised Dialogue Segmentation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new dataset, SuperDialSeg, aimed at addressing the dialogue segmentation problem. The authors propose a feasible definition of segmentation points based on document-grounded dialogue and construct a large-scale dataset of approximately 1,000 examples. They provide benchmark performance for 18 models across various dialogue segmentation tasks, demonstrating that the proposed dataset significantly enhances model performance.

### Strengths and Weaknesses
Strengths:  
- The SuperDialSeg dataset and benchmarks, which include modern architectures like BERT and GPT-based models, are valuable for future research.  
- The human verification process for data quality is well-designed and clearly explained.  
- Extensive experiments across three datasets and 18 methods yield substantial findings, contributing to the dialogue segmentation field.  
- The paper is well-structured and motivated, with a simple yet effective annotation heuristic.

Weaknesses:  
- The writing in Section 3.2 regarding the data construction approach is unclear, lacking a formal definition of the proposed segmentation annotation method.  
- The quality of derived segmentation labels may be affected by the noise from the original annotations of the two document-grounded dialogue corpora.  
- Some parameter settings are underspecified, and the training/evaluation data are not widely available, which may hinder reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 3.2 by providing a formal definition or pseudo code for the dialogue segmentation method. Additionally, addressing the ambiguity in the human verification process by directly asking annotators to accept or reject segmentation annotations would enhance understanding. Finally, ensuring that parameter settings are clearly specified and that training/evaluation data are made more accessible would improve reproducibility.