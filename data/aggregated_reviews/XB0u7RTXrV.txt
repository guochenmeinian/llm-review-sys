ID: XB0u7RTXrV
Title: SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for data augmentation of negative examples in question answering (QA) through self-training and rejection sampling using a paraphrase detector. The authors propose a framework for counterfactual sample generation, which innovatively creates new classes of negative samples. The study emphasizes the practical value of addressing unanswerable questions in QA, aiming to reduce labeling costs while improving model performance across various datasets.

### Strengths and Weaknesses
Strengths:
- The proposed method significantly reduces data collection costs and enhances performance.
- The paper is well-organized, clearly written, and supported by extensive experimental evidence across multiple QA datasets.
- The innovative SCENE framework facilitates the integration of datasets with differing labels, promoting model migration and dataset unification.

Weaknesses:
- The complexity of the pipeline leaves some aspects, such as the necessity of the paraphrase detection model, insufficiently discussed.
- Experimental results indicate that while performance on new labels improves, it negatively impacts the original labels, raising concerns about the effectiveness of generating harder samples without a subsequent fact-checking step.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the term "harder negative examples" by providing a more precise description of sample hardness. Additionally, the authors should consider discussing the necessity of the paraphrase detection model in the Filter component and its impact on results. To address performance trade-offs, we suggest exploring the inclusion of an additional fact-checker, such as an LLM, after generating answers to maintain original label performance. Finally, we encourage the authors to clarify the variations in hyperparameters $\lambda_{\rm Shuf}$ and $\lambda_{\rm Retr}$ throughout the experimental phases.