ID: 2b9aY2NgXE
Title: Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 4, 6, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of using pseudolabels for Visual Language Models (VLMs) across various downstream tasks, including semi-supervised learning (SSL), transductive zero-shot learning (TZSL), and unsupervised learning (UL). The authors propose three training strategies—FPL, IFPL, and GRIP—differentiated by the static or dynamic nature of the pseudolabels generated from CLIP. The authors investigate the Robin Hood effect, revealing that their top-K pseudolabeling strategy combined with prompt tuning leads to a more balanced class accuracy distribution. Experimental results across six datasets demonstrate significant performance improvements, although concerns regarding the accuracy of pseudolabels over iterations suggest potential overfitting as a factor.

### Strengths and Weaknesses
Strengths:
- The writing is clear and logically structured, facilitating understanding.
- The paper provides valuable insights into the application of CLIP pseudolabeling across multiple learning paradigms.
- The performance improvements observed are impressive, and the experiments conducted are thorough and well-documented.
- The authors' exploration of the Robin Hood effect and its implications for class accuracy distribution is a novel contribution.

Weaknesses:
- The novelty of the work is limited; the use of CLIP's pseudolabels is not new, and the proposed training strategies are common in self-training methods.
- There is a lack of clarity regarding why GRIP achieves substantial improvements, especially on datasets where CLIP performs poorly. Further analysis is needed to explain this phenomenon.
- Concerns about computational cost arise due to the use of 10 iterations, which should be addressed in comparisons with other methods.
- Some reviewers argue that the methodological novelty is limited, suggesting that the extension of existing methodologies to SSL and TZSL is marginal.
- Concerns about the scalability and computational costs of the proposed approach remain, with suggestions for more efficient implementations.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contributions by providing a clearer distinction between their methods and existing self-training techniques. Additionally, further analysis is needed to elucidate the reasons behind GRIP's performance, particularly in cases where CLIP's pseudolabels are unreliable. We suggest including a discussion on the computational costs associated with the 10 iterations used in the experiments and addressing scalability concerns by incorporating methods that reduce computational costs, such as caching embeddings when using a single prompt modality. Moreover, please ensure that all tables and figures are referenced correctly in the text and enhance their readability. Lastly, we encourage the authors to clarify the methodology regarding the selection of pseudolabels and the implications of balancing classes in self-training.