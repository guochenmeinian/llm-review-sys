ID: cdlmsnQkZ9
Title: Learning non-Markovian Decision-Making from State-only Sequences
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 6, 6, 7, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 2, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an offline model-based method for learning a non-Markovian Decision Process (nMDP) through the Lan-MDP algorithm. The authors propose a framework that estimates transitions and policies via maximum likelihood estimation (MLE) using Langevin dynamics, transforming behavior cloning into a reward-maximization problem by constructing a reward based on the estimated transition/policy for sequential decision-making. The authors define the posterior distribution \( p_\theta(a_t|s_{0:t+1}) \) and its relationship to the prior and likelihood in a Bayesian context, asserting that their method can effectively learn policies that account for non-Markovian rewards. The paper demonstrates the effectiveness of Lan-MDP through experiments on various tasks, showing it outperforms existing methods, although the theoretical justification for including the online loss \( L_\text{online} \) in the objective function is questioned.

### Strengths and Weaknesses
Strengths:
- The proposed method is novel and addresses the challenging problem of non-Markovian imitation learning from state-only sequences.
- The formulation as an MLE problem is interesting and relevant, particularly in contexts where expert actions are unobserved.
- The authors provide a comprehensive framework for addressing imitation learning from observations (ILfO) in nMDP, contributing to the understanding of non-Markovian dynamics.
- The paper includes clear empirical evidence supporting the effectiveness of the Lan-MDP approach, suggesting it performs comparably to existing methods in certain contexts.

Weaknesses:
- The reliance on MCMC and importance sampling raises concerns about computational efficiency and scalability to more complex scenarios.
- The theoretical justification for the inclusion of \( L_\text{online} \) and the convergence of the joint optimization process is insufficiently addressed.
- The paper lacks sufficient examples to illustrate the proposed method's applicability, particularly in real-world or synthetic problems.
- The motivation for using the cubic planning task as a representative example of real-world applications is questioned, as it may not reflect practical scenarios.
- There is a lack of clarity regarding the motivation for using nMDP and the advantages of model-based imitation over conventional methods.
- The experiments do not adequately compare against relevant baselines, such as off-policy and model-based ILfO algorithms.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing explicit explanations for the various policy-related terms used and ensuring that the final objective function is clearly formulated. Additionally, we suggest including more examples, particularly from real-world applications, to demonstrate the method's effectiveness in diverse settings. It would be beneficial to compare the proposed method against established baselines, such as OPOLO and MobILE, to better contextualize its performance. Furthermore, addressing the computational overhead of MCMC and exploring the implications of using a multimodal policy could enhance the robustness of the findings. We also recommend improving the theoretical derivation for Equation (10) to clarify the independence of trajectories from rewards and elaborating on the theoretical convergence of the joint optimization process. Lastly, clarifying the practical implications of context length on performance and providing examples where Lan-MDP outperforms modified OPOLO would strengthen the paper.