ID: Nn0daSf6CW
Title: Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 7, 5, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of return landscapes in deep reinforcement learning agents for continuous control, revealing that the mapping between policy parameters and returns can show high-frequency discontinuities, affecting agent performance. The authors introduce the notion of noisy neighborhoods in the return landscape, demonstrating that policies with similar average returns can have significantly different distributional profiles. They analyze the influence of these neighborhoods on learning dynamics and propose a distribution-aware procedure to enhance policy robustness. Additionally, the authors propose an algorithm that aims to improve policy stability in reinforcement learning by optimizing against a return landscape, utilizing a novel rejection sampling technique applied to policy parameters rather than actions. The relationship between the optimization objective and the return landscape is emphasized as central to their work.

### Strengths and Weaknesses
Strengths:
- The paper offers a novel perspective on the instability of deep reinforcement learning agents by examining the return landscape.
- The introduction of noisy neighborhoods and the characterization of post-update return distributions yield valuable insights into the relationship between policy parameters and returns.
- The proposed algorithm provides a unique perspective on policy stability and rejection sampling.
- The experiments and visualizations effectively illustrate the findings and support the authors' claims, with additional experiments strengthening the paper.

Weaknesses:
- The introduction of Algorithm 1 appears abrupt and lacks a clear connection to earlier conclusions, particularly regarding the interpolation of stability along gradient directions. The authors could explore leveraging updates that yield more stable neighborhoods.
- The algorithm's clarity makes it challenging for readers to relate it to existing methodologies; a more comprehensive comparison with related work is necessary.
- The claims about interpolating between policies from the same versus different runs are only supported by a limited number of qualitative examples, suggesting a need for quantification across a larger sample.
- The terminology used, such as "noisy" and "stability," is deemed inappropriate and misleading by some reviewers.
- The connection between the optimization objective and return landscape is inadequately explored.
- The experimental design lacks statistical robustness, with insufficient runs to support claims of significance.

### Suggestions for Improvement
We recommend that the authors improve the connection between Algorithm 1 and the earlier conclusions regarding stability interpolation, potentially by considering updates that enhance stability in neighborhoods. Additionally, a more thorough comparison with existing methodologies should be included to help contextualize the proposed algorithm. We suggest quantifying the claims made in Section 4.1 across a broader population of policy pairs to strengthen the argument. Expanding the experimental setup to include a wider variety of environments would also enhance the robustness of the findings. Furthermore, we urge the authors to improve the clarity of the paper by proposing alternative and proper naming for "noisy landscapes" and "stability," committing to these changes throughout the manuscript. Lastly, we recommend that the authors repeat the rebuttal experiments with a minimum of 20 seeds to ensure statistical reliability and to incorporate all promised changes from the rebuttal into the final version.