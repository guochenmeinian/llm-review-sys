ID: tGuMwFnRZX
Title: Latent Graph Inference with Limited Supervision
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 7, 7, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for latent graph inference (LGI) that addresses the supervision starvation problem caused by graph sparsification operations. The authors propose identifying k-hop starved nodes and mitigating their effects by incorporating a regularization adjacency matrix. They utilize CUR matrix decomposition to reduce computational costs and improve performance on various benchmarks. The method is model-agnostic, allowing integration with existing LGI models. Additionally, the authors introduce connections between labeled and unlabeled nodes to enhance model performance, leveraging the assumption that unlabeled nodes belong to specific classes. They acknowledge the complexity of the relationship between node degree distributions and model performance, suggesting further exploration is warranted. The authors also discuss the significance of labeling rates and baseline models in their results, clarifying definitions related to k-hop starved nodes and the target matrix Q.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow.
2. It identifies the supervision starvation problem in LGI, providing solid theoretical and empirical support.
3. The proposed method is model-agnostic and can be integrated into various LGI models.
4. Extensive experiments demonstrate significant improvements on benchmarks.
5. The paper effectively highlights the potential of addressing supervision scarcity in graph learning, showing significant improvements in specific scenarios.
6. The authors are responsive to reviewer feedback, committing to clarifications and enhancements that improve the manuscript's clarity and logical flow.

Weaknesses:
1. The method may be sensitive to hyperparameters $\tau$ and $\alpha$.
2. The efficiency of CUR decomposition is not clearly justified, as experimental results focus on accuracy rather than efficiency.
3. Sections 3.2 and 3.3 lack clarity, making them difficult to follow.
4. Some concerns regarding presentation clarity were noted, and the authors recognize the need for further exploration of the implications of degree distributions on model performance.
5. The discussion on the implications for inductive learning scenarios remains outside the current scope of the paper.

### Suggestions for Improvement
We recommend that the authors improve the explanation of why CUR decomposition is more efficient and provide a clearer mathematical form for $L_{reg}$ in Equation 9. Additionally, we suggest that the authors discuss state-of-the-art GNNs in more detail, as the proposed approach is model-agnostic. Clarifying the relationship between the proposed method and the generation of the residual adjacency matrix would also enhance understanding. We further recommend that the authors improve the clarity of definitions, particularly regarding k-starved weights and the exclusion of self-connections from neighborhood definitions. Including experimental results to demonstrate the efficiency of the two variants of $\Gamma(\tilde{\mathbf{U}}, n)$ would be beneficial. Finally, we encourage the authors to incorporate discussions on the implications of their findings for inductive learning scenarios in future revisions.