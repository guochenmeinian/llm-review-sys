ID: X8ItT6mGKF
Title: MAN TruckScenes: A multimodal dataset for autonomous trucking in diverse conditions
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 8, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "MAN TruckScenes," the first large-scale multimodal dataset specifically designed for autonomous trucking, addressing unique challenges such as trailer occlusions and novel sensor perspectives. The dataset comprises over 740 scenes, each lasting 20 seconds, recorded under various environmental conditions using a comprehensive sensor suite, including 4 cameras, 6 lidar, and 6 radar sensors. It features manually annotated 3D bounding boxes for 27 object classes and 15 attributes, and is notable for providing 4D radar data with 360° coverage, making it the largest radar dataset for 3D object detection. The authors also aim to enhance object detection and tracking in autonomous driving, particularly under adverse weather conditions, and propose to expand the evaluation of their dataset by including multiple baseline models across different modalities (LiDAR, radar, camera) and sensor fusion applications. They acknowledge the need for qualitative results, especially in challenging weather scenarios, and plan to include a comprehensive analysis of performance metrics and additional baseline models in the appendix.

### Strengths and Weaknesses
Strengths:  
- Significant contribution to autonomous vehicle research by addressing a critical gap with a dataset specific to autonomous trucks.  
- High-quality data ensured through a multi-stage annotation and quality assurance process.  
- Comprehensive sensor suite offers a rich multimodal resource for perception research.  
- Diverse environmental conditions enhance robustness.  
- Innovative inclusion of 4D radar data with 360° coverage.  
- Commitment to expanding baseline comparisons and including qualitative results to enhance usability.  
- Initiative to incorporate various modalities and sensor fusion is commendable.

Weaknesses:  
- The dataset is recorded exclusively in Germany, which may limit its applicability to other regions with different road conditions and traffic behaviors.  
- Data collection from a single autonomous test truck may restrict the diversity of sensor perspectives.  
- The experiments section lacks richness, with only one baseline method evaluated, primarily CenterPoint, and qualitative results in adverse weather conditions are missing.  
- The related work section is somewhat limited, overlooking other relevant datasets and algorithms, which could provide a more comprehensive context.  
- The performance of radar-based methods, such as RadarGNN, remains significantly lower than that of LiDAR-based methods.

### Suggestions for Improvement
We recommend that the authors improve the dataset's geographical diversity by expanding data collection to different countries and incorporating multiple test vehicles to enhance generalization and remove potential biases. Additionally, we suggest including a broader range of baseline models for comparison across all modalities in the experiments section, as well as providing qualitative results in adverse weather conditions to enhance user understanding of the dataset's performance. We encourage the authors to enhance the related work section by including datasets like the aiMotive dataset, K-Radar, and View-of-Delft, as well as relevant algorithms, to give readers a more comprehensive overview. To address the performance issues with radar-based methods, we recommend exploring advanced network architectures, such as encoder-decoder models, and implementing data augmentation techniques to improve detection of vulnerable road users (VRUs) and small objects. Lastly, including a picture of the real truck used for data collection in the appendix will provide valuable context for users.