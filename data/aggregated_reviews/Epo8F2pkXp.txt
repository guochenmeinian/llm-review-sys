ID: Epo8F2pkXp
Title: INVESTIGATING ANNOTATOR BIAS IN LARGE LANGUAGE MODELS FOR HATE SPEECH DETECTION
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 5, 6, 4
Original Confidences: 5, 5, 5

Aggregated Review:
### Key Points
This paper presents an exploration of annotator biases in Large Language Models (LLMs) for hate speech detection, identifying four key categories of bias—gender, race, religion, and disability—across LLMs such as GPT-3.5, GPT-4o, Llama-3.1, and Gemma2. The authors create the HateBiasNet dataset to study these biases and conduct a comparative analysis with the ETHOS dataset. The research aims to elucidate the implications of these biases for model performance and societal impact, particularly concerning vulnerable groups, thereby contributing to the discourse on responsible AI practices.

### Strengths and Weaknesses
Strengths:
- The paper addresses a critical issue in AI ethics, focusing on biases that can perpetuate discrimination against marginalized communities.
- Research questions are clearly articulated, enhancing the study's focus on annotator bias in LLMs.
- A comprehensive approach is adopted, analyzing multiple dimensions of bias and employing rigorous methodologies.
- The development of the HateBiasNet dataset fills a gap in existing datasets, facilitating deeper examination of bias in hate speech detection.

Weaknesses:
1. Lack of methodological transparency regarding data collection and annotation processes, including annotator selection and criteria for categorizing tweets.
2. Overemphasis on LLMs, potentially downplaying the role of traditional human annotators and their biases.
3. Findings may not be fully generalizable beyond hate speech detection due to the specific nature of the topic and annotator demographics.
4. Insufficient specificity in proposed mitigation strategies for bias.
5. The related works section lacks citations of relevant publications that could enhance the study's context.
6. The methodology for comparing LLM and human annotations is flawed, as it does not ensure demographic consistency between LLM personas and human annotators.

### Suggestions for Improvement
We recommend that the authors improve methodological transparency by providing detailed explanations of the data collection and annotation processes, including annotator selection criteria. Additionally, consider incorporating a balanced view of LLMs and human annotators by discussing their comparative performance and biases. To enhance generalizability, the authors should clarify the selection criteria for biases and vulnerable groups analyzed in the study. We suggest providing more concrete mitigation strategies for bias and expanding the related works section to include key publications relevant to the study. Lastly, we advise refining the methodology for comparing LLM and human annotations to ensure demographic consistency and clarify the reasoning behind the use of personas like "Not Asian" and "Not Black."