ID: SwphsE7hYO
Title: Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for model compression that enhances the performance of smaller models through a multi-step framework involving knowledge transfer from larger language models. The authors propose tuning in-domain soft prompts, utilizing reinforcement learning for model stability, constructing a knowledge store, and training smaller models. The experiments demonstrate performance improvements on various GLUE and SuperGLUE datasets, particularly with small BERT models.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach to train small-scale models, achieving strong performance against recent distillation baselines.
- The writing is clear, with well-presented figures and tables.
- The proposed method is evaluated through diverse experiments, providing a solid foundation for its claims.

Weaknesses:
- The claim that T5-3B is an "extreme large" model is an overstatement, raising concerns about the applicability of the method with only very small BERT models as students.
- The complexity of the method, involving multiple steps for training and knowledge retrieval, may hinder practical adoption.
- The paper lacks clarity in several areas, including the training process of the generator and the initialization of small models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing detailed explanations of the training process for the generator and the PPO agent. Additionally, we suggest including comparison baselines, such as simple fine-tuning of a randomly initialized BERT model, to better contextualize the proposed method's performance. To address concerns about the complexity, consider simplifying the training and compression procedure. Finally, we encourage the authors to extend their evaluation beyond NLU/classification tasks to include other domains, such as open-domain question answering, to validate the method's broader applicability.