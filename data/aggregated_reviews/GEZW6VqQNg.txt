ID: GEZW6VqQNg
Title: Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel evaluation method for large language models (LLMs) that engages them in debate-like interactions to assess their reasoning capabilities. The authors propose a task that tests whether LLMs can defend correct answers against user-presented invalid arguments. Extensive experiments reveal that LLMs struggle to maintain their beliefs in truth when faced with invalid arguments, highlighting reasoning deficiencies not captured by conventional benchmarks. The findings raise concerns about the risks of deploying LLMs in real-world scenarios without a clear understanding of the ground truth.

### Strengths and Weaknesses
Strengths:
- The paper introduces a well-motivated and innovative task formulation for evaluating LLMs through debate-like interactions.
- It reveals significant deficiencies in LLM reasoning abilities that are overlooked by traditional benchmarks.
- The study provides a thorough analysis of the implications of aligning LLMs with human feedback.
- The writing is clear and accessible.

Weaknesses:
- The paper lacks sufficient qualitative examples of failure cases across various reasoning tasks beyond mathematical reasoning.
- The lessons derived from the evaluation are unclear, particularly regarding the implications for future research.
- The treatment of uncertain answers in commonsense reasoning may favor the model unduly.
- Details regarding inter-annotator agreement and the characteristics of non-essential aspects in qualitative analysis are insufficiently explained.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation's lessons by providing comparisons with other models in addition to ChatGPT and GPT-4. It would be beneficial to analyze how user-generated invalid answers align with human responses, rather than solely relying on LLM-generated inputs. Additionally, we suggest that the authors provide specific examples of what constitutes correct uncertain answers in commonsense reasoning tasks. Finally, enhancing the qualitative analysis with details on inter-annotator agreement and the characteristics of non-essential aspects would strengthen the paper.