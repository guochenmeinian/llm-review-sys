ID: GAsRl2ElHk
Title: KAKURENBO: Adaptively Hiding Samples in Deep Neural Network Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 6, 5, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called KAKURENBO aimed at accelerating deep neural network (DNN) training by adaptively hiding samples based on their loss and prediction accuracy. The authors propose adjusting the learning rate and maximum hidden fraction for each training iteration to mitigate convergence issues. Empirical evaluations indicate that KAKURENBO can reduce training time by up to 22% with only a 0.4% decrease in accuracy compared to baseline methods. The paper also provides theoretical justification for the convergence of the proposed method.

### Strengths and Weaknesses
Strengths:
- The approach is novel, simple, and effective, demonstrating a compelling convergence-accuracy tradeoff.
- The manuscript is well-organized, with a thorough summary of related work that aids reader comprehension.
- High-quality figures and a comprehensive ablation study enhance the presentation and reproducibility of results.

Weaknesses:
- The hyperparameters are heuristically set without sufficient analysis, raising concerns about generalizability across different datasets.
- The experimental results are based on single trials, which may undermine the statistical significance of the findings.
- The calibration of the softmax confidence used for sample selection needs further discussion.
- Comparisons with stronger baselines and advanced methods are lacking, which could provide a clearer picture of the proposed method's effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the calibration of the softmax confidence and provide a more detailed analysis of the hyperparameters, including the number of trials needed to achieve the claimed acceleration. Additionally, conducting experiments on the jft-3b and lain-5b datasets would strengthen the manuscript's applicability and credibility. We also suggest including comparisons with more advanced methods and plotting the accuracy-time trade-off for different baselines to better illustrate the proposed method's superiority. Lastly, clarifying the differences between the proposed method and data distillation would enhance the manuscript's clarity.