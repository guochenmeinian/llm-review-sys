ID: WxLVYZbIew
Title: Building on Efficient Foundations: Effective Training of LLMs with Structured Feedforward Layers
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of structured approximations in the feedforward network (FFN) blocks of Transformers to enhance the efficiency of Large Language Models (LLMs). The authors propose three methods: LowRank, BlockShuffle, and BlockDense, along with a novel self-guided training technique that utilizes a dense matrix as a residual component. The experiments cover model sizes from 110M to 1.3B parameters, demonstrating a 2.5x speed-up with a 0.4 PPL increase under the same training FLOPs. The paper emphasizes the importance of addressing FFN bottlenecks as model sizes increase.

### Strengths and Weaknesses
Strengths:
- The topic of improving LLM efficiency is timely and significant, particularly as model sizes grow.
- Extensive experiments and ablations were conducted, covering various factors such as model size, pre-training tokens, and learning rates, along with scaling laws.
- The experimental setup is modern, utilizing techniques like RefineWeb, RoPE, and FlashAttention, and the writing is clear and accessible.

Weaknesses:
- The training duration (2B to 25B tokens) is insufficient for convergence, making it difficult to assess the competitiveness of the proposed methods.
- There is a lack of downstream task evaluations; only validation perplexity is reported.
- The paper does not include a baseline where the FFN remains dense, which is essential for comparison.

### Suggestions for Improvement
We recommend that the authors improve the experimental rigor by conducting additional training at 300B tokens to demonstrate the competitiveness of their methods at convergence. It is crucial to include a baseline where the FFN is not expanded, as this would provide a clearer comparison. Additionally, we suggest incorporating evaluations on downstream tasks, such as SuperGLUE, to strengthen the analysis of model quality. Furthermore, a comparison of self-guided training with other regularization methods, like spectral regularization, would enhance the understanding of its effectiveness. Lastly, we encourage the authors to acknowledge the limitations regarding model size and token count in relation to state-of-the-art models.