ID: lTUXlmjcva
Title: From Alexnet to Transformers: Measuring the Non-linearity of Deep Neural Networks with Affine Optimal Transport
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 4, 7, -1, -1, -1, -1
Original Confidences: 2, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the affinity score, a measure of the non-linearity of an activation function \(\sigma(X)\) based on the distribution of \(X\). The affinity score is defined through the approximation of the 2-Wasserstein distance \(W_2(X, Y)\) by \(W_2(N_X, N_Y)\), where \(Y=\sigma(X)\) and \(N_X\), \(N_Y\) are Gaussian approximations of \(X\) and \(Y\). The authors characterize a DNN model by the set of affinity scores of its activation functions under a given input distribution. Experimental results indicate that transformer-based vision models exhibit lower affinity scores, suggesting a more efficient use of non-linearity compared to CNN models.

### Strengths and Weaknesses
Strengths:
- The proposed affinity score offers valuable insights into the comparative non-linearity of CNN and transformer-based models, indicating that transformers utilize activation functions more effectively for higher prediction performance.
- The method is grounded in optimal transport theory, providing a robust theoretical foundation and demonstrating practical utility in predicting DNN performance.

Weaknesses:
- The affinity score shows low correlation with existing non-linearity metrics like \(R^2\), raising questions about the sufficiency of these metrics for DNN analysis. The authors should clarify how the distribution in Fig. 3(C) changes with alternative metrics.
- The affinity score's behavior at negative \(x\) for activation functions like ReLU is unexpected and requires explanation.
- The paper lacks a comprehensive discussion of various activation functions beyond ReLU, Tanh, and Sigmoid, which limits its applicability.
- Many formal results presented are well-known without proper citations, potentially misleading readers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of the non-linearity signature in Def. 3.1, as its current formulation appears ambiguous. Additionally, including a broader range of activation functions, such as GELU, would enhance the robustness of the analysis. We suggest that the authors compare their findings with existing research on non-linearity in DNNs and provide actionable insights on how their analysis can inform model architecture design or selection. Finally, addressing the misleading statements regarding the role of activation functions in function approximation would strengthen the paper's credibility.