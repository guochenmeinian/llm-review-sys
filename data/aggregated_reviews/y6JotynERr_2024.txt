ID: y6JotynERr
Title: Towards Diverse Device Heterogeneous Federated Learning via Task Arithmetic Knowledge Integration
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a knowledge distillation method, TAKFL, designed to address the challenges of heterogeneous device prototypes in federated learning. By capturing knowledge transfer among device prototypes, TAKFL aims to preserve each device's unique contributions and mitigate knowledge dilution. The authors propose a theoretical framework that optimizes global models while acknowledging overlaps in class labels across prototypes, arguing that this does not invalidate their theoretical results. They emphasize that overlapping class labels can create an i.i.d. condition that simplifies knowledge transfer, allowing for minimal information loss with the Vanilla Average Logits method. The method's effectiveness is validated through evaluations on various computer vision (CV) and natural language processing (NLP) tasks, demonstrating improvements in model accuracy and scalability. The authors also differentiate their approach from FedProto, highlighting distinct problem formulations and evaluation metrics.

### Strengths and Weaknesses
Strengths:
1. The authors tackle a significant real-world challenge in federated learning, focusing on knowledge transfer across heterogeneous devices.
2. The paper is well-structured, enhancing the clarity and accessibility of complex concepts.
3. Extensive experimental results support the claims, showcasing the effectiveness of TAKFL across diverse tasks.
4. The authors provide a clear theoretical framework that addresses the complexities of knowledge distillation in heterogeneous FL settings.
5. They effectively argue that overlapping class labels can facilitate knowledge transfer, maintaining the relevance of their theoretical insights.
6. The distinction between their method and FedProto is well-articulated, clarifying the unique contributions of their work.

Weaknesses:
1. The concept of "task arithmetic" is vague; a clearer explanation of its enhancement to federated learning design is needed.
2. The reliance on the task arithmetic property is questioned, particularly in cases of overlapping class labels, which may undermine the validity of some theoretical claims.
3. The knowledge distillation process appears largely derivative of existing methodologies, lacking exploration of novel design elements in TAKFL.
4. The reliance on outdated baselines like FedAvg and FedDF limits the evaluation's rigor; more recent baselines should be incorporated.
5. The assumption of weight disentanglement is overly strong and may not hold in practice, limiting theoretical impact.
6. The method incurs high computational costs, with resource usage scaling as O(M^2), which could be problematic.
7. The experimental design relies on public datasets, which may not reflect real-world distributions, necessitating further validation.
8. Some reviewers express concerns regarding the clarity and completeness of the theoretical propositions, suggesting that additional proof or citations are necessary to support certain conclusions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the "task arithmetic" concept and its implications for federated learning. Additionally, we suggest that the authors address the concerns raised about the validity of their claims when class labels overlap, potentially by revising the relevant sections to enhance the robustness of their theoretical framework. Exploring and highlighting any novel design elements in TAKFL would strengthen the contribution. Incorporating more recent baselines for comparison is essential for a rigorous evaluation. We also recommend addressing the strong assumptions regarding weight disentanglement, perhaps by providing empirical evidence of its practical validity. Furthermore, an efficiency analysis comparing time and storage costs with existing methods should be included. Lastly, conducting additional experiments to validate the impact of dataset choices on model performance would enhance the paper's robustness.