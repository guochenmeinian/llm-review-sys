ID: vuabr8zbCq
Title: Improving the Robustness of Summarization Models by Detecting and Removing Input Noise
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical examination of the performance decline caused by various input noises across different datasets and model sizes. The authors propose a method that detects and eliminates such noise during the inference phase, effectively addressing performance degradation without requiring additional training. The study quantifies the impact of noise on pretrained Transformer-based summarization models and analyzes how different model components are affected.

### Strengths and Weaknesses
Strengths:
- The investigation into the effects of noise on summarization models is both practical and novel, with significant implications for real-world applications.
- The paper is well-written and provides valuable insights into the performance impacts of noise, supported by empirical results.
- The proposed noise detection and removal strategies could be beneficial for future research.

Weaknesses:
- The synthetic nature of the noise introduced raises questions about the generalizability of the findings to real-world datasets, which may not exhibit similar noise patterns.
- The evaluation is limited, focusing solely on PEGASUS models, which restricts the breadth of the study.
- Some results show inconsistencies, such as a performance drop in certain scenarios, which are not adequately explained.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by incorporating a small test set with 'real noise' to validate their approach. Additionally, expanding the evaluation to include a wider range of models beyond PEGASUS would strengthen the study. The authors should also provide more insight into the unexpected performance declines observed in some scenarios to enhance clarity and understanding.