ID: 204YOrDHny
Title: Reparameterization invariance in approximate Bayesian inference
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 8, 5, 7, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the invariance of Bayesian neural networks (BNNs) under reparameterization and its impact on approximate posterior inference. The authors argue that this invariance leads to ambiguity in uncertainty estimates due to multiple reparameterizations yielding different posterior densities. They analyze the Laplace approximation's properties using linear algebraic and differential geometric tools, deriving an algorithm that outperforms existing methods in uncertainty quantification tasks.

### Strengths and Weaknesses
Strengths:
* The paper addresses a significant problem with impressive theoretical rigor and clarity.
* The presentation is clear, with helpful figures that enhance understanding.
* The theoretical tools, including the parameter manifold on the Generalized Gauss-Newton (GGN) approximation, are original and elegant.
* Comprehensive numerical results support the central claims.

Weaknesses:
* While the work is strong, minor improvements in presentation are needed, particularly in defining terms like "kernel" and "diffusion" for broader accessibility.
* The proposed method has higher computational complexity, which may limit its practicality for large-scale neural networks.
* The experiments focus on standard datasets and smaller networks, which may not fully validate the method's effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of terminology by explicitly defining terms like "kernel" and "diffusion" upon their first use. Additionally, the authors should evaluate the Riemannian diffusion method on more diverse datasets and larger models to strengthen empirical validation. Reporting time metrics for the proposed method would also help readers assess its practicality. Finally, addressing the need for new computational pipelines for handling the GGN matrix would enhance the paper's completeness.