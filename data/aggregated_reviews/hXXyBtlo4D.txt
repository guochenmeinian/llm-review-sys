ID: hXXyBtlo4D
Title: MMNMT: Modularizing Multilingual Neural Machine Translation with Flexibly Assembled MoE and Dense Blocks
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a modularized multilingual machine translation (MNMT) framework that integrates dense and mixture-of-experts (MoE) models, aiming to enhance translation performance across various resource settings. The authors introduce a three-stage training method and demonstrate the effectiveness of their approach through experiments on multiple datasets, including OPUS-100, where it outperforms both dense and MoE baselines.

### Strengths and Weaknesses
Strengths:  
- The model architecture and training method are well-structured and clear, making them easy to implement.  
- Extensive experiments provide comprehensive analysis and support for the proposed framework's effectiveness across high-resource, low-resource, and zero-shot translations.  
- The paper addresses the integration of language-specific modules, contributing to the understanding of multilingual translation.

Weaknesses:  
- The paper lacks comparisons to strong MNMT baselines such as NLLB and Lego-MT, limiting the evaluation of its performance.  
- There is insufficient detail on the size of the models and the number of parameters, which is crucial for understanding performance gains.  
- Figures 1, 4, and 5 are too small, hindering clarity.

### Suggestions for Improvement
We recommend that the authors improve the comparison by including additional state-of-the-art baselines in MNMT to better evaluate their model's performance. Additionally, the authors should provide more results from existing works and specify the number of parameters in Section 5. Clarifying the model sizes and discussing whether performance gains are due to larger model sizes would enhance the paper. Finally, we suggest enlarging Figures 1, 4, and 5 for better visibility.