ID: fT1RkAgrC3
Title: Over-parameterized Student Model via Tensor Decomposition Boosted Knowledge Distillation
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 5, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Over-Parameterization Distillation Framework (OPDF), which aims to mitigate performance degradation in limited-parameter student networks after knowledge distillation (KD). The authors propose using the Matrix Product Operator (MPO) for tensor decomposition to create an overparameterized student model, allowing for increased parameters during training without additional inference time. The proposed method integrates a tensor constraint loss to align teacher and student models, and experimental results across various KD tasks demonstrate its effectiveness.

### Strengths and Weaknesses
Strengths:  
1. The innovative application of MPO for expanding student model capacity during KD without incurring extra inference overhead is noteworthy.  
2. The methodology is well-structured, providing a clear overview followed by detailed explanations, making it accessible despite its abstract concepts.  
3. Comprehensive experiments across NLP and CV tasks validate the method's effectiveness and its orthogonality to existing KD techniques.  
4. The study effectively examines the impact of overparameterization scale and learning rate through ablation experiments.

Weaknesses:  
1. The paper lacks clarity in certain areas, such as the explanation of Figure 1 and the reshaping of weights into higher-order tensor formats.  
2. There is insufficient information on the time and memory costs associated with the overparameterization process using MPO.  
3. The experiments are conducted on smaller models, raising concerns about the scalability of the approach to large models like LLMs/VLMs.  
4. The paper does not adequately discuss the limitations of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing more detailed explanations of key concepts, such as the meaning of the arrows in Figure 1 and the reshaping process of weights. Additionally, it would be beneficial to include a discussion on the time and memory costs associated with the proposed method. We suggest that the authors address the scalability of their approach to larger models and provide distillation results on such models to validate applicability. Furthermore, we encourage the authors to clarify the statement regarding "losing its ability to think independently" and to elaborate on the differences between central and auxiliary tensors with supporting formulas or figures. Lastly, a comparison with other tensor network formats, such as tree tensor networks, could enhance the discussion on the advantages of MPO over SVD.