ID: tQOncmMEVO
Title: G-SPEED: General SParse Efficient Editing MoDel
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents G-SPEED, a lightweight general text editing model composed of a tagging model and a generation model. The authors propose an unsupervised data clustering method to curate datasets for training and employ a mixture-of-experts architecture to enhance performance. Experimental results indicate that G-SPEED achieves state-of-the-art performance on several editing tasks.

### Strengths and Weaknesses
Strengths:
- The unsupervised data clustering algorithm effectively trains editing models.
- G-SPEED demonstrates competitive performance compared to larger models.
- The paper is well-written and presents its methods clearly.

Weaknesses:
- Limited technical novelty, as many techniques used have been extensively explored in prior literature.
- Insufficient experimental comparisons, particularly with supervised baseline models.
- Lack of comprehensive analysis regarding the differences between G-SPEED and existing models.
- Concerns about potential contamination between training data and test sets.

### Suggestions for Improvement
We recommend that the authors improve the experimental comparisons by including more supervised baseline models, such as PEER, and ensuring evaluations are conducted under fair conditions. Additionally, we suggest adding human evaluation or automated evaluation metrics, such as those using ChatGPT/GPT-4, to strengthen the evaluation framework. The authors should elaborate on the advantages of their unsupervised clustering method over prior work and provide a detailed discussion of the pros and cons of LLM-based generative editing approaches compared to G-SPEED. Furthermore, addressing the potential contamination of data collected in an unsupervised manner with the test sets is crucial for validating the results. Lastly, we encourage the authors to clarify the motivation for using sparse BERT layers and to include descriptions of supervised state-of-the-art models in the appendix.