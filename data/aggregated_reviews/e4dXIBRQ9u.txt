ID: e4dXIBRQ9u
Title: Grammatical Error Correction via Mixed-Grained Weighted Training
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a mixed-grained weighted training approach for grammatical error correction (GEC), addressing discrepancies in training data by proposing weights at both token and sample levels. The authors utilize a homologous teacher model to measure accuracy and diversity in data annotation, leading to improved performance in sequence-to-sequence (seq2seq) and sequence-to-edit (seq2edit) paradigms. Empirical results indicate that the proposed MainGEC method outperforms existing baselines, although it does not achieve state-of-the-art (SOTA) results.

### Strengths and Weaknesses
Strengths:
- The proposed method shows performance improvement over strong baselines.
- The approach is easy to implement and demonstrates generality across GEC paradigms.
- The methodology is clearly presented, with helpful examples that enhance understanding.

Weaknesses:
- The concept of weighted training is not novel, positioning the paper more as an empirical contribution.
- The requirement for a homologous teacher model limits flexibility and raises questions about the necessity of training a student model from scratch.
- The paper lacks exploration of how different teacher models affect student learning effectiveness and does not validate the method's applicability in other languages or tasks.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contribution by providing a more comprehensive comparison with Lichtarge et al. (2020) through re-implementation for a fair assessment. Additionally, conducting experiments in other languages, such as Chinese and low-resource languages like Czech, would strengthen the validation of their method. The authors should clarify the targeted scenarios for using a student model and consider investigating the impact of various teacher models on learning effectiveness. Finally, it is essential to acknowledge that the results are not SOTA and to include relevant comparisons with larger models like T5-XXL, if computational constraints allow.