ID: Bf6WFWNCUP
Title: MG-ViT: A Multi-Granularity Method for Compact and Efficient Vision Transformers
Conference: NeurIPS
Year: 2023
Number of Reviews: 27
Original Ratings: 5, 5, 4, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MG-ViT, a two-stage multi-granularity method aimed at enhancing the efficiency of Vision Transformers (ViTs) by balancing computational cost and performance. The approach involves an initial lightweight ViT for single-granularity inference, followed by a multi-granularity stage if classification is uncertain. The authors acknowledge challenges in obtaining MAE pre-trained models for DeiT-S and LV-ViT-S but have conducted experiments using the MAE-Tiny model, demonstrating improved throughput without sacrificing accuracy. The method is evaluated on ImageNet, showing significant reductions in FLOPs without performance loss, and the authors plan to extend their experiments to larger MAE models and other architectures in the final version.

### Strengths and Weaknesses
Strengths:
1. The proposed method achieves a favorable speed-accuracy trade-off compared to existing methods.
2. Comprehensive experiments, particularly ablation studies, effectively support the claims.
3. The manuscript is well-structured and easy to follow.
4. Solid experimental results indicate that combining MG-ViT with MAE-Tiny significantly improves throughput.
5. The paper engages in constructive dialogue with reviewers, showing responsiveness to feedback and a willingness to improve the work.

Weaknesses:
1. Evaluation is limited to a single scale (small versions of DeiT and LV-ViT), raising concerns about generalizability to other scales.
2. The writing contains grammatical errors and confusing sentences, detracting from clarity.
3. The novelty of the method is questioned, as similar strategies have been explored in prior works.
4. Lack of throughput comparisons and results for downstream tasks like segmentation and detection limits the applicability of the findings.
5. There is inconsistency regarding whether models were trained from scratch or fine-tuned, which raises concerns about the fairness of comparisons with related works.
6. The focus on ImageNet classification is seen as limited, with a need for demonstrating advantages in other tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing by addressing grammatical errors and confusing sentences. Additionally, the authors should clarify the training methodology used for their models to address concerns regarding the comparison with other works. We suggest including evaluations on larger model scales and providing throughput comparisons alongside FLOPs in Table 3. To enhance the method's applicability, we encourage the authors to explore its integration with hierarchical ViTs and conduct experiments on downstream tasks such as segmentation and detection. Finally, we urge the authors to include experimental results for larger MAE models in the final version to substantiate their claims further and to develop comprehensive guidelines for threshold selection to aid users in practical applications.