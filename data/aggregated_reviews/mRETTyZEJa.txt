ID: mRETTyZEJa
Title: GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a retrieval-augmented story generation method that utilizes a framework called GROVE, which includes three main components: Retrieval Repository, Evidence Forest Construction via Asking Why, and Evidence Chains-supported Story Rewriting. The authors propose that this method enhances story generation by retrieving similar plots from the IMDB dataset and using them to fill in missing background information, ultimately leading to improved story quality as measured by various metrics.

### Strengths and Weaknesses
Strengths:
- The proposed method demonstrates creativity and addresses the challenge of balancing controllability and creativity in story generation.
- Experimental results indicate that the method achieves higher scores in likability, coherence, complexity, and creativity, supported by evaluations from experts in English Literature.

Weaknesses:
- The complexity of the proposed pipeline raises concerns about its necessity and effectiveness compared to simpler methods.
- The experiments lack comprehensiveness, with limited baselines and insufficient exploration of alternative approaches that could yield similar improvements.
- Some key terms and concepts, such as "evidence forest," are not clearly defined, leading to potential confusion.

### Suggestions for Improvement
We recommend that the authors improve the clarity of key terms, particularly "evidence forest," and provide a more detailed explanation of the experimental setup, including prompts used for baseline comparisons. Additionally, we suggest conducting a more comprehensive set of experiments with varied baselines to demonstrate the necessity of the proposed complex method. Specifically, exploring simpler prompt engineering techniques or alternative baselines could help validate the effectiveness of the retrieval-augmented approach. Finally, including human evaluations in future iterations of the paper would strengthen the claims regarding the method's performance.