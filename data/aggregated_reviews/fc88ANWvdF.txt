ID: fc88ANWvdF
Title: Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 5, 7, -1, -1
Original Confidences: 5, 3, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a general framework for structured matrices using Einsum, analyzing the scaling laws of these matrices in relation to rank, compute intensity, and parameters-per-FLOPs on GPT-2. The authors propose a sparse mixture of structured linear experts with high-rank, non-parameter-sharing taxonomy parameters and introduce a learning rate scaling rule for Einsum-induced structured matrices via maximal update parameterization. Additionally, the paper explores the integration of Mixture-of-Experts (MoE) with structured layers and conducts empirical studies to identify optimal design principles.

### Strengths and Weaknesses
Strengths:
- The paper provides a novel perspective on structured matrices through Einsum operations.
- It is clearly written with sufficient detail, offering valuable insights for future research.
- The topic is timely, addressing the need for efficient model compression as model sizes increase.

Weaknesses:
- The complexity of the notation, particularly the use of multiple alphabets for dimensions and taxonomy variables, hinders readability; occasional reminders of each alphabet's role would help.
- The continuity of the taxonomy space is questionable, as it appears discrete due to the integer nature of rank, FLOPs, and parameters.
- The novelty of the proposed method is unclear, and empirical results may require further improvement to substantiate the claims.
- Current experiments are limited to GPT-2 and one dataset, which may not support broad conclusions regarding scaling laws.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the notation by providing reminders of the roles of the various alphabets used throughout the paper. Additionally, we suggest addressing the continuity of the taxonomy space more thoroughly. To enhance the novelty and impact of the work, the authors should clarify the key insights and contributions of their submission, possibly by providing examples of using more factors in their framework. Expanding experiments to include different model architectures (e.g., BERT, ViT) and datasets would strengthen the conclusions drawn from the study. Finally, including comparisons with other MoE architectures, such as low-rank-MoE, would provide a more comprehensive evaluation of the proposed BTT-MoE architecture.