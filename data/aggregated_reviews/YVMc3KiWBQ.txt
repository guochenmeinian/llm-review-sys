ID: YVMc3KiWBQ
Title: Offline Reinforcement Learning with Differential Privacy
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 7, 7, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 2, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two offline reinforcement learning (RL) algorithms with differential privacy guarantees, specifically targeting tabular and linear Markov Decision Processes (MDPs). The authors demonstrate that these pessimism-based algorithms achieve instance-dependent sub-optimality bounds while ensuring differential privacy, with the privacy cost diminishing as sample size increases. The paper also introduces new confidence bounds using Bernstein concentration inequality, enhancing error estimation under privacy constraints.

### Strengths and Weaknesses
Strengths:
1. The paper is the first to provide a provable study of differential privacy in offline RL, addressing a significant gap in the literature.
2. It is well-written and accessible, despite its theoretical nature.
3. The algorithms are sound and practical, with empirical evaluations validating their performance under different privacy budgets.

Weaknesses:
1. The motivation for the problem needs clearer real-world justifications, particularly regarding the necessity of generating private policies from raw data.
2. Assumption 2.2 regarding data distribution may not hold in high-dimensional spaces; further discussion on this assumption and potential remedies is needed.
3. The addition of Gaussian noise to count statistics in the tabular MDP algorithm may lead to instability in uncertainty estimation; justifications for this approach and comparisons with alternative methods are required.
4. The requirement for two independent offline datasets of equal length in Algorithm 2 is unrealistic; clarification on the independence condition is necessary.
5. Experimental results for DP-APVI are absent, and the performance gap of DP-VAPVI compared to its non-private counterpart raises concerns about its competitiveness.
6. The impact of the privacy budget (ρ) on algorithm performance is not discussed, and the experiments are limited to synthetic datasets, which may not reflect real-world complexities.

### Suggestions for Improvement
We recommend that the authors improve the motivation section by providing a convincing real-world example that justifies the need for privacy in offline RL. Additionally, the authors should discuss the implications of Assumption 2.2 and suggest remedies for potential violations. Justifications for the Gaussian noise addition in the tabular MDP algorithm should be included, along with comparisons to other methods in the literature. Clarification on the independence condition for the datasets in Algorithm 2 is essential. We also suggest including experimental results for DP-APVI and addressing the performance gap of DP-VAPVI compared to non-private methods. Finally, a discussion on the impact of the privacy budget (ρ) and conducting experiments on real-world datasets would enhance the paper's practical relevance.