ID: BacLV3QUi8
Title: AniEE: A Dataset of Animal Experimental Literature for Event Extraction
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new dataset for biomedical Named Entity Recognition (NER) and Event Extraction (EE) focused on scientific literature about animal experiments, an area with limited existing resources. The dataset comprises 350 documents annotated with 22k entities and 10k events, utilizing a schema developed in consultation with domain experts. It includes complex structures such as discontinuous entities and nested events, which are crucial for accurately capturing experimental attributes. The authors provide a detailed account of the annotation process and report experimental results demonstrating the dataset's utility for training existing NER and EE models.

### Strengths and Weaknesses
Strengths:
- The dataset fills a significant gap in biomedical information extraction, particularly for animal experiments.
- The authors offer a thorough description of the annotation process and provide useful statistics on the dataset.
- Experimental results indicate that the dataset can effectively train models for detecting and classifying entities and events.

Weaknesses:
- Inter-annotator agreement for event annotations (0.586) is notably lower than for entities (0.973), raising concerns about the reliability of event annotations.
- The paper lacks detailed explanations of the annotation guidelines and the rationale behind certain methodological choices.
- The evaluation of models on the dataset is not sufficiently detailed, and comparisons with results from similar datasets are missing.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the inter-annotator agreement, particularly regarding the lower scores for event annotations, to clarify whether this reflects intrinsic task difficulty or issues with the guidelines. Additionally, we suggest that the authors provide more comprehensive annotation guidelines to enhance the dataset's reliability. It would also be beneficial to include comparative results from similar datasets to contextualize the performance of models trained on their dataset. Lastly, we encourage the authors to refine the related work section to focus more on event extraction datasets and tasks directly relevant to their study.