ID: mDRmX8IlBI
Title: MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 4, 8, 7, 8, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MMWorld, a benchmark for evaluating the world modeling capabilities of Multimodal Language Models (MLLMs) using a dataset of 1,910 videos across seven broad disciplines and 69 subdisciplines, comprising 6,627 question-answer pairs. MMWorld emphasizes multi-faceted reasoning tasks such as explanation, counterfactual thinking, and prediction, distinguishing itself from previous benchmarks by its multidisciplinarity and comprehensive evaluation of MLLMs' performance compared to human capabilities.

### Strengths and Weaknesses
Strengths:
1. MMWorld offers a thorough evaluation of MLLMs across diverse disciplines and reasoning tasks, providing insights into their strengths and weaknesses.
2. The benchmark includes a variety of question types that require multi-faceted reasoning, enhancing the understanding of model capabilities.
3. The study presents an in-depth analysis of MLLMs' performance, including comparisons with human performance, guiding future improvements.

Weaknesses:
1. The term "world model" may not accurately describe MLLMs, as they generate language-only outputs from multimodal inputs. The authors should provide a more precise term for MLLMs' capabilities.
2. The evaluation methodology, particularly the reliance on multiple-choice questions, may not fully capture the models' understanding of world dynamics. The authors should clarify how the benchmark differentiates from multimodal QA reasoning tasks in prior work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the benchmark's novelty by explicitly stating what unique capabilities it probes compared to existing benchmarks. Additionally, the authors should explore the potential of integrating video-specific information and address the limitations in evaluating intuitive physics capabilities. It would be beneficial to discuss how to build next-generation models based on the analysis of current models' performance and mistakes. Furthermore, we suggest that the authors consider organizing results by capabilities rather than disciplines to enhance clarity and relevance.