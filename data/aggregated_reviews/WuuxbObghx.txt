ID: WuuxbObghx
Title: Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FedPepTAO, a Parameter-Efficient prompt Tuning technique designed for efficient federated learning (FL) of Large Language Models (LLMs) using decentralized data. The authors propose a partial prompt tuning strategy that selects specific layers based on their importance to minimize communication and processing costs. Additionally, they introduce an adaptive optimization method to address client drift issues on both device and server sides. Experimental results demonstrate that FedPepTAO outperforms nine baselines across ten benchmark datasets, achieving significant improvements in accuracy and training time efficiency.

### Strengths and Weaknesses
Strengths:  
- The proposed efficient partial prompt tuning approach enhances performance while reducing the number of parameters updated during FL, making it more feasible for LLMs.  
- The adaptive optimization method effectively addresses client drift problems, improving performance in non-IID data scenarios.  
- The approach is technically sound, and the results show significant improvements in accuracy and efficiency.

Weaknesses:  
- The experiments are conducted using the RoBERTa-large model, which has only 355M parameters, raising concerns about applicability to larger LLMs typically exceeding 7B parameters.  
- Some figures and tables lack clarity and sufficient explanations, making it difficult to interpret results.  
- The paper primarily focuses on encoder-only tasks, with no results for decoder models presented.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 1 by adding a caption and enhancing its comprehensibility. Additionally, we suggest providing a detailed explanation of how target accuracies in Table 2 were determined, including the methodology for achieving these targets. It would also be beneficial to include evaluations on larger models, such as GLM XLarge and GLM XXLarge, to assess the scalability of the approach. Furthermore, we encourage the authors to report confidence intervals in their evaluations and consider comparing their method to LoRA for a more comprehensive analysis of parameter-efficient training methods.