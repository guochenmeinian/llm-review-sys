ID: hwjmEZ8561
Title: Analyzing Vision Transformers for Image Classification in Class Embedding Space
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 3, 6, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to reverse-engineer Vision Transformers (ViTs) for image classification, focusing on how internal representations at various levels are projected onto the class embedding space. The authors explore the contributions of self-attention and MLP layers in constructing categorical representations and provide insights into the interpretability of ViTs. They also introduce a framework that quantifies the alignment of intermediate layers in a neural network with the class-embedding space, challenging the assumption that such layers must inherently align. The authors demonstrate a progressive alignment to the class-embedding space throughout the network hierarchy, supported by perturbation studies. They clarify that their findings do not claim low performance due to mismatched weight matrices but rather highlight a correlation between the activation of the key-value memory pair system in MLP layer 11 and performance. The authors argue that identifying spurious correlations is crucial for enhancing the semantic robustness of ViTs.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a pioneering approach to reverse-engineering ViTs, offering novel insights into their representation construction.
2. It enhances mechanistic interpretability and explainability, identifying relevant image regions for class detection.
3. The distinct roles of self-attention and MLP layers are emphasized, illustrating their different contributions to categorical updates.
4. The framework provides a novel approach to quantifying alignment in neural networks.
5. The authors present meaningful results through perturbation studies, demonstrating progressive alignment.
6. The focus on spurious correlations offers insights into improving semantic robustness in ViTs.
7. The experimental design is clear and accessible, with well-presented tables and figures.

Weaknesses:
1. The novelty of the contributions is limited, with insufficient distinction from existing works, particularly regarding the assumption that class prototypes are encoded in the class-projection matrix.
2. The findings may not generalize to larger or differently trained ViTs, as the study primarily focuses on vanilla ViTs trained on ImageNet.
3. The rationale for summing gradients across blocks and the performance differences between block 11 and others remain unclear.
4. Some insights into network weights may be perceived as unsubstantiated due to assumptions about alignment.
5. The results and discussions, particularly in Section 5, lack adequate explanation and clarity.
6. Figures and tables in the paper are reported to be unclear, impacting the overall presentation of results.

### Suggestions for Improvement
We recommend that the authors improve the discussion in Section 3 to clarify the distinctions between their framework and similar NLP work, enhancing the clarity of technical details. Additionally, incorporating results from ViTs pre-trained on larger datasets would strengthen the findings and validate conclusions. We suggest further investigation into the performance differences between block 11 and other blocks, as well as a clearer rationale for summing gradients across blocks. Addressing the concerns regarding the assumptions about alignment could strengthen the perceived contributions of the paper. Lastly, we encourage the authors to provide a more thorough explanation of the results and discussions in Section 5 to enhance understanding, and to improve the clarity of their figures and tables, particularly by enhancing captions to provide more detailed content descriptions.