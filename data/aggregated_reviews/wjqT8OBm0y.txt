ID: wjqT8OBm0y
Title: A Refutation of Shapley Values for Explainability
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 4, 4, 3, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a formal definition of five anomalies related to explainability scores, demonstrating that for every n â‰¥ 4, Boolean classifiers defined over n features exhibit these anomalies concerning the SHAP score. The authors argue that this evidence highlights the inadequacy of Shapley values for explainability. The anomalies are defined through the lens of abductive explanation, where a feature's relevance is determined by its inclusion in minimal weak abductive explanations. Additionally, the authors provide a critical examination of Shapley values in the context of explainable artificial intelligence (XAI), asserting that their interpretation does not require an input baseline, contrary to some existing definitions. They acknowledge that future research may explore alternative definitions that include a baseline.

### Strengths and Weaknesses
Strengths:
1. The formalization of the five anomalies provides a clear representation of issues with explainability scores.
2. The paper offers valuable insights into the SHAP score, establishing a framework for evaluating its adequacy.
3. It presents one of the first formal results demonstrating the inadequacy of Shapley values for explainability.
4. The writing is clear and well-structured.
5. The authors provide a clear rebuttal to the reviewer's comments, emphasizing their interpretation of Shapley values and acknowledging limitations.

Weaknesses:
1. The results indicate that only a small proportion of Boolean classifiers exhibit the anomalies, suggesting that most classifiers may not demonstrate these issues.
2. The paper lacks discussion on practical formalisms, such as decision trees, suitable for expressing the Boolean functions that exhibit these anomalies.
3. The authors' stance may overlook the importance of baseline considerations in certain interpretations of Shapley values, which could limit the applicability of their findings.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the practical implications of their findings, particularly regarding the representation of Boolean classifiers in common formalisms like decision trees or binary decision diagrams. Additionally, we suggest that the authors clarify the significance of their results by addressing the proportion of classifiers exhibiting anomalies and exploring the implications for real-world applications. Furthermore, we encourage the authors to refine their claims about the inadequacy of Shapley values, possibly framing them as "incompatible" with certain definitions of feature importance rather than outright refutations. Lastly, we recommend that the authors enhance their discussion on the implications of their findings for definitions of Shapley values that include a baseline and provide more context on how their results relate to existing literature.