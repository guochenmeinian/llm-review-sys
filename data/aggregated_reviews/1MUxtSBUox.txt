ID: 1MUxtSBUox
Title: Budgeting Counterfactual for Offline RL
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel offline reinforcement learning (RL) algorithm, BCOL, which limits counterfactual decisions rather than enforcing policy or value regularization. The authors propose a dynamic programming approach to learn how to allocate a fixed number of counterfactual decisions across states. They argue that their method provides a unique upper bound on counterfactual decision-making, which is not addressed by existing offline RL methods. Experimental results demonstrate the algorithm's performance across various offline RL tasks, with two implementations based on TD3 and SAC. The paper also provides a theoretical justification for the budgeting approach, emphasizing the flexibility of their method across various datasets and environments, and highlighting its performance in challenging scenarios.

### Strengths and Weaknesses
Strengths:
- The authors detail their experimental setup and implementation, enhancing reproducibility.
- The empirical results are presented clearly, with effective ablation studies on hyperparameters.
- The budgeting concept for counterfactual decision-making is novel and promising, addressing gaps in existing methods.
- The proposed method offers a novel way to limit counterfactual explorations with proven guarantees.
- The authors provide a comprehensive comparison of their method against recent offline RL techniques, demonstrating its benefits.
- The inclusion of a grid-world example effectively illustrates the budgeting mechanism and its advantages.

Weaknesses:
- The paper lacks a clear explanation of how BCOL learns where to extrapolate, which is crucial for understanding its efficacy.
- The benefits of limiting counterfactual explorations need clearer formal demonstration compared to methods that do not utilize counterfactual budgeting.
- The algorithm introduces additional complexity in learning the Q function, which may increase computational difficulty.
- The choice of the budget parameter B requires further clarification to avoid potential issues in its application across different environments.
- Performance metrics show that BCOL does not consistently outperform existing algorithms like CQL and CDC, raising questions about its competitive edge.
- Concerns regarding the safety of inducing counterfactuals and their implications in real-world deployments remain inadequately addressed.
- The budget tuning process is unclear, and the method's applicability to continuous settings is limited.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for BCOL by providing formal guarantees regarding its performance and conditions under which it outperforms other methods. Additionally, we suggest improving the formal demonstration of the benefits of counterfactual budgeting compared to traditional methods. Clarifying the motivations behind counterfactual budgeting and addressing safety concerns in offline RL applications would enhance the paper's impact. Furthermore, providing more detailed guidance on selecting the budget parameter B for various environments would mitigate potential issues. Including more recent and competitive imitation learning baselines in the experiments would provide a clearer context for BCOL's contributions. Lastly, we encourage the authors to incorporate the grid-world example into the updated manuscript for enhanced clarity, as well as visualizations of critical decision-making steps and a more detailed analysis of budget expenditure over time to strengthen the empirical findings.