ID: 01GQK1gwe3
Title: Can Neural Networks Improve Classical Optimization of Inverse Problems?
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 4, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to gradient-based non-convex optimization by jointly optimizing multiple inverse problems through a reparameterization of the parameter space using neural networks. The authors implement this methodology with classical optimization techniques like BFGS and demonstrate its efficacy through experiments on various complex inverse problems, including the Kuramoto-Sivashinsky equation and the Incompressible Navier-Stokes equation, showing measurable performance improvements.

### Strengths and Weaknesses
Strengths:
- The main problem addressed is relevant, and the authors highlight the limitations of traditional optimizers in complex landscapes.
- The experimental results indicate significant improvements in solution accuracy, particularly with increasing problem sizes.
- The paper is well-structured and clearly presents the methods, experiments, and results.

Weaknesses:
- The experiments rely on synthetic inverse problems; real-life examples would enhance the applicability of the findings.
- The lack of clarity regarding the specific inverse problems and loss functions complicates understanding.
- The proposed method incurs high computational costs, which may limit its practical use.
- There is insufficient discussion on the generalizability of the results and the potential limitations of the reparameterization approach.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the inverse problems and loss functions used in their experiments to facilitate better understanding. Additionally, providing real-life examples of inverse problems would strengthen the paper's applicability. We suggest including a detailed runtime comparison between BFGS and Neural Adjoint methods, as well as a breakdown of the computational costs associated with the proposed method. Furthermore, addressing the potential limitations of the reparameterization approach and discussing its practical applicability would enhance the robustness of the findings. Lastly, we encourage the authors to normalize the loss functions and clarify the statement regarding supervised learning's relationship with complex loss landscapes.