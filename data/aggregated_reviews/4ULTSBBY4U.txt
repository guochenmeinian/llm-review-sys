ID: 4ULTSBBY4U
Title: Training-free Diffusion Model Adaptation for Variable-Sized Text-to-Image Synthesis
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 5, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the challenges associated with using fixed-resolution diffusion models for generating images of varying sizes and proposes a scaling factor to stabilize attention entropy, addressing issues of image quality. The authors argue that diffusion models should generate images of arbitrary sizes without the need for specialized models, which is cost-prohibitive. They identify problems such as inadequate object representation and repetitive patterns when using square models for arbitrary aspect ratios. The proposed scaling factor is shown to mitigate these issues, with empirical validation through quantitative and qualitative evaluations.

### Strengths and Weaknesses
Strengths:
1. The introduction of a scaling factor to counteract entropy fluctuations is a novel and computationally efficient approach.
2. The implementation is straightforward, enhancing its applicability.
3. The paper includes robust quantitative evaluations and user studies, demonstrating significant improvements in common diffusion model issues.
4. A variety of qualitative samples illustrate the enhancements in generated images.

Weaknesses:
1. The rationale for using a fixed-resolution model as a baseline is unclear, as other methods like cascaded diffusion or GAN-based super-resolution could be more effective.
2. The range of test resolutions is limited, raising questions about the method's performance with more significant scale variations.
3. Missing comparisons with simple baselines, such as down-sampling or using super-resolution techniques, weaken the justification of the proposed method's contributions.
4. The main formula lacks thorough explanation and justification, leading to potential confusion.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the main formula (Equations 6 and 7) by providing justifications for the conjectures and approximations made. Additionally, the authors should compare their method against fixed-resolution synthesis followed by super-resolution or down-sampling to establish its effectiveness. It would also be beneficial to explore a wider range of resolutions in experiments to assess the method's limits. Furthermore, including comparisons with other machine learning methods, such as MultiDiffusion, would strengthen the paper's contributions. Lastly, addressing the compensation for human annotators and clarifying the evaluation metrics used for FID would enhance the paper's rigor.