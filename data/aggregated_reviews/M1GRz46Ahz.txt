ID: M1GRz46Ahz
Title: SHARCS: Efficient Transformers Through Routing with Dynamic Width Sub-networks
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SHARCS, a method designed to accelerate the inference of Transformer-based models by introducing a router that adjusts the model's width based on the difficulty of input samples. The authors propose a heuristic method to estimate sample hardness and finetune both the router and sub-networks simultaneously. Experimental results indicate that SHARCS outperforms prior methods, achieving improved inference efficiency while maintaining comparable accuracy across various NLP tasks.

### Strengths and Weaknesses
Strengths:
- The paper is well organized and easy to follow.
- It addresses an important problem of inference efficiency in pre-trained Transformer models.
- Extensive experiments demonstrate the effectiveness of the proposed method and its novel perspective compared to existing early-exiting methods.

Weaknesses:
- The writing quality requires thorough proofreading to enhance clarity.
- The novelty of the method is questionable, lacking a compelling rationale for its necessity and differentiation from existing methods.
- Insufficient experimental comparisons, particularly regarding the validation set and the need for ablation studies on the number of non-adaptive layers (K).
- The paper primarily compares SHARCS with early-exiting methods, which may not be appropriate given that SHARCS is not an early-exiting approach.

### Suggestions for Improvement
We recommend that the authors improve the writing quality through careful proofreading to enhance clarity. Additionally, the authors should provide a clearer rationale for the novelty of SHARCS and how it differs from existing methods. Expanding the experimental comparisons to include more recent adaptive approaches and conducting ablation studies on the number of non-adaptive layers (K) would strengthen the paper. Furthermore, addressing the questions regarding the router's training and the diversity of reduction factors during inference would clarify the method's effectiveness and rationale.