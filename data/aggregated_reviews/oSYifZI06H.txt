ID: oSYifZI06H
Title: Generative Spoken Language Model based on continuous word-sized audio tokens
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents tGSLM, a generative spoken language model that utilizes 200ms-long continuous audio tokens instead of traditional 20ms or 40ms discrete units. The authors propose that this approach enhances computational efficiency while maintaining performance comparable to standard models. The alignment of acoustic and lexical encoding is innovative, and the experiments conducted are thorough and detailed.

### Strengths and Weaknesses
Strengths:
- The alignment between acoustic and lexical encoding is a novel contribution.
- The implementation choices, such as the 200ms segmentation, are well justified.
- The paper is well-written, structured, and includes a comprehensive set of metrics.
- The results demonstrate that the model can generate speech effectively using continuous embeddings.

Weaknesses:
- The description of the system is complex, making it difficult to follow, particularly regarding the upsampling to HuBERT units.
- The performance of the 200ms-tGSLM is not significantly better than conventional models, except in specific metrics.
- There is a lack of information on generation speed at inference, which is crucial for practical applications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the system description, particularly the upsampling process to HuBERT units. Additionally, consider providing benchmarks on generation speed expressed in tokens/sec and sec_of_audio/sec to address practical concerns. It would also be beneficial to include one or two more baselines for comparison, such as tokenizations from AudioLM or BEATs, and to explore the possibility of generating speech directly from the 200ms tokens using techniques like GAN. Furthermore, we suggest renaming "lexical tokens" and "acoustic tokens" to more accurately reflect their continuous nature. Lastly, addressing the memory consumption of the FAISS layer in the context of inference would enhance the paper's comprehensiveness.