ID: w7b0RC74hT
Title: ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "ClickPrompt," a novel framework that integrates click-through rate (CTR) models with pretrained language models (PLMs) to enhance CTR prediction. It addresses the limitations of traditional CTR models, such as semantic information loss, and the inefficiency of PLMs in capturing collaborative knowledge. ClickPrompt generates interaction-aware soft prompts through CTR models, facilitating the alignment of collaborative and semantic knowledge. The framework allows for flexible fine-tuning strategies, either with or without PLMs, and demonstrates effectiveness through experiments on real-world datasets.

### Strengths and Weaknesses
Strengths:
1. The writing is clear and accessible.
2. The method is simple yet effective, achieving competitive performance compared to previous baselines.
3. The choice of datasets and thorough experimentation validate the proposed framework.
4. The concept of prompt-augmented masked language modeling (PA-MLM) is intriguing, with ablation studies indicating its significance.

Weaknesses:
1. The research question regarding model compatibility is not fully addressed, as experiments are limited to specific encoder-only models without considering other architectures.
2. The role of PLMs in the CTR process and the quality of soft prompts generated by CTR models require further discussion.
3. Improvements over baseline methods are minimal, often less than 0.5%.
4. There is a lack of discussion on efficiency and time considerations when fine-tuning CTR models without PLMs.

### Suggestions for Improvement
We recommend that the authors improve the discussion on model compatibility by including experiments with encoder-decoder models (e.g., BART and T5) and decoder-only models (e.g., BLOOM and LLaMA). Additionally, the authors should explicitly address the importance of PLMs in the CTR process and the effectiveness of soft prompts. We suggest providing a more detailed analysis of the efficiency and time costs associated with the different fine-tuning strategies. Finally, exploring the applicability of ClickPrompt to other large language models and discussing the performance of ChatGPT in predicting CTR would enhance the paper's depth.