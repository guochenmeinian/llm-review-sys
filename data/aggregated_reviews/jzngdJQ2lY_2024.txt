ID: jzngdJQ2lY
Title: Solving Minimum-Cost Reach Avoid using Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Reach Constrained Proximal Policy Optimization (RC-PPO), an RL algorithm aimed at solving the minimum-cost reach-avoid problem by reformulating the optimization on an augmented system. The authors derive an optimal control problem under constraints and demonstrate how to solve it in a reinforcement learning context. The results indicate an improvement in the optimized objective compared to methods that incorporate the goal objective into the optimization cost. The authors propose new experiments demonstrating that RC-PPO outperforms existing methods, including a comparison with SAC and extensive grid searches of reward function coefficients. The findings indicate that RC-PPO directly addresses the minimum-cost reach-avoid problem more effectively than prior methods. Additionally, the authors provide theoretical proofs establishing the equivalence of their formulation to the minimum-cost reach-avoid problem and highlight the limitations of the CMDP formulation in achieving optimal solutions.

### Strengths and Weaknesses
Strengths:  
The paper provides a solid theoretical foundation, including proofs and detailed explanations. The definitions and theorems are clearly formulated, and the experimental results are well illustrated, supporting the claims made. The proposed state augmentation technique appears crucial to the solution, and the algorithm outperforms competitors when the goal is included in the objective. The paper includes new experimental comparisons that reinforce the superiority of RC-PPO over baseline methods. The robustness of RC-PPO is demonstrated through experiments involving noise, showing graceful performance degradation.

Weaknesses:  
The state augmentation technique is not novel within constrained RL contexts, although its application here is different. The importance of the sub-problem addressed is not fully justified; further elaboration on its significance is needed. Some design choices, such as minimizing the initial state \( z_0 \) with the constraint \( z_t \geq 0 \), lack clarity. Additionally, the authors should explore simpler methods for solving the problem, including experiments with heavier penalties for not reaching the goal. The paper may benefit from further elaboration on the implications of the theoretical proofs presented. Some experimental setups could be expanded to include a broader range of scenarios for validation.

### Suggestions for Improvement
We recommend that the authors improve the justification for the importance of the reach-avoid problem, possibly by formulating safety gym benchmarks. Clarifying the rationale behind minimizing \( z_0 \) while imposing \( z_t \geq 0 \) would enhance understanding. We suggest conducting experiments with more significant penalties for not reaching the goal and varying rewards to strengthen the argument for the proposed approach. Furthermore, providing performance comparisons with other RL algorithms, such as TD3 and SAC, would bolster the rationale for using the two-phase PPO framework. Additionally, we recommend improving the discussion surrounding the implications of the theoretical proofs to enhance clarity and consider expanding the experimental setups to encompass a wider variety of scenarios, which would strengthen the validation of the proposed method.