ID: LloZFVwWvj
Title: Non-autoregressive Machine Translation with Probabilistic Context-free Grammar
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PCFG-NAT, a novel approach that utilizes Probabilistic Context-Free Grammar (PCFG) to enhance non-autoregressive translation (NAT) systems by capturing complex dependencies among output tokens. The authors argue that this method alleviates the limitations of conditional independence in traditional NAT models, allowing for better representation of both grammatical and semantic information. Experimental results indicate that PCFG-NAT narrows the performance gap with autoregressive models while maintaining speed advantages.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a novel method that effectively captures dependencies in NAT using PCFG.  
- Systematic designs, including a lightweight variant (RH-PCFG), architecture support, and decoding strategies, are well-developed.  
- The results demonstrate improved translation quality, as evidenced by BLEU scores on major benchmarks.  
- The paper is well-written and reproducible, with a strong motivation for learning hierarchical dependencies.

Weaknesses:  
- The architecture does not fully deliver on the promise of introducing PCFG-style dependencies, as the conditional independence assumption limits the relationship between tokens.  
- The comparison in Table 2 lacks earlier methods that achieve better performance, and some reported results are inconsistent with original papers.  
- The analysis of training complexity and the effectiveness of the RH-PCFG design is insufficiently detailed.  
- There is limited exploration of the syntax tree analysis, and the assumption of left-to-right connectivity is not empirically supported.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the training loss function and provide a more thorough analysis of training complexity compared to baselines. Additionally, we suggest including detailed ablation studies to elucidate the impact of each technique, particularly the role of glancing training and RH-PCFG. It would also be beneficial to address the missing references to related work and clarify the relationship between the parameters used in experiments. Finally, we encourage the authors to explore the implications of their assumptions regarding left-to-right dependencies and consider integrating insights from hierarchical SMT systems.