ID: i4z0HrBiIA
Title: Applying Sparse Autoencoders to Unlearn Knowledge in Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 6
Original Confidences: 3, 1

Aggregated Review:
### Key Points
This paper presents an investigation into the use of sparse autoencoders (SAEs) for unlearning knowledge in language models, specifically targeting bioweapon-related information. The authors test their method on gemma-2b-it and gemma-2-2b-it models using the biology subset of the Weapons of Mass Destruction Proxy dataset, comparing it to the Representation Misdirection for Unlearning (RMU) technique. The work addresses a significant issue in AI safety and ethics, emphasizing the necessity of selectively removing dangerous knowledge from language models. The authors propose a novel approach utilizing interpretable SAE features for unlearning, which shows potential.

### Strengths and Weaknesses
Strengths:
(1) The paper addresses a relevant and timely issue in AI safety.  
(2) The innovative use of SAEs for unlearning may provide more interpretability than existing methods.  
(3) Detailed analysis of individual SAE features demonstrates potential for fine-grained control in unlearning.  
(4) The comparison with RMU offers useful context for evaluating the SAE-based approach.  

Weaknesses:
(1) The effectiveness of the SAE-based unlearning appears limited compared to RMU, particularly regarding unwanted side effects on unrelated tasks.  
(2) There is a lack of thorough discussion on why the SAE approach underperforms relative to RMU.  
(3) The evaluation is restricted to two specific models and one dataset, which limits the conclusions.  
(4) The paper does not adequately address potential risks, such as misuse for removing ethical constraints from language models.  
(5) The authors mention the need for significant changes to the SAE training process or intervention method but fail to provide concrete suggestions for improvement.  

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of the SAE approach compared to RMU, providing insights into the underlying reasons for its underperformance. Expanding the evaluation to include a broader range of models and datasets would strengthen the conclusions. Additionally, addressing the potential risks associated with their method is crucial. Finally, we suggest that the authors offer specific, actionable suggestions for enhancing the SAE training process or intervention method to facilitate practical applications of their approach.