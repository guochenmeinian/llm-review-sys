ID: kK23oMGe9g
Title: Immiscible Diffusion: Accelerating Diffusion Training with Noise Assignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 2, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the "Immiscible Diffusion" method, which proposes a noise-assignment strategy that reassigns noise samples within each mini-batch to enhance the training of diffusion models. The authors claim that this approach accelerates training to reach the same FID threshold and captures more details in generated images. Additionally, the paper explores conditional generation using a Stable Diffusion model to generate ImageNet images based on class conditioning, reporting a decrease in the FID score, indicating improved performance. The authors also discuss the challenges of training diffusion models with a batch size of 1, emphasizing that larger batch sizes are typically more effective. They clarify that while batch size influences diffusion performance, its impact is relatively minor compared to the effects of immiscible diffusion. However, reviewers express significant concerns about the method's novelty, effectiveness, and the overall quality of presentation.

### Strengths and Weaknesses
Strengths:
1. The metaphor of “immiscible fluids” is creative and provides an interesting perspective on diffusion training.
2. The proposed method is simple and can be implemented with minimal code.
3. The paper offers a faster diffusion model training strategy while maintaining generation quality.
4. The authors demonstrate a quantitative improvement in FID scores, suggesting enhanced performance in conditional image generation.
5. The authors provide clear clarifications regarding the influence of batch size and the starting point of FID curves.
6. The proposed alternative assignment for image-noise pairs is an interesting approach that could enhance model performance without excessive GPU memory usage.

Weaknesses:
1. The mathematical notations and derivations are imprecise and unnecessarily complex, leading to potential inaccuracies.
2. The submission fails to adequately discuss or compare with existing literature, raising questions about its novelty.
3. The quality of presentation, including figures and writing, requires significant improvement.
4. The results presented are not convincing, with concerns about cherry-picking and unclear quantitative evaluations.
5. The qualitative examples are limited and potentially cherry-picked, raising concerns about the representativeness of the results.
6. The theoretical arguments regarding the first denoising direction lack clarity and may not adequately support the proposed method.
7. The distinction between their method and existing approaches like OT-CFM is not convincingly articulated.
8. The reliance on FID alone for evaluation may not be sufficient, and the visual differences in results are not as significant as claimed.
9. The mathematical proofs lack rigor, particularly in justifying certain assumptions and the implications of the limits applied in their equations.

### Suggestions for Improvement
We recommend that the authors improve the clarity and precision of the mathematical notations and derivations, simplifying complex equations where possible. Additionally, it is crucial to discuss related works in detail to highlight the novelty of the proposed method. The authors should enhance the presentation quality, including figures and writing, to ensure better understanding. We urge the authors to provide comprehensive qualitative and quantitative results without cherry-picking, ensuring that all findings are clearly presented and substantiated. Furthermore, we recommend that the authors improve the qualitative analysis by providing a broader set of images to demonstrate the effectiveness of their method without cherry-picking. Addressing the concerns regarding the model's ability to correctly generate images based on class conditioning is essential, possibly by including more robust experimental evidence. Clarifying the theoretical framework surrounding the first denoising direction and its implications for conditional generation is also necessary. Finally, we suggest conducting further experiments to evaluate the diversity of generated images, potentially using metrics such as CLIP-Score and CMMD, as well as exploring representation learning methods to assess generalization capabilities.