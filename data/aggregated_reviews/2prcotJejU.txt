ID: 2prcotJejU
Title: Prompting with Pseudo-Code Instructions
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the use of pseudo-code as instructions for prompting large language models (LLMs) instead of traditional natural language prompts. The authors propose that pseudo-code is less ambiguous and clearer, leading to improved performance in a zero-shot setting. They provide a dataset of pseudo-code instructions for 132 tasks from the Super Natural Instructions benchmark and demonstrate the advantages of this approach through experiments with two LLM families, BLOOM and CodeGen. The results indicate significant performance improvements, particularly highlighting the importance of task-indicative function names, docstrings, and comments.

### Strengths and Weaknesses
Strengths:
1. The novel approach of using pseudo-code prompts is innovative and has potential implications for future research.
2. The authors provide a comprehensive dataset of pseudo-code instructions that could benefit the NLP community.
3. Experimental results show that pseudo-code instructions outperform natural language instructions across various tasks.

Weaknesses:
1. The fairness of the experiments is questionable, particularly regarding the performance of vanilla LLMs in zero-shot settings, as they may generate excessive outputs.
2. The rationale for not using natural language instructions in the 2-shot prompting setting is unclear, especially given the consensus that few-shot instruction-based prompting is generally more effective.
3. The paper lacks specific examples of pseudo-code instructions and comparisons with natural language prompts, making it difficult to fully grasp the differences in effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their experimental design by addressing the fairness of the zero-shot results and providing more detailed specifications of output formats. Additionally, we suggest including examples of pseudo-code instructions and natural language prompts to illustrate the differences in their effectiveness. Furthermore, we encourage the authors to clarify the rationale behind the 2-shot prompting approach and provide examples of the prompts used in their experiments.