ID: qE5vtBMbCJ
Title: LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LEXTREME, a new multilingual benchmark for evaluating NLP models on legal and legislative text data. It compiles 11 existing legal NLP datasets across 24 languages from 8 language families into a unified benchmark. The authors provide baseline results using five popular multilingual encoder models (DistilBERT, MiniLM, mDeBERTa, XLM-R base/large) and monolingual models. The benchmark aims to facilitate the future development and evaluation of multilingual legal NLP models, addressing the current reliance on monolingual resources.

### Strengths and Weaknesses
Strengths:
- The benchmark is useful for tracking multilingual progress in the NLP domain.
- It highlights the moderate performance of baseline models and the underperformance of ChatGPT, indicating gaps in the field.
- LEXTREME consolidates most open datasets in the legal domain into a single accessible space, featuring a public leaderboard and full access to code.

Weaknesses:
- The work lacks novelty, as it does not introduce new models, data, or evaluation methods; it merely aggregates existing datasets and models.
- The results and discussions are underwhelming, with insufficient explanations for observed performances and no exploration of how recent advancements in long-context models could enhance outcomes.

### Suggestions for Improvement
We recommend that the authors improve the discussion section to provide deeper insights into the results, particularly addressing why native monolingual models outperform native legal models. Additionally, we suggest exploring the influence of jurisdiction on results, as different legal systems may affect model performance. It would also be beneficial to discuss how recent works on long context, such as SLED, could be applied to improve results. Finally, we encourage the authors to consider alternative evaluation metrics, such as micro F1 or label-weighted scores, to potentially alter model rankings.