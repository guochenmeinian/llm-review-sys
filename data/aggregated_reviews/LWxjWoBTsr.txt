ID: LWxjWoBTsr
Title: Large Language Models can Implement Policy Iteration
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 6, 5, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 1, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework where large language models (LLMs) utilize in-context learning to perform policy iteration, enabling reinforcement learning (RL) without gradient updates. The authors propose the In-Context Policy Iteration (ICPI) algorithm, which predicts actions and Q-values by updating prompts with historical data. They connect foundation models with the theory of policy iteration, emphasizing the method's applicability to embodied tasks, particularly in robotics, where verbal policy descriptions are impractical. Experiments on simple environments demonstrate that ICPI can achieve decent performance, particularly with the Codex model, and the authors highlight that their work has been publicly available on Arxiv since October 7, 2022, preceding the publication dates of related works.

### Strengths and Weaknesses
Strengths:
- The application of in-context learning to policy iteration is well-motivated and presents a novel approach to RL.
- ICPI shows promising empirical results across various toy tasks, with detailed ablation studies supporting its components.
- The paper establishes a significant connection between foundation models and policy iteration theory in reinforcement learning.
- The proposed method's applicability to embodied tasks, particularly in robotics, is a notable advancement.
- The authors provide a clear distinction between their approach and previous works, enhancing the understanding of its utility.
- The paper is well-written and presents its findings clearly, with extensive experiments designed to test specific failure modes.

Weaknesses:
- ICPI's applicability is limited to small, handcrafted environments, raising questions about its performance in more complex tasks like Atari or Mujoco.
- The current formulation only accommodates discrete state and action spaces, limiting its use in continuous-action scenarios.
- Comparisons to recent deep RL baselines are lacking, which would help contextualize ICPI's advantages.
- Some claims, such as the speed of ICPI compared to gradient-based methods, are unsupported and require empirical validation.
- The reliance on the ability to verbally describe a policy in previous works may limit their applicability compared to the authors' approach.
- The timing of the authors' Arxiv submission relative to other works may raise questions about originality, despite the authors' claims.
- The writing could be more concise, particularly in the related works and methodology sections, and there are minor issues with repeated content and reference typos.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of ICPI by exploring its performance in more complex environments, such as Atari and Mujoco tasks. Additionally, the authors should consider extending ICPI to handle continuous state and action spaces. To strengthen the paper, we suggest including comparisons with popular deep RL baselines like PPO, SAC, and CQL/IQL to better illustrate ICPI's advantages. Furthermore, a detailed comparison of computation time between ICPI and gradient-based methods is necessary to substantiate claims regarding speed. We also recommend that the authors improve clarity regarding the implications of their approach compared to previous works, particularly in terms of practical applications in robotics. Lastly, we encourage the authors to refine the writing for conciseness and address minor errors in references and repeated sections, and ensure future submissions begin with an anonymous link to facilitate smoother review processes.