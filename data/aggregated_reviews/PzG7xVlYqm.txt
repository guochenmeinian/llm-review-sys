ID: PzG7xVlYqm
Title: On the Computational Complexity of Private High-dimensional Model Selection
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 5, 5, 4, 5, -1
Original Confidences: 3, 2, 2, 3, -1

Aggregated Review:
### Key Points
This paper presents a differentially private algorithm for best subset selection (BSS) in high-dimensional sparse linear regression models, utilizing the exponential mechanism. The authors demonstrate that their algorithm achieves strong statistical utility guarantees under high-privacy regimes and can recover accurate models in low-privacy regimes, achieving minimax optimal conditions akin to non-private settings. To address computational complexity, they introduce a Metropolis-Hastings algorithm with polynomial mixing time, ensuring approximate differential privacy. Experimental results on simulated data validate the algorithm's effectiveness in identifying active features with reasonable privacy budgets.

### Strengths and Weaknesses
Strengths:
- The proposed exponential mechanism enhances the $\beta_\min$ condition for the DP BSS problem.
- The Metropolis-Hastings algorithm achieves approximate differential privacy with polynomial mixing time, maintaining sparsity through a double swap update.
- The paper is clearly written, and the algorithm is simple, requiring common assumptions in privacy and sparse linear regression literature, with theoretical guarantees on privacy and utility.

Weaknesses:
- High-probability events in Theorem 3.5, Theorem 4.3, and Corollary 4.4 need further clarification.
- The rapid convergence of MCMC is contingent on spurious features having low correlation with true features, which is often unknown in practice, complicating the assessment of this correlation.
- The assumptions regarding sensitivity and margin require better justification, and the algorithm's computational intensity raises questions about the practicality of the proposed MCMC version.

### Suggestions for Improvement
We recommend that the authors improve the clarity of high-probability statements in Theorem 3.5, Theorem 4.3, and Corollary 4.4 by incorporating the failure probability into the conclusions. Additionally, we suggest citing specific theorems from Guo et al. (2020) in the proof of the utility guarantee for better context. The authors should also address the disadvantages of output perturbation compared to the exponential mechanism and explore whether it can achieve pure DP with good utility guarantees. Furthermore, we encourage a more comprehensive comparison with existing computationally-efficient sparse optimization methods and a clearer motivation for the advantages of BSS. Lastly, the authors should consider removing the data-dependent assumption for bounded sensitivity (Assumption 3.2) in favor of a data-independent sensitivity bound through clipping or truncation.