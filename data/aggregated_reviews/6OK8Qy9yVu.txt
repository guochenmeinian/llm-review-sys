ID: 6OK8Qy9yVu
Title: Why Go Full? Elevating Federated Learning Through Partial Network Updates
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 7, 3, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the FedPart method, which addresses the 'layer mismatch' phenomenon in federated learning by implementing partial network updates. The authors demonstrate that FedPart outperforms previous methods while reducing communication and computation overhead. The approach involves full network updates in initial communication rounds, followed by training only one layer per round.

### Strengths and Weaknesses
Strengths:
- The proposed idea is innovative and well-executed, with a clear communication of the main concept.
- The experiments are extensive, comparing multiple methods and metrics, and show surprisingly good performance.

Weaknesses:
- The term 'layer mismatch' lacks convincing evidence and clarity, particularly regarding its connection to inadequate cooperation among layers.
- The strategy for selecting trainable layers is unoriginal and requires further justification for its effectiveness.
- The experimental setup includes five rounds of full network training, raising concerns about the impact of these trainings on the results.
- Communication cost analysis is insufficient, as it does not adequately address the implications of requiring more communication rounds while updating fewer parameters.

### Suggestions for Improvement
We recommend that the authors improve the clarity and justification of the 'layer mismatch' concept, possibly by providing additional evidence or literature support. The authors should also enhance the discussion surrounding the layer selection strategy to demonstrate its effectiveness. Furthermore, we suggest clarifying the experimental setup to address the potential impact of full network trainings on the results. Lastly, a more thorough analysis of communication costs is necessary, particularly regarding the implications of increased communication rounds when updating fewer parameters.