ID: qIkYlfDZaI
Title: A Closer Look at the CLS Token for Cross-Domain Few-Shot Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to Cross-Domain Few-Shot Learning (CDFSL) by investigating the role of the CLS token in Vision Transformers (ViT). The authors observe that randomly initializing the CLS token improves target-domain performance, attributing this to the CLS token's absorption of domain-specific information. They propose a method to decouple this domain information during source-domain training, enhancing the transferability and generalization of ViT in CDFSL tasks. The effectiveness of the proposed method is validated through extensive experiments across multiple benchmarks.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and easy to follow, with a clear problem statement and methodology.
- The findings regarding the CLS token's role in CDFSL are novel and significant, supported by comprehensive analysis and experiments.
- The methodology is innovative, with a clear rationale for decoupling domain information from the CLS token.
- The experiments demonstrate state-of-the-art performance across various datasets.

Weaknesses:
- There is a lack of validation using models other than DINO, and results from iBOT show only marginal improvements.
- The performance differences compared to existing methods are not substantial, with some benchmarks showing less than 1% improvement.
- The paper does not sufficiently discuss the generalizability of the approach to other tasks beyond those tested.
- The writing quality needs improvement, with some statements lacking rigor and clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of statements regarding the CLS token's role in handling the few-shot learning problem, as the current expression lacks rigor. Additionally, we suggest conducting validation using a broader range of models, including CLIP-pretrained Vision Transformers, to assess the applicability of findings. The authors should also provide a more detailed theoretical analysis of their results, particularly regarding the performance metrics and the reasoning behind their training setup. Finally, enhancing the discussion on the computational efficiency of the proposed method compared to existing approaches would strengthen the paper.