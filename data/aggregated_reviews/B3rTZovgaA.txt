ID: B3rTZovgaA
Title: Doolittle: Benchmarks and Corpora for Academic Writing Formalization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Academic Writing Formalization (AWF) task, aimed at enhancing academic writing quality through grammar correction, word refinement, and structure modification. The authors propose a large-scale, non-parallel dataset named DOOLITTLE, consisting of approximately 800 expert-annotated parallel paragraphs for development and testing. They introduce a method called metric-oriented reinforcement learning (MORL) and conduct a comprehensive evaluation of neural approaches on AWF, revealing a performance gap compared to human rewrites.

### Strengths and Weaknesses
Strengths:
- The paper introduces the AWF task and constructs the DOOLITTLE dataset, which is valuable for advancing formal academic writing automation.
- It provides a detailed annotation process and quality control, aiding future research.
- Extensive experiments demonstrate that the MORL method can effectively tune large language models, achieving promising results.

Weaknesses:
- The motivation for selecting automatic evaluation metrics is unclear, and the absence of commonly used metrics like GLUE or SARI is noted.
- The non-parallel nature of the dataset limits its applicability for Seq2Seq model training.
- Insufficient ablation experiments hinder understanding the specific gains from the MORL method.
- The analysis of experimental results lacks depth, particularly regarding perplexity (PPL) differences among models.
- Some technical details, such as the classification of BART-Large as a large language model (LLM), require clarification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the chosen evaluation metrics and consider incorporating GLUE or SARI. Additionally, providing a clearer explanation of the dataset's limitations regarding Seq2Seq training would enhance its universality. We suggest conducting more ablation studies to quantify the benefits of the MORL method. Furthermore, a deeper analysis of experimental results, particularly regarding PPL discrepancies, should be included. Lastly, we encourage the authors to clarify the classification of models like BART-Large in the context of LLMs.