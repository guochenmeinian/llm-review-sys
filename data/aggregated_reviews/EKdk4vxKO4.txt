ID: EKdk4vxKO4
Title: MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 5, 7, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MDAgents (Medical Decision-making Agents), a framework for optimizing collaboration among large language models (LLMs) in medical question answering. MDAgents classifies the complexity of medical questions into low, moderate, and high categories, assigning appropriate LLM agents based on this classification. The framework employs multi-turn discussions and iterative report refinement, demonstrating superior performance on multiple medical QA datasets compared to baseline models. The authors evaluate MDAgents on ten medical benchmarks, achieving promising results.

### Strengths and Weaknesses
Strengths:
- The writing is generally clear, and the displays are informative.
- A novel medical domain-specific agent collaboration method has been proposed.
- Comprehensive experiments demonstrate the superior performance of MDAgents.
- The evaluation setup includes a sensible range of datasets and competitors, with strong results across various settings.

Weaknesses:
- Reported scores are inconsistent with existing literature, raising questions about the state-of-the-art claims. Discrepancies in performance metrics, such as those for Medprompt, need clarification.
- The complexity checker, while novel, lacks thorough evaluation; a correlation study with human assessments is recommended.
- Some figures, particularly Figure 3 and Figure 5, are confusing and require clarification regarding their content and implications.
- The paper uses only 50 samples for evaluation without sufficient justification, which may not adequately represent performance.
- The descriptions of multi-party collaboration and agent roles are vague, leaving important design decisions unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of reported scores by addressing discrepancies with existing literature. Additionally, the authors should conduct a correlation study between LLM complexity assessments and human evaluations to validate the complexity checker. We suggest revising the confusing figures and providing clearer explanations of their content. Furthermore, the authors should justify the choice of using only 50 samples for evaluation and consider including open-ended questions to better simulate real-world medical scenarios. Lastly, we encourage the authors to provide more detailed descriptions of the multi-party collaboration and agent roles to enhance understanding of the framework's design.