ID: XQm8tlPKgY
Title: SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the SCITAB dataset, which addresses limitations in existing benchmarks for scientific fact-checking. Comprising 1225 challenging claims requiring compositional reasoning with scientific tables, SCITAB reflects real-world scenarios and is derived from actual scientific statements. The authors have established benchmarks using state-of-the-art models and identified unique challenges, including ambiguous expressions and irrelevant claims. The dataset is publicly available, providing a valuable resource for further research in scientific fact-checking.

### Strengths and Weaknesses
Strengths:  
- The SCITAB dataset is a high-quality, manually verified resource that presents significant challenges for the research community, especially given the performance of LLMs on traditional tasks.  
- Detailed annotation and manual filtration steps ensure the dataset's quality.  
- The paper includes an interesting error analysis that elucidates why LLMs struggle with this task.  
- The dataset's construction and the diversity of reasoning types are well-articulated, making the paper easy to follow.  

Weaknesses:  
- The human-machine collaboration in data annotation lacks analysis on best practices for utilizing human labor effectively.  
- Some manual annotation experiments were conducted by a single annotator, which could introduce biases.  
- The distribution of atomic reasoning functions is uneven, with certain types over-represented.  
- The dataset's size is smaller compared to comparison datasets, potentially limiting its applicability.  
- The task definition is unclear in some examples, and a clearer human baseline is needed.

### Suggestions for Improvement
We recommend that the authors improve the analysis of human-machine collaboration in dataset construction, focusing on optimizing the use of human labor. Additionally, the authors should conduct further evaluations of LLMs with SCITAB, particularly regarding compositionality in reasoning, and consider incorporating more experiments or case studies into the main content. Clarifying the method for generating unverifiable claims and exploring advanced prompting techniques for reasoning would enhance the paper. Lastly, addressing the uneven distribution of reasoning functions and providing a clearer human baseline would strengthen the overall argument.