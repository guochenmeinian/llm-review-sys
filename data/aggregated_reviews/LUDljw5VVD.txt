ID: LUDljw5VVD
Title: Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the effectiveness of Large Language Models (LLMs) as few-shot solvers for information extraction (IE) tasks compared to fine-tuned small Pre-trained Language Models (SLMs). The authors conduct extensive experiments across multiple datasets, revealing that while LLMs are generally ineffective as few-shot information extractors, they can complement SLMs by handling difficult samples. The proposed adaptive filter-then-rerank paradigm allows SLMs to filter challenging samples, which LLMs then rerank, achieving an average F1-gain of 2.4% on various IE tasks while maintaining efficiency.

### Strengths and Weaknesses
Strengths:
- The paper provides a thorough evaluation of LLMs as few-shot solvers for IE tasks, enhancing understanding of their capabilities and limitations.
- The proposed adaptive filter-then-rerank paradigm offers a novel solution that optimally utilizes both LLMs and SLMs, improving IE performance while managing costs.

Weaknesses:
- The paper lacks discussion on whether the training set was included in the pre-training of the LLMs, which is critical for understanding the results.
- There is insufficient exploration of the filter-then-rerank paradigm's validity concerning small-scale generative models like LLaMA and Vicuna.
- The analysis of why LLMs struggle with easier samples is limited, and a deeper investigation could enhance the paper's contributions.

### Suggestions for Improvement
We recommend that the authors improve the discussion regarding the training set's role in the pre-training of LLMs to clarify its impact on the results. Additionally, we suggest including a quantitative analysis of LLM performance on fine-grained datasets to support claims about their limitations. A case study or deeper discussion on LLM failures with easier samples would also strengthen the paper. Lastly, we encourage the authors to qualify statements about LLMs' performance by acknowledging that current models under vanilla prompting settings may not be optimal for IE tasks.