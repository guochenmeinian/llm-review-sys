ID: bVO1sWgnTx
Title: Efficient Classification of Long Documents via State-Space Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates State-Space Models (SSMs) for long document classification tasks, conducting a comprehensive evaluation that demonstrates their superiority over self-attention-based models. The authors propose a state space pooler to progressively reduce input length for deeper layers, enhancing efficiency and robustness.

### Strengths and Weaknesses
Strengths:
- The paper provides a thorough evaluation of SSM-based models across six long text classification datasets, both with and without pre-training.
- It demonstrates that SSMs outperform transformer-based self-attention models in terms of efficiency and robustness to input noise.
- The writing is clear and accessible.

Weaknesses:
- The proposed method is viewed as a simple pooling operation, lacking novelty.
- The analysis is not sufficiently in-depth, and the work may be considered slightly incremental.
- The adjustment made for efficiency comes at the cost of performance.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their proposed method and provide a more in-depth analysis of their findings. Additionally, we suggest including a discussion on the intuition behind why SSMs perform better than transformer-based models. It would also be beneficial to explore variations of the hierarchical mechanism and its theoretical connection to the S4 model. Finally, we encourage the authors to incorporate missing references to enhance the comprehensiveness of their study.