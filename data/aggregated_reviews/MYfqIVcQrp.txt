ID: MYfqIVcQrp
Title: Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 7, 4, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into graph contrastive learning (GCL) methods, particularly focusing on their distinctions from visual contrastive learning (VCL). The authors argue that GCL does not necessitate positive samples and can operate without negative samples under certain conditions, which challenges existing beliefs in self-supervised learning (SSL). They emphasize that simple augmentations, primarily involving Gaussian noise, can achieve good performance in graph classification. The study highlights the relationship between alignment loss and graph convolution, suggesting that this connection reveals the inductive bias of graph neural networks (GNNs) in GCL. The authors acknowledge the importance of clearly distinguishing their contributions from prior works and plan to enhance the clarity of these distinctions throughout the paper.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written, presenting well-structured arguments and demonstrating a clear understanding of the unique characteristics of GCL compared to image-based methods.
- The ablation experiments are convincing and provide valuable insights, establishing a connection between GNN architecture and contrastive learning objectives, which could inspire future research.
- The authors show a commitment to addressing feedback and improving the clarity of their contributions.

Weaknesses:
- Theorem 4.1 lacks clarity regarding its similarity to existing methods like GRACE/GCA/ProGCL, particularly concerning the augmentations used.
- Some claims may be perceived as over-stated, particularly regarding the novelty of findings in relation to existing literature, including the assertion that GCL can function without negative samples.
- Discrepancies in accuracy between MLP and GCN in Table 4 raise questions about the necessity of GCN when using InfoNCE.
- Section 5.2 does not adequately explain the intuition behind ContraNorm, leaving readers confused.
- The paper does not clarify whether the findings regarding Gaussian noise as augmentation apply to MLP as well as GCN.
- The use of Chameleon and Squirrel datasets for heterophily benchmarks is questionable due to their problematic nature.
- The experimental setups across different domains may lack standardization, raising questions about the validity of cross-domain comparisons.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Theorem 4.1 by explicitly discussing the assumptions and augmentations related to GRACE/GCA/ProGCL. Additionally, we suggest improving the language used in the paper to avoid over-claiming their contributions, particularly regarding the novelty of findings related to the absence of negative samples. It is essential to clarify that while their results are valuable, they should not imply being the first to indicate that no negatives are needed. Furthermore, we encourage the authors to include a more intuitive explanation of ContraNorm in Section 5.2 and to clarify whether the findings regarding Gaussian noise apply to MLP. We also recommend reconsidering the use of Chameleon and Squirrel datasets in their benchmarks and refining the phrasing around Gaussian noise as an augmentation. Lastly, we suggest framing the relationship between GNNs and alignment loss as a hypothesis rather than a definitive conclusion, and incorporating a more extensive discussion of related works throughout the paper to better contextualize their contributions and differences from prior research.