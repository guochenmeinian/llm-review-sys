ID: 4diKTLmg2y
Title: RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RepLiQA, a novel dataset designed to evaluate large language models (LLMs) on question-answering and topic retrieval tasks using unseen reference content. The dataset includes five splits, with four not previously released, containing human-crafted reference documents, questions, ground-truth answers, and relevant paragraphs. The authors assess 18 LLMs, providing insights into their reliance on internal memory versus document comprehension, and benchmark these models against TriviaQA, highlighting differences in memorization versus reading comprehension.

### Strengths and Weaknesses
Strengths:
- The dataset effectively addresses data contamination, providing a robust benchmark for LLM evaluation.
- Extensive experiments with 18 state-of-the-art LLMs yield valuable insights into model performance on unseen data.
- High-quality, human-curated content enhances the dataset's relevance and accuracy.

Weaknesses:
- The license ambiguity regarding commercial use could hinder wider adoption.
- The annotators' pool, limited to India, raises concerns about potential biases and language consistency, which are not discussed.
- The paper lacks detailed information on annotator agreements and data quality measures, affecting transparency.
- The gradual release of dataset splits poses a risk of data leakage, and the evaluation methodology lacks error bars and variance measures.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of the annotators' pool, specifically addressing potential biases in content creation and differences in language conventions. Additionally, clarify the involvement of LLMs in dataset creation, particularly in relation to the findings in Figure 9. The authors should also investigate the surprising results where smaller models outperform larger ones, ensuring that data quality is not a confounding factor. We suggest enhancing Figure 4 for better model identification and addressing the ambiguity in topic retrieval tasks. Finally, consider including fine-tuned LLMs tailored for QA tasks to enrich the evaluation.