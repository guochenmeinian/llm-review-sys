ID: j9wGUcS30B
Title: On Masked Pre-training and the Marginal Likelihood
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 4, 6, 7, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a derivation of the equivalence between log marginal likelihood (LML) and negative masked pre-training (MPT) loss, arguing that training with MPT inherits good generalization properties due to this relationship. The authors conduct experiments on probabilistic PCA, VAE, and BERT, confirming that optimizing a biased estimate of LML through MPT leads to convergence towards the true LML value. The paper also discusses the implications of these findings for model design in AI and Bayesian learning. Furthermore, the authors clarify how MPT can be understood through the lens of LML, building on previous results from Fong and Holmes (2020) while addressing the nuances of their approach to masked token prediction.

### Strengths and Weaknesses
Strengths:
- The connection between LML and masked pre-training loss is significant, with relevance for large language models (LLMs) and Bayesian learning.
- Empirical experiments validate the theoretical claims, particularly in the context of probabilistic PCA, and the results are reproducible.
- The paper is well-written, with a clear introduction and background on masked language modeling (MLM), and demonstrates transparency in the derivations while acknowledging foundational work.

Weaknesses:
- The justification for claiming that maximizing marginal likelihood explains MPT's generalization is unconvincing, as it does not adequately address why MPT outperforms other objectives that directly maximize LML.
- Some derivations lack clarity, and the transition to Proposition 1 is abrupt; Proposition (1) is seen as a derivative result rather than a main contribution, as it can be directly derived from Proposition (2) in Fong and Holmes (2020).
- Concerns are raised about the validity of Equation (1), which assumes conditional independence of masked tokens that may not hold true, and the paper does not sufficiently explain the importance of the connection between MPT and LML, leading to skepticism about its practical relevance.
- The paper does not explore the implications of conditioning on subsets of tokens in practice, which could provide valuable insights.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the arguments surrounding the insights on marginal likelihood and generalization, particularly addressing the confounding factors that may influence their claims. Additionally, we suggest providing more detailed explanations of the derivations in Section 3 to enhance reader comprehension and clarifying the significance of the connection between MPT and LML for practical applications. Revisiting the formulation of Equation (1) to ensure that the assumptions made are valid and clearly articulated would strengthen the paper. Furthermore, including experiments on vision models like MAE could broaden the applicability of the findings, and explicitly demonstrating the implications of their findings would bolster their argument for the relevance of LML in the context of deep neural networks.