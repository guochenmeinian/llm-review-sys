ID: 0P6uJtndWu
Title: Efficient Diffusion Policies For Offline Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 7, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to enhance the efficiency of diffusion policies in offline reinforcement learning (RL) by introducing the Efficient Diffusion Policy (EDP). The authors propose three main techniques: 1) action approximation to avoid backpropagation through the diffusion sampling chain; 2) replacing intractable policy log likelihoods with the Evidence Lower Bound (ELBO); and 3) utilizing a fast DPM-Solver for action sampling. These innovations aim to improve computational efficiency and compatibility with various offline RL algorithms, achieving state-of-the-art results on the D4RL benchmark suite. The authors clarify that their method is not focused on likelihood estimation but aims to enhance compatibility with likelihood-based RL methods. They also address misconceptions regarding their training scheme and the Energy-based Action Selection (EAS) process, emphasizing the distinct steps involved and their contributions to reducing variance in diffusion policies.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- The use of action approximation allows for training with larger diffusion noising time K, making diffusion policy training practical.
- The authors provide a clear rationale for the benefits of using action approximation, including empirical support from related literature.
- The proposed method shows substantial performance gains in specific environments, indicating its effectiveness.
- The authors demonstrate compatibility with popular offline RL algorithms, enhancing the applicability of diffusion policies.
- Strong empirical performance is reported across the D4RL benchmark, particularly in challenging tasks.
- The authors are responsive to reviewer feedback and demonstrate a willingness to clarify and improve their presentation.

Weaknesses:
- The discussion of limitations is insufficient, neglecting broader impacts and technical constraints.
- The reliance on action approximation introduces high variance, and there are no ablation studies to assess its effect on the final policy.
- The novelty of the contributions is limited, primarily revolving around two main changes: action prediction and the use of DPM-Solver.
- There are concerns about the fairness of comparisons made with existing methods, particularly regarding hyperparameter settings.
- Some explanations, particularly around policy improvement and EAS, may lack sufficient clarity and detail.
- The evaluation protocol and baseline comparisons may not convincingly demonstrate the superiority of EDP.
- The paper could benefit from a more thorough discussion of related works to contextualize the contributions of EAS.

### Suggestions for Improvement
We recommend that the authors improve the discussion of limitations and broader impacts of their work. Additionally, we suggest conducting ablation studies to clarify the effects of action approximation on policy performance. The authors should also provide a more detailed explanation of the advantages of using $\hat{a}^0$ over traditional action samples and consider including stronger baseline comparisons to substantiate claims of superiority. Furthermore, addressing the concerns regarding the evaluation protocol and clarifying the computational claims related to training times would enhance the paper's rigor. We also recommend improving clarity regarding the comparison of their method with Diffusion-QL, particularly addressing the hyperparameter settings used in their experiments. If EAS is a significant contribution, the authors should provide a more comprehensive comparison with existing literature to highlight their unique contributions. Finally, further clarifications on the intuitive explanations provided for action approximation would strengthen the paper's arguments.