ID: pLsPFxqn7J
Title: Kernelized Cumulants: Beyond Kernel Mean Embeddings
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 7, 7, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents kernelized cumulants, extending classical cumulants to Hilbert-space-valued random variables, particularly in RKHS. The authors demonstrate that kernelized cumulants serve as a new set of statistics for higher-order two-sample and independence tests, generalizing MMD and HSIC. The efficiency of these tests is validated through numerical experiments on synthetic and real data.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clear, effectively communicating complex ideas despite heavy notation.
- The proposed kernelized cumulant is a novel generalization of MMD and HSIC, providing competitive results across various datasets.
- The computational efficiency of the proposed statistics remains quadratic in sample size, similar to MMD.

Weaknesses:
- The main contribution appears to be a straightforward combination of known concepts: cumulants and kernel methods in statistical testing.
- The relationship between MMD/HSIC and higher-order cumulants when changing kernels is not adequately discussed.
- The empirical results suggest that estimating kernelized cumulants may require large samples, and the convergence rate of the estimator is not addressed.
- The paper lacks theoretical guarantees regarding the effectiveness of kernelized cumulants, such as consistency.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the relationship between MMD/HSIC and higher-order cumulants, particularly regarding kernel changes. Additionally, the authors should provide theoretical guarantees for kernelized cumulants, including consistency results. It would also be beneficial to explore more kernel options in experiments, such as the neural tangent kernel, and to clarify the criteria for determining the 'optimal value' of $\sigma$. Finally, addressing the convergence rates for finite-sample estimators and the potential vulnerability to kernel misspecification would enhance the paper's robustness.