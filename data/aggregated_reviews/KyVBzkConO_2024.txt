ID: KyVBzkConO
Title: Injecting Undetectable Backdoors in Obfuscated Neural Networks and Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 5, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical pipeline for injecting undetectable backdoors into deep learning models, which includes training a neural network, converting it to a boolean circuit, injecting a non-replicable backdoor using pseudorandom generators (PRGs) and digital signatures, and then obfuscating the circuit before converting it back to a neural network. The authors argue that traditional approaches, such as using SAT instances, fail to meet the necessary conditions for undetectability, and they assert that using a PRG allows for a backdoor that is both usable and undetectable, preventing polynomial-time adversaries from distinguishing between a model with a backdoor and an honest model. They also provide a formal construction based on cryptographic assumptions, define "undetectable" and "unreplicable" backdoors, and address backdoors in language models, while discussing the role of obfuscation in protecting intellectual property (IP) and privacy.

### Strengths and Weaknesses
Strengths:
- The paper addresses a critical issue regarding undetectable backdoor attacks, which pose significant risks to production systems.
- The presentation is clear, with helpful discussions that enhance reader understanding.
- The introduction effectively builds intuition and compares the work to Goldwasser et al. (2022).
- The authors provide a clear rationale for using PRGs over SAT instances, enhancing the theoretical foundation of their approach.
- The discussion on obfuscation effectively addresses potential concerns regarding IP protection and privacy, illustrating its relevance in practical applications.

Weaknesses:
- The novelty of the contribution is unclear, as prior work has already demonstrated similar techniques for binary classification models.
- Many definitions and theorems lack sufficient explanation, complicating comprehension.
- The paper ends abruptly without a conclusion or discussion, and important definitions regarding backdoors in language models are relegated to the appendix.
- The rationale for the entire pipeline, particularly the necessity of each component, is not intuitively explained.
- The assumption of a procedure "Perturb" that ensures undetectability is questionable, as its existence is not established.
- The practicality of the proposed method is questioned, particularly regarding the potential size increase of models and inference costs due to conversion to boolean circuits.
- The conversion back from obfuscated circuits to neural networks appears unnecessary for inference purposes, raising concerns about the utility of the construction.

### Suggestions for Improvement
We recommend that the authors clarify the novelty of their contribution relative to existing work, particularly Goldwasser et al. (2022). It would be beneficial to include a conclusion section to summarize findings and implications. We suggest that the authors provide more intuitive explanations for the necessity of components like the PRG and digital signatures, and consider restructuring the paper to include critical definitions and discussions in the main body rather than the appendix. Additionally, addressing the limitations of the proposed approach and the practical applicability of their construction would strengthen the paper. We also recommend that the authors improve the practical aspects of their method by addressing concerns related to the size and expense of inference when converting to boolean circuits, and clarifying the necessity of converting back to a neural network for inference, as evaluating the obfuscated circuit itself may suffice.