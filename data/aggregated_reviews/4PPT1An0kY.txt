ID: 4PPT1An0kY
Title: Self-Ensemble of $N$-best Generation Hypotheses by Lexically Constrained Decoding
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for combining N-best generations from beam search to enhance final text generation. The authors propose training a token-level quality estimation (QE) module to assess the inclusion of tokens in the final output, followed by lexically constrained decoding. Experiments on paraphrasing, summarization, and CommonGen demonstrate improved performance over standard beam search and other reranking methods. The contribution lies in advancing NLP engineering through this novel approach.

### Strengths and Weaknesses
Strengths:  
1. The idea of combining N-best hypotheses is reasonable and the method is straightforward.  
2. Experiments across multiple generation tasks showcase the method's generalization ability.  
3. The paper is well organized and presents an intuitive approach to improving generation quality.

Weaknesses:  
1. The token-level QE component lacks consideration for compositional semantics and synonyms, leading to rigid labeling.  
2. The method requires decoding twice, which is inefficient for practical applications.  
3. A significance test is missing for claims of performance improvement, and a time comparison with reranking methods is necessary.

### Suggestions for Improvement
We recommend that the authors improve the token-level QE component to account for compositional semantics and synonyms. Additionally, we suggest providing a detailed time comparison of the proposed method against standard beam search to clarify its computational expense. Finally, we encourage the authors to include a significance test for performance claims to strengthen their findings.