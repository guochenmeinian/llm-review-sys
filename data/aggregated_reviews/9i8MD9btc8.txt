ID: 9i8MD9btc8
Title: (Almost) Provable Error Bounds Under Distribution Shift via Disagreement Discrepancy
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 3, 6, 7, 6, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper proposes a novel metric for evaluating distribution shift in datasets, demonstrating its effectiveness through the training of a surrogate model that maximizes disagreement discrepancy. The authors establish an error estimation bound under certain assumptions and validate their method through extensive experiments across various datasets, showing that it does not overestimate prediction accuracy in the target domain. Additionally, the authors present a method for bounding out-of-distribution (OOD) accuracy using a concentration term, arguing that their proposed bound is theoretically sound. However, reviewers express concerns regarding the role of the concentration term in both their method and conventional bounds, noting insufficient validation of assumptions, particularly that the assumption is only verified in 30% of settings. The evaluation of the proposed method against baseline methods is also criticized for lacking fairness, as the latter often utilize expectation forms that incorporate concentration terms.

### Strengths and Weaknesses
Strengths:
1. The authors utilize unlabeled data from the target domain, which is believed to enhance prediction accuracy.
2. The introduction of the concept "disagreement discrepancy" offers insights into domain shift investigations.
3. The assumptions for obtaining accuracy bounds are clearly articulated, and the experimental evaluations are thorough, particularly in assessing when these assumptions hold.
4. The authors provide a theoretically grounded approach to bounding OOD accuracy.
5. Empirical results suggest that the proposed method is often valid and outperforms prior methods in certain scenarios.

Weaknesses:
1. Certain evaluation metrics are unclear, particularly the advantage of consistently underestimating target accuracy, especially when the Mean Absolute Error (MAE) underperforms compared to other methods. The authors should clarify the benefits of this feature.
2. The proposed metric underperforms against baseline methods in crucial metrics, such as MAE.
3. The approach of training an additional network by maximizing disagreement may be time-consuming and sensitive to hyperparameters.
4. The writing could be improved for clarity, particularly regarding the introduction of existing divergence measures.
5. The results presentation aggregates data across datasets and shifts, obscuring performance analysis.
6. Reviewers argue that the authors do not adequately address the role of the concentration term in their method compared to conventional bounds.
7. The assumption underlying the proposed bound is only validated in a minority of cases, raising questions about its theoretical robustness.
8. The evaluation methodology is perceived as unfair, as it does not incorporate concentration terms in baseline comparisons.

### Suggestions for Improvement
We recommend that the authors improve the clarity of evaluation metrics and explicitly illustrate the advantages of the method's tendency to underestimate target accuracy. Additionally, the authors should provide a more detailed comparison with baseline methods to demonstrate the proposed metric's effectiveness, ensuring that these comparisons incorporate concentration terms for fairness. We suggest refining the writing for better clarity, particularly in introducing existing divergence measures, and presenting results in a manner that allows for objective evaluation across different scenarios. Furthermore, we encourage the authors to explore the implications of their assumptions more thoroughly, possibly by plotting the relationship between $\hat \Delta(\hat h, y^*)$ and $\hat \Delta(\hat h, h')$ to support their claims. Lastly, the authors should provide a more comprehensive validation of their assumptions across a broader range of settings, rather than relegating this information to the appendix, and clarify the metrics used to support claims about the proximity of their hypotheses to the true target, potentially by including specific metrics such as L_2 distance.