ID: ADsEdyI32n
Title: LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for LLM prompt compression, introducing a two-step method that employs an auxiliary model to drop low-perplexity demonstrations and utilize a hierarchical selection for relevant token extraction. The authors demonstrate that their method significantly outperforms previous prompt compression techniques across various datasets and compression constraints. Additionally, the paper discusses LLMLingua, which includes a budget controller and a token-level iterative compression algorithm, achieving up to 20x compression with minimal performance loss.

### Strengths and Weaknesses
Strengths:
- The proposed methods are novel and effectively tackle the important issue of prompt length in LLMs, offering practical solutions for reducing inference costs.
- Comprehensive experiments across multiple datasets provide strong empirical support for the claims made.
- The paper is well-written and presents a convincing argument for the effectiveness of the proposed compression methods.

Weaknesses:
- The reliance on multiple steps and auxiliary computations is not adequately discussed, raising concerns about the overall computational cost and latency.
- The focus on prompt token reduction may not directly correlate with improvements in end-to-end inference time, which is crucial for practical applications.
- The perplexity-based compression method may primarily eliminate stop words, suggesting that simpler baselines could achieve similar performance.

### Suggestions for Improvement
We recommend that the authors improve the discussion regarding the computational requirements of the two-step method and its impact on overall system latency. Additionally, we suggest that the authors consider including end-to-end inference time as a key metric in their evaluations. Clarifying the prompt tuning process for the GPT4 baseline could also strengthen the paper. Finally, addressing the potential effectiveness of simpler baseline methods that drop stop words while retaining entities would enhance the robustness of the findings.