ID: qo17ZiVnH2
Title: Filling the Image Information Gap for VQA: Prompting Large Language Models to Proactively Ask Questions
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for knowledge-based Visual Question Answering (VQA) tasks, enhancing existing methods by enabling Language Learning Models (LLMs) to extract detailed, question-related captions from images. The authors propose a strategy that deconstructs questions into subquestions, retrieves answers for each, and generates a comprehensive caption that integrates this information with the original question and caption. This enriched input is then utilized by the LLM to improve reasoning and answer accuracy. The main contributions include a comprehensive framework and extensive experimental validation demonstrating the methodology's effectiveness.

### Strengths and Weaknesses
Strengths:
- The paper effectively highlights the need for nuanced, query-specific information to enhance LLM performance in VQA tasks.
- The proposed framework is intuitively designed, reflecting a deep understanding of the problem.
- Comprehensive experiments, including comparative analyses and ablation studies, validate the proposed method's efficacy.

Weaknesses:
- The implementation appears overly complex relative to the marginal improvements achieved, with multiple modules contributing to increased inference time.
- Clarity issues exist in the articulation of methods and experiments, with several points requiring further elaboration, such as the calculation of scores and the consistency of results across tables.
- The novelty of the approach is questioned due to similarities with contemporaneous works, and the performance improvements on similar tasks are minimal.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their methodology by providing detailed explanations for ambiguous points, such as the calculation of scores and the use of specific terms in tables. Additionally, we suggest that the authors explore a broader range of vision-language tasks beyond OK-VQA and AOKVQA to strengthen their findings. Incorporating dense captioning could also be beneficial, and we encourage the authors to address the implications of emerging multimodal capabilities in LLMs, such as those anticipated with GPT-4. Lastly, we advise including relevant contemporaneous references to enhance the literature review.