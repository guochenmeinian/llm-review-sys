ID: xotfLEAF4u
Title: MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 9, 6, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents COMPBENCH, a benchmark designed to evaluate the comparative reasoning capabilities of multimodal Large Language Models (MLLMs). It includes approximately 40,000 image pairs across various visual domains, each associated with questions and answers focusing on eight dimensions of relative comparison: visual attribute, existence, state, emotion, temporality, spatiality, quantity, and quality. The authors curated these pairs using metadata from existing datasets and CLIP similarity scores, evaluating several recent MLLMs, including GPT-4V, Gemini-Pro, and LLaVA-1.6, revealing significant shortcomings in their comparative reasoning abilities.

### Strengths and Weaknesses
Strengths:
- The introduction of COMPBENCH fills a critical gap in evaluating MLLMs, focusing on comparative reasoning, which is often overlooked in existing benchmarks.
- The dataset is comprehensive, covering eight comparative dimensions and a wide range of visual domains, ensuring a robust evaluation platform.
- The paper provides a thorough analysis of model performances, including error cases, guiding future research and development.

Weaknesses:
- The scope is limited, focusing solely on comparative questions, which may only serve as an add-on to existing benchmarks.
- Data bias may arise from varying sources for different question types, potentially impacting the dataset's balance.
- The question generation method is relatively simple, limiting the diversity of questions.
- The evaluation includes few baselines, particularly under fine-tuning settings, which may not provide a comprehensive assessment.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by expanding the comparative reasoning scope to include varying levels of difficulty, from simple appearance comparisons to multi-step reasoning. Additionally, reporting random guess performance would help demonstrate dataset balance. To address data bias, consider standardizing data sources across question types or implementing normalization techniques. Enhancing question generation through advanced methods or human-in-the-loop approaches could increase diversity. Finally, including a wider range of baseline MLLMs in the experiments would provide a more comprehensive evaluation of COMPBENCH's performance.