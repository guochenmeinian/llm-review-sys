ID: snNuvAOQxB
Title: MEQA: A Benchmark for Multi-hop Event-centric Question Answering with Explanations
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 7, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MEQA, a benchmark for multi-hop event-centric question answering, which is the first of its kind to integrate both events and entities, introducing novel challenges. MEQA includes five types of questions: event relation, event bridging, event listing and counting, event comparison, and unanswerable questions. The authors design a bottom-up process for constructing MEQA from an information extraction (IE) dataset and introduce two new metrics—completeness and logical consistency—to evaluate performance comprehensively. Extensive experiments, including various prompting methods, demonstrate that MEQA poses new challenges.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel multi-hop event-centric reasoning task setting, providing valuable insights into assessing model reasoning abilities.
2. A semi-automated data construction process from the IE dataset is designed, offering a new direction for the IE community and potential transferability to other datasets.
3. The presentation is clear, with figures and tables effectively illustrating the five different question types.

Weaknesses:
1. The authors do not elaborate on the specific new challenges introduced by MEQA compared to previous multi-hop reasoning datasets, particularly regarding treating events as "entities."
2. The distribution of question types is excessively long-tailed, with event comparison questions comprising only 0.6%, which is insufficient for effective model evaluation. Performance metrics for each question type should be provided.
3. Validation of the new metrics is based on only 25 questions, which may lead to random variation; more comprehensive experiments are needed.
4. A data statistics comparison with previous datasets would be beneficial.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the specific new challenges posed by MEQA compared to prior datasets. Additionally, diversifying the question types to avoid dominance by two types would enhance the dataset's robustness. To validate the new metrics effectively, conducting more extensive experiments is essential. Furthermore, providing a data statistics comparison with previous datasets would strengthen the paper's contributions. Lastly, addressing potential data leakage from the WikiEvents dataset and including results from other popular LLMs would enrich the analysis and discussion.