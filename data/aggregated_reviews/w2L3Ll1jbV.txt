ID: w2L3Ll1jbV
Title: Adversarially Robust Multi-task Representation Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 6, 6, 5, -1, -1, -1
Original Confidences: 4, 2, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical exploration of adversarial multi-task representation learning, focusing on training a predictor and feature extractor on multiple source tasks with adversarial elements, followed by training another predictor on a target task. The authors provide bounds on excess risk under mild assumptions, indicating that larger sample sizes for individual source tasks, total sample sizes, and target task sizes contribute to robust learning. The results suggest that smooth and non-negative losses yield a more rapid decrease in excess risk compared to Lipschitz losses. The authors derive Theorems for both Lipschitz and non-negative losses and introduce a new reduction method from multi-task to single-task settings.

### Strengths and Weaknesses
Strengths:  
- The problem settings and assumptions regarding data distribution and function properties are mild and appropriate.  
- The derived results provide important insights into adversarial transfer learning, emphasizing the role of diverse source tasks and sample sizes in facilitating robust learning.  
- The upper bounds align with prior non-adversarial work, indicating that similar training sample preparations are sufficient in adversarial contexts.

Weaknesses:  
- Some results may be predictable from prior work, leading to concerns about the novelty of the findings.  
- The looseness of the bounds, a natural property of Rademacher complexity-based bounds, could be improved for more restrictive cases to enhance interpretability.  
- The absence of empirical experiments to support theoretical claims is a significant limitation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of proofs in section F.1 and provide formal proofs for the theoretical results. Additionally, including empirical experiments to validate the benefits of adversarial pretraining on downstream tasks would strengthen the paper. Addressing the applicability of various adversarial attacks and clarifying the definitions of key terms, such as $g$ and $\mathcal{G}$, in equation 2 would enhance understanding. Finally, we suggest discussing the implications of different sample sizes across source tasks on the bounds presented.