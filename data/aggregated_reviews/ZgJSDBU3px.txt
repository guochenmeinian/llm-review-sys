ID: ZgJSDBU3px
Title: CaseEncoder: A Knowledge-enhanced Pre-trained Model for Legal Case Encoding
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies legal case retrieval, proposing domain-specific techniques to pre-train RoBERTa using a masked language model (MLM) objective and a contrastive objective tailored for legal cases. The authors introduce a modified in-batch sampling approach that leverages domain expertise to assess case similarity and employ a modified Circle Loss as the training objective. Evaluations on three Chinese legal case retrieval datasets demonstrate that the proposed model outperforms the RoBERTa baseline by 2-4 points.

### Strengths and Weaknesses
Strengths:
- The proposed model consistently outperforms the RoBERTa baseline and other methods.
- Ablation studies effectively illustrate the impact of various components on performance.
- The introduction of legal knowledge at the pre-training step is relevant and addresses specific challenges in legal NLP.

Weaknesses:
- The approach may lack generalizability beyond the legal domain, limiting its value for the broader NLP community.
- The reliance on domain-specific techniques raises concerns about the model's applicability across different legal systems.
- Important details regarding the pre-training data and the definition of case similarity are missing, which could hinder reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their methodology by including comparisons with standard information retrieval models, such as BM25 and state-of-the-art dense and sparse retrievers. Additionally, we suggest providing detailed information about the pre-training data used, including the corpus for MLM and the case corpus for contrastive training. Addressing the limitations of the case similarity definition in the paper and adding a limitations section would enhance clarity. Finally, consider splitting Section 4.2 into two or three subsections for better readability.