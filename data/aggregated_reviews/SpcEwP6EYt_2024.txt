ID: SpcEwP6EYt
Title: EnOF-SNN: Training Accurate Spiking Neural Networks via Enhancing the Output Feature
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 7, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents methods to enhance the representation power of Spiking Neural Networks (SNNs) by employing knowledge distillation techniques and modifying activation functions. The authors propose aligning SNN output features with those of a pre-trained Artificial Neural Network (ANN) using KL-divergence loss and replacing the last Leaky Integrate-and-Fire (LIF) activation layer with a ReLU activation function. These modifications are shown to significantly improve SNN performance across various datasets.

### Strengths and Weaknesses
Strengths:
1. The proposed techniques are novel and lead to state-of-the-art results on all tested datasets.
2. The paper is well-written and structured, facilitating reader comprehension.
3. An ablation analysis is provided to evaluate the contributions of the proposed techniques.

Weaknesses:
1. The method requires training an ANN alongside the SNN, resulting in longer total training times.
2. The novelty of the methods is questionable, as they integrate established techniques without strong unique contributions.
3. The paper lacks a comprehensive comparison with other SNN training methods utilizing similar knowledge distillation approaches.
4. The definition of "EnOF" is missing in the main text, and the citation format does not adhere to NeurIPS standards.

### Suggestions for Improvement
We recommend that the authors improve clarity by defining "EnOF" in the main text and adhering to NeurIPS citation format. Additionally, the authors should provide a comprehensive comparison with seminal works in SNN and knowledge distillation to better position their contributions within the current research landscape. We suggest including the network Hessian matrix eigenvalues or convergence speed comparisons to illustrate the advances made by EnOF and RepAct. Furthermore, the authors should clarify whether the improvement is primarily due to the replacement of the last-layer activation function or the feature alignment process. Lastly, we advise moving the overhead comparison section from the appendix to the main text for better visibility.