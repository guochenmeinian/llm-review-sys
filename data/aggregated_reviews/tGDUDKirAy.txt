ID: tGDUDKirAy
Title: Verified Safe Reinforcement Learning  for Neural Network Dynamic Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 6, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents methods for learning formally verified neural network control policies for continuous-space, discrete-time non-linear dynamical systems, represented by neural networks. The authors propose a novel incremental policy synthesis approach that incorporates a variant of curriculum learning and a parameterized representation of state-dependent policies. The experimental results demonstrate improved performance on four benchmarks compared to five comparable techniques, highlighting the effectiveness of the proposed methods.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and presents well-motivated novel ideas to enhance performance on a challenging problem.
- It includes a comprehensive empirical evaluation with meaningful benchmarks and an ablation study in the appendix, showing impressive gains over baseline implementations.
- The approach of building a curriculum of horizons and reusing verification results is relatively novel and provides an interesting future direction.

Weaknesses:
- The empirical results primarily focus on safety without discussing performance metrics such as runtime or scalability of the methods.
- The algorithm lacks a rationality analysis, and the choice of benchmark problems is not well justified, requiring further clarification.
- The incremental verification approach appears common in existing tools, and the authors should discuss its novelty relative to other verification methods.

### Suggestions for Improvement
We recommend that the authors improve the discussion of performance metrics, including runtime and scalability, to complement the safety analysis. Additionally, we suggest providing a clearer justification for the choice of benchmark problems and a more thorough comparison with other control-theoretical methods, such as CBF-based approaches. Furthermore, the authors should clarify the limitations of their approach concerning the specific verification tools used, particularly the α,β-CROWN toolbox, and discuss the generalizability of their methods to other tools.