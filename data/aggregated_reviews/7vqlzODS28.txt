ID: 7vqlzODS28
Title: HyTrel: Hypergraph-enhanced  Tabular Data Representation Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 7, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel tabular language model, *HyTrel*, which transforms source tables into hypergraphs. Each cell is a node, with connections formed by hyperedges based on rows, columns, and the table itself. The authors claim that *HyTrel* achieves permutation invariance and generates robust table representations through two self-supervised objectives: (1) cell or header corruption indication prediction and (2) in-batch negative sampling with InfoNCE contrastive loss. Experiments include pre-training on 27M tables and fine-tuning across four tasks: column type annotation, column property annotation, table type detection, and table similarity prediction. Findings indicate marginal improvements from pre-training and no clear superiority of pre-training objectives.

### Strengths and Weaknesses
Strengths:
- The pursuit of permutation invariance in tabular data representation is well-justified, addressing limitations of existing serialization approaches.
- The method demonstrates superior performance through direct supervised training on target data without pre-training.
- The paper is well-structured and presents a novel approach to modeling high-order interactions in tabular data.

Weaknesses:
- The complexity of building a large hypergraph is a significant concern, particularly regarding efficiency and memory costs, especially for tables with millions of rows and thousands of columns.
- There is a lack of detailed analysis on computational efficiency, including inference time and the time required for graph construction during training and prediction.
- The method is limited to single-table scenarios, which may not reflect real-world applications.

### Suggestions for Improvement
We recommend that the authors improve the complexity analysis of the proposed method, specifically addressing the overall running time and memory costs associated with constructing the hypergraph. Additionally, a direct comparison of inference times for each sample/batch should be included. We suggest discussing how *HyTrel* differs from recent works that incorporate table structure during pre-training or fine-tuning. Furthermore, including a classification head could enhance evaluation across more benchmarks. Lastly, exploring the model's performance with tables containing numerous numerical values would provide valuable insights into its robustness.