ID: cVbd7uTvD8
Title: SC3D: Self-conditioned Generative Gaussian Model with 3D-aware Feedback
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper (SC3D) proposes a single image-to-3D reconstruction method that integrates a multi-view diffusion model with a 3D reconstruction model, utilizing 3D reconstruction results as self-conditioning to enhance the multi-view generation process. The motivation is to improve geometric consistency in the reconstruction pipeline, which traditionally generates multi-view images before performing sparse view reconstruction. The core concept of 3D-aware feedback is reasonable and aligns with concurrent works such as IM-3D and VideoMV. However, several ablation studies are needed to validate the effectiveness of the proposed feedback mechanism, and the authors must clarify their contributions relative to VideoMV. Additionally, the readability of the submission requires improvement.

### Strengths and Weaknesses
Strengths:
- The integration of 3D reconstruction rendering as a conditioning factor to enhance geometric consistency in multi-view diffusion models is a sound approach.
- The experiments conducted are comprehensive, demonstrating the robustness of the proposed feedback mechanism in multi-view reconstruction.

Weaknesses:
- The claim regarding key contributions is weakened by similarities to VideoMV, necessitating clearer justification of differences.
- The method lacks generalization results beyond the Google Scan objects, raising questions about its applicability to real-world images.
- An ablation study on SVD+RGB feedback is missing from Tables 2 and 3.
- The readability of Algorithms 1 and 2 is poor, resembling a Python program rather than abstract algorithms expected in scientific literature.
- There are multiple typographical errors throughout the paper.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by providing a more detailed comparison with VideoMV and addressing the similarities with existing methods like SyncDreamer. Additionally, we encourage the authors to include generalization results on real-world images and to conduct the missing ablation study on SVD+RGB feedback. Enhancing the readability of Algorithms 1 and 2 by presenting them in a more abstract format is essential. Finally, a thorough proofreading to eliminate typographical errors is necessary to improve the overall presentation quality.