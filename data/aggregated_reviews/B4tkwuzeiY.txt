ID: B4tkwuzeiY
Title: Grammar Prompting for Domain-Specific Language Generation with  Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 5, 8, 7, 5, -1, -1
Original Confidences: 3, 5, 4, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into grammar prompting as a method to enhance large language models (LLMs) in utilizing external knowledge and domain-specific constraints through a grammar expressed in Backusâ€“Naur Form (BNF). The authors propose that by predicting a BNF grammar based on test inputs, LLMs can generate outputs that adhere to these grammatical rules. The experimental results demonstrate that this approach improves performance across various tasks, including semantic parsing, PDDL planning, and molecule generation.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- It introduces a novel method for prompting and constraining LLM generation using BNF grammar, which is described clearly and is sound.
- The experimental results are strong and indicate that the method is effective across a range of tasks.

Weaknesses:
- The novelty of the idea is limited, as it extends standard prompting methods and the constrained generation is applied at the sub-sentence level rather than the token level.
- While improvements over simple prompting methods are validated, the method's performance against more sophisticated algorithms remains uncertain.
- The experimental superiority in tasks like molecule generation is not well established compared to other graph-based generation methods.

### Suggestions for Improvement
We recommend that the authors improve the novelty discussion by clearly stating the contributions at the end of the introduction section. Additionally, we suggest exploring other grammar forms to broaden the applicability of the method. To strengthen the validation of the approach, we encourage the authors to include results from other LLMs to demonstrate broader applicability. Lastly, clarifying the retrieval process in the retrieval-based ICL experiments and addressing minor inconsistencies in the presentation would enhance the paper's clarity.