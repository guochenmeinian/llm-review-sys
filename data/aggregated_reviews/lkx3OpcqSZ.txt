ID: lkx3OpcqSZ
Title: Compressing Large Language Models using Low Rank and Low Precision Decomposition
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 7, 2, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 5, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CALDERA, a post-training compression algorithm for large language models (LLMs) that combines quantization and low-rank decomposition techniques. The authors propose a framework that approximates a weight matrix \( W \) using a low-rank, low-precision decomposition \( W \approx Q + LR \), where \( Q \) is quantized and \( L \) and \( R \) are low-rank factors. The method aims to reduce the memory and computational footprint of LLMs while maintaining strong performance. Extensive experiments demonstrate that CALDERA outperforms existing compression techniques, particularly in the low-bit regime.

### Strengths and Weaknesses
Strengths:
- CALDERA introduces an innovative compression technique that effectively exploits the low-rank structure of LLMs, supported by theoretical upper bounds on approximation error.
- Empirical results indicate that CALDERA outperforms existing methods in terms of performance, particularly when compressed to less than 2.5 bits per parameter.
- The paper is well-written and provides a rigorous approximation error analysis.

Weaknesses:
- The performance of CALDERA is sensitive to the choice of target rank and quantization bit budget, with no systematic method provided for determining optimal values.
- The experiments primarily focus on LLaMa models, limiting the assessment of CALDERA's generalizability across different architectures.
- The iterative optimization process may require significant computational resources, raising concerns about efficiency compared to simpler methods.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including comparisons with LoftQ and LQ-LoRA, as well as providing a baseline with full parameter fine-tuning. Additionally, it would be beneficial to evaluate the computational cost of CALDERA and conduct experiments on a wider range of LLM architectures to assess generalizability. To enhance clarity, the authors should also address the potential overfitting of fine-tuned models on specific datasets and consider using more diverse datasets for fine-tuning. Lastly, we suggest exploring ways to optimize the hyperparameter selection process for rank to enhance the efficiency of the proposed method.