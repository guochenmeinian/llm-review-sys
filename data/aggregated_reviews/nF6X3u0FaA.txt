ID: nF6X3u0FaA
Title: Contrastive Training of Complex-Valued Autoencoders for Object Discovery
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 8, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents improvements to the Complex-valued AutoEncoder (CAE) for object-centric learning, specifically addressing its limitations with color images and multiple objects. The authors propose CAE++ and CtCAE, where CAE++ incorporates minor architectural enhancements, and CtCAE adds a contrastive loss term to enhance object cluster separability. The authors argue that their method improves object separability, but concerns arise regarding the evaluation protocol, particularly when using multi-channel inputs like RGB images. Experimental results demonstrate that these methods can handle color images and up to six objects per image, with evaluations based on inter-cluster and intra-cluster distances, as well as ARI and FG-ARI metrics across various datasets. However, the evaluation may intertwine object separation with color values, leading to potentially uninterpretable results, especially in datasets like dSprites and Tetrominoes.

### Strengths and Weaknesses
Strengths:
- The paper advances the state-of-the-art in synchrony-based object-centric learning methods, engaging scientific curiosity.
- The proposed contrastive loss effectively utilizes magnitudes and phases in complex-valued features, enhancing object separability.
- It is well-written and easy to understand, with comprehensive experiments covering design choices and robustness.
- The inclusion of ablation studies justifies architectural decisions and demonstrates empirical improvements over the original CAE.
- The authors demonstrate a willingness to engage with reviewers and improve their evaluation methodology.

Weaknesses:
- The evaluation procedure may lead to misleading results, particularly regarding the separation of objects in RGB images, due to the presence of pixels with low channel values.
- Claims about performance comparisons in Table 2 may not be substantiated due to large standard deviations and potential misinterpretations.
- The shift from CAEs to contrastive loss raises concerns about reverting to hard clustering, which contradicts the flexibility originally promised by CAEs.
- The paper lacks a thorough exploration of the latent space separability and does not adequately compare against slot-based methods.

### Suggestions for Improvement
We recommend that the authors improve the evaluation methodology to ensure that color information does not unduly influence segmentation results. Specifically, we suggest adjusting the analysis to account for pixels with low channel values and discussing the implications of using different thresholds. Additionally, consider including a disclaimer about the limitations of the current evaluation setup and toning down claims regarding intra-cluster distances in Table 2 while clarifying the significance of bolded results. It would be beneficial to report the performance of slot-based baselines for context and to explore the effects of the proposed improvements on latent space separability. Lastly, we encourage the authors to provide more background on the encoder-decoder architecture and the k-means clustering process for clarity, as well as conducting experiments on Tetrominoes with an improved normalization scheme that eliminates all-zero pixels. Evaluating the intermediate representations of the networks could also provide further insights into the model's performance.