ID: QG4BWnsX6m
Title: Multilingual Lottery Tickets to Pretrain Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to address the multilingual curse in multilingual pretrained language models (mPLMs) while maintaining the same number of parameters and FLOPS as the baseline model, mBERT. The authors propose scaling up the language model and searching for language-specific lottery tickets using a computationally cheap approach based on zero-shot neural architecture search (NAS). They compute SNIP scores for language-specific inputs to retain parameters with the highest values, demonstrating that the resulting lottery tickets perform better than the baseline and random tickets, albeit with marginal improvements. Additionally, they analyze gradient interference, showing that the new tickets exhibit less interference than the baseline, and find that closely related languages share more overlapping sub-networks.

### Strengths and Weaknesses
Strengths:
- The proposed computationally cheap approach for finding sub-networks could be beneficial for the community, as existing methods like iterative magnitude pruning are more resource-intensive.
- The analysis of overlapping sub-networks among similar languages supports previous findings regarding language-neutral subnetworks in mPLMs.

Weaknesses:
- The concept of training large models and then compressing them is not novel and has been previously explored, raising questions about the cost-effectiveness of the proposed method compared to existing approaches.
- The significance of the proposed method is unclear when compared to simple magnitude pruning, as the paper lacks comparisons to baseline methods beyond random tickets.
- The marginal improvement over the baseline and the retention of the same number of parameters limit the perceived benefits, particularly regarding gradient interference's impact on empirical performance.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their approach by providing a more comprehensive comparison with baseline methods, including non-iterative magnitude pruning. Additionally, clarifying whether the scaled-up model is trained from scratch or utilizes fixed layers from mBERT would enhance transparency. Addressing the potential challenges of their method in larger-scale scenarios and discussing the impact on positive transfer would also strengthen the paper. Finally, we suggest providing more details on the proposed metric for gradient interference to clarify its methodology.