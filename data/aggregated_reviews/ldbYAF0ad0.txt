ID: ldbYAF0ad0
Title: Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper tests the evaluation ability of ChatGPT and GPT-4 on the SummEval dataset using various assessment methods (RTS, MCQ, and H2H). The analysis indicates that ChatGPT correlates better with human judgments than traditional metrics, although these metrics are criticized for being candidate-dependent and dimension-dependent. The authors propose a temporary efficient framework that utilizes the correlation between RTS and MCQ results as an indicator of reliability. The paper also defines a meta-correlation metric to assess how LLM-based evaluators' performance is influenced by the quality of the evaluated systems.

### Strengths and Weaknesses
Strengths:
- The paper presents a timely analysis of LLMs for human-like evaluation, revealing limitations of LLM-based evaluation.
- It provides detailed experiments, case studies, and a constructive framework for assessing LLM reliability.
- The findings contribute valuable insights for researchers in selecting effective evaluators for abstractive summarization.

Weaknesses:
- The reliance on a single dataset, SummEval, limits the generalizability of the conclusions.
- The analysis of ChatGPT's reasoning correctness is insufficient, requiring more detailed examination and human validation.
- The proposed framework lacks experimental validation and practical applicability, making it difficult for researchers to determine the reliability score $r$.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their findings by conducting experiments on additional datasets and exploring the performance of other LLMs beyond ChatGPT. A more thorough analysis of the candidate-dependence and dimension-dependence of traditional evaluation metrics and human annotators should be included to provide a comprehensive understanding of LLM evaluators' limitations. Additionally, we suggest incorporating more detailed human examinations of LLM reasoning correctness and providing experimental results to validate the proposed framework. Finally, addressing the formatting and presentation issues in the appendix and bibliography will enhance the paper's clarity and professionalism.