ID: JvOZ4IIjwP
Title: Train Hard, Fight Easy: Robust Meta Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to robust Meta-Reinforcement Learning (MRL) by introducing Conditional Value-at-Risk (CVaR) optimization, leading to the Robust Meta RL algorithm (RoML). The authors theoretically prove that biased gradients in traditional robust RL objectives are mitigated in the meta RL context, enhancing sample efficiency by oversampling lower-return tasks. The experimental results demonstrate RoML's effectiveness across various tasks, showcasing improved robustness and performance.

### Strengths and Weaknesses
Strengths:
1. The application of CVaR optimization in Meta RL addresses robustness issues prevalent in reinforcement learning.
2. The theoretical proof of immunity to biased gradients in MRL is a significant contribution.
3. The experiments provide solid evidence of RoML's advantages in terms of robustness and efficiency.

Weaknesses:
1. The definition of robustness in the context of meta RL is unclear, particularly regarding disturbances in dynamic systems.
2. The intuition behind the advantages of using CVaR in meta RL versus traditional RL is not sufficiently articulated.
3. The experimental evaluation lacks thoroughness, as it does not utilize standard meta-RL benchmarks or include high-risk tasks, and misses potential simple baselines.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of robustness in meta RL and elaborate on the differences in using CVaR in this context compared to traditional RL. Additionally, we suggest that the authors conduct experiments on standard meta-RL benchmarks and include high-risk tasks to strengthen their evaluation. Finally, discussing the choice of parameters $\alpha$ and $\beta$ in more detail, as well as considering simple baselines like oversampling based on return, would enhance the robustness of their findings.