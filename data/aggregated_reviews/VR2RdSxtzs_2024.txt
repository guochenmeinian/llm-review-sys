ID: VR2RdSxtzs
Title: MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 7, 4, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Multi-Agent System for conditional Mining (MACM) that addresses the limitations of existing methods requiring distinct prompt designs for various mathematical problems. The authors propose a framework involving three agents (Thinker, Judge, Executor) that iteratively generate conditions, verify their effectiveness, and execute calculations. Experimental results demonstrate significant accuracy improvements of MACM over traditional prompting methods like ToT and GoT across various datasets. Additionally, a comparative analysis reveals that MACM consistently outperforms CR in accuracy on the MATH and SciBench datasets, particularly on challenging logical reasoning problems. The authors also plan to enhance the presentation of their results by breaking down complex tables and figures for better comparability.

### Strengths and Weaknesses
Strengths:
1. The motivation for a method that eliminates the need for distinct prompt designs is compelling.
2. The authors provide a clear explanation of MACM, facilitating understanding.
3. Significant accuracy improvements are observed in experiments across diverse datasets, with MACM achieving higher accuracy than both CR and ToT on the MATH dataset and the diff/stat/calculus subsets of SciBench.
4. The authors have committed to enhancing the clarity of their data presentation, making it easier to compare results across different models and methods.

Weaknesses:
1. The technical contributions may be limited, necessitating a clearer explanation of the method's effectiveness.
2. The theoretical analysis lacks clarity and requires further elaboration.
3. Additional experimental results are needed to substantiate the proposed method's effectiveness.
4. The experimental setups lack rigor and clarity, making results less convincing, particularly regarding comparisons with ToT and GoT.
5. The lack of controlled variable comparisons, particularly regarding the exact versions of the models used, raises concerns about the validity of the comparisons.
6. The marginal gains of MACM on open-source models suggest that further analysis is needed to clarify its effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis by clarifying how the thought space T can be traversed and ensuring that the conditions generated are genuinely effective rather than random. Additionally, we suggest rerunning the results for GPT4-ToT instead of relying on previous studies to account for improvements in GPT4. It is also crucial to compare the number of API calls from MACM with other prompting methods to provide a clearer measure of efficiency. Furthermore, the authors should enhance the clarity of their experimental setups and ensure that the number of responses for each model is highlighted in the results to facilitate fair comparisons. We also recommend improving the clarity of their comparisons by providing exact versions of the models used in all experiments, ensuring consistency across all tests. Conducting experiments with varying temperatures for majority voting, such as 0.3 or 0.7, would provide a more comprehensive understanding of MACM's performance. Lastly, including comparisons with ToT and other competitive baselines on the MATH, SciBench, and TheoremQA datasets would strengthen their claims about MACM's advantages.