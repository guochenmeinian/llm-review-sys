ID: Vota6rFhBQ
Title: Fine-Tuning Language Models with Just Forward Passes
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 7, 7, 8, 8, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new zeroth-order optimizer, MeZO, designed for fine-tuning large language models (LMs) with significantly reduced memory requirements, achieving up to 12x memory savings compared to backpropagation. The authors provide theoretical insights into the fast convergence of MeZO, particularly under the assumption of "low effective rank," which allows the learning rate to scale with rank rather than the number of parameters. The method is shown to perform well across various benchmarks, outperforming in-context learning and linear probing while maintaining comparable performance to traditional fine-tuning methods.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to understand.
2. The method is simple and provides useful theoretical insights into zeroth-order optimization for large pre-trained models.
3. Strong experimental results and thorough ablation studies support the efficacy of MeZO.
4. The approach addresses a critical need for memory-efficient fine-tuning methods in the community.

Weaknesses:
1. The empirical verification of the low effective rank assumption is lacking, and the authors do not explore the Hessian spectra of the downstream fine-tuning loss in LLMs.
2. The treatment of convergence behavior is brief and could benefit from expansion and comparison to backpropagation.
3. The necessity of prompts for MeZO's performance is not well-explored, raising questions about its applicability across different tasks and datasets.

### Suggestions for Improvement
We recommend that the authors improve the empirical verification of the low effective rank assumption by studying the Hessian spectra of the downstream fine-tuning loss in LLMs. Additionally, we suggest conducting simulated experiments to confirm that effective rank determines convergence rates, as predicted in the theory. It would also be beneficial to expand the discussion on the convergence behavior of MeZO compared to backpropagation, particularly in the context of Section 4 Theory. Furthermore, we encourage the authors to investigate the role of prompts in MeZO's performance and clarify the conditions under which this technique is effective, including its applicability to various tasks and datasets.