ID: 8cNJyqs45T
Title: Interleaving Text and Number Embeddings to Solve Mathemathics Problems
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 3, 6, 7
Original Confidences: 3, 3, 3

Aggregated Review:
### Key Points
This paper presents a technique called Multimodal Decoding (MMD), which extends the numerical encoding technique from XVal. MMD employs an MLP to encode numbers and incorporates a lightweight routing layer that acts as a soft classifier on the transformer representation, routing the embedding between text and numeric heads. Unlike previous approaches that utilized a decoder-only architecture, the authors explore their technique on encoder-decoder architectures.

### Strengths and Weaknesses
Strengths:
- The MMD approach enhances the flexibility of XVal by encoding numbers with an MLP and eliminating the intermediate token generation stage in decoding, directly activating the numerical head on the internal representation.
- The paper is technically sound, well-written, and logically structured, with clear claims in the method section.

Weaknesses:
- The choice of encoder-decoder architecture lacks motivation, limiting the comparison of numerical embedding techniques and applicability.
- Several relevant details are missing, including specifics of the routing layer, dataset characteristics, and loss functions for the numerical head and routing layer.
- The contributions appear incremental, as the paper does not address key limitations of XVal or compare against open-source LLMs, raising questions about the value of MMD.
- Experiments are not well-motivated, as they focus on a simple dataset while current LLMs excel at more complex problems.

### Suggestions for Improvement
We recommend that the authors improve the motivation for using the encoder-decoder architecture and clarify its advantages over the decoder-only architecture. Additionally, please provide more details on the routing layer, including its architecture and training losses. It would be beneficial to include specifics about the numeric and text-numeric datasets, such as size and train/eval splits, to enhance reproducibility. We suggest conducting experiments to isolate the effects of the embedding change and the routing layer to determine their individual contributions to performance improvements. Furthermore, discussing the computational trade-offs of adding a routing layer and providing examples where MMD outperforms XVal would strengthen the paper. Finally, including significant tests when comparing with other methods would enhance the robustness of the evaluation.