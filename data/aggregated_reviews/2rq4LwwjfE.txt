ID: 2rq4LwwjfE
Title: What Do Deep Saliency Models Learn about Visual Attention?
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 4, 6, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework that decomposes learned features of a saliency model into trainable bases, which are then combined to create a final saliency map. The contribution of each basis is quantified, allowing for exploration of their semantic meanings through a probe dataset, specifically Visual Genome. The authors aim to understand the factors influencing saliency based on visual concepts, such as object types and relationships. Additionally, the paper examines center bias, category imbalance, and dataset selection in saliency research. The authors acknowledge the existence of center bias in datasets due to human compositional tendencies and propose methods to mitigate this bias through careful dataset selection and methodological adjustments. They highlight the OSIE dataset as an effective alternative to traditional datasets, demonstrating reduced center bias, and introduce a new normalization technique (Z) to quantify the contributions of semantics to visual saliency.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clearly organized, with detailed explanations of the proposed method.
- It addresses fundamental questions regarding the features important to saliency, contributing to the understanding of deep learning models in this domain.
- The framework allows for the modeling of both salient and non-salient features, providing insights into human attention.
- The authors provide a comprehensive analysis of center bias and category imbalance, demonstrating a deep understanding of the challenges in saliency research.
- The introduction of the normalization technique (Z) adds a novel dimension to the analysis of semantic contributions.
- The use of the OSIE dataset as a countermeasure to center bias is well-justified and supported by data.

Weaknesses:
- The reliance on the Visual Genome dataset raises concerns about the validity of matched concepts, as the hyper-parameter thresholding may not generalize across different datasets.
- The assumption that saliency can be solely explained by visual concepts is overly simplistic; contextual factors and dataset biases may significantly influence results.
- The paper lacks a comparison of model performance against state-of-the-art saliency models using standard metrics, which could provide valuable context.
- The definition of what constitutes a "large enough" dataset for saliency analysis remains ambiguous and could benefit from further clarification.
- The authors' responses to reviewer queries, while informative, may not fully address the complexities of the issues raised.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the limitations of using the Visual Genome dataset, particularly regarding its biases and the implications for the validity of matched concepts. Additionally, we suggest incorporating a comparison of their model's performance with existing saliency models using standard metrics to enhance the paper's contributions. Clarifying the definition of "action" in static images and addressing the contextual nature of saliency in their analysis would also strengthen the paper. Furthermore, we recommend that the authors improve the clarity around the definition of a "large enough" dataset for saliency analysis, providing specific criteria or benchmarks. We also suggest that the authors elaborate on the implications of their normalization technique (Z) in the context of existing literature to enhance its significance. Finally, addressing any remaining ambiguities in the concept of "relation" as mentioned in the reviews would strengthen the overall coherence of the paper.