ID: phnN1eu5AX
Title: Learning Probabilistic Symmetrization for Architecture Agnostic Equivariance
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a probabilistic approach to symmetrization, utilizing an input-conditional distribution to replace the untractable Haar measure distribution of infinite groups. The authors identify conditions under which this symmetrization yields an equivariant function and evaluate the method on benchmarks involving permutation and rotation symmetries. The proposed method aims to construct equivariant architectures from non-equivariant backbones, overcoming limitations of previous methods like symmetrization and canonicalization. Additionally, the paper provides an empirical analysis of the convergence and sample complexity of the algorithm using the EXP-classify dataset, revealing that smaller training sample sizes lead to lower variance estimations and better sample efficiency, while larger sample sizes enhance inference accuracy.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and well-motivated, providing a unified perspective on existing methods.
- The proposed method demonstrates flexibility and competitive performance across various contexts, including the use of general-purpose transformer architectures.
- The findings regarding the performance of pre-trained vision models on non-vision tasks are intriguing.
- The analysis provides valuable insights into the relationship between sample size and model performance, showing that smaller training sample sizes can regularize models towards low-variance estimations.

Weaknesses:
- The computational cost of the proposed method is not sufficiently discussed, particularly in comparison to the canonicalization method, which is $N$ times less expensive.
- The paper lacks a thorough discussion of prior works, missing important literature on learning symmetrizing distributions and existing methods like Equivariant Augerino.
- The objective function used for training is unclear, raising concerns about the potential collapse of the symmetrization distribution into a delta peak at the identity.
- The paper does not adequately address the limitations of the proposed method, including the additional computational costs and the need for different designs for various compact groups.
- Models trained with smaller sample sizes take longer to converge, which may hinder practical applications.
- The paper does not fully address the data efficiency of symmetrized models compared to equivariant architectures.

### Suggestions for Improvement
We recommend that the authors improve the discussion on computational efficiency, particularly addressing the increased costs associated with sampling and additional parameters. It would be beneficial to include a related works section that cites relevant literature on equivariant networks and discusses how the proposed method differs from existing approaches. We suggest clarifying the objective function used for training and providing insights into how the model prevents the collapse of the symmetrization distribution. Additionally, including an ablation study to test the necessity of the probabilistic model and a sensitivity analysis regarding the sample size's effect on model performance would strengthen the paper. We also recommend improving the organization of results by including more visualizations and plots in the final version of the paper. Lastly, we encourage the authors to explore the data efficiency of their method in comparison to equivariant architectures more thoroughly, particularly regarding the effects of training set size on learning the symmetrizing distribution, and to move the limitations discussed in the supplementary material into the main text for better visibility.