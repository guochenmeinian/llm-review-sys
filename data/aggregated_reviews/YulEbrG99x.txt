ID: YulEbrG99x
Title: Jogging the Memory of Unlearned LLMs Through Targeted Relearning Attacks
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 6, 4
Original Confidences: 4, 4, 2

Aggregated Review:
### Key Points
This paper, *"Jogging the Memory of Unlearned LLMs Through Targeted Relearning Attacks,"* investigates the vulnerabilities of machine unlearning in large language models (LLMs). The authors demonstrate that even after unlearning specific information, models can be coerced into recalling that information through targeted relearning attacks using loosely related auxiliary data. They explore this vulnerability across various benchmarks, emphasizing the limitations of current unlearning techniques and providing novel insights into the robustness of these methods.

### Strengths and Weaknesses
Strengths:
1. The simplicity and effectiveness of the relearning process highlight significant vulnerabilities in current unlearning techniques.
2. The paper is well-written, with clear explanations that enhance understanding.
3. High-quality visuals effectively illustrate the experimental setup and results.

Weaknesses:
1. The paper lacks a discussion on defensive strategies to mitigate or prevent relearning attacks.
2. There is limited theoretical exploration regarding why certain models or unlearning methods are more susceptible to relearning.
3. The selection process for the "relearning" dataset is unclear, raising questions about the variability of results with different datasets.
4. The paper does not provide a general overview of existing literature on machine unlearning, nor does it compare its findings with other techniques or datasets.
5. Key sections, such as related work and experimental setup, are located in the appendix, which detracts from the paper's synthesis.

### Suggestions for Improvement
We recommend that the authors improve the discussion on defensive strategies to mitigate relearning attacks and expand the theoretical exploration of model susceptibility. Additionally, clarifying the selection criteria for the relearning datasets and comparing their findings with existing literature on LLM unlearning would strengthen the paper. Finally, we suggest relocating the related work and experimental setup sections to the main body of the paper to enhance coherence and synthesis.