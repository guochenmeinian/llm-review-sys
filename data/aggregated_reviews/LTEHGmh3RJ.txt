ID: LTEHGmh3RJ
Title: DualCL: Principled Supervised Contrastive Learning as Mutual Information Maximization for Text Classification
Conference: ACM
Year: 2023
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel supervised contrastive learning model, DualCL, for text classification, which is parameter-free, augmentation-easy, and label-aware. The authors critique existing supervised contrastive learning methods for their training effectiveness and deployment challenges, proposing three design principles to address these issues. DualCL leverages a mutual information maximization lower bound and demonstrates superior performance across various datasets, showcasing its efficacy in capturing text representations and classifier parameters.

### Strengths and Weaknesses
Strengths:
1. The model description is well-articulated.
2. Experimental results convincingly demonstrate the method's efficacy.
3. The theoretical foundation is robust, and the framework is easy to understand.

Weaknesses:
1. The necessity of the supervised contrastive learning model in the context of large language models is unclear.
2. The performance improvements appear marginal, particularly on binary classification datasets, raising questions about its effectiveness on multi-class datasets.
3. The motivation for the new framework and the concept of "duality" require clearer explanations.
4. The experimental design could benefit from including more diverse datasets and stronger baseline comparisons.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the proposed framework, particularly regarding the limitations of existing methods. Further explanation of the "duality" concept is needed, specifically how the dot product signifies this relationship. Additionally, we suggest expanding the experimental scope to include large-scale topic classification datasets and stronger baseline models beyond PLM with different loss functions. Clarifying the connection between mutual information maximization and classification accuracy, especially in the presence of semantically correlated classes, would enhance the paper's depth. Lastly, addressing the issues with the source code link, references, and LaTeX formatting is essential for improving the paper's overall quality.