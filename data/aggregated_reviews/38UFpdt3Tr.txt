ID: 38UFpdt3Tr
Title: Exploiting Activation Sparsity with Dense to Dynamic-k Mixture-of-Experts Conversion
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 3, 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Dense to Dynamic-k Mixture-of-Experts (D2DMoE) method, which aims to enhance the efficiency of transformer models by implementing a dynamic-k routing mechanism that selects a variable number of experts based on input. The authors claim that their approach reduces computational costs by up to 60% without compromising performance, leveraging the inherent sparsity of activations in transformer models. They argue that dynamic-k routing provides consistent improvements over static top-k gating, particularly for MoEs converted from dense models. The method includes innovations such as enforcing higher activation sparsity, predicting the norm of expert outputs, and generalizing to any standalone linear layer. The authors emphasize that while loss values are used for evaluation, accuracy metrics are also reported, especially for classification tasks. They assert that comparisons with other multi-task methods are inappropriate due to fundamental differences in objectives and methodologies.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured, with clear explanations of the proposed method and thorough empirical evaluations across multiple tasks and architectures.
- The approach demonstrates a better FLOP-quality trade-off compared to existing methods, achieving significant computational savings while maintaining model performance.
- The authors provide a clear justification for their contributions, particularly the dynamic-k routing, which enhances model performance.
- They demonstrate the effectiveness of their method through empirical results, including downstream evaluations on the BoolQ dataset.
- The paper addresses reviewer concerns thoroughly, showcasing a commitment to improving the work based on feedback.

Weaknesses:
- The contribution appears limited, primarily representing a minor modification to existing MoE techniques, with insufficient novelty in the dynamic-k routing mechanism.
- Some reviewers perceive the dynamic-k routing as a minor modification rather than a substantial innovation.
- The experiments lack comprehensive comparisons with other relevant methods, particularly in terms of practical latency and performance metrics, as only theoretical FLOP reductions are reported.
- The reliance on loss values for performance representation has been criticized, with suggestions for clearer performance metrics.
- The authors face challenges in comparing their method with existing multi-task approaches due to differing objectives.
- The method introduces multiple hyperparameters, complicating reproducibility and deployment.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including a more comprehensive comparison with alternative sparsification and pruning techniques, such as those mentioned in the reviews. Additionally, it would be beneficial to report actual inference acceleration results on real hardware to substantiate claims of computational savings. We suggest that the authors improve the clarity of performance metrics by providing direct accuracy comparisons alongside loss values in their evaluations. Furthermore, we recommend that the authors clarify the practical implications of their method regarding latency and hardware compatibility, as well as provide a more detailed discussion of the theoretical foundations supporting their approach. Lastly, including comparisons with more recent baselines and addressing the potential integration of quantization techniques would enhance the paper's impact and comprehensiveness.