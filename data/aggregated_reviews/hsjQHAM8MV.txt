ID: hsjQHAM8MV
Title: Can We Edit Factual Knowledge by In-Context Learning?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called IKE for editing factual knowledge in large language models (LLMs) using in-context learning (ICL), which avoids the computational costs of traditional gradient-based approaches. The authors propose a framework that utilizes demonstrations formatted as copy, update, and retain to guide the model in learning to edit knowledge effectively. Empirical results indicate that IKE is competitive with gradient-based methods, achieving a strong performance across multiple LLMs.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and easy to follow.
- It proposes an interpretable method for LLM editing, enhancing community understanding and control over knowledge editing processes.
- The empirical evaluation is solid and comprehensive, demonstrating the benefits of ICL in model editing.

Weaknesses:
- Some statements lack rigorous derivation, particularly regarding the quantification process in Table 1, which is confusing.
- The experiments are limited to a single dataset, raising questions about generalizability to other benchmarks.
- The methodology lacks clarity on how demonstrations are formatted and retrieved, and there is insufficient analysis of the retrieved demonstrations.
- The approach may be too restricted to specific formats, limiting its applicability in open-ended generation contexts.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the derivation process for the conclusions in Table 1 and provide detailed explanations of how the editing methods are measured. Additionally, we suggest expanding the experimental evaluation to include common black-box models like GPT-3.5 and GPT-4 to assess scalability and generalizability. The authors should also clarify the methodology regarding demonstration retrieval and provide insights into the number and types of demonstrations used. Finally, we encourage the authors to explore the implications of ICL editing on the model's internal beliefs and reasoning processes, particularly in scenarios involving multiple edits.