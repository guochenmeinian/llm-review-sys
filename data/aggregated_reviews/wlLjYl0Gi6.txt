ID: wlLjYl0Gi6
Title: Efficient LLM Scheduling by Learning to Rank
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a learning-based rank predictor for scheduling LLM inference to mitigate Head-of-Line (HoL) blocking issues, significantly outperforming existing LLM serving systems. The authors propose a method that predicts the relative ranks of output lengths in a batch of requests, facilitating an approximation of shortest-job-first (SJF) scheduling strategies. The evaluation demonstrates substantial improvements in latency and throughput.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a critical issue in LLM serving.
2. It is well-written, with clear methodology and results.
3. The experiments are well-designed, showcasing practical improvements when integrated with current serving techniques.

Weaknesses:
1. The approach may accumulate unused KV cache, raising concerns about handling memory limits on GPUs.
2. The impact of the ranking model size on prediction and throughput performance is unclear.
3. The novelty is limited as it builds on existing SJF/SRTF scheduling benefits without thoroughly discussing related work.
4. The current scheduling only considers output length, neglecting prompt length, which could also affect execution time.

### Suggestions for Improvement
We recommend that the authors improve the discussion on handling accumulated unused KV cache when GPU memory limits are reached. Additionally, the authors should analyze how the ranking model size affects performance and consider including results that compare the ranking-based method to an Oracle to establish a performance upper bound. Furthermore, we suggest incorporating prompt length into the scheduling considerations and discussing the implications of using different models, such as Llama3, on the predictor's accuracy. Lastly, addressing the potential starvation of longer requests in SJF scheduling would enhance the robustness of the proposed method.