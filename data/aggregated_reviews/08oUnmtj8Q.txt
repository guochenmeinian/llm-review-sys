ID: 08oUnmtj8Q
Title: FSEO: A Few-Shot Evolutionary Optimization Framework for Expensive Multi-Objective Optimization and Constrained Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 4, 4, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a meta-learning framework for few-shot optimization to enhance surrogate modeling in expensive evaluation settings. The authors parameterize a mapping function to derive hidden features of the solution space, integrating it into a Gaussian kernel function as a deep kernel. They facilitate meta-training of this deep kernel across related tasks to develop an experience model by maximizing posterior likelihood. The experience model is adapted and updated during online optimization of the target task. Experimental results indicate that the proposed framework achieves competitive performance against strong baselines on EMOPs and ECOPs benchmarks.

### Strengths and Weaknesses
Strengths:
1. The integration of meta-learning into kernel-learning based surrogate methods is novel and may improve surrogate-based optimization in generalizable settings.
2. Experimental results demonstrate that the proposed FSEO framework is competitive with existing baselines, which is commendable for further development.
3. The overall writing is clear and well-structured.

Weaknesses:
1. The rationale for using gradient descent instead of gradient ascent to maximize the likelihood-based loss function (Eq. 4) is unclear.
2. The explanation regarding the U update iterations stemming from a smaller number of related tasks is insufficient.
3. The use of a 2-layer MLP in the deep kernel function restricts FSEO to tasks with the same solution dimension, which may not reflect practical scenarios. The authors should provide realistic examples where FSEO is effective and verify its effectiveness on traditional single-objective tasks.
4. While the writing is generally clear, Sections 3.2 and 3.3 require refinement for better comprehension, as they currently appear too simplistic and ambiguous.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the rationale behind the choice of gradient descent for the likelihood-based loss function. Additionally, please elaborate on the U update iterations and provide a more detailed explanation of how the neural network architecture can accommodate tasks with varying dimensions. It would be beneficial to include realistic scenarios demonstrating the effectiveness of FSEO and to validate its performance on traditional single-objective tasks. Lastly, we suggest refining Sections 3.2 and 3.3 to enhance reader understanding of the DKL and MDKL operations.