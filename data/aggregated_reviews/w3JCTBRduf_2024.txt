ID: w3JCTBRduf
Title: Optimization Can Learn Johnson Lindenstrauss Embeddings
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 8, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a deterministic optimization procedure to find a matrix \( A \) that satisfies the Johnson-Lindenstrauss (JL) guarantee, mapping a set of \( n \) vectors to a lower-dimensional space while preserving pairwise distances with a chosen multiplicative distortion. The authors demonstrate that directly optimizing \( A \) can lead to local minima, but by optimizing the mean and entry-wise variance of the distribution from which \( A \) is sampled, they can maintain a fixed probability of \( A \) being a JL-embedding while ensuring the variance approaches zero. They prove that second-order stationary points (SOSPs) have low variance and provide a method for finding these SOSPs to solve the optimization problem.

### Strengths and Weaknesses
Strengths:  
The paper is clearly written and well-motivated, with an innovative approach to optimizing parameters of a random matrix distribution to preserve the JL-embedding property while reducing entry-wise variance. The empirical results indicate that this method achieves lower distortion compared to randomized constructions for a fixed dimension.

Weaknesses:  
The iterative method for finding matrix \( A \) has a complexity of \( \operatorname{poly}(n, k, d) \) but lacks explicit determination, raising concerns about practical efficiency. The paper is primarily theoretical with limited experiments, and it could benefit from discussing practical applications of the deterministic JL embedding. Additionally, the relationship between deterministic JL embeddings and deep learning embeddings remains unclear.

### Suggestions for Improvement
We recommend that the authors improve the complexity analysis of their algorithm and provide explicit details on its efficiency in practice. Clarifying the practical applications of the deterministic JL embedding and how it relates to deep learning embeddings would strengthen the paper. Additionally, organizing Section 4 with subsections could enhance clarity. Addressing the questions regarding the relaxation of the objective function and the potential randomness in the proposed method would also be beneficial.