ID: S4YRCLbUK1
Title: Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2)
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 7, 7, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents T2IScoreScore (TS2), a benchmark and set of meta-metrics for evaluating text-to-image (T2I) faithfulness metrics. TS2 features higher image-to-prompt ratios than existing benchmarks, enabling the organization of semantic error graphs (SEGs) for evaluating T2I metrics such as embedding-based (CLIPScore/ALIGNScore), QG/A-based (TIFA/DSG), and caption-based (LLMScore/VIEScore) metrics. The authors conclude that simple embedding-based metrics often outperform more computationally intensive metrics in terms of separation criteria.

### Strengths and Weaknesses
Strengths:
- Introduction of meta-metric benchmarks for recent T2I metrics, including a large collection of image-text pairs and SEGs.
- Comprehensive experiments utilizing various VLM backbones for TIFA/DSG.
- Valuable dataset construction for future evaluations of text-image alignment metrics.
- Clear and sound methodology, with insightful analysis regarding the cost of evaluation metrics.

Weaknesses:
- The treatment of QG/A metrics as score regressors overlooks their advantages in providing comprehensive skill-specific performances. This limitation should be clarified in the introduction to avoid misleading new readers.
- The evaluation framework's error counting system is ambiguous, potentially introducing noise into the analysis. The authors should address how sensitive the results are to this design.
- Limited scope in evaluating a broader range of T2I models and datasets, which could enhance the generalizability of findings.

### Suggestions for Improvement
We recommend that the authors improve the introduction by clarifying the limitations of treating QG/A metrics solely as score regressors. Additionally, addressing the ambiguity in the error counting system and its impact on analysis would strengthen the paper. Expanding the evaluation to include a wider variety of T2I models and datasets, such as OpenMUSE or aMUSEd, would enhance the robustness of the findings. Finally, providing a clear recommendation for defining "success" within the evaluation framework could facilitate its adoption in the research community.