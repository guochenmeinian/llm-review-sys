ID: YmEDnMynuO
Title: GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 5, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GraphAdapter, a novel adapter-style tuning strategy for vision-language models that explicitly captures dual-modality structure knowledge through a dual knowledge graph. The authors validate their method on 11 popular benchmarks in a few-shot classification setting, demonstrating its effectiveness in leveraging task-specific structure knowledge from both textual and visual modalities.

### Strengths and Weaknesses
Strengths:
- GraphAdapter effectively models dual-modality structure knowledge, allowing for enhanced performance in few-shot learning across various datasets.
- The authors conducted extensive evaluations, showing consistent performance improvements with different backbone architectures.

Weaknesses:
- The motivation regarding the explicit exploitation of structure knowledge is weakened by experimental results showing limited gains from combining text and visual adapters.
- The construction of the knowledge graph at the outset may hinder adaptability to new data, which contradicts the goal of efficient adaptation.
- Performance improvements appear trivial, with most enhancements being around 1-2%, raising concerns about the method's overall promise.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the efficiency of GraphAdapter by comparing the training time and parameters with previous methods to demonstrate its scalability. Additionally, we suggest exploring the use of more advanced GNN mechanisms, such as GAT and GraphSAGE, and including an analysis of the time complexity associated with constructing the knowledge subgraphs. Furthermore, addressing the potential for open-world scenarios where classes are unknown could enhance the applicability of the proposed method.