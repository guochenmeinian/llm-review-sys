ID: nBOdYBptWW
Title: UniTS: A Unified Multi-Task Time Series Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents UNITS, a multi-task time series model that integrates predictive and generative tasks using task tokenization. UNITS effectively processes heterogeneous time series data without altering the network architecture and demonstrates strong few-shot and prompt learning capabilities across various tasks. The authors propose a unified model trained on multiple datasets for tasks such as forecasting, imputation, anomaly detection, and classification, supported by extensive empirical comparisons with baseline methods. Additionally, the paper provides a comprehensive evaluation of the UniTS model, focusing on its pre-training strategies, task interrelationships, and performance across various datasets. The authors conduct ablation studies to assess the impact of pre-training epochs and dataset sizes on model performance, revealing that both factors significantly enhance forecasting and classification tasks. They explore cross-task and cross-domain pre-training, indicating that the amount of data is more critical than the type of data for effective pre-training, and address the handling of varying channel-wise relationships.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a unified multi-task time series model capable of handling diverse tasks.
- The proposed architecture achieves state-of-the-art performance and adapts to new tasks through parameter-efficient prompt learning.
- Comprehensive experiments, including single-task, multi-task, and few-shot learning, are conducted.
- The authors provide extensive experimental results, including ablation studies that clarify the contributions of different components of the UniTS model.
- The systematic evaluation of pre-training tasks and their interdependencies adds depth to the analysis.
- The model shows promising performance in both forecasting and classification tasks, with robust results across various configurations.

Weaknesses:
- The contributions of the paper are overclaimed, particularly regarding generalizability to new tasks, as the model is pre-trained on all datasets used in experiments.
- The writing requires improvement, especially in detailing the methodology for prompt learning and the role of tokens.
- Empirical evidence lacks comparisons to similar unified training methods, making it difficult to assess the model's performance relative to existing approaches.
- Some experimental setups are not rigorous, with datasets used for few-shot learning being derived from those seen during pre-training.
- The claims regarding the zero-shot capabilities of UniTS may be overstated, as the model primarily focuses on few-shot learning.
- The discussion on the adaptability to new channel-wise relationships lacks clarity, potentially leading to misconceptions about the model's generalization abilities.
- The manuscript could benefit from clearer organization and more detailed descriptions of the pre-training process.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology, particularly in Section 4.3, by providing formal equations and detailed descriptions of the roles of the three types of tokens. Additionally, we suggest reorganizing the experimental settings in Section 5 to clarify which datasets correspond to each task. The authors should include more rigorous comparisons with similar methods that utilize unified training or prompt-based approaches. Furthermore, we encourage the authors to conduct ablation studies to better understand the impact of various pre-training datasets and the relationship between pre-training tasks. We also recommend moderating claims regarding the zero-shot ability of UniTS to avoid overstatement and ensuring that the limitations of the proposed approach are adequately discussed. Lastly, we suggest providing detailed descriptions of the pre-training process, ideally with visualizations to illustrate the role of the two towers in pre-training, and improving the representation in the final version by explicitly incorporating the detailed plan for tuning down claims in the revised manuscript.