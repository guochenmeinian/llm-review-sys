ID: 8xx0pyMOW1
Title: Training neural operators to preserve invariant measures of chaotic attractors
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 5, 6, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two methods aimed at preserving invariant measures of chaotic systems when training neural operators, specifically through the use of optimal transport (OT) and contrastive learning (CL) techniques. The authors argue that their OT method serves as an "oracle" for performance measurement, demonstrating superior results compared to traditional methods, particularly in high-noise settings. Both methods show improved performance in reproducing invariant statistics of noisy chaotic systems, as evidenced by experiments on the Lorenz-96 and Kuramoto-Sivashinsky equations. The authors also provide a comprehensive evaluation of their methods, including performance metrics such as the Lyapunov spectrum and leading Lyapunov exponent, while clarifying their focus on deterministic chaos in high-dimensional systems.

### Strengths and Weaknesses
Strengths:
- The problem of learning solution operators for chaotic dynamics is both challenging and significant, with the authors providing a novel perspective on preserving invariant measures.
- The introduction of physics-informed optimal transport and contrastive learning represents a new approach in this domain.
- The paper is well-structured and logically organized, contributing to clarity.
- The inclusion of additional evaluations on Lyapunov exponents and fractal dimensions strengthens the results and supports the claims of improved generalization ability.

Weaknesses:
- The proposed methods appear to be straightforward combinations of existing technologies, lacking substantial novelty.
- The relationship between the optimal transport and contrastive learning approaches is not clearly articulated, leading to a lack of clarity in the paper's structure.
- The experimental evaluations are weak, primarily demonstrating that methods optimized for specific properties perform well on those properties, without addressing broader performance metrics.
- The authors' assertion that existing methods using only rMSE perform poorly in chaotic setups is contested, as some reviewers argue that those methods were designed for such scenarios.
- There is insufficient discussion on the limitations and advantages of the proposed methods, particularly regarding the optimal transport approach's reliance on extensive equation constraints and the data diversity required for contrastive learning.
- There is a perceived lack of balanced discussion regarding alternative approaches that achieve long-term behavior in chaotic systems.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relationship between the optimal transport and contrastive learning methods, possibly by separating their descriptions in the paper. Additionally, we suggest including a discussion on the numerical error associated with long-term forecasting in autoregressive settings and exploring the possibility of forecasting at arbitrary time intervals during training and testing. It would also be beneficial to clarify the motivation for using optimal transport and contrastive learning, as well as to discuss the advantages and disadvantages of these methods in detail. Furthermore, we recommend that the authors improve the discussion surrounding the performance of existing methods, particularly those using only rMSE, to provide a more balanced perspective on their effectiveness in chaotic dynamics. Finally, we encourage the authors to enhance the experimental section by comparing their methods against previous studies, including statistical error measures in their results, and to include the results of the Lyapunov spectrum and fractal dimension evaluations in the final paper to enhance the robustness of their findings.