ID: gDpWYpocE1
Title: Pandora's Box: Towards Building Universal Attackers against Real-World Large Vision-Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to universal adversarial attacks on large vision-language models (LVLMs), focusing on a black-box setting where only the model's inputs and outputs are accessible. The authors propose a query-based targeted attack that utilizes a proxy model to assess similarities between the target model's responses and target texts. The method demonstrates effectiveness in exposing safety concerns of popular LVLMs through comprehensive experiments.

### Strengths and Weaknesses
Strengths:
- The threat model, allowing access only to the model's responses, is realistic for real-world applications, as LVLMs are often provided as services.
- The proposed method is technically sound, with well-explained components and sufficient ablation studies demonstrating empirical performance.
- The safety concerns highlighted could significantly impact the AI safety field.

Weaknesses:
- The potential for detection of numerous similar queries by defenders is a concern; experiments on defense mechanisms could enhance the paper.
- Clarification is needed regarding the model used for evaluation in comparisons with CroPA, as discrepancies between OpenFlamingo and Flamingo could affect results.
- The limited length of target texts used in experiments may not fully represent attack performance; longer sentences should be included to assess query budgets.
- Solely reporting semantic similarity scores for targeted attacks lacks depth; incorporating word/token-based evaluations like BLEU would provide a clearer understanding of performance.

### Suggestions for Improvement
We recommend that the authors improve the paper by including experiments comparing the proposed universal attack with non-universal attacks on specific tasks to clarify advantages and limitations. Additionally, the authors should provide a clearer explanation of query efficiency, particularly regarding the total number of queries allowed. The claim about "some of them may have opposite directions" needs empirical support. Clarifying how an attacker would obtain the judge model in practical scenarios is essential, as is using more specific target texts for experiments. The visibility of the adversarial patch should be addressed, along with the inclusion of physical attack experiments. Finally, exploring the transferability of the attack to other image-capable language models would enhance the paper's applicability.