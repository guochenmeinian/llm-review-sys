ID: wiI8ycNfgJ
Title: LLM-FP4: 4-Bit Floating-Point Quantized Transformers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel floating-point (FP) post-training quantization (PTQ) technique for transformer-based models, integrating Hessian-based loss into quantization parameter search to establish a strong FP-PTQ baseline. The authors propose per-channel activation quantization and demonstrate that reparameterizing additional scaling factors as exponential biases incurs minimal computational cost. The method achieves impressive compression, quantizing weights and activations in BERT to 4 bits with only a 3.6-point drop in average GLUE score compared to the full-precision model. The paper also addresses high inter-channel variance in transformers and reports competitive performance across various benchmarks.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and accessible, providing a solid background on linear and floating-point quantization.
- The proposed methods demonstrate promising empirical performance and establish a strong baseline for FP-PTQ.
- Ample implementation details facilitate reproducibility by other researchers.

Weaknesses:
- The presentation of results requires improvement, particularly in specifying whether baselines in Tables 1 and 2 use linear or floating-point quantization techniques.
- The inference latency comparison with other methods is lacking, making it difficult to evaluate the comprehensive cost performance.
- Some mathematical formulations may be dense, potentially hindering reader comprehension.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the results section by specifying the quantization techniques used in the baselines of Tables 1 and 2. Additionally, including a comparison of inference latency between the proposed method and existing techniques would enhance the evaluation of feasibility. To address concerns regarding mathematical density, we suggest incorporating more figures and visual aids to illustrate key concepts and processes. Finally, we encourage the authors to supplement experiments on LLM generative tasks to further validate the effectiveness of their method.