ID: 7Dep87TMJs
Title: Learning with Fitzpatrick Losses
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 9, 7, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Fitzpatrick loss, a refined version of the Fenchel-Young loss, grounded in monotone operator theory. The authors detail the fundamental properties of the Fitzpatrick loss, illustrating its distinctions from Fenchel-Young losses through concrete examples. They demonstrate that the Fitzpatrick loss can be viewed as a Fenchel-Young loss influenced by target-dependent regularizers and linked to generalized Bregman divergences, with its primal-dual nature ensuring convexity. A lower bound for the Fitzpatrick loss, analogous to its Fenchel-Young counterpart, is also provided, alongside experimental comparisons on classification tasks.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel framework for designing loss functions based on monotone operator theory, expanding beyond the existing Fenchel-Young loss framework, which is beneficial for the machine-learning community.
2. The detailed discussion of the Fitzpatrick loss properties and its connections to other concepts aids readers familiar with existing notions.

Weaknesses:
1. The experimental results lack compelling evidence of the Fitzpatrick loss's superiority, with only slight improvements noted in specific cases, raising questions about its practical utility.
2. Concerns exist regarding the completeness of proofs in the appendix, with potential omissions that could affect the overall soundness of the claims.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by providing a broader range of tasks and metrics, such as accuracy or F1-score, to better demonstrate the Fitzpatrick loss's advantages over existing losses. Additionally, clarifying the relationship between Fitzpatrick loss and cost-sensitive Fenchel-Young loss, as well as addressing the potential for establishing calibration of target and Fitzpatrick excess risks, would enhance the paper's depth. We also suggest ensuring that all assumptions in the mathematical statements are explicitly stated, particularly regarding the compactness of the domain, to avoid any ambiguity. Lastly, addressing minor comments regarding definitions and citations would improve clarity and presentation.