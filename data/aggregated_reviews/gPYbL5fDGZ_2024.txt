ID: gPYbL5fDGZ
Title: CopRA: A Progressive LoRA Training Strategy
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 8, 3, 6, 8, -1, -1
Original Confidences: 1, 4, 3, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to enhancing Low-Rank Adaptation (LoRA) through the CopRA method, which aims to address limitations in merging, pruning, and generalization. The authors propose a progressive training strategy that incorporates cooperative game theory, specifically using Shapley value optimization to evaluate layer contributions. The methodology is well-supported by theoretical grounding and comprehensive experimental results across multiple tasks, demonstrating CopRA's advantages. However, discussions on computational complexity and trade-offs are insufficiently detailed.

### Strengths and Weaknesses
Strengths:
- The introduction of a progressive training strategy for LoRA is timely and relevant.
- CopRA shows promising results in model merging, with potential applications in federated and multi-task learning.
- The innovative use of cooperative game theory provides a structured perspective on layer contributions.

Weaknesses:
- The paper lacks a rigorous theoretical foundation for claims regarding linear mode connectivity and Shapley value optimization.
- Experimental validation is limited to image classification tasks, necessitating a broader range of evaluations.
- There is insufficient comparison with state-of-the-art methods, making it difficult to assess CopRA's relative contribution.
- Clarity issues exist in the methodology section, and reproducibility is hindered by a lack of implementation details.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the proposed method to support claims about linear mode connectivity and Shapley value optimization. Expanding the experimental evaluation to include a wider range of tasks and model architectures would strengthen the paper's claims about generalizability. Additionally, providing a more comprehensive comparison with state-of-the-art methods in LoRA training and merging is essential. Improving clarity in the methodology section and overall presentation will enhance understanding, and including more details on hyperparameters and implementation specifics will ensure reproducibility of the results.