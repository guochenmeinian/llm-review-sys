ID: hn1oJO7lg6
Title: Computing Approximate $\ell_p$ Sensitivities
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 7, 7, 5, -1, -1, -1
Original Confidences: 3, 4, 2, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents randomized algorithms to approximate $\ell_p$ sensitivity functions for $p \in [1,\infty)$, extending leverage scores beyond the $\ell_2$ norm. The authors focus on three tasks: estimating all sensitivities, total sensitivity, and maximum sensitivity, providing different approximation types for each. They demonstrate their algorithms for $\ell_1$ and generalize to all $\ell_p$ norms with $p > 1$, proving a hardness result by reducing $\ell_p$ regression to sensitivity estimation. The algorithms utilize techniques such as hashing with Rademacher combinations, subspace embeddings, and leverage score intervals, and are empirically validated on two datasets.

### Strengths and Weaknesses
Strengths:  
- The problem of sensitivity estimation is relevant for regression and has been underexplored for general $p$. Approximating total sensitivity is crucial for understanding sample complexity in learning arbitrary functions.  
- The algorithms combine known results from sensitivity sampling and randomized numerical linear algebra (RNLA), with the total sensitivity approximation being particularly noteworthy due to its computational efficiency.  
- The analysis of the algorithms is largely clear, enhancing understanding of their guarantees.

Weaknesses:  
- Algorithm 4 lacks significant novelty, primarily integrating generalized sensitivity with an existing $\ell_\infty$ subspace embedding technique.  
- The motivation for estimating maximum sensitivity is unclear.  
- There are typos and missing definitions, such as the unclear use of the leverage scores vector $\tau(C)$ in Algorithm 3 and the undefined variable $\omega$.  
- The writing and clarity of the paper could be improved, particularly in discussing related works and the implications of the lower bounds.

### Suggestions for Improvement
We recommend that the authors improve the clarity and motivation for the maximum sensitivity estimation. Additionally, addressing the open problems and limitations in a dedicated section would enhance the paper. We suggest including a comparison of approximate $\ell_p$ sensitivities with Lewis weights and discussing alternative sampling methods, such as using row norms of a well-conditioned basis. Furthermore, providing a table comparing various $\ell_1$ subspace embedding methods in terms of time, approximation factors, and the number of rows required would be beneficial. Lastly, clarifying the exact computation of leverage scores in Algorithm 2 and elaborating on the implications of the lower bounds would strengthen the paper.