ID: GkHXBasQwm
Title: HOI-Swap: Swapping Objects in Videos with Hand-Object Interaction Awareness
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HOI-Swap, a diffusion-based video editing framework designed for object swap editing in hand-object interaction (HOI) scenarios. The authors propose a two-stage approach: the first stage trains an image-editing model to swap objects in a single frame, while the second stage uses optical flow to warp video sequences and trains a video diffusion model to reconstruct the entire video. HOI-Swap effectively addresses challenges such as HOI-awareness, spatial alignment of objects with hands, and temporal alignment with the source video, demonstrating superior performance compared to existing methods.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a novel task in HOI video editing.
2. It identifies three main challenges in this domain.
3. The use of optical flow in the video-editing stage enhances temporal alignment.
4. HOI-Swap outperforms existing video-editing approaches in HOI contexts.

Weaknesses:
1. The first stage does not explicitly address HOI-awareness; the authors should clarify how the image-editing model perceives HOI.
2. Generalizability across unseen object types is not discussed.
3. Several details regarding the experimental setup are missing, which are crucial for understanding the claims made.
4. The second stage lacks dynamic HOI awareness, limiting its effectiveness in complex video sequences.

### Suggestions for Improvement
We recommend that the authors improve the explanation of how the image-editing model in stage I perceives HOI, possibly by including additional ablation studies. It would be beneficial to test the model on a broader range of object types to assess generalizability. The authors should provide clarity on the test split creation for datasets like HOI4D and EgoExo4D, as well as a breakdown of results across all datasets. Additionally, we suggest that the authors discuss the protocols followed for training baselines and consider validating the two-stage approach quantitatively against a one-stage alternative. Lastly, a deeper discussion on the downstream applications of the model and the potential for automatic input mask annotation would enhance the manuscript.