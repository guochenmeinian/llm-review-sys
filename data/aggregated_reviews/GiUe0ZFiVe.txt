ID: GiUe0ZFiVe
Title: Bi-Level Offline Policy Optimization with Limited Exploration
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 7, 7, 6, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a bi-level structured policy optimization algorithm for offline reinforcement learning (RL), addressing distributional shift and function approximation challenges. The authors propose a hierarchical framework where the lower level constructs a confidence set of value estimates, while the upper level maximizes a conservative estimate. The framework includes a penalized adversarial estimation algorithm and provides theoretical guarantees without requiring data coverage. Empirical evaluations are conducted on both synthetic and real-world datasets, comparing the proposed method against established offline RL algorithms. Furthermore, the authors conduct a thorough investigation of the proposed algorithm through sensitivity analyses and hyperparameter tuning across various D4RL benchmarks, demonstrating stable performance under extreme hyperparameter settings. They utilize a two-layer fully connected neural network for function approximation, employing Adam as the optimizer and adhering to a structured learning rate schedule, while providing detailed comparisons with competing methods.

### Strengths and Weaknesses
Strengths:
1. The hierarchical structure for solving offline RL problems is novel and interesting.
2. The proposed method effectively incorporates uncertainty and prevents excessive pessimism through a confidence set of value estimates.
3. The paper is well-organized, providing solid theoretical results and demonstrating applicability without data coverage assumptions.
4. Extensive sensitivity analyses and additional experiments address concerns about hyperparameter tuning.
5. Clear presentation of hyperparameter selection rules and their impact on policy performance.
6. The paper effectively clarifies theoretical aspects, including boundedness and sample complexity.

Weaknesses:
1. The empirical evaluation lacks validation of theoretical claims, particularly regarding the regret bounds of Theorems 4.1, 5.1, and 5.2, and does not adequately demonstrate the benefits of the bi-level approach in addressing distribution shift.
2. The connection between the information-theoretic results and the practical algorithm remains unclear, and the significance of these results is not well articulated.
3. The algorithm's complexity compared to alternatives raises concerns about computational cost and stability.
4. Some reviewers noted that the discussion on the tightness of bounds could be better integrated into the main text.
5. There is a suggestion to enhance the clarity of the empirical algorithm's motivation in Section 4.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by validating the theoretical claims, specifically demonstrating whether the regret bounds hold and comparing performance with and without the upper/lower levels. Additionally, we suggest clarifying the significance of the information-theoretic results and their connection to the penalized adversarial estimation algorithm. It would also be beneficial to provide a detailed explanation of the optimal selection criteria for hyperparameters like $\tilde{\sigma_{n}}$ and to include more standard benchmarks, such as D4RL tasks, in the experiments to enhance the positioning of the work within existing literature. Furthermore, we recommend improving the integration of the discussion on the tightness of bounds into the main text, particularly in Section 4, to better highlight its significance, and consider including this discussion in the Appendix to further clarify its role in motivating the empirical algorithm.