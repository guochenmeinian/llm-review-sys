ID: N5H4z0Pzvn
Title: On Divergence Measures for Training GFlowNets
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates alternative training methods for Generative Flow Networks (GFlowNets) by evaluating various divergence measures, including Renyi-α, Tsallis-α, reverse, and forward Kullback-Leibler (KL) divergences. The authors propose efficient estimators for the stochastic gradients of these divergences and introduce control variates to reduce gradient variance. Empirical results demonstrate that minimizing these divergence measures accelerates training convergence and enhances stability compared to traditional methods. The paper establishes theoretical connections between GFlowNets and variational inference, extending insights to arbitrary topological spaces.

### Strengths and Weaknesses
Strengths:
- **Comprehensive Evaluation**: The paper thoroughly evaluates multiple divergence measures and their impact on GFlowNet training.
- **Theoretical Insights**: Establishing theoretical connections between GFlowNets and variational inference broadens the understanding of these models.
- **Practical Contributions**: The design of control variates to reduce gradient variance is a significant practical contribution applicable in various optimization scenarios.
- **Variance Reduction Techniques**: The introduction of variance reduction techniques effectively addresses high variance in gradient estimates, enhancing learning stability and efficiency.

Weaknesses:
- **Theory**: The theoretical contribution lacks novelty compared to previous works, and stronger experiments are required to substantiate claims.
- **Experiments - Part 1**: The paper lacks sufficient real-world tasks to illustrate its main contribution, raising questions about the practical significance of the proposed methods.
- **Experiments - Part 2**: Synthetic task plots are incomplete, and there is no clear best loss function, necessitating stronger evidence for the use of alternative divergence measures.
- **Scalability**: The tasks considered are relatively small, making it unclear how scalable the proposed objectives are.

### Suggestions for Improvement
We recommend that the authors improve the theoretical contributions by providing novel insights beyond existing literature. Additionally, the authors should include a wider range of real-world tasks, such as fragment-based molecule generation, graph combinatorial optimization, and RNA sequence generation, to better illustrate the practical significance of their methods. For synthetic tasks, we suggest including complete plots and providing a guideline on selecting the best divergence measures based on prior information. Finally, we encourage the authors to explore the scalability of their proposed objectives in more complex tasks to strengthen their empirical analysis.