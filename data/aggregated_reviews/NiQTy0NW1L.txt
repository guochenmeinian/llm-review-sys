ID: NiQTy0NW1L
Title: Lexinvariant Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 7, 8, 3, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents lexinvariant language models, where the probability of any sequence remains unchanged under vocabulary permutation. The authors replace token embeddings in a Transformer model with random Gaussian vectors that are consistent within a sequence but vary across different examples. They demonstrate that lexinvariant models can converge to the true language distribution, particularly in synthetic reasoning tasks, despite practical limitations for larger vocabularies. The paper also discusses potential applications of lexinvariance, including regularization.

### Strengths and Weaknesses
Strengths:
- Lexinvariance is a novel concept in natural language processing, previously unexplored.
- The experiments are thorough, showcasing convergence, cipher recovery, and superior performance on synthetic reasoning tasks.
- The paper is well-organized, clearly introducing lexinvariance and its theoretical properties.

Weaknesses:
- Convergence can be slow as vocabulary size increases, limiting practical applicability.
- The motivation behind the approach lacks clarity, and comparisons with byte-level LMs could enhance understanding.
- The experiments primarily focus on synthetic data, raising questions about real-world applicability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind lexinvariance and provide a more thorough comparison with byte-level LMs. Additionally, addressing the practical limitations of convergence speed for larger vocabularies would strengthen the paper. Clarifying the treatment of spaces in character-level examples and providing details on prompt lengths for synthetic reasoning tasks would also enhance the paper's comprehensibility. Finally, we suggest including more examples of semantic deciphering and discussing potential pitfalls in the probing experiments to differentiate model knowledge from probe knowledge.