ID: h4r00NGkjR
Title: VideoComposer: Compositional Video Synthesis with Motion Controllability
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 6, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VideoComposer, a framework for conditional video generation that incorporates textual, spatial, and temporal conditions. It utilizes motion vectors from compressed videos as explicit control signals to guide temporal dynamics and introduces a Spatio-Temporal Condition encoder (STC-encoder) to integrate spatial and temporal relations of sequential inputs, enhancing inter-frame consistency. Extensive experiments validate that VideoComposer effectively controls spatial and temporal patterns in synthesized videos.

### Strengths and Weaknesses
Strengths:
1. The introduction of motion vectors as a flexible user-guided signal enhances control over video synthesis.
2. The STC-encoder serves as a unified interface for effectively incorporating spatial and temporal relations.
3. The results demonstrate impressive temporal consistency and flexibility in various video editing tasks.

Weaknesses:
1. The distinction between the roles of the "Style" of CLIP and "Single Image" of the STC-encoder is unclear, as both seem to provide content to videos.
2. VideoComposer achieves comparable performance to prior models but lacks clarity on efficiency regarding training cost and inference time.
3. There is insufficient visualization and qualitative comparison with existing video generative models, particularly in the ablation studies and discussions on the integration of conditions.

### Suggestions for Improvement
We recommend that the authors clarify the differences between the "Style" of CLIP and the "Single Image" of the STC-encoder. Additionally, the authors should provide comparisons regarding training costs and inference times to address efficiency concerns. We encourage the authors to include extensive qualitative comparisons with existing models and to conduct a more comprehensive ablation study that evaluates the impact of each component and the combination of conditions on performance. Furthermore, exploring alternative design choices for integrating Condition Fusion into the U-Net, such as cross-attention, could enhance the model's effectiveness.