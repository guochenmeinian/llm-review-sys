ID: 70iM5TBkN5
Title: A Large-Scale Human-Centric Benchmark for Referring Expression Comprehension in the LMM Era
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 8, 6, -1, 7
Original Confidences: 5, 3, -1, 2

Aggregated Review:
### Key Points
This paper presents HC-RefLoCo, a benchmark for referring expression comprehension that includes 13,452 images, 24,129 instances, and 44,738 detailed annotations averaging 93.2 words. The authors propose a comprehensive evaluation framework with diverse protocols, including accuracy with different IoU criteria and scale-aware assessments, aimed at advancing large multimodal models (LMMs). The dataset's extensive annotations cover various subjects, enhancing the evaluation of LMMs.

### Strengths and Weaknesses
Strengths:
1. The HC-RefLoCo dataset is well-designed and provides a wealth of information, making it a valuable resource for the LMM research community.
2. The benchmark significantly surpasses existing datasets like RefCOCO, RefCOCO+, and RefCOCOg in scale and detail, offering a richer context for evaluation.
3. It includes comprehensive evaluation protocols that extend beyond traditional metrics.
4. The paper features thorough evaluations of several LMMs, yielding valuable insights.

Weaknesses:
1. The dataset lacks sufficient discussion on potential biases, such as demographic representation and scene diversity.
2. Current annotations are limited to bounding boxes; introducing segmentation annotations could enhance the dataset's utility for detailed object recognition tasks.
3. The applicability of Referring Expression Generation (REG) to this benchmark is not addressed, which could enhance its perceived versatility.
4. The introduction does not clearly define "human-centric" context, leading to potential confusion about the task's focus.

### Suggestions for Improvement
We recommend that the authors improve the discussion on human and scene diversity within the dataset. Additionally, a brief mention of plans to incorporate segmentation annotations in future work would be beneficial. To enhance task validity, we suggest reframing the dataset to include instances where all details in the referring expression are necessary for accurately identifying the correct person. Furthermore, clarifying the definition of "human-centric" context in the introduction would aid reader comprehension.