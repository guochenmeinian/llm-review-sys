ID: ODbTlAs0Oj
Title: M$^3$GPT: An Advanced Multimodal, Multitask Framework  for Motion Comprehension and Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 6, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents M3GPT, an advanced Multimodal, Multitask framework for motion comprehension and generation. M3GPT aims to create a unified representation space for various motion-relevant modalities, employing discrete vector quantization for seamless integration into a large language model (LLM). The framework operates directly in the raw motion space to avoid information loss associated with discrete tokenizers and learns to model connections among different motion tasks, utilizing text as a bridge for mutual reinforcement. The model demonstrates competitive performance across multiple tasks and strong zero-shot generalization capabilities.

### Strengths and Weaknesses
Strengths:
1. M3GPT integrates multiple modalities (text, music, motion) within a single framework, addressing a significant gap in existing research.
2. The framework achieves competitive performance across various motion tasks and showcases impressive zero-shot generalization capabilities.
3. The joint training of LLM and motion de-tokenizer optimizes performance in both discrete semantic and continuous motion spaces.

Weaknesses:
1. Key experimental comparisons are missing, particularly against models like MoMask, T2M-GPT, and MotionGPT, which could strengthen the argument for M3GPT's superiority.
2. The complexity of the framework may require significant computational resources for implementation and extension.
3. The paper lacks detailed discussions on evaluation metrics, benchmarks, and the potential real-world applications and limitations of M3GPT.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including comparisons with additional baselines, such as MoMask and T2M-GPT, to validate the framework's effectiveness. Additionally, the authors should provide more details on the parameterizations in Section 3 and include an ablation study regarding the text encoder, particularly the use of T5. A more thorough discussion of the evaluation metrics and the training process, including computational resources and time, would enhance the paper's clarity. Finally, we suggest that the authors address the qualitative aspects of generated motions to provide a comprehensive understanding of their realism and naturalness.