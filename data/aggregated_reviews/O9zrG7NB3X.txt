ID: O9zrG7NB3X
Title: Learn Your Tokens: Word-Pooled Tokenization for Language Modeling
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel tokenization technique for language modeling that pools characters or bytes into word representations using word boundaries. The method compresses these representations into a fixed number of CLS tokens (n=1 and n=4), demonstrating improved efficiency over byte/char tokenization and significant language modeling accuracy, particularly for rare words. The authors argue that tokenization is a critical step in NLP tasks, especially for LLMs, and provide a clear exposition of the limitations of existing methods.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and easy to understand.
- It addresses a timely and relevant topic in NLP, with strong experimental results showing significant improvements in accuracy.
- The methodology is sound, and the experiments cover various languages and specific tasks, such as arithmetic.

Weaknesses:
- Experiments are conducted on a single autoregressive language model, raising concerns about generalizability and scalability.
- The evaluation conditions for different tokenization strategies may not be consistent, potentially affecting the validity of comparisons.
- The reliance on a small dataset may limit the applicability of findings to real-world scenarios.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by testing their method on larger and more diverse language models to assess scalability. Additionally, clarifying the evaluation conditions for all tokenization strategies would strengthen the validity of the comparisons. We also suggest providing a more detailed explanation of metrics such as "word accuracy" and ensuring that all technical terms and references are properly cited. Finally, addressing the granularity of the learning signal in relation to other methods would enhance the robustness of the findings.