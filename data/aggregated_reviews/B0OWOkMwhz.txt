ID: B0OWOkMwhz
Title: MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MVSplat360, a framework for 360-degree novel view synthesis from sparse observations, utilizing an improved MVSplat for coarse geometry and a stable video diffusion model for appearance refinement. The authors condition the diffusion model on rendered features and CLIP embeddings to enhance semantic integration. The paper emphasizes the challenges of synthesizing 360-degree scenes from limited views and demonstrates that MVSplat360 shows improved consistency in rendering novel views compared to existing methods, particularly ReconFusion. Experimental results indicate improvements over state-of-the-art methods on the DL3DV and RealEstate10K datasets, especially in qualitative assessments where plausible content is generated beyond the input images. The authors acknowledge the potential for improving the efficiency of video diffusion models and plan to incorporate additional features and comparisons in future work.

### Strengths and Weaknesses
Strengths:
1. Originality: The combination of generalizable Gaussian splatting with a stable video diffusion model addresses limitations in current feed-forward neural techniques for 360Â° synthesis, establishing a new benchmark for sparse view reconstruction.
2. Clarity: The paper is well-written, making the proposed method accessible to readers.
3. Experimentation: Extensive evaluations on challenging datasets show that MVSplat360 consistently outperforms existing methods, particularly in handling complex camera trajectories.
4. Justification of Efficiency: The authors provide a clear justification for the efficiency of their approach compared to other methods requiring per-scene optimization.

Weaknesses:
1. Contribution: The technical contribution is ambiguous, as the integration of generative models and conditioning on rendered features resembles existing works like latentSplat and MVSplat.
2. Incremental Nature: The incremental nature of the proposed method relative to previous works limits its overall impact.
3. Experiments: Quantitative improvements are limited, with PSNR and SSIM results not meeting satisfactory levels. The refinement module's impact on pixel-wise metrics is minor.
4. Efficiency: The rendering speed is significantly hindered by the heavy diffusion model, reducing performance from over 100 FPS to approximately 1.75 FPS.
5. Clarity of Method: The optimization process and loss terms used during training are not clearly articulated, leading to confusion regarding the architecture's training dynamics.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by explicitly distinguishing their work from existing methods like latentSplat and ReconFusion. Additionally, providing a thorough evaluation using MVSplat+latentSplat Decoder could clarify the necessity of the heavy diffusion model. We suggest including ablation studies with PSNR results using more than 7 input views to assess the model's generalization capabilities. Furthermore, addressing the limitations of multi-view consistency in complex scenes and validating the effectiveness of the proposed post-processing method would strengthen the paper. We also recommend improving the clarity of the writing throughout the paper, particularly in highlighting the importance of joint generation of novel views using a video diffusion model and evaluating the resulting 3D consistency. Lastly, we advise implementing a mesh reconstruction approach from generated novel views, similar to that used in latentSplat, for a more convincing evaluation of 3D consistency.