ID: 3kDWoqs2X2
Title: Fearless Stochasticity in Expectation Propagation
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 8, 5, -1, -1, -1
Original Confidences: 3, 2, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two methods for Expectation Propagation (EP), aimed at addressing the sensitivity of EP to stochastic noise from Monte Carlo (MC) estimates. The authors reinterpret the moment-matching update as a Natural Gradient Descent (NGD) step, which mitigates bias and enhances stability and computational efficiency with single-sample moment-matching updates. The empirical results indicate that the proposed methods are easier to tune than alternatives and demonstrate promising performance.

### Strengths and Weaknesses
Strengths:
- The submission offers a well-reasoned technical improvement to EP, which is underexplored in the literature, supported by empirical results that validate the proposed algorithms.
- The authors effectively reduce dependence on hyperparameter tuning, enhancing confidence in the empirical findings.
- The novel NGD interpretation may inspire further EP variants, and the work provides a clear overview of EP and its variants.

Weaknesses:
- The exposition is dense and at times difficult to follow, with excessive focus on extraneous modeling details that detract from algorithmic clarity.
- The novelty of the NGD perspective may be overstated, as the relationship between moment matching in the exponential family and NGD is well-known.
- The paper lacks a comparison to variational inference, which remains a relevant benchmark.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the exposition by reducing extraneous details and focusing more on the algorithm's implications and results. Additionally, we suggest emphasizing the domain of $\theta_i$ in Section 2.1 and clarifying the relationship between the proposed methods and existing literature on NGD. Including a benchmark against variational inference would also strengthen the paper's contributions. Finally, we encourage the authors to provide more detailed explanations of experimental results within the main text rather than relegating them to the appendix.