ID: rIHx6puY5b
Title: SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 7, 6, 7, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SELECT, a benchmark for evaluating data curation strategies in image classification, introducing IMAGENET++, which extends ImageNet with five additional training data shifts using various curation methods. The authors propose metrics for assessing model performance, including in-distribution accuracy, out-of-distribution robustness, transfer learning, and self-supervised learning capabilities. The findings indicate that the original ImageNet dataset generally outperforms the new datasets across metrics, and the cost of curation does not correlate with benchmark performance.

### Strengths and Weaknesses
Strengths:
- The benchmark SELECT addresses a critical problem in data curation, providing a comprehensive evaluation framework that significantly impacts model performance.
- The paper offers a thorough analysis of multiple metrics, enhancing the understanding of data curation quality.
- The release of IMAGENET++ and associated code promotes reproducibility and further research.

Weaknesses:
- The focus is limited to image classification, which may restrict the generalizability of the findings to other computer vision tasks.
- The dataset shifts primarily derive from ImageNet, potentially lacking diversity in real-world data distributions.
- Some proposed metrics, such as CLIPScore, have limited utility in predicting downstream performance, indicating a need for more informative metrics.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by streamlining content and focusing on two or three main conclusions, moving less relevant material to the appendix. Additionally, we suggest expanding the benchmark to include other computer vision tasks beyond classification to enhance generalizability. The authors should also consider incorporating more diverse data sources to strengthen their findings. Furthermore, developing more effective metrics that correlate better with downstream performance would be beneficial. Lastly, we advise addressing the sparse related work section by discussing additional data-centric literature to provide better context.