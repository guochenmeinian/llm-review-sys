ID: drpJ7KOr3F
Title: LLMs Can Evolve Continually on Modality for $\mathbb{X}$-Modal Reasoning
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 5, 6, -1, -1, -1
Original Confidences: 4, 1, 5, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework, PathWeave, which integrates continual learning into multi-modal large language models (MLLMs) to enhance their reasoning abilities across diverse modalities while mitigating knowledge retention issues. The authors propose an Adapter-in-Adapter (AnA) structure that facilitates efficient modality alignment and collaboration. They establish a challenging benchmark, MCL, to evaluate the performance of their method on new modalities and previously learned knowledge. Experimental results indicate significant reductions in training parameter burdens while achieving state-of-the-art performance.

### Strengths and Weaknesses
Strengths:
1. The paper is logical, fluent, and easy to understand.
2. The proposed method allows pre-trained large models to expand on multiple modalities without joint training, presenting a novel approach that could inspire further multimodal research.
3. The establishment of the MCL benchmark is a promising contribution for evaluating cross-modal continual learning and generalization.
4. Extensive experiments across five modalities and over 20 datasets demonstrate comparable performance to joint training methods while significantly reducing parameter burdens.

Weaknesses:
1. The adapter fine-tuning method's performance compared to fine-tuning all parameters shows room for improvement, necessitating further discussion or experimentation.
2. A comparative analysis between this method and VPGTrans is needed to clarify their similarities and differences.
3. Minor issues include mismatches in modality sequences in figures and tables, unclear representation of parameters in the AnA structure, and insufficient clarity on training details such as loss functions and hyperparameters.

### Suggestions for Improvement
We recommend that the authors improve the discussion and analysis of the adapter fine-tuning method's performance relative to full parameter fine-tuning. Additionally, a comparative analysis with VPGTrans should be included to elucidate the distinctions and similarities between the two approaches. Furthermore, we suggest addressing the minor issues identified, such as ensuring consistency in modality sequences, clarifying parameter representation in figures, and providing detailed training information, including loss functions and hyperparameters for each modality.