ID: Y2hnMZvVDm
Title: Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 8, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the global convergence of gradient descent for training two-layer networks to learn a high-dimensional quartic function. The authors demonstrate that gradient descent converges with a sample size of \( n = O(d^{3.1}) \) and that the neural network's width can grow polynomially in \( d \). They also establish that any kernel method with \( n \ll d^4 \) cannot achieve the same accuracy, highlighting the advantages of neural networks over kernel methods in this context.

### Strengths and Weaknesses
Strengths:  
The paper is well-written and provides significant insights into the non-asymptotic mean field analysis of gradient descent training for two-layer neural networks. The convergence analysis introduces novel ideas, particularly in demonstrating how projected gradient flow dynamics can escape saddle points and in bounding the coupling error between empirical and population dynamics.

Weaknesses:  
The main limitation is the focus on a specific quartic activation function, which may restrict the generalizability of the results. Additionally, the comparison with prior works is insufficient, making it challenging to evaluate the significance of the findings. There are also concerns regarding the clarity of certain assumptions and the need for more detailed discussions on the implications of the results.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their results by exploring whether the proof techniques can extend to more general activation and target functions. Additionally, a detailed comparison with relevant prior works, particularly regarding the sample complexity and optimization dynamics, would enhance the paper's significance. It would also be beneficial to visualize the training dynamics in Section 5 using simulations of synthetic or real-world data to provide a more convincing illustration of the findings. Finally, clarifying the assumptions and addressing the questions raised regarding the relationship to previous studies would strengthen the overall presentation.