ID: 4PkBhz18in
Title: High Precision Causal Model Evaluation with Conditional Randomization
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel estimator for causal error that leverages Inverse Probability Weighting (IPW) to achieve lower variance than traditional methods. The authors propose a pairs estimator that applies IPW to both model-predicted and observed treatments, demonstrating its effectiveness through empirical tests across various scenarios. The theoretical foundation asserts that this approach can reduce variance under specific assumptions, and results indicate performance comparable to Randomized Controlled Trials (RCTs).

### Strengths and Weaknesses
Strengths:
- The proposed method is simple yet effective in reducing causal error variance.
- The assumptions supporting the main results are clearly articulated.
- Extensive empirical testing on both compliant and non-compliant datasets strengthens the findings.

Weaknesses:
- The theoretical comparison primarily focuses on the naive IPTW estimator, neglecting other existing variance-reduction methods.
- The paper lacks clarity regarding the overall goal, which should emphasize estimating true causal effects rather than just causal error.
- Missing supplementary materials hinder a thorough review of theoretical proofs and empirical setups.

### Suggestions for Improvement
We recommend that the authors improve clarity in the writing, particularly regarding the implications of skewed propensity scores and the relationship between causal effect estimation and causal error. Additionally, the authors should explicitly state whether the model is trained on a different dataset than that used for the IPTW estimator. Providing supplementary materials, including proofs and detailed experimental setups, would greatly enhance the review process and the paper's overall rigor.