ID: SQKUZSieVg
Title: Challenges in Explaining Representational Similarity through Identifiability
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 8, 5, 8, 7, -1
Original Confidences: 4, 4, 3, 2, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the phenomenon of representational similarity between models, emphasizing that two models with similar losses may not yield similar likelihoods or representations. The authors identify two main challenges: the rarity of exact likelihood equality and the potential for dissimilar representations even among models with near-optimal loss values. They extend identifiability results by linking representation deviations to log-likelihood differences and provide a toy example illustrating these concepts.

### Strengths and Weaknesses
Strengths:
- The paper offers significant theoretical insights into representational similarity and identifies open questions for future research.
- It includes a theoretical proof demonstrating that representations from one model can be expressed as a linear transformation of another's, plus an error term, contingent on equal likelihoods.
- The toy example effectively illustrates the disparity in representations from models with close loss values.

Weaknesses:
- The paper lacks concrete solutions or empirical evaluations to validate its theoretical claims.
- The definition of unembedding functions in Section 2 is unclear, which is critical for understanding the paper's key results.
- The experimental example relies on manually created representations, necessitating verification with neural networks trained on similar datasets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of unembedding functions in Section 2 to enhance understanding. Additionally, we suggest including empirical evaluations or experiments to test the theoretical claims, particularly using neural networks trained on similar datasets with near-optimal loss values. Finally, we encourage the authors to explore the practical implications of their findings for broader representation learning scenarios.