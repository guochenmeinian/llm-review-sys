ID: bdgUPZhF9b
Title: InstructoR: Instructing Unsupervised Conversational Dense Retrieval with Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called INSTRUCTOR for unsupervised conversational dense retrieval, leveraging large language models (LLMs) to generate supervised signals for retriever training without labeled data. The main contributions include: 
1. An unsupervised training framework where LLMs estimate session-passage relevance scores.
2. Three strategies for calculating relevance: modeling retrieval as conversation generation, question rewriting as a latent variable, and using question responses as a posterior guide.
3. Empirical results demonstrating significant improvements over supervised methods across multiple datasets.

### Strengths and Weaknesses
Strengths:
- The paper addresses a critical issue in conversational retrieval training and proposes a creative method for leveraging LLMs.
- Extensive and rigorous experiments validate the effectiveness of the proposed method, showing state-of-the-art results.
- The writing is clear and the methodology is well-documented, facilitating reproducibility.

Weaknesses:
- Limited analysis on how different conversation types affect LLM supervision quality.
- The approach's scalability with larger datasets is unclear, raising concerns about memory and compute costs.
- The reliance on LLMs introduces potential biases and lacks transparency, with insufficient error analysis.
- Some claims regarding the difficulty of annotating session-passage pairs are questionable, as the datasets used involve prior passage searches.

### Suggestions for Improvement
We recommend that the authors improve the analysis of how the quality of LLM-generated supervision signals varies across different types of conversations or questions. Additionally, please clarify how the approach scales with dataset size and provide details on memory and compute costs. We suggest including a more thorough error analysis to understand the limitations of using LLMs as black-box models. Furthermore, consider discussing the potential of using LLM generation probabilities directly for ranking passages instead of relying solely on retriever fine-tuning. Lastly, ensure that recent literature is adequately referenced in the related work section to enhance the completeness of the literature review.