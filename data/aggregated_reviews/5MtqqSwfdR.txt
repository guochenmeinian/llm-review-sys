ID: 5MtqqSwfdR
Title: Distillation vs. Sampling for Efficient  Training of Learning to Rank Models
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: 0, -1, 2, 1, -1
Original Confidences: 4, 4, 4, 4, 4

Aggregated Review:
### Key Points
This paper empirically evaluates the effectiveness of dataset distillation in learning to rank (LTR) tasks compared to random sampling. The authors adapt established dataset distillation techniques to LTR, conducting thorough experiments across three datasets. While the results indicate a deterioration in performance with dataset distillation relative to random sampling, the paper raises questions about the generalizability of these findings to more complex LTR models.

### Strengths and Weaknesses
Strengths:  
- The problem is clearly stated, and the methods are well-adapted to the LTR context.  
- The paper is easy to read and presents original contributions to the field.  
- The experimental analysis is thorough, providing insights into the performance of the proposed methods.

Weaknesses:  
- The motivation for dataset distillation is not clearly articulated, leading to confusion about its applicability to LTR.  
- The methodologies lack clarity, particularly in Sections 3 and 4, which should be comprehensible independently.  
- The results are disappointing, showing no improvement over random sampling, raising concerns about the generalizability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind dataset distillation, specifically addressing its effectiveness in LTR compared to other tasks. Additionally, we suggest that Sections 3 and 4 include diagrams to enhance understanding of the model training process and data representations. To strengthen the paper, we encourage the authors to conduct further analyses on the generalizability of their results to more complex LTR models and provide initial experiments to support their claims regarding performance deterioration.