ID: m0RbqrUM26
Title: StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 7, 7, 7, 5, -1, -1
Original Confidences: 5, 4, 4, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents StyleTTS 2, a GAN-based text-to-speech (TTS) model that improves upon its predecessor, StyleTTS, by incorporating a diffusion-based style encoder and directly producing waveforms instead of spectrograms. The authors propose several innovations, including a differentiable duration prediction module and the use of pretrained speech language models (SLMs) as discriminators, which enhance the quality of synthesized speech. The paper claims that StyleTTS 2 sets new standards for TTS synthesis, surpassing existing models.

### Strengths and Weaknesses
Strengths:
1. The proposed method is sound, with audio samples demonstrating clear improvements over multiple baselines.
2. The paper includes thorough evaluations, including ablation studies, which effectively support the claims made.
3. Several interesting modules are introduced, such as the differentiable duration modeling method and the WavLM adversarial module.

Weaknesses:
1. The model's complexity raises concerns, as it involves multiple training stages, including pre-training and joint training.
2. There are several technical ambiguities, such as the clarity of the evaluation methods and the definitions of certain terms and components.
3. The writing style lacks coherence in some sections, making it challenging to follow the main ideas.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation methods, particularly in Section 5.2, by specifying whether the styles analyzed pertain to prosodic or acoustic styles and clarifying the basis for emotion labels. Additionally, we suggest that the authors address the technical questions raised regarding the end-to-end training process and the coordination of different training data types, particularly in relation to SLM adversarial training. It would also be beneficial to enhance the organization and coherence of the writing to better convey the contributions and innovations of the paper. Finally, we encourage the authors to consider releasing the source code and trained checkpoints to support future research in TTS.