ID: V6891G9dWu
Title: M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents M2Lingual, a fully synthetic multilingual, multi-turn instruction fine-tuning (IFT) dataset aimed at enhancing large language models' performance across various languages and tasks. Key contributions include the creation of a dataset with 182K instruction-response pairs from 70 languages and 19 NLP tasks, demonstrating its effectiveness in fine-tuning LLMs to improve performance on benchmarks such as MT-Bench, XQuAD, and others. The dataset particularly benefits smaller LLMs (1.8B parameters) and includes a detailed analysis of the synthesis steps involved.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel method for generating synthetic multilingual, multi-turn data.
2. Extensive evaluation across multiple benchmarks shows consistent performance improvements.
3. The dataset ensures equal representation of 70 languages, significantly aiding low-resource languages.

Weaknesses:
1. The discussion on low-resource languages lacks depth regarding specific challenges and solutions beyond dataset size.
2. Insufficient implementation details hinder reproducibility of results by other researchers.
3. The comparative analysis with existing datasets could benefit from additional qualitative insights into dataset characteristics.
4. The advantage of M2Lingual over other datasets is limited, with some benchmarks showing performance similar to or worse than existing datasets.

### Suggestions for Improvement
We recommend that the authors improve the depth of discussion regarding the challenges and solutions for low-resource languages. Additionally, providing more detailed implementation steps would facilitate reproducibility. We suggest enhancing the comparative analysis with qualitative assessments of dataset characteristics. Furthermore, we encourage the authors to clarify the reasons behind the varying performance of LLMs fine-tuned on M2Lingual, possibly by investigating the unbalanced distribution of datasets across languages. Lastly, we advise including the number of steps used for fine-tuning on different datasets and elaborating on the "Evol" methodology within the text for clarity.