ID: 8Oukmqfek2
Title: Rethinking Gauss-Newton for learning over-parameterized models
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 3, 4, 7, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical and empirical investigation of the Gauss-Newton (GN) optimization algorithm applied to over-parameterized one-hidden-layer neural networks. The authors derive conditions for convergence and provide a loss convergence guarantee that is faster than that of networks trained with gradient descent (GD). They explore the dynamics of learning and generalization properties in a teacher-student framework, emphasizing the impact of initialization and step size on convergence and feature learning. Additionally, the authors propose a two-stage training algorithm for optimizing a linear layer in a neural network, arguing that random initialization is insufficient for achieving optimality. They clarify that the convergence result relies on the linear layer being pre-trained to meet a gradient threshold, which is influenced by parameters \( R \) and \( C_R \). The authors also address the computational cost and implications of using mini-batch methods.

### Strengths and Weaknesses
Strengths:  
- The theoretical analysis of GN is well-articulated and provides valuable insights into convergence dynamics.  
- The authors provide a clear theoretical framework for GN's convergence in the mean-field limit, which is a significant contribution to the field.  
- The paper is well-structured, with clear documentation of proofs and experimental results, making it accessible to a broader audience.  
- Updated experimental results show consistency with theoretical predictions, particularly regarding the behavior of GN compared to GD.  
- The experimental setup is sound, particularly in the teacher-student context, and the results on feature learning dynamics are informative.  
- The authors provide clarifications that enhance the understanding of their methodology and assumptions, particularly regarding the initialization and training process.

Weaknesses:  
- The contribution is limited, as the application of GN to neural networks is not novel and its advantages over GD are unclear, especially given the higher computational demands of GN.  
- The novelty of the work is questioned, as some reviewers find the conceptual differences from existing literature insufficient.  
- The empirical results are inconsistent, particularly the MNIST experiments, which contradict the main findings regarding learning rates and optimization of the last layer.  
- The experiments lack strong motivation from the theoretical framework, leading to concerns about the theory-empirical gap.  
- There are unresolved issues regarding the practical scalability and computational cost of GN, particularly for larger datasets.  
- The reliance on the parameters \( R \) and \( C_R \) for the threshold in the linear layer pre-training raises practical concerns about the applicability of the proposed method.  
- The overall contribution may be perceived as limited, particularly if the main focus is on proofs rather than broader implications for larger machine learning workloads.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contribution by explicitly comparing the global convergence rates of GN and GD in the main text. Additionally, we suggest that the authors address the discrepancies observed in the MNIST experiments, potentially by refining the experimental design or providing a thorough discussion of the results. It would be beneficial to clarify the stopping criterion for GN and ensure it is justified within the context of the experiments. We also recommend that the authors improve the MNIST experiments by re-running them with a larger number of epochs to validate the expected behavior regarding learning rates. Furthermore, we encourage the authors to explore connections to other mean-field regimes and to provide a more comprehensive discussion on the practical implications of their findings, particularly regarding the parameter-dependent damping constant. Clarifying the implications of mini-batch methods on the effectiveness of GN and providing more extensive experiments on larger datasets would strengthen the paper. Lastly, we recommend that the authors improve the clarity regarding the practical implications of the parameters \( R \) and \( C_R \) in the context of their training algorithm and emphasize the broader significance of their findings beyond the proofs, particularly in relation to scalability and real-world applications of their method.