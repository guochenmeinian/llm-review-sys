ID: TPeAmxwPK2
Title: Parameter and Computation Efficient Transfer Learning for Vision-Language Pre-trained Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 4, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called dynamic architecture skipping (DAS) aimed at achieving parameter and computation efficiency for transfer learning in vision-language pre-trained models (VLP). DAS seeks to optimize the routing of VLP models by dropping certain transformer layers during inference, thus reducing computational complexity while maintaining performance. The authors introduce a new problem setting termed parameter and computation efficient transfer learning (PCETL), which emphasizes the need for efficient transfer learning methodologies in the context of large-scale pre-trained models. However, the necessity of DAS is questioned due to a lack of thorough analysis and comparison with existing pruning methods. Reviewers express confusion regarding the dynamic nature of DAS, noting that the parameter $\theta_K$ remains unchanged during inference, and there is a need for clarification on the usage of the validation set in experiments, particularly whether PEFT methods utilize it.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel approach to reduce computational complexity in VLP models, providing a strong motivation for the proposed methodology.
2. The writing is clear and effectively communicates the ideas.
3. The experimental results demonstrate the potential of DAS across various benchmarks, with solid analysis and ablation studies.
4. The authors have made efforts to address previous concerns and provide new results, which have positively influenced the reviewer's rating.

Weaknesses:
1. The authors do not clearly present the advantages and disadvantages of the proposed methods.
2. The training cost and benefits of DAS are not sufficiently detailed, particularly regarding running time and GPU memory usage.
3. The paper lacks experiments on diverse tasks beyond classification, such as image captioning, raising questions about the applicability of the method.
4. There is insufficient comparison with existing dynamic architecture skipping methods and other pruning techniques, which limits the evaluation of DAS's necessity and effectiveness.
5. The dynamic aspect of DAS is unclear, and the validation set's role in the experiments is insufficiently explained.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the training cost and benefits of DAS, including specific metrics such as running time and GPU memory usage. Additionally, we suggest expanding the experimental scope to include tasks like image captioning to validate the method's versatility. The authors should also provide retrieval results on larger datasets like MSCOCO and compare DAS with established pruning methods to justify its necessity. Furthermore, clarification on why DAS is considered dynamic despite the unchanged parameter $\theta_K$ during inference is needed. Finally, we suggest providing more details on the usage of the validation set in the experiments, specifically regarding its application in PEFT methods.