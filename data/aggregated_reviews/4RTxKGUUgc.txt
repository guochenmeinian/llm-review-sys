ID: 4RTxKGUUgc
Title: Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay
Conference: NeurIPS
Year: 2024
Number of Reviews: 39
Original Ratings: 5, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel evaluation approach, ChildPlay, utilizing a suite of non-language-based games—Tic-Tac-Toe, Connect Four, Battleship, LEGO Connect Language (LCL), and a shapes game—to assess the reasoning, strategic capabilities, and spatial reasoning of large language models (LLMs) like GPT-3.5 and GPT-4. The authors argue that these games minimize dataset contamination and provide structured environments for evaluating cognitive abilities. The study explores the emergent cognitive capabilities of LLMs through structured game-based tasks, focusing on zero-shot learning. Findings indicate that despite proficiency in standard benchmarks, these models exhibit significant limitations in strategic gameplay and spatial reasoning. The authors analyze the effects of temperature settings on model performance, noting that while higher temperatures can increase randomness, they do not uniformly enhance move exploration. They clarify the differences in game mechanics between Tic-Tac-Toe and Connect Four and emphasize the importance of understanding how LLMs internalize and apply rules through gameplay.

### Strengths and Weaknesses
Strengths:
- The motivation for using non-language-based tasks to evaluate LLMs is compelling and innovative.
- The introduction of new tasks, such as LCL and the shapes game, challenges LLMs to demonstrate spatial reasoning and pattern recognition.
- The paper effectively probes the reasoning capabilities of LLMs in novel contexts, providing valuable insights into their problem-solving abilities.
- The authors have made significant revisions to improve clarity and detail in the experimental setup and results presentation.
- The inclusion of error bars enhances the interpretation of experimental results, addressing previous lapses in statistical significance.

Weaknesses:
- The selection of games may be limited, raising questions about whether LLMs learn tasks according to specified rules.
- The reliance on only GPT-3.5 and GPT-4 may limit the generalizability of findings, despite the authors' belief in the sufficiency of these benchmarks.
- Performance can vary significantly with different temperature settings, complicating the interpretation of results.
- The indirect method of assessing LLM understanding raises questions about the models' true grasp of the rules, which the authors acknowledge but do not fully resolve.
- Encoding games in ASCII may restrict task complexity, potentially limiting insights into model capabilities.
- The potential for overfitting on new benchmarks remains a concern, as zero-shot evaluations cannot completely prevent this issue.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing more detailed captions for figures, particularly in Figure 4, to explain the represented data. Additionally, ensure that the number of runs used for experiments, especially in Figure 5, is clearly stated to enhance reproducibility. We suggest expanding the range of models tested beyond GPT-3.5 and GPT-4 to strengthen the findings and provide a more comprehensive analysis of LLM capabilities. Investigating whether LLMs' poor performance is due to playing against random players and testing against stronger opponents like minimax could yield more robust insights. We also encourage the authors to explore the effects of prompts or pre-prompts on model performance and consider few-shot learning scenarios. Lastly, we advise incorporating a clearer explanation of how the models are prompted with game rules to ensure readers understand the basis for evaluating LLM understanding.