ID: Llu9nJal7b
Title: MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 8, 8, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MaskLLM, a method for introducing learnable semi-structured sparsity (N:M mask patterns) in Large Language Models (LLMs). The authors propose modeling N:M masks with a categorical distribution optimized through Gumbel softmax, demonstrating that end-to-end learning of mask parameters yields more effective sparsity patterns than one-shot methods. The approach supports transfer learning for downstream tasks, allowing for the learning of lossless masks for deployment. Extensive evaluations indicate that MaskLLM achieves state-of-the-art performance compared to other sparse LLM methods.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important problem with a novel solution, showcasing practical applications for LLMs.
- Results indicate that the learnable mask method outperforms state-of-the-art approaches while keeping LLM weights frozen, suggesting significant potential for N:M sparsity improvements.
- The methodology is well-structured, with clear explanations and thorough evaluations across various models and datasets.

Weaknesses:
- Concerns arise regarding the reliance on non-public datasets and LLMs, which may hinder reproducibility; more results with publicly available data are recommended.
- The computational cost of the proposed technique compared to simpler sparse pretraining/finetuning methods remains unclear.
- The best results depend on SparseGPT as a prior for mask initialization, raising questions about the method's performance without such priors.

### Suggestions for Improvement
We recommend that the authors improve the related work section by incorporating discussions on learning sparsity, including mask learning with STE or STR. Additionally, clarifying the actual storage costs and data formats used during fine-tuning would enhance the paper's transparency. It would also be beneficial to provide more results using publicly available datasets and open-source models, such as LLaMA-3, to bolster reproducibility. Finally, addressing the hyper-parameter tuning concerns and clarifying the discrepancies in dataset usage would strengthen the paper's overall rigor.