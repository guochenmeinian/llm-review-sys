ID: vIGNYQ4Alv
Title: Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 8, 6, 5, 7, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a first-order optimization method for convex optimization that leverages recent advances in quasi-Newton methods, achieving a convergence rate that matches the optimal rate ($1/k^2$) when $k = \Omega(d)$ and improves upon it when $k >> d$. The authors propose a quasi-Newton method with a backtracking line search and a projection-free online algorithm to approximate the Hessian of the objective. The algorithm demonstrates a global convergence rate of $\mathcal{O}\bigl(\min\{\frac{1}{k^2}, \frac{\sqrt{d\log k}}{k^{2.5}}\}\bigr)$, particularly outperforming Nesterov's accelerated gradient (NAG) under certain conditions.

### Strengths and Weaknesses
Strengths:
- The proposed method improves upon the best-known convergence rates, particularly outpacing NAG when the number of iterations exceeds the dimension.
- The authors effectively present the main steps of their results, making the technical contributions accessible despite the complexity.

Weaknesses:
- The experimental validation does not adequately support the theoretical claims, particularly regarding the convergence rates observed.
- The paper lacks detailed descriptions of the online method and its subroutines, and the presentation of the regret bound is insufficient.
- The total computational complexity of the proposed method may be higher than that of NAG due to the additional Hessian-vector products required.

### Suggestions for Improvement
We recommend that the authors improve the experimental section to better align with the theoretical claims, possibly by using the log-sum exp loss instead of strictly convex losses like logistic loss. Additionally, including a plot with time on the x-axis could clarify the computational bottlenecks. The authors should provide a more intuitive description of the online method and explicitly state the main result regarding the regret bound. It would also be beneficial to address the absence of the probability $p$ in Theorem 1's bound. Furthermore, we suggest that the authors explore the implications of using standard projection-free no-regret methods and consider discussing the potential for modifications under strong convexity assumptions. Lastly, correcting the typographical error "subourtine" to "subroutine" is necessary.