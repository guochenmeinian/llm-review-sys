ID: IWWWulAX7g
Title: Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 4, 6, 7, 5, 5, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Fine-grained Late-interaction Multi-modal Retrieval (FLMR) approach aimed at enhancing knowledge retrieval in retrieval-augmented visual question answering (RA-VQA). The authors propose that traditional methods suffer from incomplete image understanding and lossy compression of visual scenes and questions. By leveraging multi-dimensional embeddings and late interaction, the method captures fine-grained relevance, achieving state-of-the-art performance on the OK-VQA dataset.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- Extensive experiments demonstrate the effectiveness of the proposed method, showing improved retrieval performance.
- The introduction of fine-grained visual representations significantly enhances the model's capabilities.

Weaknesses:
- The novelty of the approach is somewhat trivial, as multi-modal late interaction has been previously proposed by ColBERT.
- The paper lacks clarity on the main contributions compared to existing methods like DPR.
- The ablation study results in Table 1 are confusing, particularly regarding the performance of ROI&VE in different settings.
- The authors do not provide sufficient justification for the choice of benchmark datasets, and the reliance on a single dataset raises concerns about generalizability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by explicitly comparing FLMR with existing methods like DPR and ColBERT in a side-by-side format. Additionally, we suggest including a performance curve in Table 1 that illustrates the relationship between the number of knowledge sentences and VQA scores. It would also be beneficial to address the confusion surrounding the results of ROI&VE and clarify the rationale behind using max in Equations 3 and 5. Finally, we encourage the authors to explore additional datasets, such as A-OKVQA, to validate the generalizability of their findings.