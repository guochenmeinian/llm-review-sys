ID: gLfgyIWiWW
Title: Labeling Neural Representations with Inverse Recognition
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 5, 5, 3, 5, -1, -1, -1
Original Confidences: 3, 4, 5, 2, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new explainability method called INVERT, which links learned representations to human concepts without relying on segmentation masks. INVERT is computationally efficient and generalizable across various neuron types, demonstrating its applicability in identifying neurons influenced by spurious correlations and fine-tuning representations. The authors also introduce an interpretable metric for assessing the alignment between representations and explanations, enhancing understanding of model decision-making structures.

### Strengths and Weaknesses
Strengths:
- INVERT is generalizable across architectures, not limited to convolutional neurons.
- The method is efficient, requiring less computational time and resources.
- It does not necessitate additional annotations, such as segmentation masks.
- The writing quality is strong, with clear equations and comprehensible figures.

Weaknesses:
- There is insufficient evidence demonstrating that INVERT generalizes to other configurations beyond ResNet-18.
- The evaluation is limited to a narrow range of examples, necessitating broader testing across various models and datasets.
- Certain methodological aspects lack clarity, particularly in the appendix, which could be better integrated into the main text.
- The concepts linked to learned representations must be pre-defined or manually selected.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by expanding experiments to include various neuron types and architectures beyond ResNet-18. Additionally, we suggest enhancing the clarity of the methodology by relocating Section C in the Appendix to Section 3 for better coherence. To address the limitations of pre-defined concepts, consider exploring an "open vocabulary" approach for concept selection. Furthermore, we encourage the authors to provide more qualitative examples and quantitative validation of their claims, such as through a Mechanical Turk study, to substantiate their assertions regarding AUROC and IoU metrics. Lastly, including a discussion of the limitations of INVERT in the conclusion would strengthen the paper.