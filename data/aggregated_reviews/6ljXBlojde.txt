ID: 6ljXBlojde
Title: Mask Propagation for Efficient Video Semantic Segmentation
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 5, 6, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a mask propagation method, MPVSS, for video semantic segmentation (VSS). MPVSS employs Mask2Former for mask predictions and queries from key frames, utilizing a motion encoder to extract pixel-wise motion between key frames and adjacent frames. The method generates query-based flow maps and binary mask predictions for adjacent frames, producing semantic segmentations through matrix multiplication. The authors validate their approach with a detailed ablation study and demonstrate competitive performance against state-of-the-art methods on VSPW and Cityscapes.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- The motivation for leveraging motion estimation to enhance VSS efficiency is sound.
- The novel method of propagating key frame queries to motion features is interesting.
- Experiments are well-designed, yielding convincing results.
- MPVSS captures segment-level information, providing better propagation results than previous pixel-level methods.

Weaknesses:
- Key frames are selected at fixed intervals, potentially not addressing redundancy issues.
- The efficiency of the proposed method requires more detailed discussion, particularly regarding the use of FlowNet for motion feature extraction.
- There are typos, such as "TLOPs" in Fig3 (b) and "wrapped" instead of "warped."
- Insufficient information is provided about training time and strategies used in experiments.
- The proposed mask propagation framework closely resembles existing methods, necessitating further experimentation to validate the effectiveness of the query-based flow.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the efficiency of MPVSS, particularly clarifying the role of FlowNet in motion feature extraction. Additionally, the authors should address the redundancy problem associated with fixed interval key frame selection. We suggest correcting the identified typos and providing more detailed information about training time and strategies. To strengthen the claims regarding the novelty of the query-based flow, the authors should unify the keyframe segmentation network for clearer comparisons with existing methods. Finally, including visualizations of the query-based flow and pixel-based flow separately would enhance understanding of their contributions.