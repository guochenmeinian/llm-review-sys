ID: mrARDvuKi2
Title: 2INER: Instructive and In-Context Learning on Few-Shot Named Entity Recognition
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a text-to-text approach for Named Entity Recognition (NER), extending InstructionNER by introducing an auxiliary task to predict entity types and incorporating in-context learning alongside fine-tuning. The authors demonstrate performance improvements over InstructionNER and other few-shot baselines across three datasets. An ablation study highlights the benefits of both the auxiliary task and in-context learning. Additionally, the authors propose a new method for zero-shot NER using sequence-to-sequence modeling and a task called type extracting, which enhances the model's understanding of semantic relations between entity labels and input sentences.

### Strengths and Weaknesses
Strengths:
- The modifications to InstructionNER lead to substantial performance improvements on two out of three datasets.
- The introduction of the auxiliary task, type extracting, is novel and enhances the model's semantic understanding.
- The paper is well-structured and presents detailed empirical results across multiple datasets.

Weaknesses:
- The modifications to InstructionNER are relatively minor and straightforward, lacking intrinsic novelty.
- The experimental setup is confusing, particularly regarding the naming of 2INER and InstructionNER, and the clarity of in-context examples.
- The contribution of type extraction to overall performance is unclear, as the ablation study does not adequately illustrate its impact.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup by explicitly distinguishing between 2INER and InstructionNER, and clarifying the role of in-context examples. Additionally, the authors should provide more detailed results comparing 2INER and 2INER+TE to better illustrate the contribution of type extraction. We also suggest revising the paper for conciseness and relevance, ensuring that each section directly pertains to the paper's contributions. Lastly, addressing the perceived lack of novelty in the auxiliary task could strengthen the paper's overall impact.