ID: NYlL3oACU2
Title: Comparing Biases and the Impact of Multilingual Training across Multiple Languages
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multilingual study of race, religion, nationality, and gender biases across six languages using monolingual and multilingual encoder models. The authors employ novel metrics for extrinsic bias evaluation with sentiment analysis as a downstream task. The findings reveal how models associate positive sentiment with specific races/nationalities and religions tied to particular languages. The study introduces a new multilingual dataset that could aid future bias research in both monolingual and multilingual contexts. The experiments are extensive, utilizing multiple BERT models pre-trained from scratch to analyze the effects of pre-training in different settings.

### Strengths and Weaknesses
Strengths:
- The study is well-designed, addressing potential issues such as varying training data across languages and classifier performance.
- It provides a comprehensive evaluation and contributes valuable insights into biases in multilingual models.
- The paper is well-written and presents a thorough analysis of the topic.

Weaknesses:
- The interpretation of the adopted metrics is unclear, making it difficult to gauge the extent of bias for specific languages/attributes.
- The presentation of results, particularly in Table 9, lacks clarity, hindering trend identification.
- There is insufficient analysis regarding the amplification of bias during fine-tuning compared to pre-training.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the metrics and results interpretation, particularly in Table 9, to enhance trend visibility. Additionally, we suggest including a discussion on the reasons behind the amplification of bias during fine-tuning. Making the pre-trained models available on HuggingFace would also improve reproducibility. Lastly, we encourage the authors to clarify the concept of multilingual fine-tuning and address the relevance of the evaluation on BLOOM within the paper's context.