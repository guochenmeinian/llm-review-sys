ID: MGPST5I9DO
Title: Learning Efficient Surrogate Dynamic Models with Graph Spline Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GraphSplineNets (GSN), a hybrid model that combines graph neural networks (GNNs) with orthogonal spline collocation methods to enhance prediction accuracy in physical simulations across continuous space and time. The authors propose an adaptive collocation sampling strategy and techniques that reduce computational overhead by eliminating the need to learn interpolators or collocation weights. Experimental results across various PDE settings indicate that GraphSplineNets outperforms state-of-the-art baselines in both accuracy and runtime. Additionally, the paper compares GSN with MultiScale MeshGraphNets (MS-MGN), highlighting differences in message passing and autoregressive rollouts. GSN performs message passing at low resolution and utilizes an Optimal Sampling Criterion (OSC) for high resolution, while MS-MGN operates at both resolutions. The authors provide a detailed overview of their experimental setup, including total rollout times and resolutions for various experiments, and demonstrate a Pareto improvement through the inclusion of Pareto frontier plots.

### Strengths and Weaknesses
Strengths:
- The paper introduces innovative techniques, including a novel loss function, the COLROW algorithm for efficient linear equation solving, and adaptive collocation sampling for prioritized sampling.
- Extensive experiments demonstrate the proposed method's superiority over baselines in terms of prediction quality and speed.
- The clarity of the writing and the quality of the graphics enhance understanding of the complex concepts.
- The paper effectively clarifies the differences between GSN and MS-MGN, enhancing the understanding of their innovations.
- The detailed experimental overview and the inclusion of Pareto frontier plots provide strong empirical support for the claims made.
- The authors' responsiveness to reviewer feedback indicates a commitment to improving the manuscript.

Weaknesses:
- The comparison of GraphSplineNets is limited to space-continuous baselines, making it difficult to assess the benefits of incorporating time-oriented collocation.
- The evaluation lacks large-scale or 3D problems, which are critical for understanding the method's efficiency in practical applications.
- Some sections, such as the explanation of Figure 5.1 and the construction of graphs from snapshots, are unclear or overly detailed, detracting from the overall presentation.
- The manuscript could benefit from a more explicit comparison with MS-MGN in the related work section to further clarify the authors' contributions.
- Some aspects of the experimental setup may require additional clarification to ensure comprehensive understanding.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the explanation regarding the dataset for interpolation points in equation (2) and provide more detail on how collocation points are selected. Additionally, we suggest compressing the detailed descriptions in section 3.4, as they primarily cover standard textbook facts. The authors should also clarify the relationship between the number of processor blocks and the resulting error, and consider evaluating their method on larger-scale problems to demonstrate its practical relevance. Furthermore, we recommend enhancing the related work section by providing a more explicit comparison with MS-MGN, particularly focusing on the differences in coarsening methods and message passing. Lastly, addressing minor typographical errors and ensuring consistent notation throughout the manuscript would enhance its overall quality.