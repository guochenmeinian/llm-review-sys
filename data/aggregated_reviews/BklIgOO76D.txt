ID: BklIgOO76D
Title: How many samples are needed to leverage smoothness?
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the transitory regimes in the generalization error of kernel methods and learning rates in high-dimensional spaces, emphasizing how smoothness can mitigate the curse of dimensionality. The authors prove Theorem 2, which bounds the generalization error, and explore lower bounds on this error using Taylor and Fourier expansions. They argue that classical analyses often overlook exponential constants in the dimension, illustrating that while polynomials of bounded degree and band-limited functions can be learned at a linear rate, the constants involved may be exponential in dimension. The authors investigate both theoretical and empirical implications of their findings, particularly in the low-sample transitory regime, and discuss a kernel setting that allows for sharper bounds along with a meta-algorithm framework for kernel selection. However, concerns arise regarding the optimality of their specific choices, and the Sobolev setting is noted to provide explicit characterizations of kernel eigenfunctions, with results potentially applicable to arbitrary kernels.

### Strengths and Weaknesses
Strengths:
1. The paper addresses significant issues regarding sample complexity and learning rates in high-dimensional settings, filling a notable gap in the literature.
2. The mathematical foundation appears solid, with considerable effort put into the proofs, although some notational inconsistencies exist.
3. The authors incorporate experimental findings to complement theoretical results, demonstrating a strong grasp of the material and a commitment to improving clarity.

Weaknesses:
1. Several important results are left as conjectures, raising questions about the completeness of the analysis.
2. The mathematical presentation suffers from clarity issues, including inconsistent notation and scattered assumptions that complicate understanding.
3. The relationship between sections 2.1 and 2.2 is unclear, particularly regarding the implications of different rates and constants, and the necessity of the meta-algorithm for showcasing arbitrary convergence profiles is questioned.
4. The focus is primarily on RKHSs, limiting the applicability of results to other hypothesis spaces, such as neural networks, which should be more clearly stated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by addressing notational inconsistencies and ensuring that all symbols are defined upon first use. It would be beneficial to provide a clearer connection between the results and their implications, particularly in the discussion of figures and their relevance. Additionally, we suggest that the authors clarify the conjectures presented in the paper, potentially providing proofs or more detailed explanations where possible. We also recommend improving the clarity of the relationship between sections 2.1 and 2.2, particularly how the different rates and assumptions interact. It would be advantageous to compute explicit forms of $\min_\lambda A(\lambda, n)$ in certain cases to strengthen the paper's arguments. Furthermore, we encourage the authors to reevaluate the necessity of the meta-algorithm framework and ensure that the presentation of kernel selection is as clear and straightforward as possible. Lastly, we urge the authors to explicitly state the limitations of their results regarding the applicability to other function classes beyond RKHSs and to refine the language throughout the paper to enhance overall readability for the audience.