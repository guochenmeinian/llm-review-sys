ID: zn5ihqknGj
Title: An Alternating Optimization Method for Bilevel Problems under the Polyak-Łojasiewicz Condition
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 3, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a generalized alternating method for nonconvex bilevel optimization with a PL condition in the lower level. The authors provide convergence analysis for this method using a new stationary metric and demonstrate its efficiency through experimental results. The work reformulates the bilevel optimization problem as a constraint optimization and establishes necessary conditions via calmness, proving improved iteration complexity under mild assumptions. Additionally, the paper explores optimality conditions in bilevel optimization problems, specifically addressing the role of Hessians in establishing necessary conditions for global optimality. The authors propose an algorithm that converges to an $\epsilon$-stationary point, supported by theoretical proofs that demonstrate the tightness and informativeness of their conditions. They clarify the implications of their assumptions regarding eigenvalues and the uniqueness of solutions in various cases, including nonconvex-strongly-convex and nonconvex-Morse-Bott scenarios.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and clearly presents the limitations of previous works while stating its goals.  
- It significantly relaxes assumptions and proves improved iteration complexity for bilevel problems, aligning with optimal complexity in terms of ε.  
- The introduction of a stationary metric for evaluating optimality in non-convex lower-level problems is a timely contribution to the literature.  
- The authors provide a clear and rigorous theoretical foundation for their proposed algorithm, including proofs of convergence and optimality conditions.  
- The clarification of assumptions regarding eigenvalues enhances the understanding of the conditions under which their results hold.

Weaknesses:  
- Some mathematical clarity is lacking, particularly in definitions and figures, which could confuse readers.  
- The algorithm's novelty is questioned, as it closely resembles existing methods for strongly-convex lower-level objectives.  
- The paper primarily addresses deterministic cases, limiting its applicability compared to previous works that handle stochastic settings.  
- Assumption 2 is considered too strict, raising concerns about the implications of the eigenvalue conditions on the algorithm's validity.  
- The applicability of the stationary condition is limited to problems satisfying the PL condition, which may reduce the broader impact of the work.  
- There is a noted lack of practical performance improvements in test accuracy compared to existing methods, which diminishes the excitement surrounding the proposed algorithms.

### Suggestions for Improvement
We recommend that the authors improve the mathematical clarity of statements, particularly in Definition 2 and Figure 2, to ensure rigorous understanding. Additionally, we suggest removing the right plot from Figure 5, as comparing actual convergence rates with theoretical rates may be misleading. Clarifying the convergence of terms, especially for $\mathcal R_w$, in Theorem 2 would enhance the paper's rigor. Furthermore, we encourage the authors to provide more details on the relationship between parameters in Theorem 2 and to clarify the implications of Assumption 2, particularly regarding the eigenvalue conditions and their necessity for the algorithm's performance. We also recommend that the authors improve the practical performance of their proposed algorithms in terms of test accuracy, as this is a critical measure in the machine learning community. Lastly, consider relaxing assumptions regarding eigenvalues to allow for negative eigenvalues along the optimization trajectory, which could enhance the applicability of the results.