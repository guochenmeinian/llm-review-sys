ID: 5ujp72CiYB
Title: Efficient Large Multi-modal Models via Visual Context Compression
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 5, 6, 5, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the redundancy of visual tokens in MLLMs, demonstrating that visual tokens can be compressed significantly without major performance loss. The authors propose the Visual Context Compressor (VCC) using simple average pooling for compression and introduce a Stage-wise MLLM Training strategy that starts with heavy compression and gradually reduces it, achieving a 16% reduction in training costs while enhancing performance compared to the baseline.

### Strengths and Weaknesses
Strengths:
1. The observation regarding the redundancy of visual tokens and the potential for substantial compression is noteworthy and may guide future research.
2. The paper includes comprehensive empirical studies on various compression methods and proposes effective training strategies based on visual token compression.
3. The proposed methods yield improved performance while reducing training costs.

Weaknesses:
1. The paper lacks detailed discussions on why other visual compressors perform worse than average pooling, which could provide valuable insights for designing effective compressors.
2. The study is limited to LLaVA-1.5-7B; demonstrating scalability to larger models like 13B or 34B would strengthen the findings.
3. The reported 16% training efficiency improvement may be marginal in practical scenarios, especially considering the complexity of the four-stage training compared to the baseline.
4. Evaluation is primarily on visual question answering tasks; additional assessments on other multi-modal tasks could enhance the claims.
5. The paper does not analyze potential information loss due to compression in tasks requiring dense visual information.
6. Essential experiments are missing, such as evaluating larger input resolutions and testing on larger models like LLaVA-13B.
7. The best stage-wise training scheme may be challenging to adapt to different model and data settings due to its complexity.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the comparative performance of different visual compressors to provide deeper insights. Additionally, the authors should demonstrate the scalability of their method by testing it on larger models and input resolutions. Expanding the evaluation to include various multi-modal tasks would strengthen the paper's claims. We also suggest providing a detailed analysis of potential information loss due to compression and simplifying the stage-wise training scheme to enhance its applicability across different settings.