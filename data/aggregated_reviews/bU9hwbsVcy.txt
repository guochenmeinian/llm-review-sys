ID: bU9hwbsVcy
Title: The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 8, 6, 4, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive study on "essential sparsity" in large pre-trained vision and language transformers, proposing that a sharp drop in performance occurs after one-shot pruning relative to sparsity levels. The authors validate the existence of essential sparsity across various models, revealing significant findings such as abrupt sparsification during pre-training, the influence of pre-training data on knowledge condensation, and the superior emergent sparsification properties of self-supervised learning compared to supervised learning. Additionally, the authors introduce a novel approach to model pruning, emphasizing a one-shot, magnitude-based, training-free, and data-free method that achieves comparable performance to more complex techniques like LTH within an essential sparsity range. They provide new results for Vicuna-13B on the MMLU benchmark, showing that one-shot pruning performs comparably to SparseGPT without the latter's computational overhead, while also highlighting unexpected findings related to sparsity dynamics during pre-training and the effects of data on model sparsity.

### Strengths and Weaknesses
Strengths:  
- The paper validates the existence of essential sparsity across large pre-trained transformer models, providing valuable insights for the sparsity research community.  
- It effectively demonstrates that simple pruning methods can match the performance of more complex techniques within certain sparsity ranges.  
- The paper includes thorough experiments and new results that contribute to the understanding of model sparsity dynamics, which are of broad interest.  
- The authors express a willingness to clarify and adjust their claims based on reviewer feedback.

Weaknesses:  
- The paper lacks results on hardware-friendly sparsity patterns such as N:M and does not discuss potential GPU runtime benefits from essential sparsity.  
- There are concerns regarding the practicality of the proposed sparsity patterns, with some reviewers questioning the validity of N:M patterns beyond 2:4.  
- The authors' claims of being "data-free" and "training-free" have been criticized as potentially misleading, necessitating clearer definitions.  
- The trade-offs between one-shot pruning and other methods, such as LTH, are not adequately analyzed in the paper.  
- Some conclusions appear over-claimed, particularly regarding the abrupt performance drop at sparsity thresholds, which has been documented in prior literature.  
- The mathematical definition of essential sparsity is unclear, and the paper does not adequately discuss the limitations of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the discussion on hardware-friendly sparsity patterns and their implications for practical applications. Additionally, we suggest that the authors clarify their terminology regarding "data-free" and "training-free" claims, revising these to "inference-free" and "post-optimization free," respectively. The authors should include more baseline comparisons to strengthen their claims regarding essential sparsity's performance relative to other methods, particularly addressing how essential sparsity compares to random pruning and SparseGPT results. A more balanced assessment of the trade-offs between one-shot pruning and other methods, including a clearer analysis of the implications for training time, test-time accuracy, and speed improvements, would enhance the paper's rigor. Finally, clarifying the mathematical definition of essential sparsity and addressing the limitations of their approach would provide valuable insights.