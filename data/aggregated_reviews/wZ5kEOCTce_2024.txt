ID: wZ5kEOCTce
Title: Rethinking Patch Dependence for Masked Autoencoders
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 4, 5, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CrossMAE, a novel pre-training approach that enhances representation learning by utilizing cross-attention in the decoder instead of self-attention, thereby improving the efficiency of masked token reconstruction. The authors argue that MAE achieves coherent image reconstruction through global representations learned in the encoder, rather than through interactions between patches in the decoder. The methodology allows for partial reconstruction of masked patches and incorporates inter-block attention to fuse features across layers.

### Strengths and Weaknesses
Strengths:
- The approach of analyzing the reconstruction process through self-attention and cross-attention is intriguing and well-motivated by practical observations.
- The writing is clear, and the visualizations, particularly Figure 5, effectively illustrate the contributions.
- The paper demonstrates that CrossMAE generally improves upon the vanilla MAE and other similar methods.

Weaknesses:
- The claim that MAE reconstruction relies on global representation learning needs more support, especially given recent studies on the role of mask tokens in capturing local information.
- The absence of self-attention in CrossMAE raises concerns about the validity of observations made on vanilla MAE, necessitating further explanation.
- The paper lacks a structured ablation study to clarify the impact of individual contributions on performance.
- The performance evaluation is limited, with only linear-probing and fine-tuning results reported for IN1K; additional datasets and tasks should be considered for generalizability.
- Limitations of the proposed work are insufficiently discussed, with only a brief mention in the last sentence of section 5.

### Suggestions for Improvement
We recommend that the authors improve the support for their claims regarding the role of global representations in MAE and clarify the implications of removing self-attention in CrossMAE. A comprehensive analysis of the individual contributions' impact on performance and computational complexity should be introduced, ideally in a structured table format. Additionally, we suggest training both models for 1600 epochs to evaluate if performance gains are sustained. Finally, the authors should disclose more detailed limitations of their work to provide a clearer understanding of its scope and applicability.