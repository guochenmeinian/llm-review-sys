ID: NU2kGsA4TT
Title: SceneScape: Text-Driven Consistent Scene Generation
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 4, 3, 8, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for text-driven perpetual view generation that synthesizes long-term changing views of a scene and its corresponding 3D mesh using free-vocabulary text prompts and camera trajectories. The authors leverage pretrained text-to-image and depth prediction models, applying a test-time finetuning scheme to enhance geometric and textural consistency throughout the video sequence. The proposed method demonstrates promising results in generating coherent and visually appealing videos, particularly in indoor settings.

### Strengths and Weaknesses
Strengths:
- The method effectively utilizes pretrained models, allowing for diverse and zero-shot scene generation without extensive training data.
- The generated videos exhibit impressive consistency and geometric plausibility, with users able to influence the output through text prompts.
- The continuous scene generation approach, supported by a unified mesh representation and test-time finetuning, significantly enhances the quality of the results.

Weaknesses:
- The method is primarily limited to backward camera movement, which restricts the exploration of scenes and may lead to unrealistic structures, such as infinite hallways.
- The experiments lack systematic rigor, with a small number of prompts and videos, making it difficult to draw robust conclusions.
- There is insufficient analysis of cumulative errors and the potential for distortion in generated scenes, particularly regarding dynamic content and complex trajectories.

### Suggestions for Improvement
We recommend that the authors improve the diversity of camera trajectories in their experiments to include side-to-side movement and more complex scenarios, which would better test the method's robustness. Additionally, we suggest providing a more systematic evaluation of prompts, potentially through programmatic generation, to enhance the consistency and representativeness of the results. The authors should also address the analysis of cumulative errors and consider incorporating instance-level priors to rectify shape distortions in generated scenes. Lastly, clarifying the necessity of decoder finetuning and the initialization of the mesh representation would strengthen the paper's technical foundation.