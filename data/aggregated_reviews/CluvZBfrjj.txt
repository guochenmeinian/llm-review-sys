ID: CluvZBfrjj
Title: From Instance Training to Instruction Learning: Task Adapters Generation from Instructions
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 4, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Task Adapters Generation from Instructions (TAGI), which aims to enhance cross-task generalization in large language models (LLMs) by converting task instructions into task-specific adapters using a hypernetwork. The authors propose a two-stage training process involving hypernetwork pretraining and fine-tuning, demonstrating that TAGI can match or outperform traditional instruction fine-tuning methods while significantly reducing computational requirements. Experimental results on datasets like Super-Natural Instructions and P3 validate the effectiveness of TAGI.

### Strengths and Weaknesses
Strengths:
- The novel approach of "learning with instructions" represents a significant shift from instance-based training to instruction-based learning, enhancing adaptability.
- TAGI effectively reframes cross-task generalization as a parameter generation problem, addressing key limitations in existing methods.
- Comprehensive evaluations and ablation studies strengthen the validity of the results, showcasing TAGI's performance against various baselines.

Weaknesses:
- The exploration of model sizes is limited, primarily focusing on models up to 3B parameters, leaving questions about scalability to larger models.
- The methodology is restricted to encoder-decoder models, which may limit the generalizability of findings.
- There is insufficient analysis of how instruction quality impacts performance, and hyperparameter sensitivity is not thoroughly explored.

### Suggestions for Improvement
We recommend that the authors improve the exploration of larger model sizes to assess TAGI's scalability. Additionally, expanding the analysis beyond encoder-decoder models would enhance the generalizability of the findings. A more detailed discussion on the impact of instruction quality on performance is necessary, along with a comprehensive examination of hyperparameter sensitivity, including:
- Exploring the effects of different LoRA ranks beyond the chosen value of 32.
- Analyzing how the size of the hypernetwork affects performance and efficiency.
- Investigating the trade-offs between pretraining and fine-tuning steps.