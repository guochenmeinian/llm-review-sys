ID: rXn9WO4M2p
Title: Self-Influence Guided Data Reweighting for Language Model Pre-training
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to data reweighting during pre-training using self-influence (SI) scores to indicate sample importance. The authors propose PRESENCE, which utilizes the gradient norm for reweighting gradients during backpropagation. They demonstrate that this method improves the performance of the mT5 model on downstream cross-lingual transfer tasks compared to models not pre-trained with PRESENCE. 

### Strengths and Weaknesses
Strengths:
- The use of gradient norm/self-influence for sample weighting during pre-training is a novel contribution to NLP.
- The experimental study is comprehensive, showing promising results.
- The paper is well-written and easy to follow.

Weaknesses:
- There is a lack of comparison with other pre-training data filtering methods, limiting the validation of PRESENCE's effectiveness.
- The method incurs significant pre-training computation overhead, which may hinder real-world applicability.
- Experimental results do not convincingly demonstrate the advantages of PRESENCE, particularly regarding the relationship between temperature $\tau$ and downstream performance.
- The choice of multilingual transferability as the evaluation task raises questions about the method's generalizability beyond this context.

### Suggestions for Improvement
We recommend that the authors improve their comparison with existing pre-training data filtering methods to substantiate the advantages of PRESENCE. Additionally, addressing the significant pre-training computation overhead with empirical justifications would enhance the paper's applicability. We suggest expanding the experimental evaluation to include other language models beyond T5 and incorporating a broader range of tasks beyond multilingual evaluations. Finally, clarifying the two-stage reweighting process earlier in the paper and providing the source code for reproducibility would strengthen the overall contribution.