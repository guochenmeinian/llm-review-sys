ID: TKXMPtCniG
Title: Accelerating Large Batch Training via Gradient Signal to Noise Ratio (GSNR)
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 5, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 2, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a variance-reduced gradient descent technique (VRGD) aimed at enhancing the training of deep neural networks with large batch sizes. The authors focus on small to medium model sizes (10M to 300M parameters) and datasets (up to 1M images or ~3B words) while achieving faster convergence and improved generalization compared to traditional methods. The proposed method leverages the gradient signal-to-noise ratio (GSNR) to adaptively scale learning rates, demonstrating effectiveness through experiments on benchmarks like ResNet and BERT. Additionally, the authors analyze the generalization gap in large-batch training, specifically comparing their method to LARS, and report a 65.7% reduction in the BERT pretraining scenario. However, they observe a performance drop when scaling the batch size to 32k, raising questions about the effectiveness of their hyper-parameter selection strategy.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a significant problem in neural network training, particularly for large batch scenarios.
2. The proposed method is straightforward and shows promising empirical results across various tasks, including significant generalization gap reduction in transformer models like BERT.
3. The theoretical analysis of convergence and generalization is compelling, and the authors provide training time results that enhance the paper's robustness.

Weaknesses:
1. The contribution is limited, as the importance of GSNR in large batch training is not sufficiently justified.
2. The method lacks rigorous motivation and analysis, raising questions about its effectiveness compared to existing techniques like SAM.
3. The convergence analysis metrics used are questionable, focusing on mid-training performance rather than optimal solutions.
4. There is a notable performance drop in the proposed method when increasing the batch size to 32k, which is not observed with LARS.
5. The visualization of the loss landscape is unclear and lacks detail, which could hinder understanding.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind using GSNR in large batch training, providing a more comprehensive rationale for its relevance. Additionally, we suggest including comparisons with flat-minima-based methods such as SAM to substantiate claims regarding generalization improvements. Furthermore, the authors should provide detailed visualizations and analyses of training times and convergence rates, specifically addressing wall-clock time metrics and the implications of batch size on performance. We also recommend improving the clarity of the loss landscape visualization, potentially by including a figure similar to that in the referenced paper. Additionally, we suggest that the authors provide a thorough analysis of the generalization gap in transformer models, such as vision transformers or other NLP models. Finally, we encourage the authors to address the performance drop observed at a batch size of 32k by refining their hyper-parameter tuning strategy, particularly the learning rate, to ensure optimal performance across varying batch sizes.