ID: JkqrZqBO7d
Title: PETRA: Parallel End-to-end Training with Reversible Architectures
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 3, 3, 5, 3, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method that integrates reversible neural networks with parallel distributed training to minimize memory usage while incurring slight communication and computation overhead. The authors propose a novel approach that eliminates the need for storing intermediate activations, thereby enhancing parallelism. However, the experiments are limited to a single A100 GPU and do not adequately demonstrate the performance benefits in a real distributed environment.

### Strengths and Weaknesses
Strengths:
- The adaptation of reversible architectures to pipelined training is promising and could become a popular method if validated.
- The innovative use of reconstructed inputs instead of stored weights is a novel contribution.
- The problem setup is intriguing, addressing high memory consumption effectively.
- The paper is well-written, with clear figures and tables that enhance understanding.

Weaknesses:
- The experimental size is insufficient, with only three ResNet variants tested, particularly concerning the largest model, ResNet50.
- There is no empirical data on speedup or training time, which is crucial given the increased communication overhead.
- The final accuracy drops significantly across datasets, raising concerns about the method's applicability to larger models.
- The analysis lacks proof or detailed reasoning on why the proposed scheme would converge or be a good approximation.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by including a broader range of models, especially larger ones like ResNet152 or ViTs, to assess the method's scalability. Additionally, it is essential to provide empirical data on training time and speedup in a real distributed environment to substantiate claims of computational benefits. We also suggest including a detailed analysis of convergence and approximation errors to strengthen the theoretical foundation of the proposed approach. Finally, consider extending the method to non-reversible architectures to compare performance comprehensively.