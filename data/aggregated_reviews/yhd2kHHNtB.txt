ID: yhd2kHHNtB
Title: Avoiding Undesired Future with Minimal Cost in Non-Stationary Environments
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 7, 7, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the Avoiding Undesired Future (AUF) problem in non-stationary environments, introducing an optimization problem aimed at minimizing action costs. The authors formulate this as a convex quadratically constrained quadratic program (QCQP) and propose a rehearsal-based algorithm, AUF-MICNS, which is supported by theoretical guarantees and empirical validations. The work also addresses decision-making in scenarios with insufficient interactions, leveraging observed variable structures to enhance decision-making in dynamic environments.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clearly articulates its motivation and novelty.
- It provides a comprehensive theoretical analysis and algorithmic design, presenting a more general and practical problem formalization than existing methods.
- The proposed algorithm demonstrates effectiveness and efficiency through empirical results.

Weaknesses:
- The algorithms lack a regret bound or other theoretical guarantees on the cost, which is crucial given the aim to minimize costs in avoiding undesired futures.
- The exploration-exploitation tradeoff is not adequately addressed, particularly concerning the implications of uninformative alterations.
- Some discussions on offline reinforcement learning and the role of $\tau$ in the algorithm are missing, and the writing contains awkward phrasing that could be improved.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework by establishing a regret bound for the cost and addressing the challenges involved if such a bound cannot be established. Additionally, we suggest that the authors clarify how their algorithms handle the exploration-exploitation tradeoff and provide more discussion on the implications of offline reinforcement learning in their context. Furthermore, we encourage a careful revision of the writing to enhance clarity and coherence, particularly in sections where awkward phrasing occurs.