ID: 3JP1Jsng4G
Title: mReFinED: An Efficient End-to-End Multilingual Entity Linking System
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the first technique for creating an end-to-end multilingual entity linker (MEL), mReFinED, based on the monolingual version ReFinED. The authors propose a bootstrapped dataset to annotate unlabeled mentions in Wikipedia, allowing for evaluation on two datasets, Mewsli-9 and TR2016^hard, where the end-to-end model outperforms two-stage models in recall across most languages. The paper also introduces a novel bootstrapping mention detection framework to enhance training data quality, demonstrating a 44-fold efficiency improvement.

### Strengths and Weaknesses
Strengths:  
- The paper pioneers an end-to-end MEL system that generally outperforms two-stage approaches in recall.  
- The methodology is clearly described, allowing for reproducibility, particularly in the bootstrapping process.  
- Empirical results indicate significant performance improvements over previous models like SpaCy, XTREME, and WikiNEuRal.  

Weaknesses:  
- The analysis lacks consideration of F1 or precision metrics, raising concerns about the usability of the mReFinED model.  
- The focus on dataset analysis rather than algorithm analysis in Section 3.4 detracts from understanding the algorithm's performance.  
- There is no comparative analysis with mainstream large language models, limiting the assessment of the proposed methods.  
- The term "end-to-end" is misleading, as prior works also identify mentions, which questions the novelty of the claim.

### Suggestions for Improvement
We recommend that the authors improve the analysis by including F1 and precision metrics to better assess the usability of the mReFinED model. Additionally, we suggest relocating the dataset analysis to the appendix and incorporating a qualitative analysis of the algorithm in Section 3.4. To strengthen the evaluation, we advise conducting comparative tests with mainstream large language models and clarifying the use of the term "end-to-end" to accurately reflect the contributions of their approach.