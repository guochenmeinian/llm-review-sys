ID: RADrFxYqIH
Title: How hard are computer vision datasets? Calibrating dataset difficulty to viewing time
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 7, 7, 5, 10, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 5, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel metric called Minimum Viewing Time (MVT) to quantify the difficulty of object detection tasks in vision datasets. The authors compute MVT by measuring the time required for human annotators to identify objects in images, specifically analyzing large datasets like ImageNet and ObjectNet. They demonstrate that these datasets are skewed towards easier images and highlight a significant performance gap between human and model recognition, particularly for harder images. The authors propose that MVT can be utilized to create more balanced datasets by providing feedback during the data collection process. They acknowledge the limitations of their current analysis, particularly regarding the applicability of MVT to datasets beyond ImageNet and ObjectNet, and the need for broader dataset evaluations. The authors also recognize the importance of understanding the factors contributing to image difficulty and plan to investigate this further.

### Strengths and Weaknesses
Strengths:
1. The MVT metric is a novel and intuitive approach for assessing image classification difficulty.
2. The experimental design is efficient and logical, effectively addressing a relevant problem in the field.
3. The analysis of widely used datasets, ImageNet and ObjectNet, is significant and can influence future research.
4. The authors demonstrate awareness of the limitations of their methodology and express a commitment to addressing these in future work.
5. The inclusion of a toolkit for MVT collection simplifies the process for dataset creators.

Weaknesses:
1. The applicability of the MVT metric to other datasets is limited due to the challenges of using crowdsource workers.
2. The gap in viewing times (250 ms to 10 seconds) is not well-explained, raising concerns about its implications.
3. The paper lacks sufficient citations of relevant literature on difficulty estimation, human judgment in vision tasks, and recent works on ImageNet evaluation.
4. The dual experimental setups create confusion regarding participant selection for MVT determination and human performance metrics.
5. The interpretation of MVT results and the factors contributing to image difficulty are not fully explored, and figures lack clarity, making it difficult to extract key insights.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of the MVT metric by exploring alternative platforms for dataset creators to compute it. Additionally, we suggest providing a clearer explanation for the large gap in viewing times and including a broader range of difficulty estimation methods in their evaluation. It would also be beneficial to analyze the factors contributing to image difficulty and clarify the sampling process for ImageNet and ObjectNet images to ensure no bias in difficulty assessment. We encourage the authors to enhance the clarity of figures, particularly Figures 5, 6, and 7, to facilitate better understanding of the results presented. Furthermore, we recommend that the authors evaluate a wider range of datasets beyond ImageNet and ObjectNet to strengthen their claims about MVT applicability. Lastly, we advise the authors to include a formal data card for their annotations, outlining maintenance plans and use cases, and to clarify the relationship between MVT and the performance of different model architectures, particularly in terms of recurrent versus feed-forward models.