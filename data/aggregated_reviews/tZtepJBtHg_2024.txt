ID: tZtepJBtHg
Title: Transductive Active Learning: Theory and Applications
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 5, 4, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on active learning with Gaussian Processes in a transductive setting, focusing on optimizing model performance on a target space \( A \) while sampling from a sample space \( S \). The authors propose algorithms that sequentially select examples to minimize variations of "posterior uncertainty" on \( A \), providing theoretical guarantees and empirical evaluations in scenarios like fine-tuning and safe Bayesian optimization. Additionally, the paper introduces new theoretical results addressing extrapolation to points outside the sample set \( \mathcal{S} \) and directed learning where \( \mathcal{A} \subseteq \mathcal{S} \). The authors claim that their findings lead to a new state-of-the-art algorithm for Safe Bayesian Optimization (BO) and demonstrate significant improvements over existing methods in both Safe BO and fine-tuning applications. The paper also emphasizes the importance of capturing correlations and estimating uncertainty effectively, contributing novel generalization bounds for active learning.

### Strengths and Weaknesses
Strengths:
- The paper addresses a relevant setup in active learning and is mostly well-written.
- The proposed methods are theoretically sound, supported by empirical evaluations.
- It contributes novel generalization bounds and demonstrates compelling empirical results.
- The authors provide a comprehensive analysis of the dependence on initial variance and the linear scaling with respect to \( |\mathcal{S}| \), which is standard in kernelized bandit analysis.
- The paper enhances its relevance to the machine learning community by contributing new results in multiple settings of transductive active learning.

Weaknesses:
- Concerns about novelty and significance arise, as minimizing posterior uncertainty is a well-known strategy in active learning.
- The theoretical guarantees may not be sufficiently nontrivial, lacking clarity on challenges, tightness of bounds, and comparisons with baseline strategies.
- The empirical evaluation relies on well-studied datasets like MNIST and CIFAR100, which may not showcase sufficient novelty.
- Concerns were raised regarding the standardization of the notion of "constant" \( C \), particularly its linear dependence on \( |\mathcal{S}| \) and the maximum initial variance.
- The transition between different learning rates lacks clarity, and the authors acknowledge the potential for tighter results in specific cases.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contributions by providing clearer distinctions between their work and existing methods in active learning. Additionally, enhancing the theoretical section with more detailed explanations, discussions, and comparisons to existing literature would strengthen the paper. Specifically, clarifying the challenges in obtaining theoretical results, the implications of the bounds, and how they compare to other sample strategies would be beneficial. We also suggest that the authors improve clarity by providing specific examples to explain the notion of "constant" \( C \) and its implications. Furthermore, enhancing the transition explanation between different learning rates to better illustrate the differences would be advantageous. It would also be beneficial to explicitly state the dependence on \( |\mathcal{S}| \) in the main theorem statement if deemed important. Finally, we encourage the authors to clarify the relationship between their methods and the work cited in [1] to avoid any misinterpretation regarding extensions of their approach.