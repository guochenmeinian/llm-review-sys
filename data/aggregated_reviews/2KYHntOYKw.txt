ID: 2KYHntOYKw
Title: Cooperative Learning for Cost-Adaptive Inference
Conference: NeurIPS
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: 3, 4, 1, 3, 4

Aggregated Review:
### Key Points
This paper presents a cooperative training framework that enables neural networks to adaptively change their size to meet varying resource constraints during inference. The approach involves two subnetworks learning from a true model (self-learning) while communicating soft labels (interactive learning) and a leader network guiding the process (guided learning). The architecture-agnostic nature of the method allows it to enhance various pre-trained models. Results on CIFAR and Tiny ImageNet indicate that the sub-networks perform comparably to the full network, with predictable accuracy drops as size decreases.

### Strengths and Weaknesses
Strengths:  
- The motivation for the framework is well-articulated and relevant, with potential applications in complex domains like medical data annotation.  
- The paper is well-written, with a clear presentation of technical aspects and methodologies.  
- The cooperative learning framework is novel and flexible, allowing for various model sizes.

Weaknesses:  
- The accuracy of sub-networks does not clearly outperform existing methods like BranchyNet and BYOT.  
- The empirical evaluation lacks thoroughness, particularly in explaining the necessity of multiple subnetworks and the results of ablation studies.  
- Clarity issues exist in the presentation, particularly in Section 4, where definitions and explanations of key components are missing or unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing and presentation, particularly in Section 4, by providing clear definitions for terms such as the global mask $B$ and the parameters in equations (9) and (13). Additionally, we suggest that the authors conduct more comprehensive empirical evaluations, including ablation studies that clarify the necessity of the two teammate networks and address the discrepancies in accuracy trends compared to baselines. Furthermore, a qualitative explanation for the observed balance of losses during the interaction learning phase would enhance the understanding of the framework's dynamics.