ID: PaSpImjKm2
Title: Performance Bounds for Policy-Based Average Reward Reinforcement Learning Algorithms
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 7, 3, 4, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 1, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents performance bounds for approximate policy iteration algorithms in the average reward setting, addressing the challenges of deriving such bounds compared to discounted reward settings. The authors propose a novel approach utilizing the Schweitzer transformation to achieve finite-time error bounds for average reward approximate policy iteration and extend these results to various reinforcement learning (RL) methods. Additionally, the paper provides a theoretical framework for analyzing policy-based methods in reinforcement learning, distinguishing between stochastic approximation theory and dynamic programming/optimization perspectives. The authors argue that vanilla policy gradient methods do not generally converge, while variants rooted in mirror descent do. They provide a finite-time error bound on expected performance, asserting that their results are novel within the context of bounded function approximation errors.

### Strengths and Weaknesses
Strengths:
- The technical claims are sound, and the motivation for the need for bounds in the average setting is well articulated.
- The paper is well-written, providing a clear explanation of the challenges in analyzing performance bounds for policy iteration.
- The authors achieve finite-time error bounds for average reward approximate policy iteration and present a refined analysis of time-varying policy evaluation and improvement errors.
- The authors effectively clarify the distinction between different analytical approaches in reinforcement learning and provide a comprehensive theoretical framework with novel convergence results under specific assumptions.

Weaknesses:
- There is a lack of discussion on existing bounds related to average reward problems, particularly in comparison to actor-critic algorithms.
- The paper does not include experimental validation, which would enhance its contributions.
- The analysis of asymptotic convergence is insufficiently comprehensive, and assumptions made regarding the Markov chain structure may not hold universally.
- The algorithm's implementation details are unclear, particularly regarding updates and the handling of contractions in the average-reward setting.
- Assumption 3(a) is perceived as strong by some reviewers, raising concerns about its implications for stability and convergence.

### Suggestions for Improvement
We recommend that the authors improve the discussion on previous bounds available in the context of average reward problems and provide a thorough comparison with actor-critic schemes. Additionally, including experimental results would significantly enhance the paper's value. We suggest a more comprehensive analysis of asymptotic convergence for the algorithms and clarification of the assumptions regarding the Markov chain structure. Furthermore, the authors should specify the implementation details of the algorithm, particularly the update mechanisms and the role of parameters like \(\epsilon\) and \(\delta\). We also recommend that the authors address concerns regarding the strength of Assumption 3(a) and its implications for stability on sample paths, as well as incorporate a clearer explanation of the challenges in adapting the analysis from [VdW80] for the MDP setting.