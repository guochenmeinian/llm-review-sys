ID: 5MIk4VFn1c
Title: Private Attribute Inference from Images with Vision-Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 4, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a significant investigation into the privacy implications of Visual Language Models (VLMs) by evaluating their ability to predict sensitive personal information from online images. The authors introduce a new dataset comprising images from selected subreddits, eight categories of private attributes, and 554 manually-labeled attributes with assigned "hardness scores." The evaluation of several VLMs shows GPT4-V achieving the highest average accuracy of 77.6%. The authors also explore prompt engineering and automated zooming through prompting.

### Strengths and Weaknesses
Strengths:
1. The paper addresses the crucial and under-explored issue of privacy risks associated with VLMs.
2. The dataset's unique approach, with only 9.7% of annotated attributes depicting humans, allows for a nuanced evaluation of VLMs' inference capabilities.
3. The writing is clear, with well-documented annotation and evaluation procedures.

Weaknesses:
1. The ground truth labels created by annotators may introduce subjectivity and may not reflect real-world truths.
2. The dataset's relatively small size and lack of standard deviation reporting hinder the assessment of result robustness.
3. The paper lacks alignment rates between different annotators and does not evaluate against human observers, limiting its impact.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their findings by increasing the dataset size and reporting standard deviations. Additionally, providing alignment rates between annotators would enhance the credibility of the labeling process. The authors should also consider including a comparison against human observers to strengthen the evaluation. A discussion on the time and financial costs of human annotation versus VLM-based methods would be beneficial. Lastly, evaluating the impact of hyperparameters and discussing potential defenses against the proposed attack would add depth to the analysis.