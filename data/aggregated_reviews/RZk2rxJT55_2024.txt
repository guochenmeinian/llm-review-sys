ID: RZk2rxJT55
Title: Hamiltonian Mechanics of Feature Learning: Bottleneck Structure in Leaky ResNets
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 6, 6, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the "Leaky ResNet" ordinary differential equation, a variant of NeuralODE that includes an additional vector field attracting trajectories to the origin, controlled by a parameter $\tilde{L}$. The authors define the "Cost of Identity" (COI), which couples the ODEs and evolves along their trajectories based on the training dataset. They demonstrate that solutions predominantly occupy regions where the COI approaches an optimal value. The paper also proposes three discretization schemes for the ODE and provides numerical results. Furthermore, it investigates the dynamics of deep neural networks (DNNs) through the lens of Bottleneck structures and their relationship with NeuralODEs, proposing that the dynamics switch between fast and slow regimes corresponding to high and low-dimensional representations. The authors aim to clarify the convergence of discrete neural network architectures to continuous models, particularly in the context of leaky ResNets, and emphasize the importance of understanding the interaction between the number of layers and the Bottleneck structure.

### Strengths and Weaknesses
Strengths:
- The approach of demonstrating neural network properties through a model where trajectories spend time in specific regions is intriguing.
- The authors articulate their intuition, assumptions, and limitations effectively.
- The COI appears to be a novel concept reflecting interesting properties of ODE models for neural networks.
- The mapping to Hamiltonian mechanics provides valuable intuition regarding feature learning dynamics.
- The exploration of the Bottleneck structure and its implications for DNN dynamics is a novel contribution that could enhance understanding of feature learning in deep networks.
- The authors provide rigorous statements and proofs, ensuring that assumptions are clearly stated and do not rely on subjective interpretations.
- The authors plan to include a convergence analysis in the appendix, addressing concerns about the relationship between discrete and continuous models.

Weaknesses:
- The main results are not clearly articulated in the abstract or introduction, and a rigorous statement of the study's goals is lacking.
- The claim regarding the emergence of a bottleneck structure in ResNets is not substantiated by the paper's results.
- There is insufficient justification for the applicability of results to neural networks with a finite number of layers.
- The derivation of key ingredients is inadequately supported, and the paper lacks rigor, with many informal discussions that could benefit from formal statements and proofs.
- Propositions and theorems often lack clear assumptions, complicating the understanding of their applicability.
- The paper mixes formal definitions with informal arguments, hindering readability.
- The use of non-standard notation (e.g., $p$ as a continuous layer index) may hinder comprehension for some readers.
- There is a lack of clarity regarding the convergence of the discretization scheme to the expected continuous-time process, particularly concerning the dependence of parameters on $L$.
- Some reviewers express uncertainty about the practical relevance of the findings to real-world neural networks, particularly regarding the assumptions made in the theoretical framework.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the main results by explicitly stating them in the abstract and introduction. A rigorous justification for the bottleneck structure claim should be provided. Additionally, the authors should clarify the applicability of their results to finite-layer neural networks. Including a short proof or explanation of key derivations in the appendix would enhance the paper's self-containment. We suggest replacing informal discussions with rigorous statements and proofs to bolster the paper's rigor. Clear statements of assumptions for each proposition and theorem are necessary to improve understanding. We also recommend that the authors improve the clarity of the contribution section to better summarize the results and their implications. Providing a formula for a discrete leaky ResNet that converges to the ODE with Bottleneck structure as $L$ approaches infinity would enhance the manuscript's rigor. It would also be beneficial to address the concerns regarding the interaction between the limits of $\tilde{L}$ and $L$ more explicitly, perhaps by including a numerical scheme that captures this relationship. Lastly, we suggest that the authors clarify the notation used throughout the paper to ensure accessibility for a broader audience.