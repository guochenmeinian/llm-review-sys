ID: TYdzj1EvBP
Title: How Do Large Language Models Acquire Factual Knowledge During Pretraining?
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 5, 4, 7, 8, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into how large language models (LLMs) acquire knowledge during pretraining, utilizing a dataset of fictional entities and various probes to assess knowledge retention. The authors find that knowledge acquisition improves with repeated exposure, but models tend to forget this knowledge over time, particularly when fine-tuned with other data. Key findings include: 
- Knowledge retention is influenced by model size and batch size, with larger models and larger batch sizes aiding retention.
- The study identifies a power-law relationship between training steps and forgetting rates, and highlights the challenges LLMs face with long-tail knowledge.
- Additionally, the paper analyzes the overlap between pre-training data (PTD) and Fictional Knowledge (FK), estimating this overlap through average BM25 scores from the Dolma corpus. The results indicate that higher overlap may lead to increased effectivity but also faster forgetting, although these findings are inconclusive due to the small-scale nature of the study. The authors propose measuring log probability increases to assess the impact of FK on model performance, emphasizing the need for careful interpretation of these metrics.

### Strengths and Weaknesses
Strengths:
- The authors conduct comprehensive investigations into LMs' knowledge acquisition, examining factors such as knowledge recurrence types, model scales, batch sizes, and levels of acquisition.
- The introduction of the FICTIONAL KNOWLEDGE dataset allows for controlled experiments and provides a quantitative framework for assessing knowledge dynamics.
- The analysis provides valuable insights into the relationship between FK and PTD, particularly regarding effectivity and forgetting rates.
- The authors have made efforts to clarify their methodology and improve metric definitions, enhancing the paper's overall clarity.

Weaknesses:
1. The interpretation of batch size effects may be overly assertive, lacking empirical support for claims about retention rates.
2. The evaluation metric may not effectively reflect knowledge acquisition, as the target phrase may not be the only correct continuation.
3. The clarity of the writing could be improved, particularly in organizing information and defining symbols consistently.
4. Important experimental details, such as the generation of the fictional knowledge and the construction of evaluation probes, are insufficiently described.
5. The relationship between model size and forgetting is not thoroughly explored, leaving questions about how larger models retain knowledge.
6. The interpretation of log probability metrics remains unconvincing, as the metrics may not adequately reflect model performance without addressing confounding factors.
7. The study's conclusions are based on a limited dataset, raising concerns about the statistical significance of the findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing, particularly by reorganizing complex sections and providing more detailed descriptions of the dataset creation and evaluation probes. It would be beneficial to plot the relationship between batch size and effectiveness to support their claims. Additionally, we suggest that the authors investigate the impact of learning rate on knowledge acquisition and forgetting dynamics, as this could provide a more comprehensive understanding of their findings. We also recommend improving the interpretability of their log probability metrics by addressing potential confounding factors that may affect the values. Furthermore, we suggest conducting a more extensive investigation into the overlap between FK and PTD to strengthen the validity of their conclusions. Finally, we encourage the authors to provide concrete examples and demonstrations to clarify their methodology and findings further.