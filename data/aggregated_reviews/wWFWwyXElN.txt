ID: wWFWwyXElN
Title: LLM-powered Data Augmentation for Enhanced Cross-lingual Performance
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of data augmentation methods for multilingual commonsense reasoning tasks by leveraging few-shot prompting techniques on large language models (LLMs) like ChatGPT and GPT-4. The authors generate task-specific English data, which is then used to fine-tune smaller multilingual models such as mBERT, evaluated for zero-shot crosslingual generalization across three datasets. The findings indicate that the proposed method effectively enhances zero-shot performance, supported by extensive human evaluations of data quality.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant NLP challenge in multilingual commonsense reasoning, contributing valuable insights into leveraging large language models for lower-resource languages.
- It features a comprehensive experimental setup, analyzing three datasets and employing multiple multilingual models, with robust human evaluation of generated data quality.
- The writing is clear and well-structured, making the methodology easy to follow.
- The authors transparently discuss the limitations of their work.

Weaknesses:
- The experiments focus on smaller multilingual models (<500M parameters), leaving the impact of larger models like mT5 or BLOOM unexplored.
- There is a lack of baseline experiments using the large language models directly for the tasks, raising questions about their inherent capabilities.
- The paper does not adequately address the diversity of synthesized examples, which may limit the effectiveness of the data augmentation.
- Clarity issues exist regarding the generation process of synthetic data and the rationale behind the number of examples produced for each dataset.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by explicitly detailing the data generation process, including whether all instances were synthesized in one step or through multiple prompts. Additionally, discussing the choice of the number of synthesized samples in relation to the original dataset sizes would enhance understanding. To address concerns about diversity, consider generating examples in smaller batches with varied seed examples. Furthermore, we suggest including comparisons with other data augmentation methods and discussing the implications of using closed generative LLMs, as well as exploring the potential of more recent models like LLAMA 2 or TULU for future work.