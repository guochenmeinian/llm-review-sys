ID: pMaCRgu8GV
Title: Artificial Generational Intelligence: Cultural Accumulation in Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 7, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two methods for learning agents to balance imitation and exploration across generations, utilizing the behavior of noisy oracles and the best-performing agents from prior generations. The authors explore two setups: in-context learning, where a meta-RL algorithm learns a recurrent RL algorithm using observations from a noisy oracle, and in-weights learning, which trains the RL algorithm from scratch at each generation without a train-test split. The authors clarify that they use a population size \(N\) to train \(N\) different seeds for the baseline model, selecting the best one. They also emphasize the importance of tuning hyperparameters, particularly learning rate schedules, for both baseline and accumulation methods. The paper demonstrates that these methods outperform non-cumulative baselines in three partially observable tasks and aims to model cultural accumulation rather than achieve state-of-the-art performance.

### Strengths and Weaknesses
Strengths:
- The approach is novel and interesting, particularly in its emphasis on cultural accumulation in RL.
- The work is well situated among related prior work in generational methods and social learning.
- The authors provide thorough clarifications and address reviewer concerns effectively.
- The manuscript includes detailed comparisons of performance metrics across generations, highlighting potential real-world applications.

Weaknesses:
- The writing about the algorithms is unclear, particularly regarding the training algorithms for in-context and in-weights accumulation, which are only partially explained in the main text.
- Observing an oracle with privileged information seems unrealistic, especially in the in-weights setting, as it may artificially simplify the problem.
- There is only one baseline used in each setting, raising concerns about the fairness of comparisons and tuning.
- The environments tested do not adequately demonstrate the complexity needed to validate claims about procedural memory.
- The paper lacks comparisons to a broader range of multi-agent RL techniques, limiting the scope of empirical validation.
- There is insufficient clarity regarding the evaluation versus training phases in the figures, and the noise levels used in evaluation plots are not clearly stated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the algorithm descriptions, particularly by providing more thorough explanations in the main text rather than relegating details to the appendix. Additionally, we suggest that the authors explicitly state that the figures relate to the evaluation phase and clarify the comparative scores against state-of-the-art methods. It would be beneficial to include a broader range of comparisons to multi-agent RL techniques and specify the noise levels used in the evaluation plots. We also encourage the authors to address the unrealistic aspects of using a noisy oracle and explore how agents might learn robustly from prior generations without such assistance. Furthermore, we suggest that the authors consider testing in more complex environments that require skill development to better demonstrate the concept of procedural memory. Lastly, we recommend highlighting the efficiency gains from cultural evolution more prominently in the introduction or discussion sections and elaborating on how the models could be adapted for specific real-world applications, potentially including case studies to illustrate their impact.