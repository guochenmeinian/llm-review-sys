ID: pi764D1Xrx
Title: Ask Language Model to Clean Your Noisy Translation Data
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method utilizing a large language model (GPT) to clean the target side of the noisy MTNT dataset, enhancing its utility for evaluating machine translation (MT) robustness. The authors propose a cleaner version, C-MTNT, and demonstrate the few-shot capabilities of large language models for generating parallel corpora tailored to MT tasks. The paper evaluates the effectiveness of different input types—monolingual, bilingual, and translation—against various noise types, showing that the bilingual approach yields the best results for certain noise categories.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a significant issue in machine translation by cleaning noisy datasets, which is beneficial for real-world applications.  
- The experimental results provide valuable insights into the effectiveness of different cleaning methods and their impact on MT robustness.  
- The authors' rebuttal adds clarity and addresses several reviewer concerns, including the introduction of human evaluation.

Weaknesses:  
- The contributions of the paper are questioned, particularly regarding the novelty of the proposed methods, which appear to leverage existing few-shot capabilities of large language models.  
- The evaluation relies heavily on the BLEU metric without exploring other metrics like BLEURT-20 or COMET-QE, limiting the assessment of model performance.  
- The paper lacks a detailed explanation of the cleaning process and the sensitivity of the models to different prompting strategies.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in Section 4.4, by explicitly defining positive and negative samples for contrastive learning. Additionally, a more detailed description of the cleaning process using large language models should be provided. To strengthen the evaluation, consider incorporating human assessments to validate the effectiveness of C-MTNT. We also suggest using a more robust baseline than the language_tool_python module, such as human evaluations or other libraries like emoji library, better_profanity, and profanity_check. Finally, the authors should explore and cite recent works on parallel corpus augmentation to enhance their experiments and provide a broader context for their findings.