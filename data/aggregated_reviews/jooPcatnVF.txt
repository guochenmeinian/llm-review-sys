ID: jooPcatnVF
Title: Implicit Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 5, 6, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for integrating outlier detection into an end-to-end learnable framework, utilizing a differentiable implicit Out-of-Distribution (OOD) detection layer. The authors propose a Gaussian mixture model that allows for efficient differentiation through techniques such as unrolling, implicit differentiation, or Jacobian-free back-propagation. Additionally, the authors integrate external knowledge triplets into the training pipeline, addressing both in-distribution (ID) and OOD setups. Quantitative results demonstrate that while convergence rates are similar for ID and OOD setups, significant improvements in accuracy for Visual Question Answering (VQA) tasks are observed when external knowledge is utilized. The authors also explore the sensitivity of their model to incomplete knowledge triplets, showing that accuracy improves over iterations despite slower convergence with higher levels of incompleteness. The experiments, particularly in VQA, show promising results, although the contribution to the field is debated.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and clearly written, with a smooth narrative throughout.
- The proposed differentiable method for outlier detection is sound and demonstrates state-of-the-art results with fewer samples and less training time.
- Comprehensive experiments and ablation studies are included, supporting the claims made in the introduction.
- The experiments convincingly demonstrate the generalization abilities of implicit layers to OOD setups.
- The integration of external knowledge triplets enhances model performance, particularly in VQA tasks.
- The paper effectively discusses the convergence properties and sensitivity to incomplete knowledge.

Weaknesses:
- There are missing references to prior work on differentiable optimization, which may limit the novelty of the contributions.
- The results from implicit differentiation are only valid under convergence, which is not guaranteed due to the predetermined number of iterations.
- The number of parameters in the models compared in Table 1 raises questions about fairness and consistency, as does the lack of clarity regarding the time per epoch and the forward pass execution.
- The OOD detection layer's effectiveness is questioned, as performance gains are not substantial compared to simpler baselines.
- The presentation of results could be improved for clarity and emphasis on key findings.
- Some technical details regarding the implementation and assumptions may require further elaboration.

### Suggestions for Improvement
We recommend that the authors improve the integration of prior related work on differentiable optimization to enhance the novelty of their contributions. Additionally, please clarify the convergence behavior of the implicit differentiation method, particularly regarding the validity of gradients when the forward pass does not converge. Address the discrepancies in the number of parameters across models in Table 1 and provide separate timings for the forward and backward passes. We also suggest improving the clarity of the discussion surrounding the integration of external knowledge triplets and emphasizing the results that validate the OOD generalization of implicit models. Lastly, consider discussing the necessity of the OOD component in relation to performance gains, enhancing the discussion on convergence properties and sensitivity to incomplete knowledge triplets to elevate the technical soundness of the work.