ID: XyAP8ScqLV
Title: An Empirical Study Towards Prompt-Tuning for Graph Contrastive Pre-Training in Recommendations
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 4, 4, 5, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical study on prompt-tuning for graph contrastive pre-training in recommendation systems, proposing a method that integrates graph neural networks (GNNs) and contrastive learning. The authors conduct extensive experiments across various real-world datasets, demonstrating significant improvements in recommendation accuracy, coverage, and diversity compared to baseline models. The contributions include a novel method combining graph contrastive learning with prompt-tuning, insights into design choices, and ablation studies analyzing component impacts. The findings highlight the potential of prompt-tuning in enhancing recommendation systems.

### Strengths and Weaknesses
Strengths:
- The exploration of prompt-tuning techniques in graph contrastive pre-training is innovative and expands the understanding of prompt-based methods in recommendation systems.
- Systematic evaluations of different prompt strategies are well-designed, providing a comprehensive analysis of effectiveness.
- Experiments on diverse datasets enhance the generalizability and validity of findings, with careful attention to hyperparameter tuning.
- Detailed information on code implementation and data availability promotes reproducibility and further research.
- Practical implications for enhancing recommendation systems through demonstrated performance improvements.

Weaknesses:
- The paper assumes a strong understanding of graph contrastive learning and prompt-tuning, potentially alienating less familiar readers.
- The effectiveness of prompt-tuning could be further contextualized by comparing it with prompt-less models.
- Generalizability of the prompt engineering techniques to other recommendation tasks is not discussed.
- Some empirical details, such as the specific GCL methods used, are not fully explained.
- Improvements over existing baselines are not significant, and the visualization in Figure 2 lacks convincing clarity.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the generalizability of prompt engineering techniques to other recommendation tasks. Additionally, a comparison with prompt-less models would provide deeper insights into the benefits of their approach. Clarifying the specific GCL methods adopted in experiments and addressing the computational complexity compared to existing approaches would enhance the paper's rigor. Finally, we suggest providing a more detailed exploration of potential challenges and limitations of prompt-tuning in recommendation systems.