ID: FlcdW7NPRY
Title: Approaching Human-Level Forecasting with Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 5, 7, 6, 8, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a forecasting system based on Language Models that aims to achieve human-level forecasting capabilities. The authors propose a system that autonomously searches for relevant information, generates forecasts, and aggregates predictions. They collect a large dataset of questions from competitive forecasting platforms to test the system's end-to-end performance, demonstrating that it performs nearly on par with the crowd aggregate of competitive forecasters and surpasses it in certain scenarios. The authors also contribute a dataset of forecasting questions and conduct a series of ablations to highlight the contributions of each component to the system's forecasting ability.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel retrieval-augmented LM system that effectively combines information retrieval, reasoning, and prediction aggregation to enhance forecasting accuracy.
2. The self-supervised fine-tuning method leverages the model's own forecasts to generate training data, improving prediction accuracy.
3. The comprehensive dataset enhances the breadth and reliability of the results.
4. The careful ablations and clear writing contribute to the paper's quality and significance in the field of predictions.

Weaknesses:
1. The system requires significant computational resources due to its summary and multi-sampling operations; reporting token statistics and costs used by the system and baseline may be necessary.
2. The comparison with the baseline may be unfair due to differing sampling frequencies; an obvious baseline could involve sampling the baseline 6 times and then voting or averaging.
3. Some descriptions are unclear, such as the reference to the averaged Brier score without specifying the source table.
4. The dataset is relatively small, limiting the human baseline for benchmarking.

### Suggestions for Improvement
We recommend that the authors improve the clarity of descriptions, particularly in section 6.1, by specifying the source of results. Additionally, we suggest incorporating the contents of Appendix I into the main text to enhance accessibility. More qualitative examples at each step of the system would provide better context, and a discussion on the importance of prediction tasks in fields like social science would strengthen the introduction. Finally, we encourage the authors to explore the potential of open-source models like Llama 2/3 under similar fine-tuning setups and consider including a thorough error analysis of the final system.