ID: klqhrq7fvB
Title: On the Scalability of GNNs for Molecular Graphs
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 3, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the scaling of GNNs across various settings, including width, depth, number of molecules, number of labels, and diversity in pretraining across 12 datasets. The authors propose a foundational model named MolGPS, which integrates various findings and demonstrates superior performance in molecular property prediction. The study reveals that scaling laws for GNNs significantly enhance performance on downstream tasks, and it examines the effects of different pre-training strategies.

### Strengths and Weaknesses
Strengths:  
1. The paper is well-written and easy to follow.  
2. Extensive experiments are conducted, providing interesting findings on scaling laws for GNNs.  
3. The empirical results of MolGPS across various datasets are promising and meaningful to the community.  
4. The investigation into fine-tuning versus probing provides valuable insights.

Weaknesses:  
1. The pre-training strategy is limited to a supervised method, neglecting established self-supervised approaches in molecular representation learning.  
2. The pre-training dataset is constrained to only 5 million molecules, which is insufficient for exploring scaling laws effectively.  
3. The presentation of results, particularly in Figure 2, is poor, making it difficult to interpret findings.  
4. There is a lack of analysis regarding the performance of models when datasets are removed, which raises questions about the importance of dataset characteristics versus size.

### Suggestions for Improvement
We recommend that the authors improve the pre-training strategy by incorporating self-supervised methods to enhance the robustness of their findings. Additionally, increasing the scale of the pre-training dataset would provide a more comprehensive exploration of scaling laws. We suggest enhancing the clarity of result presentations, particularly by revising Figure 2 for better readability. Finally, we encourage the authors to include a detailed analysis comparing fine-tuning and probing to clarify their respective impacts on performance.