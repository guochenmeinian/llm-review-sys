ID: az1SLLsmdR
Title: Elucidating the Design Space of Dataset Condensation
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Elucidate Dataset Condensation (EDC), a framework that integrates strategies like soft category-aware matching and learning rate scheduling to address the limitations of existing dataset condensation methods. The authors provide both theoretical and empirical analyses, demonstrating EDC's state-of-the-art accuracy across various datasets and model architectures.

### Strengths and Weaknesses
Strengths:  
1. The paper conducts a thorough investigation into effective strategies for expanding the design space of dataset distillation while reducing computational costs, supported by solid theoretical foundations.  
2. Empirical results show substantial improvements in accuracy, particularly on ImageNet-1k with a ResNet-18 model, highlighting the practical efficacy of the proposed methods.  

Weaknesses:  
1. The comparison experiments are not comprehensive, lacking evaluations against other convolutional architecture methods beyond the baseline RDED.  
2. The paper's structure is confusing, making it difficult to discern the primary novelty and motivation behind the work. Key concepts like "bi-level" and "uni-level" optimization are not clearly defined.  
3. The provided code is not executable due to missing packages and insufficient setup instructions, hindering reproducibility.  
4. The definition of generalized data synthesis is unclear, and the paper does not adequately address potential information loss or scalability concerns with larger datasets.

### Suggestions for Improvement
We recommend that the authors improve the comprehensiveness of the comparison experiments by including additional convolutional architecture methods to provide a more thorough evaluation of EDC. We also suggest clarifying the paper's structure and explicitly defining key terms such as "bi-level" and "uni-level" optimization to enhance reader understanding. Furthermore, the authors should provide detailed instructions for code execution, including a complete list of dependencies and environment setup guidelines. Lastly, we encourage the authors to refine the definition of generalized data synthesis and explore the implications of potential information loss and scalability with larger datasets.