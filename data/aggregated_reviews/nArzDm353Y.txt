ID: nArzDm353Y
Title: Training Transitive and Commutative Multimodal Transformers with LoReTTa
Conference: NeurIPS
Year: 2023
Number of Reviews: 27
Original Ratings: 6, 6, 7, 6, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a self-supervised learning framework, LoReTTa (Linking mOdalities with a tRansitive and commutativE pre-Training sTrAtegy), designed to address the challenge of training models with incomplete multimodal datasets. The authors propose a method that utilizes commutative and transitive modeling to enable the model to learn from available modality pairs and generate pseudo-targets for unseen combinations. The authors argue that LoReTTa differs conceptually from cycle consistency by focusing on predicting a missing modality without relying on the original input for reconstruction. The framework is evaluated on both synthetic and real-world datasets, demonstrating significant improvements in performance, particularly in handling unseen modality combinations. Additionally, the authors conducted further experiments that strengthen their contributions and propose a methodology for hyperparameter selection, although concerns regarding reproducibility and code accessibility remain.

### Strengths and Weaknesses
Strengths:
- The paper tackles a relevant problem in multimodal learning, particularly in scenarios where data alignment is challenging, such as in medical datasets.
- The proposed method is straightforward and well-motivated, with a clear presentation that enhances readability.
- The authors provide a clear distinction between their method and existing approaches, particularly cycle consistency.
- Experimental results indicate that LoReTTa outperforms existing baselines that do not incorporate transitive modeling.
- The methodology for hyperparameter selection is well-articulated, referencing established best practices in the field.
- The authors are responsive to reviewer feedback and willing to clarify and improve their submission.

Weaknesses:
- The methodological novelty is limited, as the approach largely builds on existing concepts like CycleGAN and causal masked modeling without significant innovation.
- The experimental scope is narrow, relying on small-scale datasets (e.g., SVL-MNIST and TCGA-OMICS) and lacking diverse applications or larger benchmarks.
- There are concerns regarding the clarity and rigor of the theoretical analysis, particularly in Section 4, which could benefit from better organization and clearer notation.
- The absence of upper-bound experiments raises questions about the completeness of the evaluation.
- Some reviewers express skepticism regarding the authors' claims about the uniqueness of their transitive modeling approach compared to cycle consistency.
- The argument against using BERT for generative tasks is not fully convincing to all reviewers, indicating a need for stronger justification.
- Concerns remain regarding the accessibility of the paper to fellow researchers, particularly in terms of reproducibility and the availability of code.
- The hyperparameter tuning process for CLIP is not fully detailed, raising questions about the adequacy of the chosen batch size and its impact on results.

### Suggestions for Improvement
We recommend that the authors improve the methodological novelty by clearly articulating how their approach differs from existing works and providing more substantial theoretical analysis, including generalization error bounds. Additionally, we suggest conducting experiments on larger and more diverse datasets to validate the generality of the approach. The authors should also include upper bound results to strengthen their claims and clarify the computational requirements compared to existing models. Furthermore, we recommend improving the clarity of their argument regarding the limitations of BERT in generative tasks, possibly by providing more explicit evaluations of its performance compared to LoReTTa. We also suggest that the authors discuss the similarities and differences between their approach and the MCTN method more thoroughly. To enhance accessibility, please ensure that all training details necessary for reproducing results are adequately described, including how grid search of the hyperparameters is performed using the validation set. Finally, consider releasing the full training code to improve reproducibility and clarify the computational cost of using VQVAE compared to CLIP.