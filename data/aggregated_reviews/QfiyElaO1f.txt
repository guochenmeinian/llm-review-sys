ID: QfiyElaO1f
Title: Data-Efficient Variational Mutual Information Estimation via Bayesian Self-Consistency
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 8
Original Confidences: 4, 5

Aggregated Review:
### Key Points
This paper presents a novel approach to estimating mutual information (MI) through Bayesian self-consistency, proposing a bound that serves as an alternative to VNMC and VPCE. The authors derive a variational estimator that incorporates a variance penalty to enhance consistency in marginal likelihood estimates, addressing challenges in MI estimation, particularly in high-dimensional contexts. The empirical evaluation shows that the proposed estimator converges faster and yields more accurate MI and posterior approximations compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The manuscript introduces an innovative application of self-consistency penalties, enhancing sample efficiency and maintaining asymptotic consistency.
- The rigorous empirical evaluation demonstrates the method's efficacy in specific tasks, providing compelling evidence of improved performance over baselines.
- The structure of the manuscript facilitates a logical progression from foundational concepts to detailed methodology.

Weaknesses:
- The explanation of self-consistency in the derivation of the bound is informal and lacks rigor, leading to potential misunderstandings.
- The proposed loss function has been previously presented in different contexts, which may undermine its novelty.
- The complexity of mathematical derivations may challenge readers without a strong background in probabilistic modeling and Bayesian inference.
- The metrics used for evaluation, such as the estimation error of the log posterior density and MMD, may not effectively capture the performance of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the explanation of self-consistency to provide a more formal motivation for arriving at Eq (6), potentially drawing from Richter et al. (2020). It is essential to clarify how the proposed loss function relates to existing literature. Additionally, the authors should elaborate on the conditions under which their estimators have finite variance and how these compare with VNMC/VPCE. We suggest providing a clearer definition of the metrics used for evaluation, particularly regarding the estimation error of the log posterior density and the use of MMD, and consider including boxplots of estimates versus true values for better clarity. Finally, expanding the empirical evaluation to include a broader range of applications would strengthen the claims of generalizability.