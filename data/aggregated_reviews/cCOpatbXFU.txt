ID: cCOpatbXFU
Title: Investigating Variance Definitions for Stochastic Mirror Descent with Relative Smoothness
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 4, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates a new definition for the stochastic gradient variance in mirror descent, addressing limitations of existing analyses that require a strongly convex distance generating function. The authors propose a definition that is more applicable in practice and derive convergence rates in a convex setting. They demonstrate the application of this new variance definition to bound the estimation error of maximum a posteriori (MAP) for one-dimensional Gaussian distributions. Additionally, the paper presents a theoretical exploration of mirror descent methods, focusing on the assumptions necessary for their effective application. The authors propose replacing the classical Legendre-type assumption with a more general Assumption 1, which includes conditions like twice continuous differentiability and strict convexity, while acknowledging that assuming $h$ is Legendre may be necessary for certain applications.

### Strengths and Weaknesses
Strengths:
- The paper makes significant progress in generalizing mirror descent analyses, particularly in Section 2.2, where the proposed definition is shown to be superior to existing ones.
- It provides a non-asymptotic bound for the estimation error of Gaussian MAP, addressing a fundamental problem previously lacking theoretical guarantees.
- The introduction of a new variance definition allows for convergence results under less restrictive assumptions, which is a notable contribution to the field.
- The authors engage constructively with reviewer feedback, indicating a willingness to clarify and improve their work.
- The proposed framework allows for a broader understanding of mirror descent methods by relaxing certain assumptions while maintaining rigor.

Weaknesses:
- The convergence rates are derived using constant step sizes, which do not allow the optimality gap to vanish; diminishing step sizes are not addressed.
- The application of the new definition is currently limited to maximum likelihood estimation for one-dimensional Gaussian distributions, raising questions about its generalizability to multivariate cases.
- There is a lack of clarity regarding the assumptions required for various results, particularly in distinguishing between different assumptions used throughout the paper.
- The paper lacks clarity regarding the specific assumptions on $h$ and their implications, leading to confusion among reviewers.
- The justification for non-asymptotic guarantees on $f_\eta$ is deemed insufficient, raising concerns about the robustness of the results.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the assumptions used in the paper, ensuring that it is explicitly stated which results depend on which assumptions. Specifically, we suggest that the authors clarify the assumptions related to $h$ by explicitly stating them at the beginning of each section and introducing a clear "Assumption 2" to outline the necessary conditions for the iterates. Additionally, we suggest exploring the generalization of the new variance definition to multivariate Gaussian distributions and providing further applications to justify its necessity. To enhance the paper's presentation, we encourage the authors to address minor issues such as notation consistency and the clarity of certain passages. Furthermore, we advise the authors to provide a more robust justification for the non-asymptotic guarantees on $f_\eta$, as the current rationale does not convincingly connect these guarantees to the overall objectives of the paper. Lastly, we recommend including a concrete example beyond MAP/MLE to illustrate the application of their results.