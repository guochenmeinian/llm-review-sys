ID: LAGxc2ybuH
Title: Explaining the Uncertain: Stochastic Shapley Values for Gaussian Process Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 7, 6, 8, 7, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to explaining Gaussian processes (GPs) through stochastic Shapley values, introducing methods GP-SHAP and BayesGP-SHAP. These methods leverage the analytical covariance structure of GPs to provide explanations that reflect epistemic uncertainty, differing from previous methods like BayesSHAP. The authors also propose a Shapley prior for learning Shapley values in predictive tasks, supported by experiments on benchmark datasets.

### Strengths and Weaknesses
Strengths:  
- The work is theoretically well-formulated and motivated, effectively combining uncertainty research with explainable AI (XAI).  
- The experimental analysis of GP-SHAP and BayesGP-SHAP is convincing, and the paper adheres to style guidelines, including a well-rounded appendix for reproducibility.  
- The introduction of stochastic cooperative games and the ability to measure explanation uncertainties are significant contributions to the XAI community.  

Weaknesses:  
- The predictive approach using the Shapley prior feels underdeveloped, lacking a comparative analysis with FastSHAP and an in-depth discussion of its benefits.  
- Some concepts may be challenging for machine learning readers, particularly in the definitions section, which could benefit from clearer explanations.  
- The connection to uncertainty research could be strengthened by discussing the types of uncertainty captured in the confidence intervals.  
- The exposition could be improved by emphasizing key ideas earlier in the paper and clarifying certain mathematical statements.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the Shapley prior by including a comparative analysis with FastSHAP to clarify its advantages. Additionally, we suggest enriching the definitions section to enhance accessibility for machine learning readers. Strengthening the connection to uncertainty research by elaborating on the types of uncertainty captured in the explanations would also be beneficial. Lastly, we advise revising the exposition to highlight key ideas earlier and clarifying the statements in Propositions 11 and 12 for better understanding.