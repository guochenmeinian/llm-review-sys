ID: pgEIr2HY2E
Title: Improving Summarization with Human Edits
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach, referred to as SALT, to enhance clinical text summarization by incorporating human edits. The method utilizes a combination of likelihood and unlikelihood training to adjust the probabilities of tokens based on human feedback. The authors conduct experiments on two datasets, demonstrating improvements across various metrics, including ROUGE and UMLS-F1. The study aims to address the integration of user feedback effectively and explores the implications of imitation edits.

### Strengths and Weaknesses
Strengths:
- The motivation for using human edits as feedback is well-founded, particularly as it can be naturally obtained from user workflows.
- The application of unlikelihood training to incorporate human edits is sound and could inspire further research in NLP.
- The experiments are comprehensive, with sufficient analysis and ablation studies.

Weaknesses:
- The presentation of Sections 3 and 4 is poor, lacking clarity and necessary citations.
- The effectiveness of imitation edits is questionable, with insufficient statistical analysis of alignment between AI-generated summaries and ground truth.
- The paper lacks comparisons with existing methods in the literature, limiting the contextual understanding of its contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of Sections 3 and 4 by providing clearer citations, defining terms, and introducing the datasets more effectively. Additionally, we suggest including a figure to illustrate key concepts and addressing the significance of E-C and E-NC tokens in the context of the proposed method. The authors should also consider expanding the human evaluation to a larger sample size and integrating established edit-based metrics like SARI to contextualize their findings. Finally, we encourage the authors to explore iterative feedback processes and clarify the training details of their model to enhance reproducibility.