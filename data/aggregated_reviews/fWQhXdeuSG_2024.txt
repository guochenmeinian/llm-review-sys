ID: fWQhXdeuSG
Title: Pretrained Optimization Model for Zero-Shot Black Box Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 1, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on zero-shot black box optimization, introducing a pretrained optimization model (POM) that learns to generate effective optimization strategies based on a dataset. The authors achieve notable results on the BBOB benchmark and two robot control tasks. Key components of the POM include the LMM and LCM, which facilitate sample strategy generation. The LMM module generates a strategy matrix using population ranking and relative advantage information, while the LCM adapts the crossover strategy based on similar information. POM aims to optimize strategy parameters in the strategy space and demonstrates resilience to conflicting objective functions. The training algorithm, MetaGBT, enhances stability by integrating gradient information across tasks. Empirical results show strong performance across both low and high-dimensional scenarios.

### Strengths and Weaknesses
Strengths:
1. The design of the POM is logical, drawing from established optimization processes and incorporating thoughtful design elements for the LMM and LCM.
2. The training process is well-structured, with a training dataset that effectively covers landscape features, and the loss function design is commendable for balancing convergence and diversity.
3. The algorithm shows impressive results on both synthetic datasets and real-world robot control tasks.
4. The authors effectively clarify how POM addresses conflicting optimization objectives, enhancing understanding of the framework's adaptability.
5. The integration of ranking and relative performance information in the LCM and LMM modules is well-articulated, resolving previous concerns about conflicts.

Weaknesses:
1. Despite the extensive hand-crafted designs in POM, the reliance on mutation and crossover raises questions about the motivation for this design choice.
2. The 'Ablation Study' indicates that the `mask` is a critical component, yet it lacks theoretical backing, which undermines the framework's overall integrity. Additionally, the performance of the NOTRAIN variant raises further concerns.
3. The paper could benefit from presenting more comprehensive results, particularly for higher dimensions in the BBOB tests.
4. The simplicity of the training dataset may lead to overfitting issues.
5. The related work section is not exhaustive; for instance, it could include recent studies on using LLMs for black-box optimization.
6. Some design choices regarding the proposed method compared to standard layers could be better organized, as they may distract from the main idea if presented too early.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the design choices in POM, particularly regarding the mutation and crossover mechanisms. Additionally, we suggest providing a more comprehensive analysis of the `mask` component and its implications for the framework's robustness. The authors should also expand the results section to include more diverse scenarios, particularly in higher dimensions, and enhance the related work section to encompass relevant literature on LLMs in optimization. Finally, we encourage the authors to clarify the implementation details of baseline methods and improve the organization of the writing by deferring detailed discussions of design choices related to standard layers to later sections. This will allow readers to first grasp the main concept without distraction.