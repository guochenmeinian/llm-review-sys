ID: ChKCF75Ocd
Title: PutnamBench: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 8, 7, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PutnamBench, a multi-language and challenging theorem-proving benchmark targeting undergraduate-level math competitions. It comprises 1,337 formalizations of 514 problems from the Putnam Competition, covering diverse mathematical domains. The authors manually formalize natural language problems into Lean, Isabelle, and a significant portion into Coq, introducing a new task that requires theorem provers to identify solutions and provide proofs of correctness. Experiments reveal that existing neural and symbolic methods can solve only 5 problems, indicating the benchmark's difficulty.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated and well-written, providing a comprehensive benchmark that bridges formal and informal theorem proving.
- PutnamBench is the first benchmark of its kind for undergraduate-level math competitions, larger than the miniF2F benchmark, and includes problems in three formal languages.
- The manual efforts in constructing the dataset are commendable, and the evaluation of neural baselines and hammer tools is detailed and convincing.

Weaknesses:
- The term 'multilingual' may be misleading; 'multi-language' could be more appropriate.
- The discussion of miniF2F may not accurately reflect its coverage of other languages.
- The evaluation setting lacks clarity regarding the specific tasks used.
- There is a need for more details on the annotation process and the performance of various language models.

### Suggestions for Improvement
We recommend that the authors improve the terminology by using 'multi-language' instead of 'multilingual.' Additionally, clarify the discussion around miniF2F to accurately represent its language coverage. It would be beneficial to evaluate the autoformalization capabilities of LLMs on PutnamBench and consider setting a time limit for neural baselines to better mimic human performance in the Putnam Competition. Furthermore, providing additional details on the annotation process, including the qualifications of annotators and semantic alignment, would enhance the paper. Lastly, incorporating more baselines, particularly those based on open-source models, would provide a more comprehensive evaluation of performance.