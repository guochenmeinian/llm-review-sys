ID: eNhW9UnlGG
Title: Contextual Gaussian Process Bandits with Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extension of Gaussian Process (GP) based contextual bandits, introducing a reward model called the neural network-accompanied Gaussian process (NN-AGP). The authors propose the NN-AGP-UCB algorithm, which models the context dependency through a multi-output GP and a neural network, determining rewards via their inner product. The work includes empirical evaluations and theoretical analyses, focusing on complex reward functions influenced by contextual variables.

### Strengths and Weaknesses
Strengths:
- The paper is well-written with clear language, thorough experiments, and strong results.
- It introduces a flexible reward model that allows for various neural network applications, demonstrating its effectiveness through diverse empirical evaluations.
- The theoretical analysis, including regret bounds and maximum information gain, is solid.

Weaknesses:
- The methodological novelty is limited, primarily extending previous work with minimal new insights.
- The reward model's structure is questioned for its naturalness, and comparisons with related algorithms like NeuralUCB are lacking.
- The assumptions for regret analysis are strong, particularly regarding the convexity and compactness of the parameter space, which may not hold for neural networks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 3.3 to better articulate its purpose. Additionally, the authors should include comparisons of their method against NeuralUCB in queuing and pricing tasks. It would be beneficial to discuss the limitations of the proposed approach more thoroughly. Furthermore, we suggest including additional metrics in the empirical evaluation, such as computation cost and prediction latency, to provide a more comprehensive assessment of the model's performance.