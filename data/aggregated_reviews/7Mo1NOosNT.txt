ID: 7Mo1NOosNT
Title: COLD: Causal reasOning in cLosed Daily activities
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the COLD (Causal reasOning in cLosed Daily activities) framework, which aims to bridge the gap between open-ended causal reasoning and symbolic representation-based question answering. The authors create a substantial dataset of approximately 8 million causal queries, leveraging human understanding of daily activities to evaluate the causal reasoning capabilities of various Large Language Models (LLMs). The findings indicate that LLMs struggle with causal reasoning, even in seemingly trivial scenarios, and the authors utilize theories of causal inference, such as the backdoor criterion, to analyze these capabilities. Additionally, the paper discusses the challenges of conducting human evaluations for the proposed models, Corr2Cause and CLadder, due to their symbolic forms. The authors acknowledge that while a full evaluation on the entire dataset is impractical, a small random sample could be sufficient for assessment, and they express a commitment to incorporating human evaluation in the updated version of the paper to enhance the quality of their work.

### Strengths and Weaknesses
Strengths:
- The COLD framework effectively connects open-ended causal reasoning with symbolic question answering, grounded in human understanding of daily activities.
- The large dataset of causal queries is a significant contribution, providing a robust basis for evaluating LLMs' causal reasoning abilities.
- The paper is well-written, with clear explanations and a logical flow, enhancing the communication of key findings.
- The exploration of LLMs' causal reasoning using the backdoor criterion offers valuable insights into the strength of causal relationships.
- The authors demonstrate a willingness to engage with reviewer feedback and improve their work.
- The proposal to include human evaluation on a random sample of the dataset shows a proactive approach to addressing concerns.

Weaknesses:
- The reliance on automatically generated queries necessitates human annotations or expert evaluations to confirm their reliability.
- The observational graphs, created through human annotations, limit the range of concepts covered; automated approaches for constructing such graphs should be discussed.
- The potential for synthesized queries to be used for fine-tuning is unclear; exploring whether training on these queries could improve performance would enhance the paper's comprehensiveness.
- The paper lacks evaluations of additional baselines focusing on zero-shot commonsense question answering, which could provide insights into the benefits of transformations from commonsense knowledge bases.
- The symbolic nature of the models complicates human evaluation, which may hinder the assessment process.
- There remains a lack of clarity on why human annotators can create ESDs but struggle with conducting evaluations.
- The presentation could be improved, as some important details are relegated to the appendix, and there are grammar typos that need correction.

### Suggestions for Improvement
We recommend that the authors improve the reliability of the generated queries by incorporating human annotations or expert evaluations. Additionally, discussing automated methods for constructing observational graphs would facilitate large-scale causal benchmarking. The authors should explore the feasibility of using synthesized queries for fine-tuning and evaluate additional baselines related to zero-shot commonsense question answering. Furthermore, we suggest that the authors improve the clarity around the challenges faced in human evaluation, particularly regarding the symbolic forms of their models. Explicitly outlining the methodology for the proposed random sampling of the dataset for human evaluation will ensure transparency and rigor in their approach. Finally, enhancing the clarity of the presentation and correcting grammar typos will improve the overall quality of the paper.