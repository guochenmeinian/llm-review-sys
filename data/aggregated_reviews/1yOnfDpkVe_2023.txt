ID: 1yOnfDpkVe
Title: Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 8, 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Battle of the Backbones (BoB), a benchmark evaluating various pretrained model backbones across multiple computer vision tasks, including classification, detection, segmentation, out-of-distribution generalization, and image retrieval. The authors provide insights into the performance of different architectures and pretraining methods, recommending specific models such as Sup. ConvNeXt-B and Sup. SwinV2-B for practitioners. They detail notable differences in behavior between self-supervised learning (SSL) and supervised pretraining, as well as between convolutional and transformer architectures. The authors highlight the surprising performance of supervised ConvNeXt compared to state-of-the-art SSL methods and acknowledge the need for further exploration of structural differences that influence performance. Additionally, they benchmark various models, including the Segment Anything Model (SAM) ViTs, and include experiments with smaller architectures like ResNet-18 and EfficientNet-B0.

### Strengths and Weaknesses
Strengths:
- The paper offers a comprehensive benchmark of pretrained models, revealing their strengths and weaknesses across diverse tasks, which is valuable for both practitioners and researchers.
- Extensive experiments were conducted using various datasets, enhancing the generalizability of the findings.
- The authors provide open-source code and detailed documentation, supporting reproducibility.
- The inclusion of diverse tasks and benchmarks enhances the paper's applicability.
- The authors have addressed feedback by adding experiments and clarifying details regarding model performance and training costs.

Weaknesses:
- The paper primarily focuses on performance without addressing fairness aspects, such as biases introduced by different pretraining methods.
- It lacks statistical tests for comparisons, which could strengthen the conclusions.
- There is limited discussion on lightweight models and classic architectures, which are relevant in practical applications.
- The conclusions regarding object detection may lack sufficient solidity, as noted by some reviewers.
- The computational feasibility of including larger models in the benchmark remains a concern, limiting the generality of claims about model performance.

### Suggestions for Improvement
We recommend that the authors improve the paper by including statistical tests, such as the post-hoc Nemenyi test, to demonstrate the relative rank and statistical significance of the results. Additionally, consider including a broader range of pretraining methods and classic convolutional networks like EfficientNet and RegNet. It would also be beneficial to clarify the reasons behind the observed performance differences between CNNs and Transformer-based models. Furthermore, the authors should report the computational costs associated with different finetuning strategies and enhance the readability of the tables in the appendix by standardizing their formats. We also suggest that the authors improve the robustness of their conclusions regarding object detection by providing more empirical evidence or theoretical analysis to support their claims. Additionally, discuss the generality of their findings, particularly regarding the performance of larger models, as there is potential for ViTs to outperform CNNs at greater scales. Lastly, addressing the implications of model size on performance and including discussions on fairness and biases would enrich the paper's contributions.