ID: FkAwlqBuyO
Title: Direct Preference-based Policy Optimization without Reward Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DPPO, a method for offline Preference-based Reinforcement Learning (PbRL) that learns a policy from paired trajectory comparisons without requiring a reward function or online rollouts. The authors propose a pseudo-Boltzmann model that focuses on the similarity of the learned policy to preferred trajectories rather than returns. The implementation employs binary classification loss and contrastive learning loss, maximizing the number of contrastive learning terms from a given batch size. DPPO incorporates a BC-like regularizer to maintain low divergence from trajectories and includes an auxiliary loss for unlabeled data. The method is evaluated on D4RL benchmarks, demonstrating competitive performance against existing offline PbRL algorithms. The authors also introduce a new policy score metric that incorporates multiple negative samples and address concerns regarding the Boltzmann rationality assumption and the implications of using rewards versus returns in their evaluation.

### Strengths and Weaknesses
Strengths:
- The need for online rollouts is addressed, making the method practical for systems where rollouts are costly.
- The exposition is clear, with solid motivation for the algorithm's components and adequate ablation studies.
- Empirical results are strong, particularly in the Adroit and Franka Kitchen environments.
- The authors effectively resolved major concerns regarding baselines and provided additional experiments that enhance the paper's credibility.
- The implementation of a preference model offers a fresh perspective on policy optimization, potentially aligning better with human decision-making processes.
- The clarity in addressing technical aspects of the loss functions and experimental results demonstrates thoroughness in their methodology.

Weaknesses:
- The baselines may be too weak, as the method appears similar to BC, and comparisons with stronger baselines are lacking.
- There remains skepticism about the applicability of the proposed method beyond offline settings, particularly in learning sequential decision-making policies from scratch.
- The implications of the new loss function diverging from traditional Boltzmann PbRL are not thoroughly explored.
- The discussion on the Boltzmann rationality assumption lacks depth, particularly in comparing it to alternative models in real data contexts.
- Some references are missing, and the formal definition of the preference model is not provided, leading to confusion regarding its relationship to reward modeling.
- The reliance on reward measurements instead of returns may not adequately capture the nuances of different reward functions.

### Suggestions for Improvement
We recommend that the authors improve the comparison of their approach with foundational works in preference-based RL, such as Liu et al. [1], and include results for methods that utilize similar trajectory distance comparisons. Additionally, we suggest that the authors clarify the connection between their approach and behavior cloning, particularly in Section 3.2, to enhance understanding of how minimizing the distance metric leads to policy learning. It would be beneficial to include an ablation study demonstrating the importance of the contrastive learning aspect of DPPO versus the segment smoothing objective. We also recommend improving the discussion surrounding the Boltzmann rationality assumption by incorporating a comparative analysis with alternative models, referencing relevant literature to strengthen their argument. Furthermore, we suggest redesigning the evaluation metrics to focus on returns rather than rewards, as this may provide a more robust measure of performance. Finally, we encourage the authors to explore the feasibility of their approach in purely online settings and to present empirical assessments that address this aspect in future work.