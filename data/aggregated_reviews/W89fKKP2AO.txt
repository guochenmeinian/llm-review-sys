ID: W89fKKP2AO
Title: Universal Neural Functionals
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 5, 5, 5, 7, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 2, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Universal Neural Functionals (UNFs), which are models that process neural network weights in a permutation equivariant manner. The authors develop an algorithm for constructing maximally expressive equivariant linear layers applicable to various architectures, including RNNs and Transformers. The evaluation shows that UNFs improve generalization prediction for RNNs and enhance learned optimizers across several tasks.

### Strengths and Weaknesses
Strengths:
- The approach is well-motivated mathematically, focusing on permutation equivariant functions, and the experiments adhere closely to the proposed methodology.
- The paper is well-written, effectively explaining complex mathematical concepts.
- The work generalizes previous methods to a broader class of neural networks, contributing significantly to the field.

Weaknesses:
- The RNN generalization prediction experiment lacks sufficient baselines, making it difficult to assess the significance of the results.
- The authors do not test methods without permutation symmetries, which would provide a clearer comparison of performance.
- The presentation of the methodology and results is unclear in certain sections, particularly around specific equations and the overall organization.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology and results, particularly around Equations 9 and 10. Additionally, it would be beneficial to include more baseline comparisons, such as Deep Sets and a simple MLP, to contextualize the findings. We also suggest explicitly defining terms and concepts in the Preliminaries section to enhance readability. Finally, addressing the lack of comparisons to recent works on permutational equivariant models would strengthen the paper's contribution.