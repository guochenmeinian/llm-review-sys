ID: lHnbPVKbts
Title: Constrained Multi-objective Bayesian Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 8
Original Confidences: 3, 4

Aggregated Review:
### Key Points
This paper presents a method for *Constrained Multi-objective Bayesian Optimization* (CMOBO), building on previous work by Golovin & Zhang (2020) and Deng & Zhang (2019). The proposed method optimizes a random scalarization of the multivariate objective using the upper confidence bound, effectively optimizing hypervolume improvement while incorporating constraints modeled with Gaussian Processes (GPs). The authors demonstrate that their CMOBO method outperforms well-known unconstrained alternatives in optimizing multivariate problems with fewer constraint violations.

### Strengths and Weaknesses
Strengths:
- The paper exhibits mathematical rigor, providing theoretical proofs that ensure regret bounds for cumulative hypervolume and constraint violations.
- Empirical validation is strong, with tests on both synthetic functions and real-world problems, showcasing versatility and robustness.
- The integration of random scalarization within constrained multi-objective optimization is a novel contribution, addressing a significant gap in the literature.

Weaknesses:
- The choice of budget for experiments lacks justification, particularly for the Penicillin example.
- The algorithm's reliance on GPs may limit its applicability in scenarios where GPs do not accurately model the data.
- Some sections contain technical jargon that could hinder accessibility for readers unfamiliar with GP-based optimization.

### Suggestions for Improvement
We recommend that the authors improve the justification for the experimental budget choices by consulting practitioners to understand typical evaluation limits. Additionally, clarifying the constants associated with CMOBO-0.05 and CMOBO-0.1 in Algorithm 1 would enhance understanding. To improve clarity, consider simplifying technical jargon and providing more intuitive explanations of the algorithm's workings. Finally, exploring adaptive or learning-based approaches to scalarization could further enhance the originality and applicability of the method.