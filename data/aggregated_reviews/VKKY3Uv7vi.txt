ID: VKKY3Uv7vi
Title: BPQP: A Differentiable Convex Optimization Framework for Efficient End-to-End Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 8, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents BPQP, a differentiable convex optimization framework aimed at efficient end-to-end learning for convex problems using KKT conditions. The authors propose a novel approach that reformulates the backward pass as a quadratic program (QP), avoiding the need for large Jacobian matrix computations and thereby improving speed and space efficiency compared to existing methods like OptNet. The evaluation demonstrates significant performance improvements in efficiency, particularly for linear programming (LP), quadratic programming (QP), and second-order cone (SOC) problems.

### Strengths and Weaknesses
Strengths:  
1. The paper is well-organized and clearly written, making it accessible.  
2. The BPQP framework offers significant speedups over existing differentiable layers, with promising experimental results against both conventional and neural network-based solutions.  
3. The proposed method allows for algorithmic decoupling of forward and backward passes, enhancing flexibility.  

Weaknesses:  
1. The differences between BPQP and OptNet are insufficiently justified, particularly regarding the choice of optimization methods (interior-point vs. ADMM).  
2. The experimental section is limited, focusing primarily on synthetic tasks and a single portfolio optimization problem, raising concerns about the generalizability of results.  
3. The paper lacks a discussion on limitations and potential future work, as well as performance measures beyond computational time in the results.  

### Suggestions for Improvement
We recommend that the authors improve the justification for the differences between BPQP and OptNet, particularly the rationale for using ADMM over the interior-point method. Additionally, we suggest expanding the experimental section to include a wider variety of tasks, such as Total Variation Denoising and Sudoku, to better showcase the effectiveness of BPQP. It would also be beneficial to include performance measures in Tables 1 and 2 beyond just computational time. Furthermore, we encourage the authors to address the limitations of their method and explore the extension of their approach to non-convex problems. Lastly, we advise enhancing the introduction to better motivate the relevance of differentiable optimization layers in practical contexts.