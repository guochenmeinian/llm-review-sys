ID: vVdRgpC1Oh
Title: An Empirical Study of Multimodal Model Merging
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical investigation into merging modality-specific vision and language transformers into a modality-agnostic architecture. The authors systematically test various merging methods, including tree merging, interpolation, arithmetic, and RegMean, while varying key factors such as initialization and model architectures. Experiments are conducted on VQA and COCO image-text retrieval tasks, yielding insights into multimodal margining.

### Strengths and Weaknesses
Strengths:  
- The work provides multiple interesting observations about multimodal margining and achieves comparable performance to models pre-trained from scratch.  
- The empirical results regarding seed pretraining iterations and weight sharing strategies offer valuable insights for multimodal pretrained models.  
- The thorough experiments and analysis on merging mechanisms contribute to understanding how merging works.

Weaknesses:  
- The novelty is limited as the study is primarily empirical, and the target tasks (VQA and COCO) are considered outdated.  
- Only a single baseline (ViT-B/16) is tested, and the proposed metrics are largely based on previous work.  
- The evaluation benchmarks are insufficient for a comprehensive understanding of multimodal pretraining, lacking text-only and image-only tasks.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the work by discussing how modality merging differs from previous studies on single/multi-task merging, identifying unique technical challenges or new findings. Additionally, it would be beneficial to expand the evaluation to include more recent benchmarks and a wider variety of tasks, including text-only and image-only evaluations, to substantiate claims of modality-agnostic performance. Clarifying the difference between seed pretraining and VL-pretraining would enhance understanding, and a deeper analysis of why merging works could strengthen the paper.