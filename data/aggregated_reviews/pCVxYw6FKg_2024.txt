ID: pCVxYw6FKg
Title: The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 7, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two modifications to neural networks aimed at removing permutation symmetries: fixing certain weights and employing a non-elementwise activation function. The authors demonstrate that these "asymmetric" networks enhance performance metrics related to linear mode connectivity, Bayesian network efficiency, and metanetwork functionality. Additionally, the paper explores W-Asymmetric Networks, clarifying their differences from lottery tickets, which require fixed initialization and specific training protocols. In contrast, W-Asymmetric Networks can be trained from any initialization, allowing for broader applicability. The authors also introduce $\sigma$-Asymmetric networks, which differ significantly in structure from pruned networks. The study provides a thorough examination of the implications of permutation symmetry on neural network training and outcomes.

### Strengths and Weaknesses
Strengths:  
- The paper serves as an effective ablation study on permutation symmetries, offering a valuable tool for further investigations.  
- The methods proposed are both novel and logically sound, supported by a comprehensive literature review.  
- The organization of the paper is clear, with well-motivated methodologies and experiments that are easy to follow.  
- The authors provide a clear distinction between lottery tickets and W-Asymmetric Networks, highlighting the flexibility of their approach.  
- The introduction of $\sigma$-Asymmetric networks adds novelty to the research, as noted by other reviewers.  
- The authors are responsive to reviewer comments and willing to clarify technical details.

Weaknesses:  
- The reliance on experimental results over theoretical foundations limits the paper's robustness, particularly for the $W$-asymmetry method, which employs non-standard training settings.  
- A lack of code availability hampers reproducibility and relevance.  
- The results for $\sigma$-asymmetry are weaker and lack sufficient analysis to explain their performance, particularly in relation to linear mode connectivity and Bayesian networks.  
- Some reviewers perceive certain findings as not new, which may detract from the perceived novelty of the work.  
- There is a need for clearer communication regarding the use of normalization techniques in the experiments.

### Suggestions for Improvement
We recommend that the authors improve the theoretical grounding of their findings, particularly regarding the $W$-asymmetry method's training settings, by providing a comparison with standard models initialized to the same weights. This would enhance the comparability of metrics like $L^2$ weight distance. Additionally, an analysis of how learning rate and warmup duration affect experimental outcomes is necessary to clarify the observed results.

Further, we suggest including more details in tables 5-7 to facilitate replication of the asymmetry methods, specifically clarifying how $n_{\text{fix}}$ and $\kappa$ are determined for each architecture and providing a clearer mapping of block names to ResNet20 layers.

We also recommend that the authors improve clarity regarding the normalization techniques used in their experiments, specifically addressing whether layer or batch normalization is employed in the LMC results. Lastly, we encourage the authors to address the limitations of the $\sigma$-asymmetry method by discussing its weaker performance and exploring the potential for fixing biases to achieve asymmetry. Additionally, the authors should further emphasize the unique contributions of $\sigma$-Asymmetric networks to reinforce their novelty in comparison to existing methods.