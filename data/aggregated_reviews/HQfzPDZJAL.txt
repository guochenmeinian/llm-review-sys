ID: HQfzPDZJAL
Title: Expository Text Generation: Imitate, Retrieve, Paraphrase
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an architecture for generating stylistically consistent expository texts through a guided approach called "Imitate, Retrieve, Paraphrase" (IRP). The authors propose a three-step method: creating a content plan based on stylistic imitation, retrieving factual information, and paraphrasing the facts to match the style. The methodology is evaluated against gold standard documents using both automated metrics and human evaluation, demonstrating superior performance compared to other models. The paper also introduces three datasets to support further research in factuality and style.

### Strengths and Weaknesses
Strengths:
- The evaluation is thorough, incorporating a wide range of metrics and human assessments, which is unusually comprehensive.
- The proposed IRP framework is innovative and effectively combines pretrained models to enhance expository document generation.
- The analysis of factual errors is particularly insightful and adds depth to the study.

Weaknesses:
- The quality of the newly introduced dataset is difficult to assess quantitatively, limiting the evaluation of the approach.
- The applicability of the IRP method to other domains is not explored, which could restrict its broader relevance.
- The three-step architecture may lead to error propagation.

### Suggestions for Improvement
We recommend that the authors improve clarity in section 3.3 by explicitly stating that training utilizes the ground-truth document, as this could reduce confusion. Additionally, in the "with doc" scenario, please clarify whether only back-translated versions are included and if the original text is excluded. We also encourage the authors to consider the potential benefits of larger models for their approach, even if this is not tested in the current work. Lastly, we suggest revising the terminology in eq 1 and 5 to specify that it refers to Language Modeling loss and reconsider the use of "ensembled" in line 338, opting for "combined" instead.