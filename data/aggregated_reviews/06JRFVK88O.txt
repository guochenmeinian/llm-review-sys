ID: 06JRFVK88O
Title: Mimicking To Dominate: Imitation Learning Strategies for Success in Multiagent Games
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 8, 6, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for multi-agent reinforcement learning (MARL) that addresses training instability and slow convergence by modeling opponents' behaviors through imitation learning. The authors propose reducing uncertainty during training by predicting opponents' next states, providing thorough derivation and analysis. They tested their approach across multiple environments, achieving significant improvements in final rewards compared to state-of-the-art (SOTA) algorithms.

### Strengths and Weaknesses
Strengths:  
- The paper is well-organized, with clear motivation and method introduction, making it easy to follow.  
- The adaptation of IQ-Learn is straightforward, and the derivation process is technically sound.  
- Extensive experiments demonstrate the proposed IMAX-PPO algorithm's effectiveness, achieving the highest expected rewards in most tasks across three environments.  
- The theoretical analysis of the framework is interesting and contributes to the understanding of the method.  

Weaknesses:  
- The training curves in some environments are unstable, raising concerns about the algorithm's stability and the impact of adversarial imitation learning.  
- The assumption that opponents do not learn may limit the framework's applicability.  
- Related work needs improvement, particularly regarding the claim that existing methods require access to opponents' observations, which is not universally true.  
- The presentation suffers from heavy notation and lacks sufficient intuition in key sections.  
- An ablation study is missing, and the framework may require extensive hardcoding for practical implementation.

### Suggestions for Improvement
We recommend that the authors improve the related work section by accurately representing the landscape of opponent modeling in MARL and including missing references. We suggest enhancing the presentation by reducing heavy notation and providing more intuitive explanations of the framework and its objectives. Additionally, we encourage the authors to conduct an ablation study and explore the performance of their method on top of other MARL algorithms. Finally, we advise that the authors consider the implications of opponents learning and address the stability issues observed in training curves, possibly through hyper-parameter tuning or additional training techniques.