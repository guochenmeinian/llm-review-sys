ID: rwbzMiuFQl
Title: Break It Down:  Evidence for Structural Compositionality in Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 8, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on structural compositionality in neural networks, defining it as the ability of networks to decompose complex tasks into modular subroutines. The authors conduct various experiments demonstrating that many models implement these subroutines within their subnetworks, shedding light on compositional generalization in deep learning. The research indicates that unsupervised pretraining enhances compositional structures in language models. Overall, the paper is well-structured and contributes significantly to the field of mechanistic interpretability.

### Strengths and Weaknesses
Strengths:
1. The focus on the composition of different rules rather than downstream generalization is novel and persuasive.
2. The experimental designs are ingenious, effectively linking subroutines to subnetworks through a mask function.
3. The investigation spans various model architectures (ResNet, ViT, BERT) and input types (image and language), enhancing the study's robustness.

Weaknesses:
1. The main results are primarily empirical, raising questions about their generalizability across architectures and tasks.
2. The requirement to specify subroutines in advance limits the applicability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the non-compositional solution panel in Figure 1 by repositioning classes A and B and adding a decision boundary. Additionally, the authors should clarify the bottom-right panel of Figure 1 before the experimental section and correct the filling colors in Figure 2. In Section 7, providing an explanation for ViT's performance on the proposed problem would be beneficial. For Figure 4, adding legends for the symbols used would enhance understanding. Furthermore, we suggest exploring the patterns in subnetworks by drawing masks for different subnets and considering the relationship between their structures. Lastly, we recommend contextualizing the work more thoroughly with respect to Csord√°s et al. (2021) and including relevant references to enhance the discussion on compositionality in neural networks.