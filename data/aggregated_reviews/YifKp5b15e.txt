ID: YifKp5b15e
Title: Near-Optimal Bounds for Learning Gaussian Halfspaces with Random Classification Noise
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 6, 7, 5, 7, 6, -1, -1, -1
Original Confidences: 3, 2, 2, 2, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an efficient algorithm for learning $d$-dimensional halfspaces under random classification noise (RCN) with Gaussian marginals. The authors focus on the in-homogeneous case where the bias $p$ is not fixed. The algorithm achieves a sample complexity of $\tilde{O}(d / ((1 - \eta)\epsilon) + d / \max(p(1 - 2\eta), \epsilon)^2) \log \frac{1}{\delta}$ and includes an initialization procedure, an optimization step using Riemannian subgradient descent, and a testing phase to identify the optimal threshold. The paper also establishes lower bounds on sample complexity, demonstrating the near-optimality of the proposed algorithm.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written, with mathematically interesting bounds and clear proofs.  
- The matching dependence on $1 / p^2$ in the upper and lower bounds provides a strong argument for the near-optimality of the results.  
- The algorithmic contributions are clearly articulated, making the technical overview accessible.

Weaknesses:  
- The pseudocode for Algorithm 1 could be more digestible, and additional details on sample complexity for Initialization and Testing would enhance clarity.  
- There is a substantial gap in $d$ between the upper and lower bounds, which raises questions about potential future work to close this gap.  
- The Optimization procedure's reliance on a leaky ReLU rather than a scaled ReLU could be better justified.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the pseudocode for Algorithm 1 to enhance reader comprehension. Including more details on the sample complexity for Initialization and Testing would also be beneficial. Additionally, addressing the gap in $d$ between the upper and lower bounds in future work could strengthen the paper. We suggest clarifying the rationale behind using a leaky ReLU in the Optimization procedure and considering whether a more sensitive dependence on the classification noise $\eta$ could be established.