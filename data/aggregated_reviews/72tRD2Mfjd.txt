ID: 72tRD2Mfjd
Title: Offline Multitask Representation Learning for Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 1, 2, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Multitask Offline Representation Learning (MORL) algorithm, which aims to enhance sample efficiency in offline multitask reinforcement learning (RL) by learning a shared representation from pre-collected datasets modeled by low-rank Markov Decision Processes (MDPs). The authors provide theoretical results demonstrating the benefits of this method for various downstream RL tasks, including reward-free, offline, and online scenarios. However, the paper lacks empirical evaluation.

### Strengths and Weaknesses
Strengths:
1. The paper addresses the novel problem of offline multitask representation learning in RL, filling a relevant gap in the literature with a well-grounded theoretical approach.
2. It includes rigorous theoretical analysis and detailed proofs supporting the proposed algorithm.
3. The organization and clarity of the writing facilitate understanding of the problem, algorithm, and theoretical results.

Weaknesses:
1. The empirical validation is limited, lacking comparisons with state-of-the-art algorithms.
2. The theoretical proofs require additional explanations or visual aids to improve accessibility for a broader audience.
3. The paper is difficult to follow for readers not specialized in theoretical aspects of RL, and the notation can be confusing.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their presentation by summarizing the main results at the end of Sections 3 and 4 to enhance readability. Additionally, when introducing Equation 3.4, the authors should clarify its purpose related to improving sample complexity through exploration. We suggest using curly braces for notation clarity, particularly for $h \in [H]$, $t \in [T]$, and $i \in [N]$. 

Furthermore, we advise the authors to provide a clearer discussion of the limitations associated with their assumptions, including practical implications. The paper would benefit from a comparison with other multitask RL algorithms and a more explicit discussion of the contributions to downstream online and offline RL. Lastly, we encourage the authors to include visual aids and a proof roadmap to facilitate understanding of the theoretical proofs.