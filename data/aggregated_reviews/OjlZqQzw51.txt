ID: OjlZqQzw51
Title: Percentile Criterion Optimization in Offline Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 5, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel dynamic programming algorithm aimed at optimizing the percentile criterion in offline Reinforcement Learning (RL) using the Value-at-Risk (VaR) Bellman operator. The authors demonstrate that this operator serves as a valid contraction mapping, optimizing a tighter lower bound on the percentile criterion compared to Robust Markov Decision Processes (RMDPs) with Bayesian credible region (BCR) ambiguity sets. The authors argue that their method extends VaR optimization from supervised learning to RL without assumptions on prior distributions. Theoretical analyses and performance bounds are provided, alongside empirical results that showcase the VaR framework's effectiveness across various domains. The authors clarify the theoretical superiority of their VaR framework over existing Bayesian and frequentist methods, particularly in handling epistemic uncertainty.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant problem in offline reinforcement learning by proposing a novel dynamic programming algorithm.
- Comprehensive theoretical analysis supports the proposed VaR framework, including rigorous proofs and performance guarantees.
- Empirical results indicate that the VaR framework outperforms baselines, demonstrating robust performance across multiple tasks and effectively addressing model uncertainty.

Weaknesses:
- Clarity issues arise from inconsistent notation, such as the use of "w" and "w_{s,a}", which may confuse readers.
- The key question regarding the optimality of Bayesian credible regions for the percentile criterion is not clearly defined, lacking explicit criteria for optimality.
- The empirical results may be insufficient, particularly in the comparison with frequentist methods, raising concerns about the practical significance of the improvements.
- The paper does not explore the application of the VaR framework to continuous state-action spaces, limiting its broader applicability.
- The reliance on a precisely estimated transition model $\tilde{p}_{s,a}$ poses challenges in offline RL due to limited datasets, potentially affecting the algorithm's performance in complex tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of notation throughout the paper to avoid confusion, particularly regarding the use of "w" and "w_{s,a}". Additionally, we suggest that the authors clearly define the optimality criteria for Bayesian credible regions in relation to the percentile criterion. It would also be beneficial to discuss the applicability of the VaR framework to continuous state-action spaces to enhance understanding of its potential use in a wider range of problems. Furthermore, we encourage the authors to address the limitations of the transition model estimation and consider providing a more comprehensive empirical evaluation against a broader set of baseline algorithms, including those from the frequentist perspective. Finally, we recommend that the authors include more diverse tasks and baseline comparisons in their empirical evaluation, particularly with frequentist methods, and provide a thorough discussion and comparison with prior works on non-Bayesian VaR methods to strengthen the paper's claims. Clarifying the implications of the Gaussian case and its practical applications could also enhance the argument for the framework's significance.