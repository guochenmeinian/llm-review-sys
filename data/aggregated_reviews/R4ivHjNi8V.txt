ID: R4ivHjNi8V
Title: Efficient and Learnable Transformed Tensor Nuclear Norm with Exact Recoverable Theory
Conference: NeurIPS
Year: 2023
Number of Reviews: 28
Original Ratings: 6, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel Adaptive Tensor Nuclear Norm (ATNN) aimed at improving low-rank tensor recovery through a data-adaptive transformation. The authors claim that their method captures the low-rank structure of tensors more effectively than existing fixed or nonlinear approaches, providing theoretical guarantees for exact recovery. The proposed framework is applied to tensor completion and tensor robust principal component analysis, demonstrating superior performance and efficiency in experiments. The authors assert that ATNN introduces enhancements over SALTS, leading to improved speed and theoretical guarantees, and they emphasize that their work contributes significantly to the tensor nuclear norm field.

### Strengths and Weaknesses
Strengths:
- The paper offers solid theoretical results for a challenging problem, with efficient algorithms for recovery that are well-explained.
- Extensive experiments validate the theoretical claims, showcasing the effectiveness of the ATNN approach, particularly in terms of PSNR and processing time compared to SALTS.
- The adaptive transformation matrix proposed enhances expressiveness compared to fixed transformations, and the authors clarify the differences in regularization methods between ATNN and SALTS.

Weaknesses:
- The paper is dense in certain sections, particularly in the definitions at the end of Section 2, which hampers readability.
- The theoretical novelty is limited, as the proof structure closely follows established methods, and the authors overlook significant related works.
- Reviewers express skepticism regarding the novelty of ATNN, particularly questioning the differences between the equations presented in this paper and those in SALTS, and there are claims of inaccuracies in the authors' characterization of SALTS as an extension of Qrank.
- The performance of ATNN is constrained by its linear transformation, and it does not consistently outperform existing methods like S2NTNN and TCTV.

### Suggestions for Improvement
We recommend that the authors improve the readability of the paper by simplifying the dense sections, particularly the definitions in Section 2. Additionally, it would be beneficial to discuss the practical implications of the strict assumptions made in the theoretical results, such as incoherence and uniformly distributed support sets. We suggest elaborating on the importance of data-dependency in lines 46-52 and clarifying what constitutes "good" performance metrics in this context. Furthermore, we encourage the authors to include a related works section that highlights differences between their approach and existing methods, particularly addressing the computational efficiency and theoretical advancements over SALTS and other adaptive TNN methods. Lastly, we recommend that the authors improve the clarity of their explanations regarding the novelty of ATNN, particularly in relation to Eq.(5) and Eq.(15), and provide further clarifications on the differences between ATNN and SALTS, especially concerning the regularization methods and the implications of the parameter "k." Addressing these points may help alleviate reviewer concerns and enhance the paper's reception.