ID: w4zZNC4ZaV
Title: How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 8, 6, 8, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 1, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
The paper systematically analyzes the impact of instruction tuning on open-source models, particularly focusing on the LLaMa family. It evaluates various instruction datasets and their effects on model performance across multiple tasks, releasing both code and model checkpoints for community use. The authors clarify their definition of instruction tuning, emphasizing its distinction from typical task-specific supervised finetuning, and argue that their findings, while potentially obvious in hindsight, are valuable for the community given the rapid developments in the LLM landscape. Key findings include the nuanced effects of instruction datasets on model performance, particularly that no single dataset optimally enhances all tasks. The paper has been updated to include additional experiments on societal biases and the performance of LLaMa-2 models, demonstrating significant improvements over LLaMa-1.

### Strengths and Weaknesses
**Strengths:**
1. Clear and timely problem formulation addressing the effects of instruction datasets.
2. Comprehensive evaluation providing valuable insights into instruction tuning.
3. Well-presented and systematically analyzes instruction-tuning resources, offering timely results for the community.
4. Release of model checkpoints and data suite, facilitating further research.
5. Well-documented code and models, enhancing reproducibility.
6. Additional evaluations on toxicity and truthfulness, enhancing the paper's contributions.
7. Strengthened correlation investigation in Figure 2, revealing a high correlation between unique token counts and model evaluations.

**Weaknesses:**
1. Limited exploration of the reasons behind performance variations across datasets.
2. Ambiguity in the definition and scope of "instruction tuning," with some initial confusion stemming from the broad definition and title.
3. Lack of societal impact evaluations, particularly concerning bias and toxicity, despite some updates on these aspects.
4. Some findings may not offer new insights to the community, as they align with existing knowledge.
5. Concerns about the paper's fit within the dataset-oriented track, suggesting it leans more towards model-centric analysis.

### Suggestions for Improvement
1. The authors should include evaluations on multi-turn dialogue to address real-world applications of language models.
2. The authors are encouraged to enhance the discussion on societal impacts, including bias and toxicity, potentially using datasets like TruthfulQA.
3. The authors could clarify the rationale for comparing models to Davinci-003, as this may introduce evaluation biases.
4. The authors might consider refining the title to better reflect the content and focus of the study, minimizing potential confusion.
5. The authors should further clarify the distinction between instruction tuning and traditional finetuning in the paper to enhance understanding.
6. The authors should strengthen the correlation analysis in Figure 2 by exploring alternative evaluators and controlling for unique token effects.
7. The authors should reference Appendix A in relevant sections of the main text to improve discoverability of discussions on model-based evaluation limitations.
8. The authors might consider discussing related findings from recent literature to enrich the context of their results.