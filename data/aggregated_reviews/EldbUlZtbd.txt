ID: EldbUlZtbd
Title: Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 8, 7, 8, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an experimental study on the localization claims of causal tracing and editing methods, particularly focusing on ROME. The authors find that localization results are nearly uncorrelated with editing success, revealing that a wide range of MLP layers can be edited effectively, despite localization suggesting that only early to mid-layers store factual information. The authors propose new tasks and datasets to address this discrepancy.

### Strengths and Weaknesses
Strengths:  
- The paper provides a solid empirical study with convincing conclusions supported by experimental evidence.  
- It offers a clear and novel conceptual contribution, particularly the surprising uncorrelatedness of localization and edit success, which has not been previously noted in the literature.  
- The writing is clear and well-structured, with comprehensive experiments that support the findings.

Weaknesses:  
- Minor comments include the need for additional techniques like mean ablation and resampling ablation, which may be more principled than zero ablation.  
- There are questions regarding the noise level choice and its comparison to ROME's methodology.  
- The paper lacks a detailed analysis of the inconsistencies between causal localization and editing performance metrics, particularly in relation to GPT models.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the metrics used for tracing effect and model editing performance, particularly in relation to ROME. Additionally, consider incorporating tracing methods related to other editing problem variants. It would be beneficial to provide a more detailed analysis of the storage of factual knowledge in GPT models and how this influences the design of editing methods. Finally, addressing the conflict regarding layer switching in the revised version could enhance the paper's robustness.