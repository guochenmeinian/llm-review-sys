ID: pC44UMwy2v
Title: Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 6, 7, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Reasoning Granularity (RG) framework aimed at quantifying and optimizing the Chain-of-Thought (CoT) reasoning capabilities of large language models (LLMs). The authors introduce a new metric, RG, to measure the complexity of reasoning tasks and establish a combination law to integrate multiple reasoning tasks, categorizing RG into three types: Completely Feasible, Partially Feasible, and Completely Infeasible. The study validates this framework through extensive experiments on 25 models and 4 tasks, demonstrating the effectiveness of various optimization strategies, including Minimum Acceptable Reasoning Paths (MARP), Tool Usage, and Program-of-Thought (PoT).

### Strengths and Weaknesses
Strengths:
1. The innovative RG framework provides a novel approach to quantify and optimize CoT reasoning in LLMs, leading to a concrete metric for assessing CoT capabilities.
2. The combination law for RG effectively addresses the integration of multiple capabilities for a single task, which is essential for real-world applications.
3. The categorization of RG types aids in systematically optimizing CoT performance, while MARP offers practical solutions to enhance CoT and reduce token consumption.
4. Extensive validation across 25 models and 4 tasks showcases the robustness and applicability of the framework.

Weaknesses:
1. Concerns exist regarding the generalizability of results to other tasks or models not included in the study, as current solutions appear more task/RG-specific.
2. The combination law for RG is based on assumptions that may not universally apply across all reasoning tasks or model architectures, necessitating further empirical validation.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by including a broader range of tasks in their evaluations. Additionally, a more rigorous theoretical analysis of the RG framework would strengthen the paper's contributions. It would be beneficial to provide detailed examples of tasks that fit into each RG category and discuss how these categories influence the modelâ€™s optimization process. Furthermore, we suggest exploring the robustness of the combination law across different reasoning tasks and addressing how to evaluate RG for tasks without an explicit difficulty level. Lastly, incorporating experiments based on models like GPT-4 could help clarify the improvements attributed to the RG framework versus inherent model capabilities.