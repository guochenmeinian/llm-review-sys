ID: yXW2dCTQdi
Title: Controlled maximal variability along with reliable performance in recurrent neural networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a principle for selecting actions to drive recurrent neural network activities, aiming to maximize variability while avoiding unwanted states, defined as states where no action is possible. The authors utilize a reinforcement learning framework to analyze the coupling between action choice and network dynamics, applying their approach to tasks where maximizing entropy within certain boundaries is deemed successful. The paper proposes the maximum occupancy principle (MOP) as a normative theory of neural variability, suggesting that agents maximize future occupancy of their state-action space while avoiding terminal states. Additionally, the authors emphasize the importance of defining goals in terms of terminal states, suggesting that agents learn to avoid these states through environmental interaction. However, concerns are raised about how agents can learn what constitutes a terminal state if encountering it leads to their end, indicating that the focus on terminal states may be excessive.

### Strengths and Weaknesses
Strengths:  
1. The framework is original and well-explained, providing a unique perspective on controlled variability in neural networks without compromising task performance.  
2. The writing is clear, and the experimental results validate the central claims, demonstrating that induced variability does not negatively impact task adherence.  
3. The paper includes specific controls for the proposed mechanism, comparing networks with and without input current modulation.  
4. The paper is technically robust and has moderate-to-high impact potential, with no major concerns regarding evaluation, resources, reproducibility, or ethical considerations.  

Weaknesses:  
1. The tasks are primarily limited to those with several terminating states, raising questions about the generalizability of the approach to more complex reinforcement learning tasks.  
2. The motivation for the paper is unclear, lacking comparisons with experimental data to show how the neural network behavior aligns with natural behaviors or how the proposed reward may be superior.  
3. The argument regarding terminal states is limiting, as many tasks inherently reduce action space when reaching a goal, which should be acknowledged.  
4. The emphasis on "terminal states" may be misplaced, as agents might benefit more from avoiding non-terminal but still detrimental states.  
5. The mechanism by which agents learn about terminal states is not clearly articulated, raising questions about its feasibility.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation by providing comparisons with experimental data to demonstrate how their approach aligns with natural behaviors. Additionally, the authors should explore the implications of their framework in more complex tasks and discuss potential biological inspirations for the proposed mechanism, including neuromodulatory mechanisms. It would also be beneficial to address how the framework could scale to multi-task settings and clarify the connections to existing theories of neural variability. Furthermore, we suggest that the authors improve the clarity regarding the concept of terminal states, particularly addressing how agents can learn to avoid them. It may be beneficial to downplay the focus on terminal states and instead highlight the importance of avoiding surrounding "bad" states that could lead to reduced future state occupancy. Finally, we recommend acknowledging the evolutionary perspective on warning signals in the revised manuscript, as this could enrich the discussion on how biological systems preemptively address terminal states.