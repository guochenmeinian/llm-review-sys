ID: GTDKo3Sv9p
Title: Discrete Flow Matching
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 8, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Discrete Flow Matching, a novel method for generating discrete data, such as language, by utilizing a general family of probability paths between source and target distributions. The authors propose a formula for sampling from these paths using learned posteriors, which enhances generative perplexity compared to prior models. The approach effectively bridges autoregressive models and discrete flow models, achieving notable performance on benchmarks, including HumanEval and MBPP coding tasks. The methodology is inspired by the Flow Matching algorithm and Continuous Time Markov Chains (CTMC), integrating discrete state spaces and time-dependent schedulers into a unified framework.

### Strengths and Weaknesses
Strengths:
- The methodology is solid, with formal analysis and extensive empirical studies demonstrating significant improvements in language modeling, code generation, and image generation tasks.
- The paper is clearly written and well-organized, providing rich technical details about methodologies, including masked source training and probability path schedulers.
- Impressive empirical studies with a model scaled up to 1.7B parameters, showcasing state-of-the-art results.

Weaknesses:
- The method's description lacks clarity, making it difficult to distinguish between the authors' contributions and those of Campbell et al.
- There are unanswered questions regarding the flexibility of the method in handling non-Gaussian target distributions and the definition of posteriors in Eq. 15.
- The paper does not include qualitative analysis of unconditional generation or sufficient discussion of diversity-related metrics.
- Some experiments on common benchmarks used in existing discrete diffusion models are missing, which is important for justifying the effectiveness of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the method's description to better delineate their contributions from previous work. Additionally, please provide a qualitative analysis of unconditional generation in Section H and clarify the handling of non-Gaussian target distributions. We suggest including results on diversity-related metrics and addressing the performance on common benchmarks like LM1B and OWT. Furthermore, it would be beneficial to elaborate on the instantiation of the scheduler, as it is critical for performance. Lastly, please ensure that the code is made available upon publication.