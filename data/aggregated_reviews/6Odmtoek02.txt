ID: 6Odmtoek02
Title: Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantisation.
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 5, 6, 7, 5, -1, -1, -1
Original Confidences: 4, 2, 3, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for quantizing weights using Bayesian neural networks (BNN) through a probabilistic weight fix network (PWFN). The authors propose clustering weights around powers-of-two values, leveraging a probabilistic model where weights follow a Gaussian distribution. The mean ($\mu$) and standard deviation ($\sigma$) of the weights are learned via Bayes-by-backprop (BBP), and quantization is performed iteratively based on these learned parameters. PWFN aims to enhance compressibility and uncertainty quantification in neural networks, demonstrating improved performance over existing methods.

### Strengths and Weaknesses
Strengths:
- The probabilistic representation of weights enhances flexibility and noise resilience, allowing for robust performance.
- The iterative clustering and regularization term contribute to improved performance compared to traditional methods.
- The approach achieves higher quantization efficiency by utilizing a model-level codebook rather than layer or channel-level codebooks.

Weaknesses:
- The computational complexity is significantly increased compared to conventional weight fix networks (WFN), complicating training and hyperparameter exploration.
- The choice of prior and the learning of $\mu$ and $\sigma$ can lead to inconsistent performance, with potential drops in accuracy.
- Insufficient explanations for certain symbols in equations and a lack of clarity regarding figures and specific terms hinder understanding.
- The writing style is somewhat difficult to follow, particularly in method presentation and the explanation of results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manuscript by providing explicit explanations for symbols in equations and ensuring that figures are adequately referenced in the main text. Additionally, the authors should address the training difficulties associated with Bayesian neural networks and discuss the implications of their approach on training time compared to other quantization methods. It would also be beneficial to explore the scalability of the method on more complex downstream tasks and provide detailed ablation studies regarding the sensitivity of results to the chosen prior and clustering steps. Lastly, we suggest using the term 'Quantization' consistently throughout the paper for coherence with existing literature.