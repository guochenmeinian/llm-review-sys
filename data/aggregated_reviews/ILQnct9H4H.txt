ID: ILQnct9H4H
Title: TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TRIGO, a dataset focused on Trigonometric Expression Reduction (TRIGO) to enhance the evaluation of symbolic and numerical reasoning capabilities in Language Models (LMs). The authors propose a novel data generation methodology that integrates expert annotation with a formal environment (Lean), resulting in three subsets: TRIGO-web, TRIGO-real, and TRIGO-gen. The paper includes a thorough evaluation of generative models, particularly GPT-2 and GPT-4, assessing their performance and limitations.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and motivated, contributing valuable resources to the underexplored area of mathematical proof reduction.
- TRIGO is an extensive resource that can support future research in mathematical and symbolic reasoning.
- The original methodology for data generation can inspire further work in the field.
- Empirical evaluations on state-of-the-art models are comprehensive.

Weaknesses:
- The verification of the quality of the generated dataset lacks depth, particularly regarding inter-annotator agreement and quality checks.
- The evaluation primarily focuses on a single family of generative models (GPT), limiting broader applicability; comparisons with open models like T5 would enhance the findings.
- The dataset's size may restrict its use to fine-tuning, and some results are not surprising, raising questions about the novelty of the contributions.

### Suggestions for Improvement
We recommend that the authors improve the discussion on how the quality of the annotated data is verified, including details on inter-annotator agreement and quality checks. Additionally, we suggest incorporating comparisons with other generative models, such as T5, to provide a more comprehensive evaluation. Clarifying the specifics of the dataset curation process, including the sources of additional problems and the roles of annotators, would enhance transparency. Finally, addressing the clarity and fairness of the GPT-4 comparisons in the paper is essential for a more accurate representation of the results.