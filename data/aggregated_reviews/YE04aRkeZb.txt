ID: YE04aRkeZb
Title: $\textbf{A}^2\textbf{CiD}^2$: Accelerating Asynchronous Communication in Decentralized Deep Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel decentralized asynchronous training method, A2CiD2, which effectively decouples communications and computations while minimizing overhead. The method employs a provable, accelerated, randomized gossip procedure based on continuous momentum and time, demonstrating superior performance over existing methods like AllReduce-SGD and AD-PSGD on ResNet benchmarks with 64 asynchronous A100 GPUs. Key contributions include extending the asynchronous decentralized training framework to non-convex settings and implementing the A2CiD2 mechanism to enhance communication efficiency. Additionally, the authors provide a quantitative analysis of their algorithm for decentralized optimization, emphasizing that it does not adopt an asymptotic approach and that dataset sizes do not affect convergence analysis. They argue that the theoretical attributes of their algorithm align well with experimental outcomes, which they plan to illustrate with a figure if permitted.

### Strengths and Weaknesses
Strengths:
1. The proposed A2CiD2 algorithm significantly improves communication speed and minimizes the gap between centralized and decentralized setups, making it suitable for large-scale machine learning tasks.
2. The study broadens the analytical framework for decentralized deep learning, providing new insights into asynchronous training.
3. The method has been validated through both theoretical analysis and empirical experiments, with code provided for reproducibility.
4. The authors have addressed most reviewer concerns, leading to an improved score.
5. The experimental setup utilizes a robust computing environment and datasets, demonstrating the algorithm's effectiveness in challenging scenarios.

Weaknesses:
1. The theoretical analysis lacks clarity regarding the tightness of the upper bound and its correlation with experimental results for smaller sizes.
2. The evaluation of A2CiD2 is limited to 64 asynchronous GPUs, which may not represent diverse real-world hardware configurations.
3. There is insufficient comparison with other distributed training methods regarding computational cost and performance, which is necessary to assess the proposed method's advantages.
4. The method's requirement for an additional model copy on each worker raises concerns about its applicability to larger models, particularly in memory-constrained environments.
5. The connection between theoretical and experimental analyses is perceived as weak, with gaps noted between results.
6. The choice of baselines for comparison is criticized, as AD-PSGD is considered outdated, and the lack of advanced methods in the experiments is highlighted.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical analysis in Section 3.3, particularly regarding the upper bound's tightness and its relationship with experimental results. A broader evaluation of the A2CiD2 method across various hardware configurations and scales should be conducted to enhance its applicability. Additionally, we suggest including a detailed comparison with existing distributed training methods in terms of computational cost and communication overhead. We also recommend improving the clarity of the paper's structure and strengthening the connection between theoretical and experimental analyses. Increasing the experimental component, even at the expense of theoretical analysis, could enhance the paper's impact. Furthermore, we encourage the authors to consider incorporating more advanced baselines for comparison, such as DADAO, despite its limitations in deep learning contexts, to address reviewer concerns. Finally, addressing the memory overhead issue and exploring the method's applicability to larger models, such as Transformers, would strengthen the paper's contributions.