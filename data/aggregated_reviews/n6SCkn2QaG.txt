ID: n6SCkn2QaG
Title: The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 7, 8, 7, -1
Original Confidences: 4, 5, 5, 3, -1

Aggregated Review:
### Key Points
This paper presents the FineWeb dataset, a 15-trillion token collection derived from 96 Common Crawl snapshots, aimed at training large language models. It introduces FineWeb-Edu, a filtered subset containing educational content, and conducts detailed ablation studies to demonstrate the impact of various design choices on performance across eight benchmark tasks. The authors make significant observations regarding deduplication and the quality of text in relation to duplication rates.

### Strengths and Weaknesses
Strengths:
- The paper provides comprehensive documentation of the dataset curation process, enhancing reproducibility.
- FineWeb is one of the largest and highest quality open datasets available, likely to support robust training for language models.
- The writing is clear and accessible, making the content easy to follow.
- The datasets show improved performance on benchmarks like MMLU and ARC, validating their effectiveness.

Weaknesses:
- The evaluation protocol relies on a relatively narrow set of benchmarks, limiting the scope of the findings.
- The aggregation of benchmark scores as averages may misrepresent the data due to differing scales.
- The filtering methodology for FineWeb-Edu lacks comparison with other approaches, and concerns exist regarding potential licensing issues related to the classifier used.

### Suggestions for Improvement
We recommend that the authors broaden the evaluation scheme to include additional benchmarks and consider evaluating perplexity across different domains. It would be beneficial to present a more holistic aggregation of scores, such as using normalized scores or rank sums. Additionally, providing more statistics on dataset composition, including topic distribution, would strengthen the paper. Clarifying the ablation experimental conditions and releasing progressively processed dataset versions would enhance reproducibility. Finally, addressing the filtering methodology for FineWeb-Edu in comparison to legacy approaches and elaborating on the classifier training process would provide valuable insights.