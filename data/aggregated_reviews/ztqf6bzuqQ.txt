ID: ztqf6bzuqQ
Title: Hybrid Distillation: Connecting Masked Autoencoders with Contrastive Learners
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 4, 5, 4, 5, 6, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 5, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new hybrid distillation method for Vision Transformers that combines two teacher models pre-trained through Contrastive Learning (CL) and Masked Image Modeling (MIM). This approach aims to leverage the diversity from the MIM teacher and the discriminability from the CL teacher. Additionally, a progressive masking strategy is introduced to minimize redundancy during distillation. The effectiveness of the proposed method is demonstrated through experiments on various classification, detection, and segmentation benchmarks.

### Strengths and Weaknesses
Strengths:
- The concept of utilizing two homogeneous teacher models is intriguing, allowing the student model to benefit from both teachers' strengths.
- The paper is well-written and presents a solid experimental design, achieving significant accuracy improvements over existing methods.

Weaknesses:
- Many experimental details are lacking, particularly in Section 2, making it difficult to evaluate the soundness of the experiments. Key questions remain about the distillation approach, the relevance of metrics like average head distance and normalized mutual information, and the clarity of notations.
- The paper does not adequately clarify whether the student model is trained solely by the distillation objective or if a supervised training objective is also employed.
- There are discrepancies in reported baseline results compared to existing literature, raising concerns about the empirical gains of the proposed method.
- The presentation quality is poor, with insufficient explanations of the experimental setup and metrics, and a lack of discussion on related works.

### Suggestions for Improvement
We recommend that the authors improve the clarity of experimental details in Section 2, including the specific distillation approach used and the training objectives. It is crucial to provide a clearer explanation of the metrics employed and their relevance to downstream performance. Additionally, we suggest addressing the discrepancies in baseline results by providing a detailed comparison with existing literature. The authors should also ensure consistent terminology regarding "asymmetric" concepts and clarify any notations introduced in the paper. Finally, a more thorough discussion of related works and fair comparisons with strong baselines that utilize multiple networks for distillation would enhance the evaluation of the proposed method.