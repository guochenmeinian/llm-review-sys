ID: cgmlfA1sPl
Title: Late Fusion of Transformers for Sentiment Analysis of Code-Switched Data
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach for sentiment analysis of code-switched data, utilizing a late fusion technique that combines the outputs of two fine-tuned transformer models. The authors evaluate their method on English-Hindi and English-Spanish datasets, demonstrating improved performance compared to single-model approaches. 

### Strengths and Weaknesses
Strengths:  
- The paper shows a clear improvement on the datasets used for sentiment analysis.  
- The methodology is presented clearly, making it easy to understand and reproduce.  
- The approach utilizes well-known public datasets and discusses hyperparameters adequately.  
- The proposed method outperforms the baseline and is well-justified for the problem.

Weaknesses:  
- There is insufficient empirical evidence supporting the effectiveness of the proposed model, with a lack of experiments involving additional models and diverse datasets.  
- The focus on a single dataset and comparison against a single 2020 model raises concerns about generalizability.  
- The methodology lacks clarity, particularly regarding how the models are fine-tuned before fusion.  
- The paper is perceived as incremental, not introducing novel solutions that leverage context.  
- The advantage of not using ensemble techniques is not adequately explained.  
- There are contradictory statements in the literature review regarding data availability and transformer performance with less training data.

### Suggestions for Improvement
We recommend that the authors improve the empirical support for their model by conducting additional experiments with more models and diverse datasets. Clarifying the methodology, especially how the models are fine-tuned before fusion, is essential. We suggest addressing the perception of the work as incremental by exploring novel solutions that leverage context. Additionally, the authors should explain the advantages of not using ensemble techniques more clearly. Finally, resolving the contradictory statements in the literature review regarding data availability and transformer performance would enhance the paper's coherence.