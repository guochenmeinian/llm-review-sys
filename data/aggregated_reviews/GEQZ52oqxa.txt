ID: GEQZ52oqxa
Title: $L_2$-Uniform Stability of Randomized Learning Algorithms: Sharper Generalization Bounds and Confidence Boosting
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 6, 7, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new notion of algorithmic stability termed *L2-uniform stability*, which serves as an intermediate framework between on-average stability and high probability stability. The authors establish generalization bounds for randomized algorithms, particularly focusing on stochastic gradient descent (SGD) under convex loss functions. The results leverage this new stability notion to derive sharper generalization bounds, including high probability bounds through boosting techniques.

### Strengths and Weaknesses
Strengths:
- The introduction of *L2-uniform stability* is a significant technical advancement that leads to stronger generalization bounds compared to existing methods.
- The paper is well-written, particularly the introduction, which is accessible even to those less familiar with algorithmic generalization.
- The results for SGD and other meta-algorithms are interesting and potentially impactful for the machine learning community.

Weaknesses:
- The clarity and structure of the paper need improvement, as some sections are confusing and contain typos.
- The comparison of Theorem 1 and (4) is questionable due to the stronger assumptions of the *L2* case, and the paper lacks a detailed analysis of lower bounds.
- The results in Section 4 are criticized for being weak, with slow decay rates that may limit their practical relevance.

### Suggestions for Improvement
We recommend that the authors improve the clarity and structure of the paper by providing formal definitions and correcting typos, such as elaborating on specific terms like $\mathcal{Z}$ and $\mathcal{R}$. Additionally, we suggest that the authors clarify the redundancy in the definition of $\gamma_{L_2,N}$ and provide a more rigorous comparison of Theorem 1 with known lower bounds. 

Furthermore, we encourage the authors to address the questions raised regarding the increasing upper bound in Theorem 2, the potential for deriving sub-gaussian bounds, and the implications of their results for standard algorithms beyond boosted versions. Lastly, we advise revising Section 4 to highlight the novel insights regarding the L2-uniform stability parameters and their implications for SGD.