ID: h2lkx9SQCD
Title: Faster Differentially Private Convex Optimization via Second-Order Methods
Conference: NeurIPS
Year: 2023
Number of Reviews: 26
Original Ratings: 5, 5, 5, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on differentially private (DP) convex optimization using second-order methods. The authors propose a private variant of the cubic regularized Newton method, demonstrating its quadratic convergence and optimal excess loss for strongly convex functions. Additionally, they introduce a second-order algorithm specifically for logistic regression, which significantly outperforms existing methods like DP-GD and DP-SGD, achieving speedups of 10-40 times. The authors argue that while second-order methods may initially seem brittle in the presence of noise, they can actually accelerate optimization significantly when close to the optimum, challenging conventional beliefs. They acknowledge the complexities of comparing their results with first-order methods, particularly regarding diameter dependence and performance in non-convex optimization.

### Strengths and Weaknesses
Strengths:
- The paper addresses the under-explored area of second-order methods in DP optimization, providing novel algorithms and theoretical results.
- The proposed methods achieve optimal excess loss and demonstrate significant computational efficiency in empirical tests.
- The authors provide a compelling argument for the effectiveness of second-order methods with DP, contributing to the optimization theory literature.
- The paper is well-organized, with rigorous proofs supporting the claims made and practical performance concerns addressed through experimental results.

Weaknesses:
- The theoretical justification for the benefits of second-order methods is insufficient, particularly when first-order methods can achieve similar rates in linear time.
- The convergence analysis for logistic regression lacks completeness, with no specific rates or comparisons to first-order methods provided.
- The comparison of diameter dependence between first-order and second-order methods lacks clarity, particularly regarding the implications of strong convexity assumptions.
- The computational complexity of the proposed methods may be high due to the requirement of second-order information, which could hinder scalability in large datasets.
- The paper does not initially consider the potential benefits of combining first-order and second-order methods for optimization.
- The presentation is awkward, with critical information relegated to the appendix, making it difficult for readers to follow.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the advantages of second-order methods over first-order methods, particularly in terms of oracle complexity. Additionally, we suggest providing a more comprehensive convergence analysis for the logistic regression case, including specific rates and comparisons with first-order methods. The authors should clarify the computational costs associated with the second-order methods, particularly in large-scale settings. We also recommend improving the clarity of the discussion on diameter dependence, particularly in relation to the assumptions made in existing literature. Furthermore, we suggest including a discussion on the strategy of running a first-order method for initial iterations before switching to their second-order method, as this could enhance the practical applicability of their approach. Finally, we encourage the authors to address the awkward presentation by integrating key information into the main text rather than the appendix and to explicitly mention the limitations of their methods in the revised paper to adequately address reviewer concerns.