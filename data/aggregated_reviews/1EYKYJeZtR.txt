ID: 1EYKYJeZtR
Title: Large language models transition from integrating across position-yoked, exponential windows to structure-yoked, power-law windows
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 8, 3, 8, 5, 8, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 3, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the temporal integration windows of transformer language models, particularly focusing on GPT-2. The authors evaluate the impact of word swaps on activations across layers, characterizing the integration curves as a convex combination of exponential and power-law functions. They explore whether these windows are static to token proximity or dynamically tied to sentence boundaries, finding that later layers are sensitive to these linguistic structures.

### Strengths and Weaknesses
Strengths:  
- Excellent problem presentation within the context of related work.  
- The core “word swap” approach is well-designed and motivated, with carefully constructed experiments.  
- Clear writing and organization, with important results presented and discussed appropriately.  
- The study provides empirical evidence for the robustness of results across different models and experimental setups.  

Weaknesses:  
- The motivation for the proposed functional form is underspecified, lacking detail on how the convex combination was determined.  
- The analysis does not rigorously evaluate alternative fits or consider the broader research literature on power laws.  
- The focus on only three language models limits the extensiveness of the analyses, and the use of the Brown corpus raises questions about the generalizability of the findings.  
- The experimental setup is somewhat difficult to follow, and the paper could benefit from clearer explanations of the methods and data sources.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation for the functional forms used in the analysis, providing more detail on the selection process and exploring a broader range of functional forms. Additionally, we suggest including rigorous comparisons of different model fits and testing the approach on a more diverse set of linguistic materials. It would also be beneficial to clarify the experimental setup, potentially by extracting detailed methodologies into a separate section and highlighting findings per experiment. Finally, consider extending the analysis to higher-order structures beyond sentences to demonstrate hierarchical dependence.