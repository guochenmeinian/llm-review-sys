ID: 1IU3P8VDbn
Title: Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates whether large language models (LLMs) can perform causal reasoning at a human-like level by incorporating contextual information. The authors propose a distinction between 'shallow' (level-1) and 'human-like' (level-2) causal reasoning and introduce the "CausalProbe-2024" dataset for benchmarking level-2 reasoning capabilities. They argue that many causal inferences rely on general knowledge and intention, presenting the $G^2$-Reasoner to enhance reasoning by integrating general knowledge and goals. Evaluation of multiple LLMs on this dataset reveals a significant performance drop compared to previous benchmarks, although the $G^2$-Reasoner shows improvement over naive approaches. The authors acknowledge that their definitions for level-1 and level-2 causal reasoning lack concreteness and clarity, and they identify the formulation for Eq. (2) as a primary limitation, particularly concerning the guarantee of total probability. They also address the relationship between current and future values in LLMs, noting that while the autoregressive mechanism suggests independence, the diversity in training data may allow for implicit learning of causal relationships.

### Strengths and Weaknesses
Strengths:
- The authors provide a valid contribution by assessing LLMs' causal reasoning capabilities in context, enhancing realism and difficulty compared to existing benchmarks.
- The dataset is constructed from recent news articles, ensuring it contains novel information not present in the training sets of the tested LLMs.
- The paper clearly describes the experimental setup and the steps of the $G^2$-Reasoner, including prompt templates and comparisons to other approaches.
- The paper tackles an interesting and timely problem, contributing to the understanding of causal reasoning in LLMs.
- It proposes a novel benchmark and provides insightful analyses and methodological contributions.
- The authors demonstrate a commitment to improving the paper based on reviewer feedback, including the introduction of a new dataset and detailed responses to comments.

Weaknesses:
- The paper lacks clarity on the impact of additional context on cause-effect pairs, particularly in Sec. 5 / Fig. 4.
- The distinction between level-1 and level-2 reasoning is vague, and the $G^2$-Reasoner does not present explicit mechanisms for improving causal reasoning beyond providing general background knowledge.
- Concerns about the quality of the dataset arise, as multiple valid answers can exist for some questions, questioning the significance of the results.
- The partitioning of samples into CausalProbe-S and -H lacks clear criteria for selection.
- The evaluation results do not quantify the variance introduced by different temperature settings in LLMs.
- The formulation of Eq. (2) is a significant limitation, raising concerns about the total probability guarantee.
- The statement regarding the independence of current values from future values is overly simplistic and requires clarification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions and examples for level-1 and level-2 reasoning to enhance understanding. Specifically, the authors should refine their description of level-2 causal reasoning to emphasize the generation of new causal insights. Additionally, please provide a detailed explanation of the role of equation (1) in the $G^2$-Reasoner and how it can be computed. We suggest revising the formulation for Eq. (2) to address its limitations more thoroughly. We also recommend incorporating a human review process for quality checking the dataset to address concerns about answer validity. Furthermore, clarify the criteria used for partitioning samples into CausalProbe-S and -H. Lastly, we encourage the authors to quantify the variance introduced by temperature settings in their evaluations to strengthen the reliability of their findings and clarify the relationship between current and future values in LLMs, ensuring that the discussion accurately reflects the complexities of the autoregressive mechanism and the implications of diverse training data.