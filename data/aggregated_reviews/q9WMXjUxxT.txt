ID: q9WMXjUxxT
Title: Trust Region-Based Safe Distributional Reinforcement Learning for Multiple Constraints
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a safe reinforcement learning (RL) algorithm called SDAC, designed to handle multiple constraints in safety-critical robotic tasks. The authors propose a gradient integration method to address infeasibility issues and a TD($\lambda$) target distribution to estimate risk-averse constraints. Experimental results indicate that SDAC outperforms existing safe RL baselines, achieving fewer constraint violations and faster constraint satisfaction.

### Strengths and Weaknesses
Strengths:
- The formulation of the safe RL problem with multiple constraints is significant for real-world applications.
- The approach explicitly addresses infeasibility issues in constrained optimization.
- The proposed risk-averse safety measures are practical for safety-critical scenarios.
- The writing quality is generally good, and the paper includes sufficient technical details for understanding and reproduction.

Weaknesses:
- The contributions of the paper are not clearly demonstrated, as performance improvements and constraint satisfaction compared to baselines are not convincing.
- The two main contributions, gradient integration and TD($\lambda$), appear unrelated, necessitating a clearer explanation of their combined impact on safe RL.
- The writing could be improved by providing more background information and intuitions, particularly regarding the trust-region method and the equations presented.
- The experimental results do not adequately support the claims, as some baselines outperform the proposed method.
- The algorithm's performance is sensitive to parameters like $\alpha$ and $\lambda$, and there is a lack of guidance on their selection.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing more background information and intuitions behind key equations, particularly in Section 2 and Section 3.1. Additionally, the authors should better demonstrate the contributions of gradient integration and TD($\lambda$) by explaining their interrelation and collective impact on safe RL. We suggest enhancing the experimental section to include comparisons with relevant baselines, such as PPO Lagrangian, and to provide a more thorough analysis of the algorithm's performance under varying parameter settings. Furthermore, addressing the concerns regarding the fragility of the gradient integration method and its implications for finite-time convergence would strengthen the paper. Lastly, we encourage the authors to clarify the necessity of the policy entropy term and its implications within the trust-region framework.