ID: jSO7Vgolc6
Title: FELM: Benchmarking Factuality Evaluation of Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 8, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FELM, a new factuality evaluation benchmark designed to assess the factuality of large language models (LLMs) across five domains: world knowledge, science and technology, mathematics, writing and recommendation, and reasoning. The authors argue that FELM addresses limitations in existing benchmarks by incorporating diverse domains and advocating for segment-level analysis. The benchmark consists of 817 data instances, which are derived from diverse datasets and broken down into 3,948 segments for detailed evaluation. The authors define four error types based on manual checks of ChatGPT's failures, asserting that all factual errors can be classified within these categories. Experiments with ChatGPT and GPT-4 indicate that FELM is a challenging benchmark for evaluating factuality, while the authors also address concerns regarding the reliability of knowledge sources and the benchmark's applicability to other LLMs.

### Strengths and Weaknesses
**Strengths:**
- The benchmark evaluates factuality across various domains, enhancing the understanding of LLM performance.
- The use of human annotators for fine-grained labeling and error categorization adds depth to the evaluation.
- The rigorous annotation process involving expert annotators and the comprehensive evaluation of the benchmark's safety and validity confirm the reliability of reference links.
- The paper is well-structured and presents a clear methodology for data generation and annotation, including improvements based on reviewer feedback, such as a flowchart for clarity and self-consistency results that enhance performance metrics.

**Weaknesses:**
- The benchmark's size is limited to 817 samples, raising questions about its adequacy for multi-domain evaluation.
- The distinction between domains, particularly regarding "reasoning" and "math," is somewhat ambiguous.
- The reliance on LLMs for factuality evaluation may lead to misleading results due to their inherent limitations.
- There is a potential performance gap when applying FELM to evaluate other LLMs, as it is primarily based on ChatGPT responses.

### Suggestions for Improvement
We recommend that the authors improve the clarity of domain definitions, particularly in distinguishing between "reasoning" and "math." Additionally, we suggest including non-proprietary models, such as Alpaca or Falcon, to enhance reproducibility and generalizability of findings. It would be beneficial to provide a comparison of FELM with existing benchmarks to contextualize its performance. Furthermore, we encourage the authors to explore the inclusion of automatic metrics for factuality assessment, such as hallucination critics, to complement the current evaluation metrics. We also recommend improving the clarity of Section 4.1 by ensuring that the flowchart effectively illustrates the experimental settings. Additionally, the authors should explore the inclusion of more diverse LLM responses in FELM to mitigate potential performance gaps. Considering the implementation of fine-grained annotations for training reward models could enhance model performance. Lastly, we encourage the authors to report hyperparameters and the monetary costs associated with running experiments more transparently to enhance interpretability for readers.