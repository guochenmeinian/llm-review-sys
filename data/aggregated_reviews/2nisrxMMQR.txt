ID: 2nisrxMMQR
Title: Meta-Exploiting Frequency Prior for Cross-Domain Few-Shot Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 7, 6
Original Confidences: 4, 5, 5

Aggregated Review:
### Key Points
This paper presents a novel framework for cross-domain few-shot learning (CD-FSL) that utilizes frequency decomposition of images into low-frequency content and high-frequency structure via FFT. The PRM-Net architecture comprises three branches—low-frequency, high-frequency, and a main branch—integrating two priors to regularize the feature embedding network. The approach demonstrates state-of-the-art performance across multiple benchmarks, supported by extensive empirical validation.

### Strengths and Weaknesses
Strengths:
1. The frequency decomposition method is straightforward and implementable.
2. The framework is rigorously evaluated on various benchmarks, achieving state-of-the-art results.
3. The paper provides a clear and detailed description of its components, enhancing reproducibility.

Weaknesses:
1. Although the method avoids additional inference costs, the decomposition and additional branches may introduce significant computational overhead, and the improvement from the reconstruction prior appears marginal.
2. There is a lack of proper citation for datasets mentioned in section 3.1, such as CropDisease, CUB, and ISIC.
3. The reliance on fixed decomposition strategies like FFT may not be optimal in scenarios with similar foreground and background colors or complex backgrounds, and the model cannot be trained end-to-end due to FFT limitations.
4. The paper lacks a detailed analysis of computational complexity, including total floating-point operations (FLOPs) during training and inference.
5. There is insufficient exploration of the choice of backbone network and loss functions, which is crucial for understanding their contributions to performance.

### Suggestions for Improvement
We recommend that the authors improve the analysis of computational complexity by including a detailed evaluation of total floating-point operations (FLOPs) during both training and inference phases. Additionally, we suggest incorporating more extensive ablation studies on the choice of backbone network and loss functions to clarify the rationale behind these decisions. Furthermore, the authors should address the assumption regarding the effectiveness of low and high-frequency components in mimicking distribution shifts, particularly in unrelated datasets. Exploring alternative image decomposition methods and their impact on performance could also enhance the framework's applicability. Lastly, we encourage the authors to provide insights into the model's performance over extended training periods and consider integrating relevant experimental content from supplementary materials into the main text for clarity.