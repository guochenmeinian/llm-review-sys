ID: lENeWLXn4W
Title: A New Linear Scaling Rule for Differentially Private Hyperparameter Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 32
Original Ratings: 3, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel hyperparameter tuning method that operates within a privacy budget by linearly extrapolating from observations with minimal privacy loss. The authors propose a linear scaling rule for optimizing the learning rate and training steps in differentially private stochastic gradient descent (DP-SGD). The method is primarily empirical, demonstrating promising results on benchmark datasets like CIFAR-10 and CIFAR-100, where it achieves an average accuracy of 89%, improving over random search by 4%. The authors emphasize the importance of their approach in enabling efficient hyperparameter tuning without compromising accuracy and provide comprehensive instructions for running their code, including the use of the AWS library "fast-differential-privacy" and the option to utilize Opacus. However, the paper lacks robust theoretical backing and strong baseline comparisons.

### Strengths and Weaknesses
Strengths:
- The core technique addresses a significant gap in the literature regarding hyperparameter tuning under privacy constraints.
- The proposed linear scaling rule effectively reduces compute and privacy costs, facilitating faster research progress.
- The method demonstrates robust performance across multiple datasets and fine-tuning tasks in both computer vision (CV) and natural language processing (NLP) domains.
- The paper is generally well-written and presents a clear methodology, with extensive empirical evaluation supporting the proposed method.
- Comprehensive instructions for running the code enhance reproducibility.

Weaknesses:
- The baselines used for comparison are insufficiently strong and do not align with cited literature, raising concerns about the validity of the results.
- The average accuracy of the random search baseline is not particularly high, and the standard deviation is relatively large, indicating potential inefficiencies.
- The paper lacks clarity on the use of RDP versus GDP, leading to confusion regarding privacy guarantees.
- The contribution may appear limited, as results align closely with existing literature, and the sensitivity of the proposed method to hyperparameter choices raises questions about its robustness.
- The contribution is thin, with insufficient theoretical justification for the proposed scaling rule and vague conclusions regarding its robustness to distribution shifts.

### Suggestions for Improvement
We recommend that the authors improve the strength of their baselines by including comparisons to naive baselines and upper bounds on hyperparameter tuning efficacy. Clarifying the use of RDP versus GDP and providing a more detailed explanation of privacy loss accounting in Algorithm 1 would enhance the technical rigor. Additionally, we suggest expanding the hyperparameter search space to include a logarithmic grid over several orders of magnitude, which may enhance the robustness of the results. It would also be beneficial to provide a more detailed analysis of the hyperparameter tuning process and its implications for performance, especially in difficult cases. Finally, addressing the concerns regarding the sensitivity of the method to hyperparameter choices and refining the search algorithm to minimize the evaluation of ineffective hyperparameter values would strengthen the paper's argument.