ID: xJLEQQrFia
Title: Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 7, 6, 4, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the KARD model, which integrates knowledge distillation and knowledge base (KB) retrieval to enhance small language models (LMs) in knowledge-intensive question-answering (QA) tasks. The authors propose a method where a large language model (LLM) generates rationales that guide a retriever to obtain relevant documents, which are then used to fine-tune a smaller LM. The approach is evaluated on two QA datasets, MedQA-USMLE and StrategyQA, demonstrating superior performance compared to various baselines, particularly for smaller models.

### Strengths and Weaknesses
Strengths:
- The proposed method is well-motivated and demonstrates originality by combining reasoning distillation with knowledge augmentation.
- The experimental results are convincing, showing significant improvements in performance, especially for smaller LMs.
- The paper is well-written, with clear explanations and detailed analyses of model configurations.

Weaknesses:
- The experimental design lacks comparisons with other knowledge-augmented LMs, which may lead to inadequate assessments of KARD's contributions.
- The use of a generic knowledge base like Wikipedia may not be optimal for domain-specific tasks such as MedQA-USMLE.
- The gains in performance become marginal with larger model sizes, raising questions about the consistency of results across datasets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental design by explicitly stating the type of retriever used in baseline methods, including whether they are dense retrievers or BM25. Additionally, conducting comparisons with more neural retrieval-augmented language models across various datasets, such as NQ and TriviaQA, would provide a more comprehensive evaluation of KARD's effectiveness. We suggest exploring the use of specialized medical knowledge bases for the MedQA-USMLE task to enhance performance. Furthermore, addressing the potential issue of shortcut reasoning by evaluating the faithfulness of the rationales through simulation-based metrics could strengthen the findings. Lastly, we encourage the authors to analyze the dataset characteristics to better interpret the performance discrepancies observed between the two QA datasets.