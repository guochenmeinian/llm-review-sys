ID: FM81CI68Iz
Title: FedL2P: Federated Learning to Personalize
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 6, 5, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to federated learning by introducing a method for optimizing hyperparameters, particularly in non-IID data scenarios. The authors propose using Batch Normalization (BN) hyperparameters to address feature shifts and employ a Jacobian product to approximate the Hessian inverse for hyper gradient calculations. The study utilizes the CIFAR-10 dataset and demonstrates that their method enhances personalized performance while maintaining global model efficacy. Additionally, the authors introduce a meta-network that outputs optimal hyperparameters based on client data, showcasing its effectiveness across multiple datasets.

### Strengths and Weaknesses
Strengths:
- **Originality:** The paper's unique approach to handling non-IID data through BN hyperparameters and the Jacobian product is innovative and addresses significant challenges in federated learning.
- **Quality:** The research methodology is robust, with thorough experimental setups and validation using standard datasets, although broader evaluations could enhance its quality.
- **Clarity:** The paper is well-structured, providing clear descriptions of methodologies and findings, making it accessible to a wide audience.
- **Significance:** The contributions to optimizing hyperparameters in federated learning are substantial, potentially leading to more efficient training processes and inspiring future research.

Weaknesses:
- **Dataset and Model Constraints:** The reliance on specific datasets limits the generalizability of the findings; testing across diverse datasets and model architectures is necessary.
- **Comparison to Other Techniques:** A lack of comprehensive comparisons with state-of-the-art methods obscures the relative performance of the proposed approach.
- **Computational Overhead:** The computational cost associated with the Jacobian product remains significant compared to existing methods.
- **Applicability:** The universal applicability of the method is questioned, necessitating a discussion of limitations and potential solutions.
- **Reproduction:** The absence of available code hinders reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the experimental setup clarity, particularly regarding data usage and client participation in federated learning steps. A broader evaluation across various datasets, including non-IID and non-image datasets, would strengthen the findings. Additionally, we suggest providing a more comprehensive comparison with other state-of-the-art methods to clarify the advantages of their approach. Discussing the limitations of their method and potential solutions would offer a more balanced perspective. Finally, making the code available would enhance reproducibility and facilitate further research.