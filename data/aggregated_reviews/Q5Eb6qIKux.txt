ID: Q5Eb6qIKux
Title: VanillaNet: the Power of Minimalism in Deep Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 5, 6, 7, 5, 8, -1, -1, -1, -1
Original Confidences: 4, 2, 5, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VanillaNet, a neural network architecture that emphasizes simplicity by avoiding high depth, shortcuts, and complex operations like self-attention. The authors propose a "deep training" strategy that begins with several layers containing non-linear activation functions, which are progressively eliminated during training. Additionally, they introduce a series-based activation function to enhance the model's non-linearity. Experimental results demonstrate that VanillaNet achieves comparable performance to state-of-the-art deep models while being more efficient and resource-friendly.

### Strengths and Weaknesses
Strengths:
1. The proposed VanillaNet architecture is novel and challenges the prevailing belief that increased complexity leads to improved performance.
2. The paper is well-structured and clear, with detailed explanations and extensive experiments validating the effectiveness of VanillaNet.
3. The proposed deep training strategy and series activation functions are innovative and contribute to the model's performance.

Weaknesses:
1. VanillaNet has significantly more parameters and FLOPs than some existing methods, which may complicate training and deployment on resource-constrained devices.
2. The merging operation described in section 3.1 lacks a visual representation, making it difficult to identify merging opportunities.
3. There is a contradiction regarding the importance of FLOPs; the authors claim it is not important but later calculate complexity tied to FLOPs.
4. The latency measurements in Table 4, profiled with a batch size of 1, could be misleading; throughput or latency with larger batch sizes should also be provided.
5. The paper lacks a clear discussion of limitations, particularly regarding the increased parameter count and FLOPs.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the merging operation by including a figure that illustrates where it occurs in the model. Additionally, please clarify the distinction between novel designs and existing methods, particularly regarding Batch Normalization Folding. To resolve the contradiction about FLOPs, we suggest comparing the latency of the proposed activation functions and convolutions instead. Furthermore, we advise providing latency measurements using larger batch sizes to better reflect real-world scenarios. Lastly, we recommend including a limitations section that discusses the implications of the higher parameter count and FLOPs associated with VanillaNet.