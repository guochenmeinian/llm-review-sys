ID: y5duN2j9s6
Title: On the Importance of Exploration for Generalization in Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 4, 7, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration method for contextual MDPs (CMDPs) called EDE (Ensemble Distributional Exploration), which emphasizes the significance of exploration for generalization in reinforcement learning (RL). The authors propose that inadequate uncertainty estimation in prior methods hinders generalization and address this by separating aleatoric and epistemic uncertainties. EDE utilizes an ensemble of quantized Q-functions to estimate epistemic uncertainty and applies this uncertainty in a UCB-style exploration algorithm. The method is evaluated on ProcGen and Crafter environments, demonstrating improved performance over epsilon-greedy baselines and QRDQN, while highlighting the importance of modeling epistemic uncertainty. However, the connection between the exploration method and the CMDP insights is not clearly established, and the paper does not claim that EDE outperforms all existing methods universally.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and extensively referenced, providing a solid foundation for its claims.
- The insight that exploration plays a crucial role in CMDP generalization is compelling and well-articulated.
- The empirical results show that EDE outperforms epsilon-greedy methods and QRDQN in several ProcGen environments, supporting its effectiveness.
- The authors effectively motivate the need for exploration in enhancing generalization within RL.

Weaknesses:
- The relationship between the exploration method and the CMDP insights is weak; EDE does not leverage the specific insights from the first part of the paper.
- The novelty of EDE is questionable, as it appears to be a straightforward combination of existing methods without substantial innovation.
- The theoretical justification for why EDE explores better in specific situations is lacking, and the exploration-exploitation tradeoff is not adequately discussed.
- Comparisons to other exploration methods are limited, which may weaken the argument for EDE's superiority.
- The experiments lack important baselines, and the performance improvements of EDE over existing methods are not convincingly significant.
- Some issues raised by reviewers remain unaddressed, which may affect the overall evaluation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the connection between the exploration method and the insights on CMDP generalization. Specifically, they should define the conditions under which exploration is beneficial for CMDPs and provide a more thorough discussion on the exploration-exploitation tradeoff. Additionally, we suggest that the authors include more comprehensive comparisons to other exploration methods and clarify the differences in exploration strategies used by the baselines. It would also be beneficial to compare EDE against intrinsic reward methods and clarify the connection between the motivating example in Section 3 and the empirical results in Section 4. Finally, we encourage the authors to plot generalization gaps for all algorithms/environments to substantiate their claims regarding exploration's impact on generalization and to address any remaining unresolved issues to enhance the paper's overall quality.