ID: KZlJF8kguO
Title: Brain Treebank: Large-scale intracranial recordings from naturalistic language stimuli
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 8, 6, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Brain Treebank, a large-scale dataset of electrophysiological neural responses recorded from intracranial probes while 10 subjects watched Hollywood movies. The dataset includes detailed annotations such as word and sentence onsets, part of speech tagging, and dependency parses, making it a valuable resource for studying naturalistic language perception. The authors provide a solid analysis of the collected data, although concerns are raised regarding the dataset's applicability for visual analysis and the lack of common movies across subjects.

### Strengths and Weaknesses
Strengths:
- The dataset is comprehensive and well-organized, featuring extensive annotations.
- The paper is well-written and includes convincing analyses of word and speech onsets.
- It serves as a significant resource for future developments in brain-computer interfaces (BCI) and related fields.

Weaknesses:
- The paper lacks machine learning analyses, which limits its engagement with the ML community.
- There are concerns about the dataset's representativeness and the variability of probe coverage among subjects.
- The relevance of the dataset for the NeurIPS community is questioned due to the absence of machine learning applications.

### Suggestions for Improvement
We recommend that the authors improve the paper by incorporating machine learning experiments to address scientific questions relevant to the dataset, such as predicting speech onset or classifying sound versus no sound. Additionally, including analyses that demonstrate the utility of the part of speech tags and dependency parses would enhance the paper's contributions. We encourage the authors to consider using the BIDS standard for intracranial data sharing and to clarify the implications of their findings for the ML community.