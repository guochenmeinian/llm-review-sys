ID: zVi11zjaPe
Title: EIT: Enhanced Interactive Transformer
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of the multi-head self-attention mechanism in transformer architectures, aiming to enhance it through the principles of complementarity and consensus. The authors propose a multi-head self-attention (EMHA) model that allows for many-to-many interactions between queries and keys, thereby generating multiple attention maps. They introduce two modules: Inner-Subspace Interaction and Cross-Subspace Interaction, which aggregate information from these attention maps. Experimental results indicate that the proposed architecture outperforms the vanilla transformer across various tasks, although it incurs a higher computational cost.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clearly articulates the motivation behind the proposed enhancements.
- The experimental results demonstrate consistent performance improvements across diverse tasks, validating the utility of the EMHA formulation.
- The ablation studies provide insightful analysis regarding the contributions of different modules and their robustness.

Weaknesses:
- The second half of the paper lacks sufficient detail, particularly in the experimental setup, which is relegated to the Appendix, making it hard to fully understand the results.
- The computational efficiency of the proposed method is a significant drawback, with training requiring 1.45x the time compared to the vanilla transformer.
- The paper does not adequately compare against relevant baselines, such as the Talking Heads Transformer, limiting the assessment of the proposed method's benefits.

### Suggestions for Improvement
We recommend that the authors improve the clarity and detail of the experimental sections, ensuring that all critical information is included in the main body of the paper rather than the Appendix. Additionally, we suggest providing a thorough analysis of the computational complexity associated with the proposed architecture, including numerical comparisons of training and inference times against the vanilla transformer. It would also be beneficial to include comparisons with relevant baselines across all tasks to better gauge the advantages of the proposed interaction modules. Lastly, we advise a careful revision of the paper to address grammatical errors and improve overall presentation.