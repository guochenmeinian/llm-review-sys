ID: TKzERU0kq1
Title: Guiding LLM to Fool Itself: Automatically Manipulating Machine Reading Comprehension Shortcut Triggers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the robustness of large language models (LLMs) against shortcut triggers, defined as text patterns that models rely on for answering questions without true understanding. The authors propose a framework where one LLM (the "editor") makes minimal edits to texts to manipulate shortcut triggers, guided by the confidence of a second "target" LLM. The study generates a new dataset, ShortcutQA, and evaluates strong LLMs like GPT-4 and ChatGPT, revealing significant drops in accuracy due to adversarial shortcuts.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a critical issue regarding the robustness and vulnerabilities of LLMs, which is increasingly relevant in real-world applications.  
- It proposes a novel framework for manipulating LLM responses and generates a valuable dataset, ShortcutQA.  
- The writing is clear, and the methodology is well-structured and technically sound.  

Weaknesses:  
- The dataset coverage is limited, primarily focusing on SQuAD and NewsQA, with a need for broader question-answering categories.  
- There are numerous grammatical errors and unclear sentences that hinder comprehension.  
- The contribution of each type of editing in the pipeline is not well articulated, and concerns about the quality of ShortcutQA remain, particularly regarding potential ambiguities introduced by edits.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing by addressing grammatical issues and simplifying complex formulations. Additionally, we suggest providing a detailed analysis of the contribution of each type of editing and exploring the impact of shortcuts across different domains, such as fiction versus non-fiction. It would also be beneficial to clarify the experimental settings, ensuring comparability between the natural and edited examples in Table 2. Lastly, including examples to illustrate the editing pipeline would enhance understanding.