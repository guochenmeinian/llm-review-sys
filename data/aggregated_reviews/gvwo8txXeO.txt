ID: gvwo8txXeO
Title: Learning, Forgetting, Remembering: Insights From Tracking LLM Memorization During Training
Conference: EMNLP/2024/Workshop/BlackBoxNLP
Year: 2024
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 5, 4, 4

Aggregated Review:
### Key Points
This paper presents an analysis of memorization and forgetting in language models during training, specifically focusing on the OLMo and pythia models. The authors find that memorization rates are higher at the beginning and end of training, suggesting that sensitive data should be placed in the middle of the training process. The study provides empirical insights into the dynamics of memorization, including cycles of memorization and forgetting, and highlights the importance of data order in these processes.

### Strengths and Weaknesses
Strengths:
- The paper addresses a timely and interesting research question, exploring the nuances of how language models memorize information over training.
- The experiments are well-designed, and the paper is well-written and easy to follow.
- It presents insightful findings on memorization dynamics and provides concrete suggestions regarding the placement of sensitive data.

Weaknesses:
- The large memorization spike at the final checkpoint appears to be an artifact of the analysis, which is prominently mentioned in the abstract.
- There is no causal link established between data order and memorization, raising concerns about the validity of the conclusions drawn regarding sensitive data placement.
- The paper does not sufficiently analyze the nature of early memorized examples, and the practicality of placing sensitive data in the middle stages of training is questionable.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their findings by addressing the potential artifact in the memorization spike at the final checkpoint and revising the claims in the abstract accordingly. Additionally, we suggest that the authors conduct a more thorough analysis of the examples memorized early in training to establish a clearer causal relationship with data order. It would also be beneficial to acknowledge the limitations regarding the need for multiple data orderings to validate their core findings. Finally, we encourage the authors to explore the distribution of sequences forgotten by the final model across previous checkpoints to enhance the depth of their analysis.