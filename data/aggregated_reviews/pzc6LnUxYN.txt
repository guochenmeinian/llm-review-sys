ID: pzc6LnUxYN
Title: StateMask: Explaining Deep Reinforcement Learning through State Mask
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a model called StateMask to identify critical states that influence an agent's final reward in deep reinforcement learning. The objective of StateMask is to randomize actions at non-important time steps without altering the expected total reward. The authors utilize a PPO-based algorithm to generate this model and provide numerical examples across various tasks, demonstrating its effectiveness.

### Strengths and Weaknesses
Strengths:  
- The concept of masking actions to identify important time steps is intriguing and does not require access to the agent's value function or policy network, making it versatile across different methods and multi-agent environments.  
- Empirical results and examples effectively showcase the method's performance, particularly in adversarial attacks and error correction.

Weaknesses:  
- The claim that the method is universally applicable is questionable, especially for shortest path problems where critical paths, not states, are more relevant.  
- The optimization of Equation 2 is problematic, as it may lead to a constant policy that fails to identify important time steps.  
- The motivation for using absolute error in Equation 2 is unclear; minimizing squared error (MSE/L2 loss) would be more straightforward for regression tasks.  
- The paper lacks a theoretical analysis and clarity on several definitions and metrics, such as the fidelity score and the choice of hyperparameters.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical analysis, particularly regarding the surrogate objective and its relation to the original problem. Additionally, the authors should refine Equation 2 to ensure it effectively identifies important time steps. It would be beneficial to include more metrics for evaluation and clarify the definitions of terms like fidelity. We also suggest addressing the limitations of the method in complex environments where multiple actions influence the final reward. Lastly, enhancing the presentation of the supplementary material to specify the types of networks used would strengthen the paper.