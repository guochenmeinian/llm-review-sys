ID: kaHpo8OZw2
Title: DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 26
Original Ratings: 7, 7, 10, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive evaluation of the trustworthiness of large language models (LLMs), specifically focusing on GPT-3.5, GPT-4, and various open-source models. The authors propose a unified DecodingTrust API that facilitates the evaluation of both proprietary and open-source models across multiple dimensions, including toxicity, stereotype bias, adversarial robustness, privacy, machine ethics, and fairness. The findings indicate that while GPT-4 generally exhibits superior performance, it is also more susceptible to manipulation through adversarial prompts. The paper includes detailed metrics on computational costs associated with various trustworthiness evaluations and highlights the need for ongoing development in trustworthy LLMs.

### Strengths and Weaknesses
**Strengths:**
1. The paper offers a thorough analysis of trustworthiness across multiple dimensions, providing valuable insights into LLM vulnerabilities.
2. The unified DecodingTrust API allows for flexible evaluations of both proprietary and open-source models, enhancing usability.
3. The methodology is sound, and the paper is well-structured and clear, with detailed experimental setups and evaluations that enhance clarity and depth.
4. The authors have incorporated valuable feedback from reviewers, improving the manuscript's robustness and relevance.

**Weaknesses:**
1. The focus is primarily on proprietary GPT models, limiting the initial evaluation of open-source alternatives.
2. Key experimental setups and datasets are often relegated to the appendix, which may hinder self-containment and reader comprehension.
3. The manuscript's substantial length (24 pages of main text + 100 pages of supplementary material) may hinder thorough evaluation and engagement with specific points.
4. Concerns regarding reproducibility arise from the use of proprietary models, which may not be accessible to all researchers.
5. The potential for misuse of the findings by malicious actors is a significant concern that the paper could better address.

### Suggestions for Improvement
We recommend that the authors improve the discussion of potential solutions or strategies to mitigate the identified vulnerabilities in the LLMs. Additionally, it would be beneficial to include critical details about the stereotype dataset and experimental setups in the main text rather than the appendix to enhance self-containment. To improve conciseness, we suggest that the authors streamline the manuscript to facilitate better evaluation and engagement. Furthermore, we encourage the authors to explore whitebox attack scenarios more thoroughly, particularly in the context of open-source models, to evaluate worst-case performance effectively. Lastly, addressing the reproducibility concerns related to the accessibility and cost of using proprietary models would increase transparency and trust in the findings.