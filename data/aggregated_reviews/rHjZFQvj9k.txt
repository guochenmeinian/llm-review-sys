ID: rHjZFQvj9k
Title: Norm of Word Embedding Encodes Information Gain
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a characteristic analysis of word embeddings, focusing on the squared norm of word embeddings as an approximation of information gain. The authors find that the bias-corrected KL divergence and the norm of word embeddings serve as effective metrics for word informativeness. They conduct experiments on keyword extraction, proper noun discrimination, and hypernym discrimination to validate their claims.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured, with clear explanations and a coherent narrative.
- The authors provide both theoretical and empirical support for their findings, demonstrating a novel relationship between the norm of word embeddings and KL divergence.
- The results are reproducible, and the bias-corrected squared norm shows significant improvement in specific tasks.

Weaknesses:
- The findings may not generalize beyond SGNS word embeddings, which are not the current mainstream in NLP.
- The absence of TFIDF as a baseline in the keyword extraction experiment raises concerns about the robustness of the results.
- The theoretical analysis and experiments are limited to static embeddings, leaving questions about the applicability to other methods like GloVe or contextualized embeddings.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by exploring whether the norm of other word embeddings, such as GloVe or embeddings from deep learning models, also encodes information gain. Additionally, we suggest including TFIDF as a baseline measure in the keyword extraction experiment to strengthen the analysis. Clarifying the implications of their theoretical findings for practical applications in model design or evaluation in NLP would enhance the paper's relevance. Finally, addressing the derivation choices and their implications in the experiments would provide clearer insights into the methodology.