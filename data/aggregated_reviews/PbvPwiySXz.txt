ID: PbvPwiySXz
Title: Disentangling Linear Mode Connectivity
Conference: NeurIPS
Year: 2023
Number of Reviews: 1
Original Ratings: -1
Original Confidences: -1

Aggregated Review:
### Key Points
This paper presents an investigation into the emergence of LMC (Learning Model Complexity) between two models initialized with the same parameters at the end of training. The authors clarify that their focus on this specific aspect makes the study of permutation symmetries less relevant. They differentiate their work from that of Vlaar and Frankle (2022), noting that while both analyses cover data, optimization, and architecture, their approach centers on LMC rather than optimization paths. Additionally, the authors highlight that increasing the depth of models disrupts convexity in the logistic regression objective, and they intentionally examine LMC in small networks to isolate the effects of individual components, despite the limitations of these models in fitting data.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a unique aspect of LMC by focusing on models with identical initial parameters.  
- The authors provide a clear distinction between their work and related literature, enhancing the understanding of their contributions.  
- The examination of LMC in small networks allows for a focused analysis of individual components.

Weaknesses:  
- The study's emphasis on small networks may limit the generalizability of the findings to larger, more complex models.  
- The inability of small models to fit data could undermine the practical implications of the results.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by exploring LMC in larger networks. Additionally, addressing the limitations of small models in fitting data could enhance the practical relevance of their conclusions.