ID: kpo6ZCgVZH
Title: Functional Gradient Flows for Constrained Sampling
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 7, 7, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 2, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a particle-based method for sampling probability densities supported on constrained subsets of \(\mathbb{R}^d\). The authors adapt Stein variational gradient descent (SVGD) principles by incorporating boundary conditions for gradient flow, leading to a constrained functional gradient flow (CFG) method. Theoretical convergence in total variation is established, and empirical performance is evaluated across various datasets, demonstrating competitive results compared to existing methods.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and presents a clear, motivated approach to constrained sampling, extending SVGD effectively.  
- The convergence analysis is straightforward, and the theoretical results are solid.  
- Empirical results show satisfactory performance across diverse examples, indicating the method's potential.

Weaknesses:  
- The proposed method does not consistently outperform existing algorithms, and a more detailed cost comparison would be beneficial.  
- The handling of boundary terms raises concerns, particularly regarding small step sizes and potential particle stagnation at boundaries.  
- The choice of the function \(g\) is not sufficiently detailed for all examples, and the paper overlooks many existing Bayesian neural network baselines.

### Suggestions for Improvement
We recommend that the authors improve the clarity of notation and definitions, particularly in specifying the set \(\Omega\) and the function \(g\). For example, explicitly defining \(\Omega\) as \(\{ x \in \mathbb{R}^d : g(x) \leq 0\}\) would enhance understanding. Additionally, we suggest providing a more comprehensive discussion on the tuning of algorithmic parameters, including the choice of \(g\), the number of particles, and the architecture of neural networks used. 

To strengthen the theoretical analysis, we encourage the authors to explore faster convergence rates and clarify the implications of the bias-variance trade-off in choosing the boundary integral approximation. Addressing the potential for particle stagnation at boundaries when using small step sizes is also crucial. Lastly, we recommend including examples where the CFG method demonstrates advantages over traditional approaches, thereby reinforcing its contribution to the field.