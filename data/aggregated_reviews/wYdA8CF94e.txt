ID: wYdA8CF94e
Title: HalOmi: A Manually Annotated Benchmark for Multilingual Hallucination and Omission Detection in Machine Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new multilingual dataset for detecting hallucinations and omissions in machine translation, featuring sentence-level and token-level annotations across eighteen translation directions involving nine languages. The authors evaluate various detection methods, revealing that existing techniques perform significantly better on high-resource language pairs. Notably, sequence log-probability is identified as the most effective method for sentence-level hallucination detection, while token attribution methods excel in omission detection.

### Strengths and Weaknesses
Strengths:  
- The dataset is a valuable resource for future research on hallucinations and omissions in machine translation systems.  
- The paper is well-written, with clear descriptions of the dataset and detection methods, contributing valuable insights to the field.  
- The manual annotation effort enhances the dataset's reliability and relevance to real-world scenarios.  

Weaknesses:  
- The main text exceeds the 8-page limit, with the Conclusions section extending into the 9th page.  
- The dataset is limited to hallucination and coverage errors from the 600M distilled NLLB model, raising concerns about potential bias.  
- The limitations section is insufficient, lacking discussion on ambiguity in definitions and challenges related to word-level annotation across different languages.  
- There is minimal information regarding the annotation process and the background of the annotators, which could affect the reliability of the annotations.

### Suggestions for Improvement
We recommend that the authors improve the limitations section by addressing ambiguities in the definitions of hallucinations and omissions, as well as the challenges posed by different languages' morphology. Additionally, we suggest providing more annotation examples to enhance clarity. To mitigate potential bias, consider incorporating additional MT models in the dataset. Furthermore, we encourage the authors to elaborate on the annotation process, including how discrepancies among annotators were managed and whether annotation agreement was measured.