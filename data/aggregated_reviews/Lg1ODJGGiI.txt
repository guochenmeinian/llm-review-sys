ID: Lg1ODJGGiI
Title: Deep Stochastic Processes via Functional Markov Transition Operators
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 6, 7, 8, 8, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Markov Neural Processes (MNPs), a novel class of stochastic processes that enhance the flexibility and expressivity of traditional Neural Processes (NPs) while maintaining consistency and exchangeability. The authors propose an iterative construction that is fully generative and demonstrate the model's superior performance across various benchmarks, including 1D function regression and geological inference.

### Strengths and Weaknesses
Strengths:
- The model is theoretically proven to satisfy valid conditions, ensuring exchangeability and consistency.
- The experiments are well-designed, showcasing the model's expressiveness and advantages over traditional models, particularly on non-Gaussian data.
- The use of instance-wise conditional normalizing flows and permutation-invariant neural networks adds significant flexibility to the MNP framework.

Weaknesses:
- The notation for probabilistic concepts is unclear, complicating the exposition and making it difficult for readers to follow. Key terms such as $p$, $f$, and the notation for transition operators need clearer definitions.
- The overall explanation of the MNP structure is hard to follow, particularly in Figure 2, which lacks adequate clarification of the arrows and their meanings.
- The empirical evaluation is somewhat limited, primarily focusing on small-scale synthetic datasets. More extensive experiments on larger, real-world datasets and an analysis of time/space complexity would enhance the evaluation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of notation for probabilistic concepts, adhering to standard measure-theoretic notation to facilitate understanding. Additionally, we suggest providing a more intuitive explanation of why MNP outperforms NP in specific contexts, such as the contextual bandit example, possibly through an ablation study regarding the number of Markov stages. Expanding the empirical evaluation to include larger datasets and additional tasks, such as few-shot image classification, would also strengthen the paper. Finally, a comprehensive ablation study should be included in the revision to further validate the findings.