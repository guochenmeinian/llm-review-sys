ID: ujE83r50tR
Title: Octopus: A Multi-modal LLM with Parallel Recognition and Sequential Understanding
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Octopus, a multimodal large language model (MLLM) designed for joint localization and visual chat tasks. The authors integrate the DETR decoder into the LLM for enhanced visual grounding, utilizing the bottom LLM layers for parallel recognition and relaying results to the top layers for sequential understanding. The paper claims to demonstrate the effectiveness of Octopus through extensive results on various MLLM tasks, highlighting improved efficiency and performance compared to previous models like MiniGPT-V2 and Shika.

### Strengths and Weaknesses
Strengths:
- The writing is clear and easy to follow.
- Octopus effectively combines localization and visual chat tasks, achieving faster performance than prior models.
- The framework shows good generalization to pixel-wise recognition tasks, such as segmentation.

Weaknesses:
- The motivation for combining DETR with MLLM lacks novelty, as similar designs exist in prior works like LLaVA-Grounding, which the authors do not adequately cite.
- The technical novelty is limited, with the meta-architecture resembling previous models.
- Several ablation studies are missing, particularly regarding the effects of pre-training data and transformer layers in DETR.
- The experimental results are not comprehensive, with unfair comparisons to baseline methods and missing recent state-of-the-art models.
- There is a lack of failure case analysis and improvement analysis compared to late fusion decoders.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by providing a thorough comparison with existing models, particularly LLaVA-Grounding, and ensuring proper citations. Additionally, the authors should conduct more comprehensive experimental investigations, including ablation studies on pre-training data and transformer layers in DETR. It would also be beneficial to include a detailed analysis of failure cases and improvements over late fusion decoders. Finally, we suggest using a pre-trained DETR decoder from publicly available checkpoints to enhance performance and efficiency.