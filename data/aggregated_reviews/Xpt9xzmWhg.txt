ID: Xpt9xzmWhg
Title: A Reflection and Outlook on Clinical Adaption of Large Language Models
Conference: AAAI
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 6, 4
Original Confidences: 4, 4, 4

Aggregated Review:
### Key Points
This paper presents an exploration of large-scale language modeling (LLM) in the clinical domain, building on previous work and analyzing existing clinical LLMs with a focus on domain adaptation strategies. The authors propose a comparison of various models based on their medical knowledge infusion strategies, emphasizing the effectiveness of continuous pre-training and supervised fine-tuning on MedQA and PubMedQA benchmarks. The discussion highlights critical issues such as training data selection, retrieval enhancement generation, copyright considerations, and the challenges in aligning clinical needs with academic research.

### Strengths and Weaknesses
Strengths:
- The paper is easy to follow and covers various aspects of clinical LLMs.
- Training data and approaches are well summarized in Table 1.
- The Results section provides valuable insights into continuous pre-training versus supervised fine-tuning, model size, compute power, and training data quality.

Weaknesses:
- The evaluation in Table 1 lacks clarity regarding the source of reported results and the definition of "best performance." Performance metrics are not clearly specified.
- Some excerpts require refinement, including a brief conclusion and an inadequate discussion on downstream use cases, particularly in early disease prevention and personalized medicine.
- The paper appears to add little novelty, as many discussed points are already covered in existing surveys.
- Significant related works are omitted, which is a notable gap.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Table 1 by specifying the performance metrics used and the source of the reported results. The column names should be adjusted for precision, such as changing "Approach" to "Training approach" and clarifying the units of parameter size and training data. Additionally, we suggest elaborating on the ethical and legal implications of using copyrighted or sensitive patient data in training clinical LLMs. Finally, we encourage the authors to enhance the conclusion and expand the discussion on downstream applications to include critical areas like early disease prevention and personalized medicine.