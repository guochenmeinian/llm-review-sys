ID: UXuBzWoZGK
Title: Catastrophic Goodhart: regularizing RLHF with KL divergence does not mitigate heavy-tailed reward misspecification
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the relationship between reward models (RMs) and reinforcement learning from human feedback (RLHF), particularly focusing on the phenomenon termed "catastrophic Goodhart." The authors analyze how the reward from the RM can be modeled as a true reward plus a noise term, demonstrating that if noise is heavy-tailed, policies can achieve high rewards without improving true utility. Conversely, if noise is independent and light-tailed, KL divergence can effectively ensure good outcomes. The authors also investigate real RMs, finding that rewards on random sequences do not exhibit heavy-tailed behavior.

### Strengths and Weaknesses
Strengths:
- The paper is well written and easy to read.
- The theoretical results are relevant and provide clean answers to the problem of reward misspecification in RLHF.

Weaknesses:
- There is a discrepancy between theory and practice, as the results are asymptotic; non-asymptotic statements would be more beneficial.
- The experimental results suggest that rewards are not heavy-tailed, indicating that KL divergence should work well, which is already established in practice, thus lacking a novel take-home message.
- The writing could be more engaging and accessible, and clarity in some sections needs improvement.

### Suggestions for Improvement
We recommend that the authors improve the clarity and engagement of the writing to enhance accessibility for a broader audience. Additionally, we suggest that the authors explore the implicit regularization effects of optimization algorithms, as understanding how certain techniques can mitigate the issues raised would be valuable. It would also be beneficial to include real-world applications in the experiments to validate the findings comprehensively. Furthermore, we encourage the authors to clarify the inconsistent notation of $U$ and $V$, and to address the implications of clipping the proxy reward function on the theoretical results.