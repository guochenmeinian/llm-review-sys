ID: 7ntI4kcoqG
Title: AMAG: Additive, Multiplicative and Adaptive Graph Neural Network For Forecasting Neuron Activity
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 7, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AMAG, a graph neural network designed for forecasting neural population dynamics by modeling additive and multiplicative interactions between neurons. The authors propose a multi-step prediction approach that incorporates a temporal encoder, a spatial interaction module, and a temporal readout module. Experimental results demonstrate that AMAG outperforms various baseline methods in both synthetic and real neural signal datasets, particularly in one-step forecasting.

### Strengths and Weaknesses
Strengths:
- The model effectively captures connectivity in synthetic signals, which is also reflected in real-world data.
- Significant improvements in forecasting performance are observed compared to competing approaches.
- The rationale behind the design of AMAG is clearly articulated, and the results, particularly those involving actual monkey data, are compelling.

Weaknesses:
- The evaluation metric of forecasting in neural time series lacks clarity regarding its importance, particularly in relation to neural decoding tasks.
- The source of predictive improvements remains ambiguous, as the proposed structure does not significantly differ from existing methods.
- The experimental design raises concerns about potential overfitting, as model selection was based on the test set.
- Quantitative results lack sufficient statistical analysis, and the significance of improvements is inadequately explained.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation metrics by explicitly demonstrating how forecasting improvements translate to better performance in neural decoding tasks. Additionally, the authors should explore and clarify the differences between their approach and existing methods, particularly regarding the contributions of the GNN operating in latent space. 

We suggest conducting a more rigorous experimental design by implementing a separate validation set for model selection to avoid overfitting. Furthermore, the authors should provide more detailed statistical analyses of the results, including standard deviations, to substantiate the significance of their findings. 

Lastly, we encourage the authors to elaborate on the initialization effects of the adjacency matrices and provide insights into the interactions being learned, particularly through additional analyses of the additive and multiplicative components.