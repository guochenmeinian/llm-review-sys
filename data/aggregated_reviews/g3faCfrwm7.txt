ID: g3faCfrwm7
Title: Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation of the calibration of various RLHF fine-tuned language models (LLMs), specifically ChatGPT, GPT-4, and Claude. The authors demonstrate that verbalized confidences yield better calibration than native conditional probabilities, with findings validated across benchmark datasets. Additionally, generating multiple hypotheses enhances calibration. The study addresses the critical need for trustworthy prediction systems with well-calibrated confidence scores.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- It tackles an important and practical problem regarding confidence extraction from LLMs.
- The findings introduce novel perspectives on calibration and show promising experimental results.

Weaknesses:
- The evaluation lacks meaningful conclusions beyond additional tables, and the method may be perceived as relatively simple and lacking novelty.
- The analysis is limited to closed models, raising concerns about potential data leakage and the absence of comparisons with non-RLHF models.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including a broader range of language models, such as LLaMA, to strengthen their conclusions. Additionally, we suggest addressing the potential impact of data contamination on model overconfidence and comparing calibration results with non-RLHF fine-tuned models. Lastly, providing more evidence on how RLHF worsens calibration would enhance the paper's contributions.