ID: 8pOBo5NgTQ
Title: FLSL: Feature-level Self-supervised Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 6, 5, 5, 7, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 2, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Feature-level Self-supervised Learning (FLSL) aimed at enhancing dense prediction tasks like object detection and instance segmentation. The authors leverage mean-shift clustering and k-means perspectives through transformer architectures to achieve effective intra-level and inter-level feature clustering. Experimental results demonstrate that FLSL significantly outperforms existing methods on benchmarks such as MS-COCO, UAVDT, and DAVIS.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated, and the proposed method shows strong performance on dense prediction tasks.
- The connection between mean-shift clustering and self-attention is insightful, leading to a clean implementation.
- Extensive experiments validate the effectiveness of the approach across multiple datasets.

Weaknesses:
- The relationship between ADCLR and FLSL is unclear, particularly regarding their use of cross-attention for patch-level information.
- The term "feature-level" may be misleading, as the method aligns more closely with cluster-level approaches rather than traditional feature-wise or instance-wise methods.
- Comparisons with related works, such as DeepCluster and CDL, are insufficient, and the computational cost of the additional attention layers is not addressed.
- The robustness of the method against dataset biases and its performance on uncurated datasets like COCO remain unclear.

### Suggestions for Improvement
We recommend that the authors clarify the distinction between ADCLR and FLSL, particularly regarding their objectives. Additionally, consider revising the terminology from "feature-level" to better reflect the clustering nature of the method. It would be beneficial to include comparisons with related works like DeepCluster and CDL to strengthen the discussion. Furthermore, we suggest providing details on the computational cost associated with the additional attention layers and conducting experiments to assess the method's robustness on uncurated datasets. Finally, including evaluations on semantic segmentation across various datasets would provide a broader perspective on the model's performance.