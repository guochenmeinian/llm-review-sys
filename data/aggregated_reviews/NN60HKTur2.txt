ID: NN60HKTur2
Title: Extracting Reward Functions from Diffusion Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 7, 7, 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for extracting a relative reward function from two diffusion models—an expert and a base model—by leveraging the differences in their score functions. The authors provide theoretical justification for the uniqueness of this relative reward function and propose a practical learning algorithm to approximate it. They demonstrate the effectiveness of their approach across various tasks, including maze navigation, locomotion, and image generation, showing improvements in task performance when using the learned reward function. Additionally, the authors emphasize the theoretical contributions to interpretability and reward learning, although they acknowledge that the method may have limited practical value for locomotion experiments, as it does not outperform existing expert models. The paper raises questions about the accessibility of the original dataset required for Algorithm 1 and the rationale behind citing Sohl-Dickstein over Dhariwal et al. for classifier guidance.

### Strengths and Weaknesses
Strengths:
- The research problem is significant and well-defined, with clear relevance to the field.
- The paper is well-organized, presenting technical details coherently.
- The theoretical findings are robust, supported by reasonable assumptions and intuitive examples.
- Empirical results across diverse tasks indicate the method's effectiveness in extracting reasonable reward functions.
- The work is technically sound, with a solid theoretical foundation and impressive experimental results.
- The method is applicable to both classifier guidance and classifier-free guidance, showcasing its versatility.
- The authors have made revisions to address several reviewer concerns, including the addition of quantitative results and a graphical illustration of the models.

Weaknesses:
- The connection to inverse reinforcement learning (IRL) is insufficiently explored, particularly regarding comparisons with established IRL methods.
- The motivation for the problem setting is unclear, as the utility of extracting a reward function when the expert model is already accessible is not well justified.
- The practical utility of the method for locomotion experiments is questioned, as it does not surpass expert models.
- The requirement for access to an expert diffusion model is seen as unrealistic and inadequately motivated.
- Experiments lack comprehensive evaluation on classifier guidance beyond locomotion tasks.
- The paper's focus on background information about diffusion models may overshadow its contributions.
- The paper does not address the potential drawbacks of using diffusion models compared to other generative models, such as VAE or GAN.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the relationship between their method and existing IRL techniques, including a thorough comparison with IRL algorithms that utilize a two-player minimax framework. Additionally, clarifying the motivation for extracting a reward function in the presence of an expert model would strengthen the paper. We suggest that the authors improve the clarity of the paper by emphasizing the contributions to interpretability more prominently in the abstract and introduction. Addressing the concerns regarding the accessibility of the original dataset for Algorithm 1 and providing a more thorough discussion of inverse reinforcement learning (IRL) in the related work section would also enhance the paper. Furthermore, we encourage the authors to clarify the definition of the "medium-expert" model and ensure that the absolute nature of the learned reward function is clearly articulated. Finally, we recommend providing ablation studies on hyperparameters and qualitative analyses of learned reward functions to further substantiate the findings, as well as considering reframing the main contribution of the paper to focus more on interpretability rather than solely on reward learning.