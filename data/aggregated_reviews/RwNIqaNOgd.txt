ID: RwNIqaNOgd
Title: RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 7, 6, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new benchmark, RL-ViGen, for studying out-of-distribution generalization in visual reinforcement learning (RL) environments. The authors collect five existing RL environments and enhance them with additional scenarios to create test conditions that deviate from the standard training distribution. They categorize cross embodiment as a significant aspect of visual generalization, supported by experiments demonstrating that current algorithms perform suboptimally in this area. The authors emphasize the importance of visual distribution shifts in evaluating generalization capabilities and argue that RL-ViGen encompasses multiple environments to better assess algorithm performance across diverse scenarios, thus addressing the limitations of existing benchmarks that often promote overfitting.

### Strengths and Weaknesses
Strengths:
- The benchmark addresses a critical aspect of visual RL, specifically out-of-distribution generalization.
- The authors provide a comprehensive categorization of cross embodiment and its relevance to visual generalization.
- The inclusion of multiple environments helps mitigate overfitting and enhances the robustness of the evaluation.
- The experimental design is well-justified and aligns with prevalent practices in the field.
- The authors have made significant revisions based on reviewer feedback, enhancing the clarity and depth of the related work section and providing additional installation options.

Weaknesses:
- The distinction between out-of-distribution and in-distribution generalization is not clearly articulated.
- Clarity regarding the training distribution for each environment is lacking.
- The quality of individual environments is questioned, as combining lower-quality environments may obscure their limitations.
- Sample efficiency curves are missing x-axes and contextual horizontal lines for comparison.
- The appropriateness of certain experimental designs, such as the choice of baseline algorithms, is questioned.
- The paper lacks a quantifiable metric for difficulty levels in tasks, which could aid in evaluating performance.
- The related work section lacks a thorough review of existing environments, failing to clearly articulate their insufficiencies.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the distinction between out-of-distribution and in-distribution generalization and clearly define the training distribution for each environment. Additionally, we suggest including x-axes on sample efficiency curves and horizontal lines indicating optimal play scores for context. The authors should provide results for agents trained on the full distribution, including test scenarios, to comprehensively assess the algorithms' performance. We encourage the authors to include a structured criterion for difficulty levels in tasks, such as the number of colors or degree of contrast, and clarify the choice of baseline algorithms in their experimental design, potentially considering meta-learning as a baseline. Lastly, we recommend improving the quality of individual environments and conducting a more comprehensive review of existing benchmarks in the related work section, clearly stating their limitations, as well as providing non-conda installation instructions to enhance usability and accessibility for users.