ID: wcdF6jR0Sp
Title: Consistent Aggregation of Objectives with Diverse Time Preferences Requires Non-Markovian Rewards
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 6, 6, 6, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of multi-objective sequential decision-making, focusing on the impossibility of aggregating different objectives with varying discount factors while maintaining dynamic consistency. The authors prove that under certain axioms, an aggregated preference relation cannot satisfy the von Neumann-Morgenstern axioms, dynamic consistency, and Pareto indifference simultaneously. They explore potential solutions to this impossibility, including state augmentation and the introduction of a "historical" discount factor. The paper connects these findings to broader discussions in decision theory and reinforcement learning.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a fundamental issue in multi-objective reinforcement learning and presents a rigorous impossibility result.
- The authors demonstrate a strong understanding of temporal consistency literature and provide clear technical exposition.
- The exploration of state augmentation and historical discounting offers valuable contributions to the field.

Weaknesses:  
- The presentation and organization of the paper could be improved, particularly for accessibility to a broader audience.
- Some notations and concepts are inadequately defined or overly complex, which may hinder understanding.
- The title and central concept of "multi-objective agency" are vague and not well-defined, potentially misleading readers about the paper's focus.

### Suggestions for Improvement
We recommend that the authors improve the clarity and accessibility of the paper by providing more detailed explanations of key concepts, particularly in Section 3.1, and ensuring that all notations are rigorously defined. We suggest moving Section 4 to the appendix to allow more space for elaborating on the main ideas and results. Additionally, we encourage the authors to consider revising the title to better reflect the content, perhaps using terms like "multi-criteria objectives" or "multi-objective reinforcement learning." Lastly, we advise that the authors clarify the implications of their results, especially regarding the conditions under which their findings apply.