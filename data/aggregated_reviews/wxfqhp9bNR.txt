ID: wxfqhp9bNR
Title: Improving Multi-Hop Reasoning in LLMs by Learning from Rich Human Feedback
Conference: AAAI
Year: 2023
Number of Reviews: 3
Original Ratings: 9, 6, 6
Original Confidences: 5, 5, 4

Aggregated Review:
### Key Points
This paper presents a significant contribution to enhancing reasoning and multi-hop question answering (QA) capabilities in large language models (LLMs) by introducing two new datasets and a novel learning algorithm. The authors propose a collection of over 2,000 annotated human-feedback examples focusing on correctness, error types, and explanations for two tasks: StrategyQA and Sports understanding. However, the evaluation of the proposed algorithms is noted to be weak, with inconsistent performance across datasets.

### Strengths and Weaknesses
Strengths:
- Clear language expression and well-placed references enhance overall clarity.
- The literature review is pertinent, and the appendix provides detailed data curation and evaluation steps.
- The work is original, introducing new datasets and a learning algorithm aimed at improving LLM reasoning.
- Detailed evaluation results and ablation studies are presented, with a thorough explanation of the dataset collection process.

Weaknesses:
- Minor grammatical and punctuation mistakes detract from clarity.
- The proposed algorithms lack novelty, and the evaluation metrics are insufficient, relying solely on accuracy, which may misrepresent model performance due to imbalanced classification.
- There is no qualitative assessment of reasoning improvements through human or expert evaluation.
- The applicability of the method appears task-specific, and the collection of human preferences is costly without addressing the feasibility of AI feedback.
- The proposed objectives are variants of existing techniques, and there is a lack of comparison with RL-based objectives and a limited range of error types considered.

### Suggestions for Improvement
We recommend that the authors improve the evaluation metrics by incorporating Precision, Recall, and F1 score to provide a more comprehensive understanding of model performance. Additionally, conducting human or expert evaluations would enhance the qualitative assessment of reasoning improvements. The authors should consider exploring meta-learning tailored for reasoning and include an ablation study to identify the most valuable feedback signals. Furthermore, addressing the cost of human preference collection and clarifying the applicability of methods to unseen datasets would strengthen the paper. Lastly, we suggest including an architectural diagram to illustrate the fine-tuning and innovations at the algorithmic level.