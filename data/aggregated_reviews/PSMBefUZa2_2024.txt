ID: PSMBefUZa2
Title: Reinforcement Learning Guided Semi-Supervised Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 3, 5, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Reinforcement Learning-Guided Semi-Supervised Learning (RLGSSL), a method that formulates semi-supervised learning (SSL) as a one-armed bandit problem. The authors employ a reinforcement learning (RL)-based loss function to guide the learning process and incorporate a teacher-student framework to enhance stability. The method features a reward function that measures discrepancies between model predictions on mixed data and pseudo-labels, aiming to improve learning efficiency.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel approach by framing SSL as a one-armed bandit problem, providing a fresh perspective.
2. The ablation study convincingly demonstrates the necessity of each component of the proposed method.
3. The writing is generally clear and fluent.

Weaknesses:
1. The rationale for using the non-differentiable MSE loss over differentiable cross-entropy loss is not convincing, as the authors claim MSE is less sensitive to label noise.
2. The integration of RL introduces additional computational overhead, which may require substantial resources.
3. The presentation lacks clarity in some areas, with terms like "convex combination" and "fluid decision boundaries" potentially confusing for readers without a strong background.
4. The improvements shown in the results are not significant, particularly on the CIFAR-100 dataset, raising questions about the method's effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their explanations, particularly regarding the rationale for using MSE loss and the definition of "meaningful weight" in their equations. Additionally, the authors should provide more comprehensive comparisons with existing methods to highlight the uniqueness of their approach. We suggest including more figures to enhance the presentation and making the writing more concise and logical. Finally, the authors should address the potential limitations of integrating RL into SSL, particularly concerning training efficiency and the assumption that labeled and unlabeled data come from the same distribution.