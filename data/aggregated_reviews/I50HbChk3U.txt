ID: I50HbChk3U
Title: Provably Bounding Neural Network Preimages
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents INVPROP, an algorithm for constructing certified bounds on the pre-image of neural networks with ReLU activations. The method constructs a convex over-approximation of the pre-image by optimizing the dual relaxation of a MILP problem. The authors evaluate the approach on backward-reachability for neural control policies, out-of-distribution detection, and robustness verification, achieving promising results.

### Strengths and Weaknesses
Strengths:
1. The paper addresses an important recent development in the literature and presents a novel approach with promising results for toy problems.
2. The method is elegant and clear, potentially serving as a baseline for future works in neural network inversion.
3. The adaptation of the α,β-Crown method is innovative, and the empirical results demonstrate efficacy on various benchmarks.

Weaknesses:
1. The main experiment evaluates a very small network (12 + 7 + 2 neurons), raising concerns about the relevance of the approach for practical network sizes.
2. The paper is dense and lacks historical context, leaving readers unaware of prior works in neural network inversion.
3. The presentation appears rushed, with minor writing issues and unnumbered equations complicating referencing.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing a more comprehensive historical perspective on neural network inversion, including earlier works. Additionally, the authors should address the scalability of their approach, particularly regarding larger networks, and clarify the limitations of their method, especially in terms of input dimensionality. We also suggest that the authors refine their presentation, ensuring that claims about approximations are accurate and that all equations are numbered for easier reference.