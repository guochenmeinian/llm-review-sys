ID: uikhNa4wam
Title: FIFO-Diffusion: Generating Infinite Videos from Text without Training
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 3, 6, 4, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a training-free diagonal denoising method, FIFO-diffusion, which allows pre-trained video diffusion models to generate infinitely long videos. The authors validate their approach across various baselines, demonstrating its applicability to different video diffusion architectures. The method employs diagonal denoising, where noise levels decrease sequentially, and introduces latent partitioning and lookahead denoising to enhance video quality.

### Strengths and Weaknesses
Strengths:
- The motivation for generating long videos is commendable, addressing a significant limitation of existing models that only produce short clips.
- The method is straightforward and can be applied to various video generation models, with well-supported theoretical and experimental foundations.
- The paper is well-structured and articulate, with effective visual aids illustrating results and methodologies.

Weaknesses:
- The paper lacks sufficient discussion of prior works related to diagonal denoising, which raises concerns about the novelty of the proposed method.
- The evaluation metrics are limited, with quantitative comparisons showing that prior work (FreeNoise) outperforms FIFO-Diffusion, indicating potential degradation in video quality over time.
- The generated videos often exhibit simple, repetitive actions, lacking global coherence and meaningful alterations, which limits their practical usability.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related works, particularly addressing the contributions of prior methods such as the Rolling Diffusion model and MultiDiffusion techniques. Additionally, we suggest enhancing the evaluation section by including more quantitative metrics that measure temporal consistency and motion magnitude to better illustrate the trade-offs in video quality. Finally, we encourage the authors to explore methods for increasing controllability in video generation to enhance the usability of their approach.