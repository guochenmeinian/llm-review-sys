ID: b6XvK2de99
Title: One-Step Diffusion Distillation via Deep Equilibrium Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 5, 6, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Generative Equilibrium Transformer (GET), a deep equilibrium model designed for the distillation of diffusion models into efficient generative models that require fewer steps for image generation. The architecture utilizes noise/image pairs generated from a pre-trained diffusion model, aiming to match these pairs through a fixed-point iteration process. The authors empirically demonstrate that GET outperforms traditional models in terms of performance, model size, and inference efficiency, particularly on CIFAR-10.

### Strengths and Weaknesses
Strengths:
- The introduction of DEQ for efficient student models is novel and addresses a significant gap in existing literature.
- The paper is well-written and easy to follow, with a clear presentation of ideas.
- Extensive experimentation shows promising results, indicating scalability and efficiency compared to ViT-based models.

Weaknesses:
- The paper lacks sufficient details on DEQ training and inference, raising questions about the relevance of DEQ techniques in the proposed architecture.
- The evaluation primarily focuses on CIFAR-10, which may not adequately represent the model's performance on larger datasets.
- There is ambiguity regarding the necessity of multiple iterations for generating high-quality images, which could mislead interpretations of "one-step generation."

### Suggestions for Improvement
We recommend that the authors improve the clarity of the DEQ training and inference processes, explicitly detailing how fixed-point solvers and backpropagation shortcuts are utilized. Additionally, exploring the impact of weight tying versus untied weights on model performance would provide valuable insights. We suggest including comparisons with larger datasets, such as ImageNet, to strengthen the evaluation of GET's effectiveness. Furthermore, addressing the role of the number of noise/data samples synthesized and clarifying the necessity of the Injection Transformer would enhance the paper's depth. Lastly, we encourage the authors to publish their training protocol and code to facilitate broader community exploration of their method.