ID: VwUTz2pOnD
Title: Kernel-Based Function Approximation for Average Reward Reinforcement Learning: An Optimist No-Regret Algorithm
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of learning in Markov Decision Processes (MDPs) with a long-run average reward objective, under certain kernel assumptions. The authors propose a UCB-type learning algorithm that achieves sublinear regret when the eigenvalues of the kernel operators decay at a polynomial rate with \( p > 1 \). Additionally, the paper establishes a confidence interval bound for kernel ridge regression, which is of independent interest.

### Strengths and Weaknesses
Strengths:  
- The paper demonstrates a strong result by proving a regret bound under weak MDP assumptions and a RKHS kernel assumption, significantly relaxing prior tabular and linear assumptions.  
- The use of kernel functions enhances expressibility in representing state-action and state value functions, capturing a broader class of functions compared to traditional linear approximations.  
- The authors provide the first sublinear regret bounds for polynomially decaying eigenvalues of the kernel function, contributing valuable insights into the design of optimistic algorithms in reinforcement learning.

Weaknesses:  
- The use of Bachmann-Landau notation is confusing, particularly in Assumption 3 and Theorem 2, where the scaling variables in the o(1) and big-O notation are unclear.  
- The computational cost of inverting a matrix of dimension \( t \) at each iteration may become prohibitive as \( t \) increases, raising questions about the tractability of the algorithm for large time horizons.  
- The resetting of value functions to zero after a finite time horizon \( w \) appears to lack data efficiency, as it only incorporates past samples into the confidence bounds.

### Suggestions for Improvement
We recommend that the authors improve clarity by avoiding Bachmann-Landau notation in favor of non-asymptotic forms for theorems, providing corollaries for cases where variables scale. Additionally, we suggest including details for bounding the maximal information gain \( \gamma(\rho; t) \) and clarifying its significance in the context of reinforcement learning. The authors should also address the computational feasibility of the matrix inversion step and consider the implications of resetting value functions on data efficiency. Further, we encourage a discussion of the key steps and challenges in the proofs to enhance the technical portion of the main text.