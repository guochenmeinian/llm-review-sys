ID: pHwLbEkB0J
Title: Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called Cross-lingual Prompting (CLP) aimed at enhancing zero-shot chain-of-thought (CoT) reasoning across multiple languages. The authors propose two main components: (1) Cross-lingual Alignment Prompting, which aligns representations across languages, and (2) Task-specific Solver Prompting, which generates outcomes for reasoning tasks. Additionally, Cross-lingual Self-consistent Prompting (CLSP) is introduced to ensemble reasoning paths across languages. Empirical evaluations on the MGSM and XCOPA datasets demonstrate that CLP achieves state-of-the-art performance, with significant improvements over baseline methods.

### Strengths and Weaknesses
Strengths:
- The proposed method is intuitive and easy to follow.
- Extensive empirical studies are conducted, showing promising improvements across different languages.
- The paper is well-written and provides a clear description of the approach and results.

Weaknesses:
- The claim of the method being plug-and-play is not sufficiently substantiated, as empirical studies are limited to GPT-3.5.
- The literature review lacks coverage of recent works on zero-shot CoT and cross-lingual transfer learning.
- Important ablation studies and comparisons with existing methods, such as GPT3.5-Translate-EN, are missing.

### Suggestions for Improvement
We recommend that the authors improve the literature review by incorporating more related works on zero-shot CoT prompting and cross-lingual transfer learning. Additionally, the authors should verify the proposed prompting on more cross-lingual datasets, such as MKQA, XNLI, and XL-WiC. It would be beneficial to include an ablation study comparing the proposed method with GPT3.5-Translate-EN to clarify the contributions of the machine translation quality. We also suggest adding experiments with smaller LLMs and exploring few-shot prompting performance. Finally, addressing the minor typographical errors and ensuring clarity in the presentation of results would enhance the overall quality of the paper.