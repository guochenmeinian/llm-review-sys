ID: H9hWlfMT6O
Title: Training Transformers with 4-bit Integers
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 4, 6, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a 4-bit training method for transformer models aimed at accelerating the training process. The authors propose a Hadamard quantizer to manage outliers during forward propagation and utilize structural sparsity and bit splitting for gradient quantization in backpropagation. The method demonstrates competitive performance across various datasets, albeit with some accuracy degradation compared to the baseline.

### Strengths and Weaknesses
Strengths:
- The theoretical analysis of the proposed quantization method for both forward and backward steps is robust.
- The use of diverse models and tasks for evaluation enhances the credibility of the findings.
- The proposed method achieves competitive accuracy compared to existing 4-bit training techniques and is compatible with contemporary hardware.

Weaknesses:
- The reported speedup of up to 35% is limited, and there is no comparison with INT8 training speeds, raising questions about the advantages of INT4 over INT8.
- The speedup measurements are based on specific configurations not used in the evaluation tasks, which obscures the contribution of the method.
- Figures and tables, particularly Fig. 4, Fig. 5, and Table 3, lack clarity and detail, making it difficult to interpret the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figures 4 and 5 and consider reporting the speedup for each task directly in Table 1. Additionally, we suggest providing a detailed comparison of training speeds between INT4 and INT8 methods to justify the choice of INT4. Clarifying the contents of Table 3, including the datasets used and the meaning of "epoch 1-5," would enhance understanding. Lastly, addressing the scalability limitations of the proposed method for larger models would strengthen the paper's contributions.