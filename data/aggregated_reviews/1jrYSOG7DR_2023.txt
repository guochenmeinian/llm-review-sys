ID: 1jrYSOG7DR
Title: Revealing the unseen: Benchmarking video action recognition under occlusion
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of occlusion issues in video action recognition, proposing three benchmark datasets: UCF-101-O, Kinetics-400-O, and UCF-101-Y-OCC, which include various occlusion types and severities. The authors conduct a benchmarking study on existing CNN and transformer models, finding that transformer-based models are generally more robust to occlusions. Additionally, they introduce a novel compositional model, CTx-Net, which demonstrates improved performance in handling occlusions. The authors have updated the paper to include clearer visualizations, additional experimental results, and a more comprehensive discussion of the dataset collection process. They address performance discrepancies when training with occluded data and provide insights into the model's adaptability and pre-training procedures.

### Strengths and Weaknesses
Strengths:  
- The paper constructs benchmark datasets with systematically designed occlusions, including a natural occlusion dataset from YouTube, enhancing the credibility of the benchmarking results.  
- It provides a comprehensive evaluation of various models, revealing significant insights regarding the resilience of CNNs versus transformers to occlusions.  
- The introduction of CTx-Net as a compositional model represents a noteworthy contribution to the field.  
- The authors have made significant improvements in visualizations, particularly for confusion matrices and results in Tables 3 and 4.  
- Additional experimental results have been incorporated, enhancing the benchmarking analysis.  
- The paper provides a thorough discussion of the dataset collection process and the challenges associated with occluded data.

Weaknesses:  
- The paper lacks detailed descriptions of data collection and generation processes, particularly for the proposed datasets.  
- There is insufficient discussion of prior work on occlusion handling in video classification, which could strengthen the literature review.  
- The clarity of experimental settings and pretraining protocols is inadequate, leading to confusion regarding model training and evaluation.  
- The explanation for the drop in performance when training with occluded data remains unconvincing to some reviewers, particularly regarding the challenges posed by synthetic occlusions.  
- There is a call for deeper analysis and visualizations to support hypotheses, particularly in relation to the performance of CTx-Net and Video MAE.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the data collection and generation processes for the datasets, ensuring that sufficient detail is provided. Additionally, the authors should enhance the literature review by including relevant prior work on occlusion handling in video classification. We suggest that the authors clarify the pretraining protocols, including which datasets were used and the specifics of the training procedures for each model. Furthermore, we recommend improving the clarity of the explanation regarding the performance drop associated with occluded data, possibly by providing more detailed visualizations or examples of failures. Consider including activation maps or other forms of visual evidence in the supplementary material to substantiate the claims made about the model's performance. Lastly, we encourage the authors to provide more visualizations and graphical representations of the data to enhance comprehension and ensure consistency in the presentation of results across all tables and figures.