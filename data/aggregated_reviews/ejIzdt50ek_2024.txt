ID: ejIzdt50ek
Title: Stochastic Optimization Schemes for Performative Prediction with Nonconvex Loss
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 7, 7, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on nonconvex performative prediction optimization, introducing a new stationarity notion and demonstrating convergence for stochastic gradient descent (SGD) with a greedy deployment scheme. The authors propose the concept of "stationary performative stability," which characterizes convergence rates and bias magnitudes, extending previous work that primarily focused on convex loss functions.

### Strengths and Weaknesses
Strengths:
1. The paper significantly expands the scope of optimization in performative prediction, moving beyond strongly convex losses.
2. The results do not require a bound on the sensitivity parameter epsilon, which is a major advantage over prior works.
3. The analysis of the lazy deployment scheme, which approximates risk minimization and incurs no bias in the limit, is a valuable addition.
4. The writing is clear and the storyline is easy to follow.

Weaknesses:
1. The performative prediction problem lacks motivation, as there is no clear application that necessitates its formulation over classical optimization objectives.
2. The numerical experiments do not convincingly justify the study of the problem.
3. Some assumptions, such as the global upper bound in C2, may be unrealistic.
4. The definition of stationary performative stability (SPS) does not account for gradients over the distribution parameter, potentially misrepresenting convergence.

### Suggestions for Improvement
We recommend that the authors improve the motivation for the performative prediction problem by providing a clear application that cannot be addressed through classical optimization. Additionally, we suggest enhancing the numerical experiments to better justify the relevance of the study. Clarifying the assumptions made in the analysis, particularly regarding the global upper bound, would strengthen the paper. Furthermore, we advise revising the definition of SPS to include gradients over the distribution parameter for a more accurate representation of convergence. Lastly, we encourage the authors to simplify some discussions, such as in Section 3.1, and to clarify the use of symbols in Theorems and Lemmas for consistency.