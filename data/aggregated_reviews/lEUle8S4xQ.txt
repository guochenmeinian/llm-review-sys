ID: lEUle8S4xQ
Title: S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Structured Sparse Fine-Tuning (S²FT) for large language models (LLMs), addressing the limitations of current parameter-efficient fine-tuning (PEFT) methods that struggle to balance high-quality training, efficiency, and scalability. S²FT achieves state-of-the-art performance by selecting a few heads in the Multi-Head Attention (MHA) and channels in the Feed-Forward Network (FFN) modules, forming dense, trainable submatrices through weight matrix permutation. The method claims to enhance generalization, reduce memory usage by up to three times, and improve throughput by 1.5-2.7 times compared to existing methods. The paper also establishes an interface between S²FT and LoRA for joint computing paradigms.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a novel and valuable method that effectively addresses key issues in LLM fine-tuning.  
- It provides comprehensive mathematical proofs and a clear theoretical analysis, demonstrating solid theoretical foundations.  
- The simplicity of the method, requiring minimal code and straightforward matrix rearrangement, is a significant advantage.  
- The experiments are extensive, covering various tasks and datasets, and the results show promising performance improvements.

Weaknesses:  
- The novelty of the pruning technique is limited, as it resembles existing structured sparse training methods.  
- The proof symbols could be better organized for clarity, and the visual representations are insufficient.  
- The experimental results show some tasks where S²FT performs worse than previous methods, necessitating deeper analysis.  
- The paper lacks exploration of S²FT's application to other model architectures and does not provide the source code for reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the organization of symbols in the proof process for better readability. Additionally, we suggest including more visual representations to clarify the method's implementation. The authors should analyze the experimental results more thoroughly, particularly for tasks where performance degrades. We also encourage the authors to explore the application of S²FT to different model architectures and provide the source code to facilitate reproducibility. Finally, addressing the potential impact of dataset distribution on the U matrix in low-rank decomposition would enhance the paper's depth.