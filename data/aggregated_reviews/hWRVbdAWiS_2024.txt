ID: hWRVbdAWiS
Title: Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach combining entropy-regularized diffusion policies with Q-ensembles for offline reinforcement learning, addressing Q-value overestimation on out-of-distribution data. The authors propose using a mean-reverting stochastic differential equation (SDE) to transform action distributions into a Gaussian form, enhancing exploration and providing robust value estimation. The method demonstrates state-of-the-art performance on D4RL benchmarks, particularly in AntMaze.

### Strengths and Weaknesses
Strengths:
- The paper introduces a theoretically robust method that effectively combines entropy regularization and Q-ensembles, offering a solution to Q-value overestimation.
- Detailed theoretical contributions and a strong connection to existing diffusion literature are provided.
- The empirical results show improved performance compared to relevant benchmarks, particularly in D4RL environments.

Weaknesses:
- High computational costs and extensive training requirements limit accessibility and applicability in real-world scenarios.
- There is a disconnect between theoretical and experimental aspects regarding discounting and infinite-time horizons.
- The evaluation is primarily focused on D4RL benchmarks, with insufficient exploration of hyperparameter sensitivity and generalizability to diverse offline RL tasks.

### Suggestions for Improvement
We recommend that the authors improve the exploration of hyperparameter sensitivity, particularly regarding the entropy temperature and Q-ensemble size, to validate the robustness of the method. Additionally, we suggest including more experiments to illustrate the impact of varying $T$ values and the choice of $\eta$. Clarifying the use of the "max Q trick" in specific environments and providing a performance comparison without it would enhance understanding. We also encourage the authors to address the computational limitations more thoroughly and provide additional details on the mathematical proofs and experimental results, particularly regarding the discrepancies observed in training curves and performance metrics. Lastly, consider revising the notation for clarity and consistency throughout the paper.