ID: q0c1JTukWE
Title: On Surgical Fine-tuning for Language Encoders
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for surgical fine-tuning in NLP, utilizing the Fisher Information Matrix (FIM) to select layers for fine-tuning in encoder-based language models. The authors demonstrate that their approach can achieve competitive performance on GLUE and SuperGLUE benchmarks, particularly for tasks that do not require extensive reasoning or world knowledge. The paper also analyzes the effects of linguistic features and distributional shifts on fine-tuning performance.

### Strengths and Weaknesses
Strengths:
- The idea of using FIM for layer selection is sound and the methodology is straightforward.
- The paper is well-written and easy to follow, with effective analysis of performance variations across tasks.
- The surgical fine-tuning method shows effectiveness on most tasks, achieving results within Â±5% of full fine-tuning.

Weaknesses:
- The theoretical basis for static layer selection prior to fine-tuning is questionable, as parameter importance can change dynamically.
- The method is only validated on NLU tasks, leaving NLG tasks unexplored.
- The paper lacks comparisons with existing methods, such as AutoFreeze and LoRA, which could substantiate the advantages of the proposed approach.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for static layer selection by addressing the dynamic nature of parameter importance during fine-tuning. Additionally, the authors should expand their validation to include NLG tasks. It is crucial to include comparisons with baseline methods like AutoFreeze and LoRA, providing performance metrics and highlighting the benefits of their method in terms of parameter and sample efficiency. Furthermore, we suggest correcting the references to figures on lines 453 and 454 for clarity.