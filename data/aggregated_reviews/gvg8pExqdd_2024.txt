ID: gvg8pExqdd
Title: Diversify, Contextualize, and Adapt: Efficient Entropy Modeling for Neural Image Codec
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 5, 6, 6, 5
Original Confidences: 1, 4, 1, 4

Aggregated Review:
### Key Points
This paper presents a novel method for deep image compression that enhances entropy encoding by introducing three contexts—local, regional, and global—to capture contextual information at varying scales while maintaining computational efficiency. The authors demonstrate that their approach achieves state-of-the-art (SOTA) compression rates with a moderate model size and relatively fast decoding times. The method is built upon existing frameworks and aims to improve the accuracy of predictions related to quantized latent values produced by the autoencoder.

### Strengths and Weaknesses
Strengths:  
- The proposed method fills a gap in efficiently capturing contextual information with minimal compute cost, addressing a significant challenge in the field.  
- Extensive experiments on real-world high-resolution images show substantial improvements over baseline methods, with a notable average rate savings of 11.96% over the VTM codec.  
- The paper includes a thorough evaluation of runtime performance and the importance of different contexts, providing valuable insights into the method's effectiveness.

Weaknesses:  
- The comparison is limited to only four baseline methods, necessitating a broader evaluation against more state-of-the-art techniques.  
- The reported improvements in compression rates, while significant, may not be compelling enough to attract interest from practitioners focused on developing new standard codecs.  
- The paper lacks a theoretical foundation and reproducibility considerations, particularly regarding the performance of the entropy model across different hardware platforms.

### Suggestions for Improvement
We recommend that the authors improve the comparison by including a wider range of state-of-the-art methods to better contextualize their results. Additionally, we suggest breaking down runtime performance by subsystem to provide clearer insights into the encoding and decoding processes. Addressing the reproducibility of the entropy model across various hardware configurations is crucial, and we encourage the authors to detail the mitigations implemented for this purpose. Finally, including additional qualitative results in the appendix could enhance the paper's comprehensiveness and appeal.