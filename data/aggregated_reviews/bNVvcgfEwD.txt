ID: bNVvcgfEwD
Title: Convergence of Adafactor under Non-Convex Smooth Stochastic Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 3, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the convergence properties of Adafactor, an adaptive learning rate optimizer, in non-convex smooth settings. The authors demonstrate that Adafactor achieves a convergence rate of $\tilde{O}(1/\sqrt{T})$ in both full-batch and stochastic scenarios, supported by empirical evidence. The research aims to fill the gap in the theoretical understanding of Adafactor, particularly its performance in memory-constrained environments and its implications for training large language models.

### Strengths and Weaknesses
Strengths:  
The analysis of Adafactorâ€™s convergence is vital for training large models, contributing significantly to its theoretical foundations. The paper is well-motivated, clearly discussing the connection between Adafactor and Adam, and presents empirical findings that validate the proposed hyperparameter settings.

Weaknesses:  
The theoretical analysis lacks novelty, as similar convergence properties have been previously established for other adaptive methods. The empirical evidence provided is not comprehensive enough to convincingly demonstrate optimal convergence in practice. Additionally, the manuscript could benefit from a deeper discussion on the proving techniques and the tightness of the convergence bounds.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis by providing a more comprehensive discussion of the proving techniques and the tightness of the convergence bounds. Including proof sketches for both the full-batch and stochastic cases in the main body would enhance clarity. Additionally, we suggest conducting more extensive empirical studies across various NLP settings to substantiate the claims regarding optimal convergence rates. It would also be beneficial to reference figures when discussing experimental results and to include time-evolution data for train-loss convergence in the experiments.