ID: qJRlz3SucN
Title: VaRT: Variational Regression Trees
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 8, 5, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a non-parametric Bayesian model for regression trees, utilizing variational inference to approximate the posterior distribution. The authors introduce a prior model comprising three components: the tree structure, probabilistic splitting criteria, and conditional distributions at each leaf. Extensive experiments demonstrate the method's competitive performance against benchmarks on various datasets.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clear, with a strong original contribution.
- Numerical experiments support the proposed method, addressing challenges in Bayesian tree methods for larger datasets.
- The innovative use of variational inference offers promising computational benefits.

Weaknesses:
- The basic idea is perceived as too simple, with prior distributions and reparametrization techniques lacking novelty.
- There is insufficient introduction and discussion regarding the 18 datasets used, and the RMSEs reported do not consistently reflect the best performance.
- Comparisons with important baselines, such as soft decision tree models and ensemble methods, are missing, and hyperparameter sensitivity is not explored.

### Suggestions for Improvement
We recommend that the authors improve the introduction by providing context for the 18 datasets and discussing the prediction results more thoroughly. Additionally, including comparisons with ensemble tree algorithms and soft BART would strengthen the paper. It would also be beneficial to investigate hyperparameter sensitivity and provide more detailed experimental results, including how RMSEs were computed. Lastly, enhancing the clarity of notations and figures through careful proofreading would improve the overall presentation.