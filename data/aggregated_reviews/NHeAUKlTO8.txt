ID: NHeAUKlTO8
Title: PartialFormer: Modeling Part Instead of Whole for Machine Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a parameter-efficient Transformer model that enhances performance by merging the feed-forward network (FFN) into attention heads and employing a shared low-input dimension FFN. The authors propose modeling gate mechanisms, increasing the number of attention heads, and utilizing a global attention mechanism. Experiments on nine translation tasks demonstrate that this approach yields better performance than the vanilla Transformer with comparable parameters. The paper also introduces "PartialFormer," which modifies the FFN modeling strategy and employs depth and width scaling strategies to maintain performance while reducing parameter usage.

### Strengths and Weaknesses
Strengths:
- The proposed methods effectively reduce parameter redundancy and improve performance across multiple translation tasks.
- The paper is well-written and presents a clear motivation for the proposed approaches.
- The experimental results support the claims made regarding the effectiveness of the proposed architectures.

Weaknesses:
- The paper lacks detailed descriptions of the Cross-AFFN sub-layer and its computations, which could lead to increased decoder depth.
- The performance of the most widely used 6-layer base model is not reported, raising questions about hyper-parameter optimization.
- Some experimental results may not be comparable due to variations in tokenization methods and conditions under which they were tested.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the Cross-AFFN sub-layer's functionality and its impact on decoder depth. Additionally, the authors should report the performance of the proposed architecture in a 6-layer base setting to clarify hyper-parameter optimization. We also suggest using sacreBLEU instead of multi-BLEU in the main experiments to enhance result reliability. Finally, addressing the irregular fluctuations in head diversity and ensuring consistent experimental conditions across comparisons would strengthen the paper's findings.