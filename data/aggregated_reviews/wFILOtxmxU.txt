ID: wFILOtxmxU
Title: Syntax-Aware Retrieval Augmented Code Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents kNN-TRANX, a token-level retrieval augmented code generation method designed to reduce extraneous noise during code generation by enabling searches in smaller, task-specific datastores. The authors propose using a seq2tree model to extract AST information, which aids in applying syntax constraints for retrieval, thereby enhancing code quality. The results on two public datasets demonstrate the effectiveness of kNN-TRANX.

### Strengths and Weaknesses
Strengths:
- The paper is well written and easy to follow, with a clear structure.
- The proposed method achieves better results than baselines on two benchmarks, and the authors provide additional experimental analysis to deepen understanding of the models.
- The combination of kNN-MT and Seq2Tree is intriguing, with mechanisms like Syntax-constrained token-level retrieval, Meta-k network, and Confidence network enhancing performance.

Weaknesses:
- The paper lacks comparison with state-of-the-art retrieval-augmented methods such as REDCODER and CodeT5+, as well as widely adopted benchmarks like APPS and HumanEval, making it difficult to assess the technical significance of the proposed model.
- The use of seq2tree models is questionable, as AST information can be easily obtained via off-the-shelf parsers.
- The retrieval component of kNN-TRANX appears overly complicated, potentially sacrificing inference efficiency, which is crucial for deployment.
- The evaluation lacks thoroughness, with insufficient ablation tests to demonstrate the contribution of novel components.

### Suggestions for Improvement
We recommend that the authors improve the paper by including comparisons with state-of-the-art retrieval-augmented methods such as REDCODER and CodeT5+, as well as large language models like ChatGPT and StarCoder. Additionally, a discussion on the relevance of these comparisons should be included. The authors should clarify the necessity of using seq2tree models for AST extraction instead of static parsers. We also suggest enhancing the evaluation by adopting the Pass@K metric and providing more thorough ablation studies to substantiate the significance of the proposed components. Finally, addressing the clarity of the datastore construction equations and the incremental learning capability early in the paper would improve readability.