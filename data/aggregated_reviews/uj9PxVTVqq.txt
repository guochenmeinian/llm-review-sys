ID: uj9PxVTVqq
Title: Enhancing Knowledge Transfer for Task Incremental Learning with Data-free Subnetwork
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 4, 6, 6, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Data-free Subnetworks (DSN) approach for task incremental learning, leveraging the Lottery Ticket Hypothesis to create task-specific neuron-wise masks for optimal subnetworks. The authors argue that mask measurements are less affected by input dimensions compared to weight-based measurements, facilitating backward knowledge transfer through data-free replay. This allows for the integration of new tasks while preserving knowledge from previous tasks. Experimental results demonstrate that the DSN method shows superior performance across four benchmarks, including PMNIST and CIFAR-100, and that the mask similarity aligns closely with task similarity, despite the challenges posed by the unavailability of old samples. However, the authors emphasize that transferring knowledge to multiple old tasks incurs significant time and memory costs, leading them to focus on the most similar tasks.

### Strengths and Weaknesses
Strengths:
- The approach is novel and effectively addresses backward knowledge transfer, a critical aspect often overlooked in incremental learning.
- The organization of the paper is commendable, guiding readers through the methodology and results clearly.
- The paper introduces a clear methodology for measuring task similarity using masks, which is less sensitive to input dimensionality.
- Comprehensive experiments validate the method's effectiveness, showcasing improvements in various evaluation metrics and supporting the effectiveness of the proposed DSN in enhancing model performance.

Weaknesses:
- The mechanism of backward knowledge transfer via data-free replay is unclear, as it seems to focus solely on old task knowledge without incorporating new task information.
- The reliance on self-generated impressions for performance evaluation raises concerns about the validity of the accuracy metrics.
- The complexity of the fine-tuning process and its potential for learning confusion when using both impressions and current task data is not fully addressed.
- The related work section lacks detailed comparisons with similar architecture-based and incremental learning methods.
- Methodological details, particularly in equations and optimization objectives, require clarification.
- The approach currently limits knowledge transfer to the most similar previous task, potentially missing opportunities for broader knowledge integration.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how backward knowledge transfer is achieved through data-free replay, ensuring that it incorporates new knowledge effectively. Additionally, we suggest that the authors provide further justification for using self-generated impressions as a benchmark for performance evaluation, as this could enhance the credibility of their results. We also recommend enhancing the related work section with comparisons to architecture-based methods and discussing the space complexity of learned mask embeddings. Clarifying the method details, particularly in equations and the validity of the Dirichlet distributions, is essential. Finally, we encourage the authors to explore the potential for knowledge transfer across multiple similar tasks to enhance the robustness of the approach.