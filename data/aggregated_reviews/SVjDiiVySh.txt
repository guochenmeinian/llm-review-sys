ID: SVjDiiVySh
Title: Improving CLIP Training with Language Rewrites
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 6, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Language augmented CLIP (LaCLIP), a method that enhances CLIP training through language rewriting to address the imbalance between image and text encoders in CLIP. Extensive experiments on datasets such as CC3M, CC12M, RedCaps, and LAION-400M demonstrate that LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on LAION-400M in terms of ImageNet 0-shot accuracy, without incurring additional computational or memory overhead during training.

### Strengths and Weaknesses
Strengths:  
- The proposed method is conceptually simple and effectively leverages existing pretrained large language models (LLMs).  
- Extensive experiments across multiple datasets and backbones are conducted within a reasonable computational budget.  
- The paper is well-written, with clear technical details and thorough empirical evaluations that cover various downstream tasks.  

Weaknesses:  
- There is a lack of comparison with existing text augmentation methods, such as DeCLIP and LaViLa, which could provide context for LaCLIP's contributions.  
- The paper does not adequately address how text augmentation compares with fine-tuning or using frozen text encoders pretrained on large-scale text data.  
- The method's reliance on a well-pretrained language re-writer incurs significant computing costs and limits end-to-end training.  
- The absence of critical ablation studies, particularly regarding the influence of text encoder size and the length of rewritten descriptions, is noted.  

### Suggestions for Improvement
We recommend that the authors improve the paper by including comparisons with existing text augmentation methods applied to CLIP training, such as DeCLIP and LaViLa, to contextualize LaCLIP's contributions. Additionally, the authors should explore the relationship between text augmentation and fine-tuning or frozen text encoders pretrained on large datasets. It would be beneficial to conduct ablation studies on the text encoder size and the length of rewritten descriptions to provide deeper insights into the method's performance. Finally, addressing the potential issue of noisy captions in the training data and proposing methods to mitigate this risk would enhance the robustness of the approach.