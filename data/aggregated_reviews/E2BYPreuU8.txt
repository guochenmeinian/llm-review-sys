ID: E2BYPreuU8
Title: On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the behavior of a single unit of linear self-attention trained via gradient flow on an autoregressive (AR) loss for token sequences generated by a specific class of noiseless linear systems. The authors propose that under certain assumptions, the gradient flow from specific initializations results in a model whose in-context learning (ICL) predictions can be interpreted as those of another model updated with one step of gradient descent (GD). They further assert that a stronger assumption is necessary and sufficient for accurate predictions from this GD. The theoretical contributions are supported by numerical simulations.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant issue in ICL theory, focusing on autoregressive objectives rather than few-shot pretraining.
- The theoretical results appear correct, with clearly defined notations and a well-written proof sketch.
- Experiments effectively verify the theoretical results, providing sufficient detail.

Weaknesses:
- The results do not convincingly demonstrate that "ICL by AR pretraining is more difficult than ICL by few-shot pretraining," as the tasks compared are fundamentally different. A fair comparison requires evaluating transformers trained under the same data/task model.
- The artificial setting raises doubts about the significance of the results, as the model is limited to a single linear attention unit and assumes a deterministic linear system.
- The initialization scheme and the reliance on a specific distributional assumption limit the generalizability of the findings.
- The term "gradient clipping" is misapplied, as it refers to a masking procedure that is more restrictive than standard clipping.
- Certain assumptions and propositions are tautological or lack insight, failing to identify key properties that lead to the observed behavior.

### Suggestions for Improvement
We recommend that the authors improve the framing of their discussion to emphasize the inadequacy of the regression data model considered by existing ICL literature, as this is a key contribution. Additionally, the authors should conduct more simulations with various initialization schemes and architectures to support their claims about the impact of data distribution on mesa-optimization. Clarifying the distinction between "gradient clipping" and the masking procedure used would enhance the accuracy of the terminology. Finally, addressing the limitations of the one-layer linear model and exploring the implications of more complex architectures would strengthen the paper's contributions.