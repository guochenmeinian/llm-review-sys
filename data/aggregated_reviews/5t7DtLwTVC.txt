ID: 5t7DtLwTVC
Title: WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 7, 9
Original Confidences: 4, 4, 3

Aggregated Review:
### Key Points
This paper presents WikiContradict, a benchmark dataset mined from Wikipedia to evaluate how LLMs handle real-world knowledge conflicts. The key contributions include a novel dataset of 253 human-annotated instances of contradictory information, a comprehensive evaluation of multiple LLMs under various QA scenarios, human evaluation results highlighting current LLM limitations, and an automated evaluation method (WikiContradictEval) for large-scale assessments. The authors address a critical issue in RAG evaluation, revealing that models struggle with contradictory information even when prompted, and they provide insights into explicit versus implicit contradictions.

### Strengths and Weaknesses
Strengths:
- The dataset is novel and derived from real-world Wikipedia contradictions, utilizing inconsistency tags effectively.
- The evaluation methodology is comprehensive, testing multiple LLMs under various conditions.
- The human evaluation process is rigorous, demonstrating good inter-annotator agreement.
- The analysis, particularly regarding explicit versus implicit contradictions, is insightful.

Weaknesses:
- The dataset primarily contains public information likely encountered during pre-training, making it challenging for models to avoid using intrinsic knowledge without careful prompting. Evaluating answer factuality alongside proposed metrics would be beneficial.
- The study focuses on zero-shot scenarios but does not explore few-shot setups where examples of expected behavior are provided.
- The discussion on improving LLM performance in handling contradictions is limited.
- The reliance on Wikipedia tags may introduce bias, and some contradictory passages may not be truly contradictory.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation task in the abstract and introduction, explicitly stating that models are expected to identify and respond with both contradicting answers. Additionally, we suggest adding more discussion on related work, particularly addressing similar studies that explore knowledge conflicts. Providing finer-grain categories for contradiction types and discussing automatic evaluation with models beyond GPT-4 would enhance the paper. Insights from an error study, detailing specific reasons for partially correct or incorrect responses, would also be valuable.