ID: fwmZinFwgX
Title: Enhancing Reasoning Capabilities by Instruction Learning and Chain-of-Thoughts for Implicit Discourse Relation Recognition
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to the Implicit Discourse Relation Recognition (IDRR) task by integrating generative models with instruction learning and chain-of-thought methodologies. The authors evaluate their method against three benchmarks: PDTB 2.0, PTDB 3.0, and CoNLL16, reporting results that surpass previous Transformer-based and prompting approaches. However, the clarity of the fine-tuning process and the structure of the paper, particularly regarding figures and descriptions, raises concerns.

### Strengths and Weaknesses
Strengths:  
- The combination of generative models and chain-of-thought is original and intriguing.  
- Experimental results demonstrate the effectiveness of the proposed method.  
- Extensive experiments were conducted.

Weaknesses:  
- Some descriptions, particularly regarding fine-tuning and instruction learning, are unclear.  
- The empirical contribution lacks sufficient data points and analysis, with only marginal accuracy improvements reported.  
- The implementation of the RoBERTa baseline appears questionable, and claims of state-of-the-art performance are not substantiated.  
- Missing references and unclear figure descriptions contribute to confusion.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the fine-tuning and instruction learning sections, explicitly detailing the modeling of the fine-tuning task, including input and output specifications. Additionally, we suggest providing more empirical data points, such as comparisons between different generative AI models like Llama and Flan-T5, as well as varying model sizes (3B, 7B, and 11B). Furthermore, a more thorough analysis of the evaluation results is necessary to address the observed low performance boosting and to clarify the effectiveness of the chain-of-thought approach in the context of their findings. Lastly, we advise correcting any discrepancies in the reported performance of the RoBERTa baseline and ensuring all relevant literature is cited.