ID: Sm3RzRKCel
Title: Dialogizer: Context-aware Conversational-QA Dataset Generation from Textual Sources
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Dialogizer, a novel framework aimed at generating contextually relevant Conversational Question Answering (ConvQA) datasets from textual sources. The authors propose two training methodologies: question-answer matching (QAM) and topic-aware dialog generation (TDG), alongside dialog reconstruction, to mitigate the issue of contextually-irrelevant questions. The framework has produced four ConvQA datasets, which are claimed to exhibit higher quality compared to baseline models. The evaluation includes both automatic metrics and human assessments.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated, clearly discussing the limitations of previous methods and demonstrating the effectiveness of the proposed approach through comprehensive experiments.
- Dialogizer-generated datasets show improved quality and contextual relevance, indicating potential advancements in ConvQA research.
- The combination of quantitative and qualitative evidence enhances the credibility of the findings.

Weaknesses:
- The contribution may be perceived as incremental rather than transformative, primarily building on existing dialog inpainting methods.
- The paper lacks a thorough analysis of dataset quality and potential biases in the automatic generation process.
- Some experimental results, particularly regarding the impact of re-ranking on evaluation scores, raise questions about the true contribution of QAM and TDG.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by presenting more diverse metrics to validate the generated texts from Dialogizer and Dialogue Inpainter. A quantitative analysis of the generated texts, rather than relying solely on specific examples, would enhance persuasiveness. Additionally, the authors should consider comparing their methods with instruction tuning to assess performance differences. Clarifying the extraction of topic words and the construction of questions and answers in the TDG section is essential. Conducting a hyper-parameter analysis and using GPT-4 for text evaluation would also strengthen the study. Finally, addressing the lack of discussion on previous work in conversational question generation would provide a more comprehensive context for the proposed methods.