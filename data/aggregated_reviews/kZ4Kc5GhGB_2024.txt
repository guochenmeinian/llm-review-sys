ID: kZ4Kc5GhGB
Title: Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of representation complexity across three reinforcement learning (RL) paradigms: model-based, policy-based, and value-based RL. The authors demonstrate that representing the model is the least complex task, followed by the optimal policy, while the optimal value function exhibits the highest complexity. The analysis includes reductions to established theoretical complexity classes and introduces new classes of MDPs, such as 3-SAT MDPs and NP MDPs, to illustrate these differences.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and addresses a well-motivated and interesting problem.  
- It provides a deep theoretical analysis of representation complexity, transitioning from simple to broader scenarios, and includes empirical results that align with theoretical findings.  
- The introduction of new MDP classes enhances understanding of the complexity relationships among RL paradigms.  

Weaknesses:  
- The theoretical insights lack extensive exploration of their implications for real-world applications.  
- The use of simple MLPs for approximation error validation offers limited insights for modern deep RL.  
- The empirical section raises questions about the assumption that the Mujoco locomotion environments fit the described complexity relationships, as alternative scenarios may exist.  
- Figures are barely legible at reasonable zoom levels, necessitating redesign.

### Suggestions for Improvement
We recommend that the authors improve the clarity and legibility of all figures to enhance understanding. Additionally, we encourage the authors to address the strictness of the hierarchy more carefully, as this could impact the validity of their conclusions. Engaging with the potential for reversed complexity scenarios in MDPs would strengthen the paper. Furthermore, we suggest including more experimental validation to support theoretical claims and exploring the implications of their findings for various neural network architectures beyond MLPs. Finally, providing case studies or real-world examples would help contextualize the representation complexity hierarchy in practical applications.