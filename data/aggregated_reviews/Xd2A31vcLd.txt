ID: Xd2A31vcLd
Title: ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to enhance text-only dialogue systems by integrating visual knowledge, addressing limitations such as restricted dataset scale and quality. The authors propose a new paradigm for constructing multimodal dialogues, introducing a framework named RESEE that divides visual knowledge into finer granularities at both entity and turn levels. Extensive experiments demonstrate the effectiveness of RESEE, which incorporates visual representations into vanilla dialogue models, and the authors contribute two new datasets, RESEE-WoW and RESEE-DD, that extend text-only dialogues.

### Strengths and Weaknesses
Strengths:  
- The paper provides a significant contribution to multimodal dialogue systems, with a clear motivation and well-structured methodology.  
- Extensive ablation studies and human evaluations reinforce the findings, indicating potential real-world applications, such as in multi-modal E-commerce recommender systems.  
- The introduction of two new datasets is a valuable resource for advancing research in this domain.  

Weaknesses:  
- The novelty of the proposed approach is questioned, as it appears to be a simple extension of previous work without significant innovation.  
- Concerns about the cold-start performance and the presence of visual biases in the auto-constructed datasets are raised.  
- The complexity of the methodology may hinder reproducibility, necessitating clearer algorithmic explanations.

### Suggestions for Improvement
We recommend that the authors improve the paper by enriching it with qualitative results that provide a comprehensive assessment of the model's performance. Additionally, addressing the cold-start performance is crucial, as real-world applications may encounter user prompts outside the model's learned response domain. To enhance reproducibility, we suggest incorporating a clear and concise algorithmic box that illustrates the learning algorithm used for training and inference. Furthermore, we encourage the authors to clarify the purpose of extending dialogue datasets with images and to benchmark the computational cost of their approach.