ID: Faxkz2V56o
Title: Noisy Self-Training with Synthetic Queries for Dense Retrieval
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a self-training strategy aimed at iteratively enhancing augmented training data for dense retrieval tasks. The authors propose generating queries for unlabelled passages and improving their quality through pseudo labels from a teacher model. Experiments conducted on MS MARCO, NQ, and BEIR demonstrate that this approach enhances both bi-encoder and cross-encoder ranking models, reducing the reliance on labeled data. However, the paper's contributions are viewed as incremental, primarily saving training time compared to existing methods like GPL and PTR.

### Strengths and Weaknesses
Strengths:
- The motivation for the self-training approach is clear and reasonable.
- The experiments are comprehensive and support the main claims effectively.
- The paper is well-written and easy to follow.

Weaknesses:
- The technical novelty is limited, with significant similarities to existing methods like PTR and GPL.
- The motivation for generating pseudo labels lacks depth, and a comprehensive noise analysis of the generated queries is needed.
- The claim that soft labels are more robust to noise is not well justified, and the out-of-domain performance appears weak compared to baselines.

### Suggestions for Improvement
We recommend that the authors improve the justification for the robustness of soft labels generated by the teacher model. Additionally, a comprehensive noise analysis of the generated queries would strengthen the motivation for the proposed method. We suggest including a convergence analysis to understand the iterative training process better. Furthermore, exploring the performance of a single bi-encoder model trained with synthetic data and cross-encoder scores from GPL could provide valuable insights. Lastly, consider updating the query generator via the self-training mechanism to enhance efficiency.