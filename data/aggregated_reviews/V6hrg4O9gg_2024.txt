ID: V6hrg4O9gg
Title: CodeRosetta: Pushing the Boundaries of Unsupervised Code Translation for Parallel Programming
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CodeRosetta, an encoder-decoder transformer model designed for unsupervised translation between programming languages, particularly focusing on high-performance computing (HPC) extensions like C++ to CUDA and Fortran. The authors claim that CodeRosetta outperforms larger language models, including GPT-4 and StarCoder, in translation tasks. The model employs techniques such as Abstract Syntax Tree (AST) Entity Recognition and customized Denoising Auto-Encoding to enhance its performance. Experimental results indicate improvements in BLEU, CodeBLEU, and compilation accuracy metrics. Additionally, the authors have conducted further experiments and provided clarifications that address previous reviewer concerns, including correcting a typo in Figure 4 for consistency with the text.

### Strengths and Weaknesses
Strengths:
1. CodeRosetta introduces novel learning objectives, including AST Entity Recognition and customized Denoising Auto-Encoding, which effectively address translation tasks.
2. The model demonstrates superior performance compared to state-of-the-art baselines, achieving significant improvements in key metrics.
3. The paper is well-written, providing clear context and background on the methods employed.
4. The authors have effectively addressed the reviewer's concerns through additional experiments and clarifications, enhancing the overall quality of the paper.
5. The correction of the typo in Figure 4 enhances the accuracy of the paper.

Weaknesses:
1. The evaluation relies heavily on BLEU and CodeBLEU metrics, which may not accurately reflect translation quality, and lacks comprehensive testing of runtime correctness.
2. The ablation study does not adequately address the impact of Masked Language Modeling (MLM) and back translation.
3. The paper lacks a detailed explanation of the model's training methodology, particularly regarding the unpaired collection of C++ and CUDA source files.
4. Initial misunderstandings regarding Figure 4 may indicate a need for clearer presentation of data.

### Suggestions for Improvement
We recommend that the authors improve the evaluation metrics by incorporating functional correctness assessments alongside BLEU and CodeBLEU to provide a more comprehensive evaluation of translation quality. Additionally, clarifying the training methodology and the implications of using unpaired data would enhance the paper's rigor. We also suggest including a discussion on the potential risks of data leakage during the fine-tuning process with synthetic data. Finally, addressing the limitations of existing metrics in code translation, providing more details on the datasets used, and improving the clarity of data presentation to prevent misunderstandings in future iterations would strengthen the manuscript.