ID: qnHB2SMQLA
Title: Take a Closer Look at Multilinguality! Improve Multilingual Pre-Training Using Monolingual Corpora Only
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into multilingual pretrained language models (mPLMs), specifically focusing on improving mono-mPLMs without parallel resources. The authors analyze the token properties of XLM-R, revealing that cross-lingual token alignments are stronger at the embedding layer than at higher layers. They propose a training objective that utilizes token-level and semantic-level code-switched masked language modeling to enhance cross-lingual interactions. Experimental results indicate that this method outperforms mono-mPLMs and achieves results comparable to para-mPLMs across various natural language understanding tasks.

### Strengths and Weaknesses
Strengths:
- The paper introduces new training objectives that leverage self-induced token alignments, enhancing mono-mPLMs without external resources.
- Empirical results demonstrate significant improvements over mono-mPLMs and comparable performance to para-mPLMs.
- The clarity of argumentation and extensive empirical evidence support the proposed methods.

Weaknesses:
- Contributions are seen as incremental, with prior works addressing similar concepts, such as token alignment analysis and code-switched masked language modeling.
- Some experimental details are lacking, particularly regarding the initialization of mono-mPLM and the impact of random substitutions during early training.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the novelty of their contributions by comparing their work with existing literature, such as Cao et al. (2020) and Wang et al. (2022). Additionally, clarify the experimental setup, specifically whether a pre-trained mono-mPLM was used and how many tokens were considered for weighted aggregation. We also suggest addressing the potential impact of context on cross-lingual word substitutions by providing attention weights. Finally, consider bolding the best scores in Tables 12 and 13 for better readability.