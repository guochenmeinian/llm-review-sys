ID: 3l9zUuFo9m
Title: Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to mitigate the costs of LLM inference by utilizing a preliminary classifier to handle simpler examples. The authors propose a teacher-student framework based on a kNN classifier that leverages past LLM predictions. The decision to delegate to the LLM is determined by a learned threshold that optimizes a discounted objective function. The experimental results indicate that the proposed system achieves accuracy within 0.5% of the LLM while reducing the number of examples requiring LLM predictions to one-third. 

### Strengths and Weaknesses
Strengths:
- The proposed approach addresses a timely issue of reducing operational costs associated with LLM calls while maintaining comparable accuracy.
- The teacher-student framework is a straightforward solution for optimizing cost and latency in LLM usage.

Weaknesses:
- The framework struggles with compositional or compound queries and lacks clarity on handling new labels not present in the training data.
- The evaluation is limited to a single dataset (Banking77) and specific NLP tasks, making generalization difficult.
- The use of proprietary GPT-4 as a teacher may not align with the goal of cost reduction, and there is insufficient detail on cache management and update frequency.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including additional NLP tasks such as sentiment classification and named-entity recognition to enhance generalizability. It would also be beneficial to clarify the cache management strategy, specifically detailing when to flush and update the cache. Furthermore, we suggest that the authors elaborate on how the algorithm handles new labels and provide statistics on the percentage of requests with identical responses in both training and testing phases. Lastly, grounding the discount factor in terms of real costs would strengthen the paper's impact.