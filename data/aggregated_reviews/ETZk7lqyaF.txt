ID: ETZk7lqyaF
Title: PersonalSum: A User-Subjective Guided Personalized Summarization Dataset for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the PersonalSum dataset, a novel resource for personalized summarization, annotated in Norwegian and featuring user-specific summaries. The dataset includes approximately 1100 personalized summaries from 441 news articles, annotated by 39 crowdworkers. The authors propose that this dataset addresses a significant gap in personalized summarization research, emphasizing user preferences and the challenges of generating personalized content using large language models (LLMs). The research explores the impact of various factors on summary generation, highlighting the complexities of personalization.

### Strengths and Weaknesses
Strengths:
- The introduction of the PersonalSum dataset fills a critical gap in personalized summarization research, particularly in non-English contexts.
- The dataset is well-annotated, ensuring diverse user representation and high quality.
- The paper provides a thorough description of the data collection and annotation process, enhancing transparency and reproducibility.

Weaknesses:
- The dataset's size is relatively small, which may limit its applicability for training large models.
- The paper lacks a detailed discussion on the specific linguistic challenges of the Norwegian language, weakening comparisons with English datasets.
- Standard summarization terminology, such as "extractive" and "abstractive," is not mentioned, indicating a gap in foundational knowledge.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of working with a lesser-spoken language like Norwegian, including specific linguistic features that may affect summarization. Additionally, the authors should incorporate standard summarization terminology to strengthen the paper's credibility. It would be beneficial to enhance the evaluation strategy by including human assessments alongside GPT evaluations to provide a more comprehensive evaluation of model performance. Finally, a more detailed discussion on the ethical implications of using user-specific data would further enrich the paper.