ID: n9xVaQMJNK
Title: Few-Shot Adversarial Prompt Learning on Vision-Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a few-shot adversarial training methodology for vision-language models, adapting the TRADES loss to enhance robustness in downstream tasks. The authors propose a uni-modal adversarial-aware mechanism that allows the model to distinguish between natural and adversarial examples, promoting meaningful learning suitable for few-shot scenarios. The methodology demonstrates high performance, achieving superior clean and robust accuracy compared to existing methods. Additionally, the paper introduces a few-shot adversarial prompt framework that enhances adversarial robustness using only 1% of training data, addressing limitations of existing approaches.

### Strengths and Weaknesses
Strengths:
- The methodology effectively adapts the TRADES adversarial training method for vision-language models, demonstrating significant improvements in robust accuracy across multiple datasets.
- The approach achieves state-of-the-art zero-shot adversarial robustness with minimal training data, showcasing efficiency and practical relevance.
- The paper is clearly written, with methodologies and results presented in an accessible manner.

Weaknesses:
- The motivation for the proposed methodology lacks clarity, particularly in addressing how previous methods' drawbacks are resolved. A clearer emphasis on the limitations of existing methods in section 3.5 would be beneficial.
- There is insufficient analysis of the proposed Uni-Modal Adversarial-Aware Mechanism, particularly regarding its implementation as a multiplicative factor to the KL divergence loss and its implications for robustness.
- The analysis of experimental results is inadequate, as some datasets showed decreased performance compared to existing methods, necessitating deeper consideration and explanation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation section by explicitly addressing how the proposed methodology overcomes the limitations of previous approaches. Additionally, a comparative baseline experiment analyzing performance with both clean and adversarial examples would provide valuable insights. We suggest that the authors provide a more thorough analysis of the Uni-Modal Adversarial-Aware Mechanism, including the rationale for its implementation and its impact on robustness. Finally, a detailed comparative analysis with existing methods would strengthen the paper's contributions.