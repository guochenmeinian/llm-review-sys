ID: pJbEXBBN88
Title: Adversarial Examples Exist in Two-Layer ReLU Networks for Low Dimensional Linear Subspaces
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the adversarial robustness of two-layer ReLU neural networks when data resides in a low-dimensional linear subspace. The authors demonstrate that training primarily alters the projections of the first-layer weight vectors onto this subspace, leaving components in the orthogonal subspace unchanged. Key results include a lower bound on the gradient's projection to the orthogonal subspace, the existence of directions for universal adversarial perturbations, and the effectiveness of reducing initialization scale or applying L2 regularization to enhance robustness. The theoretical findings are supported by experiments on small datasets, suggesting potential extensions beyond the paper's assumptions.

### Strengths and Weaknesses
Strengths:  
- The paper serves as a theoretical follow-up to existing empirical work, particularly addressing the effects of initialization scale and L2 regularization.  
- Theorems are rigorously proved, with detailed appendices and helpful sketches in the main text.  
- Experiments provide intuition and suggest future research directions.

Weaknesses:  
- The proofs may lack surprising elements or complexity.  
- The experiments are limited in scope, primarily utilizing simple synthetic data, which raises questions about generalizability to more complex datasets like MNIST.  
- Some claims require further justification, particularly regarding on-manifold perturbations and their comparison to off-manifold perturbations.

### Suggestions for Improvement
We recommend that the authors improve the justification for claims regarding on-manifold perturbations, particularly by providing results that define the smallest perturbation strength leading to incorrect predictions. Additionally, sharing the code for experiments or reporting observations when training iterations are sufficiently large would enhance the reproducibility of results. We also suggest including more comprehensive experimental results, particularly on real-world datasets, to strengthen the paper's contributions. Finally, addressing the potential trade-offs between clean performance and adversarial robustness in the context of L2 regularization would provide valuable insights.