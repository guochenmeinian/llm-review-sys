ID: 7Tir0u0ukg
Title: Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 5, 5, 5, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the first provably efficient randomized exploration methods in cooperative multi-agent reinforcement learning (RL) within a parallel MDP framework. The authors propose two Thompson-sampling-type algorithms that utilize perturbed-history exploration and Langevin Monte Carlo exploration strategies. These algorithms achieve a regret bound of $\tilde{\mathcal{O}}(d^{3/2} H^2 \sqrt{MK})$ and a communication complexity of $\tilde{\mathcal{O}}(d H M^2)$, marking a significant theoretical advancement. The methods are evaluated through extensive experiments across various environments, demonstrating their effectiveness even under misspecified transitions.

### Strengths and Weaknesses
Strengths:
- The paper provides the first meaningful theoretical contribution for randomized exploration in cooperative multi-agent RL.
- The algorithms are easy to implement, computationally efficient, and avoid sampling bias.
- The theoretical framework is solid, with clear definitions and thorough descriptions of the proposed strategies.
- Extensive experimental validation is conducted, showcasing the algorithms' performance in both video games and realistic scenarios.

Weaknesses:
- The classification of the setting as "multi-agent RL" is questioned; it may be more accurately described as multi-task RL due to the independence of agents' transition and reward functions.
- The comparison with previous work lacks precision and may be misleading, particularly regarding regret bounds.
- The trade-off between regret and communication complexity is not clearly articulated, leaving ambiguity in the optimization process.
- The motivation for randomized exploration is not convincingly supported, as the theoretical results are limited to linear settings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the setting by explicitly defining the trade-offs between regret and communication complexity. Additionally, we suggest including a more precise comparison with existing work, particularly addressing the discrepancies noted in the regret bounds. The authors should also elaborate on the technical novelty of the proposed methods and clarify the synchronization framework used in the experiments. Finally, a discussion on extending the algorithms beyond linear MDPs would enhance the paper's contribution.