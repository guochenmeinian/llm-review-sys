ID: aCaspFfAhG
Title: Bandits with Ranking Feedback
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 7, 6, 5, -1, -1, -1
Original Confidences: 5, 3, 3, 2, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to the multi-armed bandit problem, specifically focusing on "bandits with ranking feedback," where feedback consists of rankings based on historical data rather than precise numerical values. The authors investigate both stochastic and adversarial settings, providing algorithms that achieve instance-dependent and instance-independent regret rates. Notably, they demonstrate that logarithmic regret is unattainable in the stochastic setting with ranking feedback, and no algorithm can achieve sublinear regret in adversarial scenarios. The proposed algorithms, DREE and R-LPE, show promise in these contexts.

### Strengths and Weaknesses
Strengths:
- The setting studied is novel and not commonly found in existing literature, offering interesting theoretical insights.
- The paper is well-structured, with clear explanations and a comprehensive analysis of both adversarial and non-adversarial settings.
- The theoretical contributions are robust, particularly the proofs regarding regret bounds under ranking feedback.

Weaknesses:
- The practical motivation for the model is lacking, and concrete applications are not well-defined.
- The dependence on the number of arms, $n$, in the instance-dependent case is suboptimal ($n^4$), and further discussion on achieving linear or sublinear dependence would be beneficial.
- The analysis of the adversarial setting lacks depth, and the algorithms' sensitivity to parameters like the time horizon $T$ needs more exploration.

### Suggestions for Improvement
We recommend that the authors improve the practical motivation for their model by providing concrete applications and examples to clarify its relevance. Additionally, discussing designs that accommodate imperfect ranking feedback would enhance the robustness of the proposed algorithms. We suggest that the authors provide a more detailed analysis of the algorithms' computational complexity and scalability, as well as insights into their performance in adversarial settings. Finally, addressing the sensitivity of the algorithms to specific parameters and exploring potential extensions to other bandit models could broaden the applicability of their findings.