ID: Fe8PxP2F2p
Title: Gradient-Based Feature Learning under Structured Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 5
Original Ratings: 7, 6, 5, -1, -1
Original Confidences: 3, 2, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into single index learning with a spiked input distribution, diverging from the typical Gaussian assumptions. The authors propose a modified SGD that outperforms vanilla SGD in learning the target direction, especially when the spike aligns with the true hidden signal. They analyze the sample complexity and identify three learning phases based on the spike's magnitude and alignment with the target direction. The work also introduces weight normalization techniques to enhance learning dynamics in structured covariance scenarios.

### Strengths and Weaknesses
Strengths:  
- The exploration of non-Gaussian input distributions is intriguing, providing valuable insights into learning dynamics.  
- The identification of phase transitions based on spike alignment offers a comprehensive understanding of the learning process.  
- The paper is well-structured, with clear explanations and rigorous mathematical proofs, enhancing its accessibility and clarity.

Weaknesses:  
- The setting is somewhat artificial, as the strong correlation between the spike and true signal simplifies the learning task compared to typical single index models.  
- The writing lacks clarity in some sections, particularly when introducing the single-neuron modification and its assumptions.  
- The practical implications of the proposed training procedure, which deviates from standard gradient descent, remain unclear.

### Suggestions for Improvement
We recommend that the authors improve the discussion on why the spiked covariance setting is not overly simplistic, possibly addressing the correlation between the spike and true signal. Additionally, we suggest rephrasing the introduction of the single-neuron modification for clarity, expanding on Proposition 4 to elucidate the role of Assumption 2, and clarifying the necessity of restricting to ReLU networks in Theorem 9. Finally, enhancing the explanation of the intermediate learning phase in Figure 1 would provide a more complete understanding of the findings.