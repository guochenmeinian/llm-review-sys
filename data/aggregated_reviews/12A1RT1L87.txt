ID: 12A1RT1L87
Title: Are Large-scale Soft Labels Necessary for Large-scale Dataset Distillation?
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a solution to the challenge of large-scale dataset distillation by focusing on reducing the storage requirements for auxiliary soft labels in ImageNet-level condensation. The authors propose Label Pruning for Large-scale Distillation (LPLD), which aims to achieve state-of-the-art performance while significantly minimizing storage needs. Key contributions include identifying the necessity of large-scale soft labels due to high within-class similarity, introducing class-wise supervision to enhance within-class diversity, demonstrating the effectiveness of random pruning of soft labels, and achieving state-of-the-art performance with a 40x reduction in label storage across various networks and datasets.

### Strengths and Weaknesses
Strengths:
1. The paper addresses the significant issue of large storage requirements for auxiliary data in dataset distillation, presenting a novel class-wise supervision approach.
2. Thorough analysis using feature cosine similarity and MMD supports claims of increased diversity in synthetic data, validated through extensive experiments on multiple datasets.
3. The manuscript is well-organized, with clear explanations of motivations, methodologies, and comprehensive result analyses, supported by effective figures and tables.
4. The substantial reduction in storage needs while maintaining or improving performance enhances the practicality of dataset distillation for large-scale applications.

Weaknesses:
1. The paper lacks a rigorous theoretical analysis, which could strengthen the findings, although the empirical study provides a valuable foundation for future theoretical work.
2. Concerns regarding the technical contributions arise, as the proposed method appears to be a class-wise adaptation of existing techniques, potentially diminishing its novelty.
3. Questions remain about the computational costs associated with class-wise optimization and the specifics of the random label pruning process.
4. Some sentences are confusing, particularly regarding the relationship between increased diversity and label pruning, which could benefit from clearer articulation.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis to complement the empirical findings, particularly regarding the relationship between class-wise supervision and diversity. Additionally, clarifying the use of Equation (8) and addressing the computational costs of the proposed method would enhance understanding. We suggest including comparisons of the proposed label pruning with other compression methods, such as Marginal Smoothing/Re-Norm with Top-K, to establish its efficacy. Furthermore, detailing the random label pruning process and addressing the performance scaling with the number of classes would provide valuable insights. Lastly, we encourage the authors to ensure clarity in the manuscript to avoid confusion in the presentation of their findings.