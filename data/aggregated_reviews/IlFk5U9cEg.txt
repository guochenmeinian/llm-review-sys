ID: IlFk5U9cEg
Title: AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database Queries
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 7, 7, 9, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new benchmark for text-to-SQL tasks, named "AMBROSIA," which addresses intrinsic ambiguities in user queries. It includes 849 multi-table databases across 16 domains and features over 4,240 ambiguous SQL queries. The authors identify three types of ambiguity: scope ambiguity, attachment ambiguity, and vagueness. They demonstrate through experiments that existing large language models (LLMs), including GPT-4, struggle significantly with these ambiguities, achieving only around 30% recall on ambiguous questions.

### Strengths and Weaknesses
Strengths:
1. The work inspects text-to-SQL ambiguity in multi-table settings, focusing on realistic scenarios rather than trivial modifications of database schema.
2. It proposes a comprehensive LLM-driven framework for collecting ambiguous examples, which could inspire dataset construction in other tasks.
3. The paper is well-written, clearly outlining objectives, methodology, and results, indicating high-quality research.

Weaknesses:
1. The three types of ambiguity may not be exhaustive or mutually exclusive, lacking formal definitions supported by linguistic references.
2. The generated databases may introduce biases and do not fully represent the complexity of real-world data, particularly proprietary data.
3. The test data relies heavily on LLMs with predefined templates, which could lead to predictable patterns that models might exploit.

### Suggestions for Improvement
1. We recommend that the authors improve the formalization of the ambiguity types by referencing established linguistic literature to ensure a comprehensive and authoritative definition.
2. Consider expanding the range of ambiguities beyond scope, attachment, and vagueness to enhance the dataset's comprehensiveness and applicability.
3. We suggest reducing the similarity between questions for scope and attachment ambiguities to ensure that the dataset captures a wider variety of patterns and improves model evaluation.
4. Clarify the precision metric used in the evaluation, especially regarding how it accounts for the multiple valid solutions associated with ambiguous questions.
5. We recommend including more human-labeled data alongside LLM-generated content to enhance the dataset's realism and reduce potential biases in the generated examples.