ID: xjyU6zmZD7
Title: Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 6, 6, -1, -1, -1, -1
Original Confidences: 5, 4, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for training spiking neural networks (SNNs) using surrogate gradient learning to address the gradient vanishing problem. The authors propose the Shortcut Back-propagation method and an evolutionary algorithm framework to balance training across shallow and deep layers. The effectiveness of these methods is validated through extensive experiments.

### Strengths and Weaknesses
Strengths:  
1) The Shortcut Back-propagation method and the evolutionary training framework are novel contributions.  
2) The paper effectively addresses the gradient vanishing problem.  
3) The writing is clear and concise.  
4) The proposed methods demonstrate excellent performance across multiple datasets.

Weaknesses:  
1) The authors should provide more mathematical proof to demonstrate the effectiveness of the residual structure in SNNs and address the complexity introduced by shortcut branches.  
2) A comparison with recent state-of-the-art works is necessary, particularly with papers [1][2] that achieve strong results using MS-ResNet-18 on large ImageNet datasets.  
3) The authors only show changes in gradient distribution in the first layer; presenting changes in mean and variance of absolute gradients for each layer would strengthen their argument.  
4) The proposed method may increase training time, and the experimental section lacks comparisons with newer methods.  
5) Figure 2 requires improved readability and comprehensibility, including horizontal and vertical coordinates.

### Suggestions for Improvement
We recommend that the authors improve the mathematical proof regarding the gradient vanishing issue and the effectiveness of the residual structure in SNNs. Additionally, the authors should include comparisons with recent state-of-the-art methods, particularly those referenced, and provide a more comprehensive analysis of gradient distribution across all layers. Clarifying the impact on training time and enhancing the presentation of Figure 2 will also strengthen the paper.