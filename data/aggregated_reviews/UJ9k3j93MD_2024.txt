ID: UJ9k3j93MD
Title: Separation and Bias of Deep Equilibrium Models on Expressivity and Learning Dynamics
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comparative analysis of Deep Equilibrium Models (DEQ) and Fully Connected Neural Networks (FNN), focusing on their structural differences and performance metrics. The authors investigate the learning dynamics of DEQ, revealing its implicit bias and demonstrating its advantages over FNN in specific scenarios. The theoretical findings quantify unique learning properties of DEQ, including its superior expressive power and ability to generalize better in out-of-distribution tasks.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and logically structured, making it accessible to readers familiar with neural network expressivity and implicit bias.
- It addresses a significant gap in the literature regarding DEQ, providing valuable theoretical insights and empirical support for its advantages over FNN.

Weaknesses:
- The paper lacks a detailed discussion on the factors influencing DEQ's separation and bias, such as initialization and gradient descent methods.
- The comparison between DEQ and FNN may not be entirely fair, particularly regarding computational costs during training.
- Certain proofs, particularly in Theorem 2(B) and Section 5, require clarification and additional justification.
- The assumption that DEQs favor dense features may be misleading, as the model primarily updates diagonal weights while keeping others zero.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the factors influencing DEQ's separation and bias, specifically addressing whether these are due to initialization, implicit differentiation, or gradient descent methods. Additionally, the authors should provide a more comprehensive comparison of memory consumption and computational costs between DEQ and FNN to justify their claims. 

Clarifying the proof of Theorem 2(B) and addressing the concerns regarding the implicit bias in Section 5 would strengthen the paper. We also suggest including discussions on the limitations of DEQs, particularly concerning computational complexity and training stability, and citing relevant works that explore the convergence and information propagation in DEQ. Lastly, conducting experiments to validate the density of learned features would enhance the paper's empirical support.