ID: HSbSATEKv7
Title: Maverick: Personalized Edge-Assisted Federated Learning with Contrastive Training
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Maverick, an edge-assisted federated learning (FL) system that addresses model drifts through personalized local training using a model-contrastive loss. The authors propose using anomalous models as negative samples to enhance convergence. Experiments demonstrate that Maverick improves convergence by up to 16.2x and accuracy by up to 12.7% compared to existing systems. However, the core idea is criticized for its similarity to prior work, particularly the paper by Moon, raising concerns about its novelty.

### Strengths and Weaknesses
Strengths:
- The introduction of contrastive training is a novel approach to optimize federated learning.
- The methodologies are detailed, and the experiments are comprehensive, showcasing significant improvements in accuracy and convergence speed.
- The paper is well-structured, with clear explanations and supportive figures.

Weaknesses:
- The rationale for using contrastive training is inadequately explained, and its advantages over existing strategies are unclear.
- The reliance on Adaptive-Krum for categorizing models lacks justification, especially given recent advancements in model poisoning attacks.
- The experiments primarily consider scenarios with a single straggler, neglecting the complexities of multiple stragglers and varying local training rounds.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the rationale behind using contrastive training and explicitly compare it with other strategies such as regularization and Bayesian inference. Additionally, the authors should provide a theoretical analysis of Maverick's convergence to enhance its theoretical contribution. It would be beneficial to include a brief description of Adaptive-Krum and quantify the communication costs associated with Maverick, particularly in scenarios with varying numbers of anomalous models. Furthermore, we suggest conducting additional experiments on smaller datasets like CIFAR-10 to report mean and standard deviation, thereby mitigating noise effects. Lastly, addressing the implications of scaling the number of clients on communication overhead would strengthen the paper's practical relevance.