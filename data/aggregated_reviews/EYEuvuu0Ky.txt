ID: EYEuvuu0Ky
Title: Language Models in Molecular Discovery
Conference: NeurIPS
Year: 2023
Number of Reviews: 2
Original Ratings: 9, 2
Original Confidences: 5, 5

Aggregated Review:
### Key Points
This paper presents a review of the application of language models, particularly transformers, in molecular discovery and computational chemistry. The authors discuss molecule representations, generative models, and enumerate current models for molecular property prediction and reaction planning. The inclusion of software packages aims to assist readers in familiarizing themselves with the field.

### Strengths and Weaknesses
Strengths:  
- The manuscript is clear and comprehensive, providing valuable background information on generative models and their applications in molecular discovery.  
- The enumeration of current models and software packages is beneficial for readers.

Weaknesses:  
- The paper lacks coherence and logical flow, with disjointed sections that transition abruptly between topics, such as the Turing test, machine learning, and applications to chemical design tasks.  
- Key issues like up-to-date data and hallucination in large models are mentioned but not elaborated upon.  
- The explanation of RNN, VAE, and transformers lacks logical connections.  
- There are typographical errors, including "additonally," and incorrect footnotes.

### Suggestions for Improvement
We recommend that the authors improve the coherence of the manuscript by ensuring logical connections between sections, particularly when transitioning between topics. It is essential to elaborate on the issues of up-to-date data and hallucination in large models for clarity. Additionally, we suggest that the authors provide clearer explanations of RNN, VAE, and transformers, establishing their interrelations. Finally, we advise proofreading to correct typographical errors and inaccuracies in footnotes.