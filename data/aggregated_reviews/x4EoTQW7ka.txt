ID: x4EoTQW7ka
Title: DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DropBP, a novel method to accelerate the fine-tuning of Large Language Models (LLMs) by selectively dropping layers during backward propagation. The authors propose that this technique reduces computational costs and activation memory, addressing significant challenges in efficient LLM training. DropBP is implemented as a PyTorch extension and is shown to improve training time, convergence speed, and maximum sequence length across various LLMs and datasets.

### Strengths and Weaknesses
Strengths:
- The concept of dropping backward propagation layers to reduce computational overhead is innovative and addresses an important issue in training large models.
- Extensive experiments validate DropBP's effectiveness in reducing training time and memory usage while maintaining accuracy.
- The development of a PyTorch extension for DropBP enhances its practical applicability.

Weaknesses:
- The motivation for the method is not well illustrated, lacking a clear distinction from previous works like LayerDrop, which also omit layer computation in the forward pass.
- The rationale for maintaining forward pass computation while dropping backward pass layers is unclear; justifying this choice would strengthen the contribution.
- The faster convergence observed with DropBP raises concerns about overfitting, necessitating a discussion on potential regularization techniques and comparisons with related work.
- More details on the sensitivity calculation process and its impact on training time are needed, as well as a theoretical analysis of DropBP's effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the motivation section by clearly distinguishing DropBP from existing methods like LayerDrop. Additionally, justifying the decision to retain forward pass computation while dropping backward layers would enhance the paper's contribution. A discussion on regularization techniques to address overfitting concerns is essential, along with a comparison to related works. Providing more details on the sensitivity calculation process and its computational overhead would clarify trade-offs. Lastly, we suggest including a theoretical analysis of why DropBP is effective and exploring best practices for tuning drop rates.