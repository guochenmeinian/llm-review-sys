ID: xzhRMsSC3T
Title: Representation Learning with Large Language Models for Recommendation
Conference: ACM
Year: 2023
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the RLMRec framework, a model-agnostic approach that enhances traditional ID-based recommender systems by integrating Large Language Models (LLMs). The framework aims to overcome limitations of existing models by capturing complex semantic aspects through advanced summarization capabilities of LLMs. The authors propose that incorporating textual signals via mutual information maximization can improve representation quality, potentially transforming recommender system functionalities.

### Strengths and Weaknesses
Strengths:  
- The RLMRec framework effectively integrates LLMs into recommender systems, demonstrating its ability to enhance representation learning and capture intricate user behaviors.  
- The theoretical foundation is well-established, and the framework shows effectiveness across multiple state-of-the-art models, with analyses of efficiency and robustness to noisy data.  
- The experiments are comprehensive and support the proposed enhancements.

Weaknesses:  
- The inference process for user/item profiling is time-intensive, raising concerns about training efficiency.  
- Baseline comparisons are limited, primarily focusing on ID-based methods without including hybrid approaches that combine ID and text embeddings.  
- The claims regarding noise robustness lack clear substantiation in the ablation study, and some derivation steps in Theorems 1 and 2 are unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the derivation steps for Theorems 1 and 2 by including a more comprehensive version in the appendix. Additionally, the authors should address the limitations of baseline comparisons by including hybrid methods to better demonstrate the advantages of LLM integration. To substantiate claims of noise robustness, we suggest providing clearer evidence in the ablation study and elaborating on the benefits of contrastive or generative alignment over simpler methods like concatenation of pre-trained BERT embeddings.