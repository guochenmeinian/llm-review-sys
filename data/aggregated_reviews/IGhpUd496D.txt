ID: IGhpUd496D
Title: Provable Editing of Deep Neural Networks using Parametric Linear Relaxation
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 7, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HRDNN, the first efficient technique for provable editing of DNNs, capable of editing a DNN to satisfy specified properties within 45 seconds. HRDNN achieves efficiency by relaxing the NP-hard provable editing problem to a linear program (LP), allowing for the construction of tight bounds parameterized by new parameters $\theta$. The approach is evaluated on various benchmarks, including vnn-comp22, CIFAR10, tiny imagenet, and bert SST DNNs.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important research topic and is well-written.
- It introduces the first verifier-in-the-loop, stopping editing once the verifier confirms property satisfaction.
- The method is applicable to various network types (sigmoid, tanh, elu DNNs).
- Extensive evaluation validates the proposed approach.

Weaknesses:
- Results in Table 1 show generalized accuracy smaller than PFT<ASBR>, and only two models are tested, limiting effectiveness demonstration.
- The significance of metrics such as ACC, Cert., and Time compared to efficacy is unclear, especially since editing methods aim to enhance certified robustness.
- The definition of Og. Effic in Table 2 is missing, and the comparison to verified training methods is not addressed.
- The parametric linear relaxation is limited to specific activation functions, lacking a general paradigm for other functions.
- The scalability of the approach is not discussed, and runtime results should be paired with predictive performance for meaningful interpretation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the metrics used, particularly in defining Og. Effic and discussing the importance of ACC, Cert., and Time relative to efficacy. Additionally, we suggest including a broader range of models in the evaluation to better demonstrate HRDNN's effectiveness. A discussion on how the proposed relaxations relate to optimal LP relaxations for ReLU neurons would enhance theoretical insights. Furthermore, addressing the scalability of the approach and providing comparisons to verified training methods would strengthen the paper. Finally, we advise the authors to ensure that the notation is accessible to all readers, avoiding reliance solely on color differentiation.