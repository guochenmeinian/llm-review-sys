ID: ARtBIBAmNR
Title: Visually Guided Generative Text-Layout Pre-training for Document Intelligence
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel pre-training method called Visually Guided generative Text-Layout Pre-training (ViTLP) for visual document understanding (VDU). The authors propose that ViTLP jointly models text and layout information from document images in a generative manner, eliminating the need for OCR preprocessing. Key contributions include: 1) ViTLP's ability to generate mixed sequences of text and layout through hierarchical modeling; 2) a multi-segment pre-training scheme that allows processing of long documents; 3) functionality as an end-to-end OCR model for text localization and recognition; and 4) state-of-the-art performance on benchmark VDU datasets, providing interpretable outputs.

### Strengths and Weaknesses
Strengths:  
- Unified pre-training approach that serves both as an OCR engine and a backbone for downstream VDU tasks.  
- Explainability through interpretable ROI visualizations for text-driven tasks.  
- Effective handling of long documents via a multi-segment pre-training scheme.  

Weaknesses:  
- Incomplete literature review, lacking comparisons with relevant recent works like LiLT, LayoutLMv3, and UDOP.  
- Performance on challenging tasks, particularly question-answering (DovVQA), is relatively lower compared to prior methods.  
- Reproducibility concerns due to insufficient implementation details, including hyper-parameters and training specifics.  
- Poor writing clarity in crucial areas, such as the explanation of token-inefficiency and multi-segment fine-tuning.  
- Lack of baseline comparisons with top-performing models, raising questions about the validity of performance claims.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including and comparing ViTLP with recent models like LiLT, LayoutLMv3, and UDOP. Additionally, incorporating benchmarks such as InfographicVQA would enhance the evaluation of the proposed method. To address performance concerns, we suggest including more comprehensive baseline comparisons, particularly with models like LayoutLMv3 and Pix2Struct. Clarifying the multi-segment fine-tuning process and providing detailed implementation information, including hyper-parameters, would significantly aid reproducibility. Lastly, we encourage the authors to refine the writing for clarity, especially regarding technical terms and claims made throughout the paper.