ID: ffNrpcBpi6
Title: Graph Convolutions Enrich the Self-Attention in Transformers!
Conference: NeurIPS
Year: 2024
Number of Reviews: 25
Original Ratings: 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to enhance the self-attention mechanism in transformers by utilizing graph signal processing principles. The authors propose the Graph Filter Self-Attention (GFSA) method, treating self-attention as a low-pass graph filter and introducing a generalized filter that can capture low-pass, high-pass, or combined filtering behaviors. The effectiveness of GFSA is evaluated across various tasks, demonstrating its potential to mitigate oversmoothing issues and improve performance in diverse applications. Additionally, the authors propose a selective application strategy to apply GFSA only to even-numbered layers, thereby reducing runtime increases while maintaining performance. They also clarify the theoretical underpinnings of GFSA, particularly Theorem 3.1, which relates the coefficients of the graph filter to its characteristics.

### Strengths and Weaknesses
Strengths:
1. The core idea is clearly articulated, and the method shows promising results across multiple transformer variants in different domains.
2. Comprehensive empirical experiments support the advantages of the proposed method, showcasing its versatility across natural language processing, computer vision, and other fields.
3. The integration of GFSA with efficient Transformers is well-documented, highlighting its potential for maintaining efficiency.
4. The theoretical analysis, including the correction of Theorem 3.1, enhances the understanding of how coefficients influence filter behavior.
5. The paper is well-organized and includes detailed implementation instructions, enhancing reproducibility.

Weaknesses:
1. The novelty of the approach is limited, as the concept of adaptive high-pass and low-pass filters has been extensively studied in the GNN field, and similar ideas have been proposed in existing graph transformer models.
2. The experiments lack focused analysis on the impact of GFSA on transformers with varying depths, and the choice of baselines may not adequately represent the current state of the art.
3. The computational complexity remains a concern, with some reviewers noting that the performance improvements are not remarkable relative to the increased complexity.
4. The notation in some equations is confusing, and the Taylor approximation may significantly reduce the capacity of the graph filters, limiting the method's effectiveness.
5. The authors do not provide specific details about the changes made in response to reviewer feedback, which may hinder the reviewers' ability to assess the improvements.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the description in Theorem F.1 and ensure that the notation used in equations is consistent and understandable. Additionally, a more systematic comparison with recent and relevant methods addressing oversmoothing in transformers should be included to strengthen the paper's claims. We suggest conducting a focused analysis of GFSA's performance across transformers with varying depths and providing clearer guidance on selecting the value of K, as its sensitivity is a critical factor. Furthermore, we recommend that the authors improve the clarity of the relationship between the choice of K and the filter characteristics, as this remains a point of confusion for some reviewers. Addressing the concerns about computational complexity in a more straightforward manner could help reinforce the practical applicability of GFSA. Finally, we encourage the authors to explicitly detail how each concern raised by the reviewers has been addressed, providing specific examples of the new experiments and clarifications to enhance the reviewers' understanding of the revisions made.