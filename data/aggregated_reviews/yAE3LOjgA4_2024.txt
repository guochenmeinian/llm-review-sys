ID: yAE3LOjgA4
Title: From Lazy to Rich: Exact Learning Dynamics in Deep Linear Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 9, 6, 8, 6
Original Confidences: 4, 4, 3, 5

Aggregated Review:
### Key Points
This paper presents a study on the gradient flow dynamics of deep linear networks (DLNs) under λ-balanced initializations, extending the findings of Braun & Dominé 2022. The authors explore learning dynamics across various regimes, from lazy to rich, and analyze how factors such as λ, architecture, and noise influence this transition. They introduce new dynamics specific to λ-balanced networks while replicating previous results, particularly highlighting the interesting "delayed rich" regime.

### Strengths and Weaknesses
Strengths:
- The extension to λ-balanced networks broadens the understanding of DLN dynamics.
- The evaluation is comprehensive, covering all prior settings and revealing new behaviors associated with λ-balanced initializations.
- The lazy to delayed rich learning transition may provide insights into the grokking phenomenon.
- The mathematical proofs appear mostly correct, with no major errors.

Weaknesses:
- The implications of the technical contributions are not well articulated.
- New behaviors for λ-balanced networks are mentioned but not discussed in detail; further investigation into delayed rich behavior and reversal learning when λ ≠ 0 is recommended.
- The introduction of the semi-structured lazy regime lacks detailed explanation and implications.
- Numerous writing and mathematical errors detract from the quality, including missing definitions, incorrect figure labels, and incomplete references.

### Suggestions for Improvement
We recommend that the authors improve the articulation of the implications of their findings, particularly regarding the new behaviors of λ-balanced networks. A more detailed exploration of the delayed rich regime and its relationship to loss saturation is necessary. Additionally, the authors should clarify the definition of "relative scale" and its implications, as well as provide empirical validation for the findings in continual and reversal learning sections. Strengthening the theoretical explanations with empirical results, particularly regarding representational similarity matrices and transfer learning, would enhance the paper's impact. Lastly, addressing the numerous writing and mathematical errors will improve the overall clarity and professionalism of the work.