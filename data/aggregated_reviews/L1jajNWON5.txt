ID: L1jajNWON5
Title: CondTSF: One-line Plugin of Dataset Condensation for Time Series Forecasting
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 4, 4, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a dataset distillation approach for time-series forecasting by decomposing the distillation loss into a value term and a gradient term. The authors derive bounds for these terms to enhance dataset distillation effectiveness. They introduce a simple data condensation plugin, CondTSF, and demonstrate its strong performance through experiments on eight time series datasets.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and structured, making it easy to follow.
- The authors provide theoretical explanations supporting their methodology.
- Strong experimental results are presented, showcasing the effectiveness of the proposed approach.

Weaknesses:
- The authors do not explain the occurrence of the non-optimizable $\epsilon$ error (p4, l121).
- The justification for deriving the gradient term bound is lacking, and the authors should clarify its computational benefits and provide supporting experiments.
- Implicit assumptions are not adequately discussed, particularly regarding the applicability of their conclusions in different contexts, such as financial time-series forecasting.
- The updating process resembles existing target smoothing/model distillation approaches, raising questions about the need for experimental or theoretical comparisons with these methods.
- The impact of the gradient and value terms is not sufficiently explored, particularly the effects of removing the L_param loss and the value term.
- The applicability of the method to regression tasks is not addressed.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the non-optimizable $\epsilon$ error and justify the need for deriving the gradient term bound, ideally with experimental evidence. Additionally, the authors should discuss the implications of their implicit assumptions more thoroughly, especially in varied forecasting contexts. We suggest including comparisons with existing target smoothing/model distillation methods to clarify the novelty of their approach. Furthermore, the authors should conduct ablation studies to assess the contributions of the gradient and value terms. Lastly, we encourage the authors to explore the method's applicability to regression tasks explicitly.