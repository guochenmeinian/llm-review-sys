ID: 6kc6Hdyknx
Title: ActionAtlas: A VideoQA Benchmark for Domain-specialized Action Recognition
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 8, 3, 6, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the ActionAtlas dataset, a new benchmark for fine-grained action understanding in video question answering (VideoQA) tasks, specifically focusing on sports. The dataset is designed to evaluate models' capabilities in recognizing detailed actions through multiple-choice questions, utilizing a novel pipeline involving LLMs for action identification, clip selection, and question generation. However, the dataset suffers from significant flaws, including the presence of multiple correct answers, which leads to incorrect assessments of model performance. The authors claim that their dataset can effectively test models' abilities to differentiate between 1161 actions, but this assertion is questioned due to the limitations of the 4-choice question format. Experimental results indicate that current models, including GPT-4o and Gemini, struggle with the dataset's challenges, highlighting the need for advanced video-centric models. 

### Strengths and Weaknesses
Strengths:
- The dataset provides a challenging benchmark that reveals the limitations of current state-of-the-art models in fine-grained action recognition.
- The innovative semi-automatic data collection and annotation process is a valuable contribution to the field of video understanding.
- The dataset aims to evaluate fine-grained action recognition, which is a relevant area of research.
- The analysis of input frame numbers offers important insights into model performance and efficiency.
- The authors have engaged with reviewer feedback and provided detailed rebuttals.

Weaknesses:
- The benchmark mixes challenges in language and video understanding, complicating the evaluation of model performance.
- The dataset design includes multiple correct answers, leading to misjudged model performance.
- There is vagueness in defining fine-grained actions, and the authors do not provide a clear definition.
- The inclusion of irrelevant factors, such as jersey number identification, complicates the dataset unnecessarily and detracts from its intended focus.
- The claim regarding the dataset's ability to test differentiation among 1161 actions lacks sufficient evidence.
- There is a lack of supervised baseline models, which could provide a clearer understanding of the dataset's challenges.
- The human verification process for annotations may introduce errors due to reliance on GPT-4-generated definitions.
- The analysis lacks qualitative insights, focusing primarily on quantitative metrics without sufficient context.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing explicit definitions of fine-grained actions and clearer objectives for the study. Additionally, we urge the authors to ensure that each question has only one correct answer to avoid penalizing models for correct selections. The authors should reconsider the inclusion of irrelevant factors like jersey numbers, which do not contribute to the evaluation of fine-grained actions. Including a human baseline as a gold standard could enhance the evaluation of the dataset's challenges. We also suggest providing evidence to support the claim that the dataset can effectively test models' capabilities in differentiating among 1161 actions, possibly by revising the relevant statements in the paper. Finally, incorporating qualitative analyses to better understand model failures and the specific challenges posed by the dataset, as well as addressing potential copyright issues with YouTube-sourced videos, is crucial. Ensuring that the dataset captures longer temporal contexts will also enhance its applicability.