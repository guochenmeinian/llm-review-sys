ID: FTpOwIaWUz
Title: On Affine Homotopy between Language Encoders
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 3, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a formal mathematical analysis of intrinsic and extrinsic homotopy between language encoders, specifically focusing on pretrained models like BERT. The authors define a language encoder as a function mapping strings from an alphabet to a vector space and introduce a metric space for encoders. They explore affine alignment measures and propose intrinsic and extrinsic similarity metrics, demonstrating their application through experiments with various BERT models. The findings indicate a correlation between intrinsic and extrinsic similarities, contributing to the understanding of encoder relationships.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach to comparing language encoders using mathematical tools from homotopy theory.
- It provides a rigorous framework for defining encoder similarity, enhancing theoretical understanding in the field.

Weaknesses:
- The paper's contribution to the Neural Information Processing Systems community is unclear, as the concepts presented are very general and their connection to neural networks seems tenuous.
- The experimental section is limited, focusing solely on BERT models with only two datasets, raising questions about the generalizability of the findings.
- The paper is difficult to read due to its dense mathematical content, formatting issues, and lack of necessary definitions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by explicitly connecting their theoretical concepts to practical applications in neural networks. Additionally, we suggest expanding the experimental section to include comparisons with other encoder architectures, such as AlBERT and ELECTRA, and utilizing more diverse datasets. To enhance readability, consider merging appendices with the main text and providing clearer explanations of key concepts, particularly the motivation for using hemi-metrics. Finally, we suggest revising the paper for submission to a journal, allowing for a more comprehensive presentation of the work.