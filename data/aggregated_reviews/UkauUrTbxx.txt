ID: UkauUrTbxx
Title: ProTransformer: Robustify Transformers via Plug-and-Play Paradigm
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 6, 4, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 1, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a robust attention mechanism, ProAttention, aimed at enhancing the resilience of transformer architectures against adversarial attacks through a plug-and-play approach that requires no additional training or fine-tuning. The authors incorporate robust token estimators within self-attention blocks and utilize the efficient Newton-IRLS algorithm, demonstrating the method's effectiveness across various tasks, including language modeling, image classification, and graph representation learning. The experiments indicate significant improvements in robustness without compromising benign performance.

### Strengths and Weaknesses
Strengths:  
1. The method is simple and effective, providing resilience with only four lines of code.  
2. Comprehensive experiments validate the claims of enhanced robustness across diverse tasks and model architectures, including language modeling and various adversarial attacks.  
3. The plug-and-play nature of ProTransformer facilitates easy integration into existing models, making it practical for real-world applications.  
4. The paper is well-written, with clear descriptions and thorough theoretical analysis.

Weaknesses:  
1. There is insufficient discussion on the runtime efficiency of the proposed method across different architectures, particularly for large language models.  
2. The paper lacks comparisons of the proposed method's performance with other robust designs in image classification tasks.  
3. Clean performance analysis in the context of jailbreak attacks is missing, raising concerns about the impact on generation quality.  
4. The absence of error margins in experimental results and a lack of detailed discussion on the method's limitations detracts from the overall presentation.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the runtime of the proposed method across various architectures, particularly for large language models, and include comparisons with other robust designs in image classification tasks. Additionally, we suggest providing a clean performance analysis in the context of jailbreak attacks to assess any potential impact on generation quality. The authors should also include error margins for experimental results and enhance the clarity and structure of the paper, particularly in figure captions and the limitations section. Finally, a more thorough explanation of why the ProAttention mechanism performs better under various attacks would strengthen the paper's contributions.