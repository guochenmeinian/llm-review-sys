ID: OYOkkqRLvj
Title: Amortized Eigendecomposition for Neural Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 8, 8, 7, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for integrating eigendecomposition into neural network training, addressing the computational cost typically associated with the backward process. The authors propose treating the "eigenvectors" U as optimizable variables and introduce an eigen loss that guides U towards the true eigenvectors of \( A_\theta \). They provide a thorough analysis of various eigen loss formulations and implement an efficient method that re-parameterizes U using QR decomposition. Empirical results demonstrate significant speed-ups in training, ranging from 1.4x to 600x, across multiple machine learning tasks.

### Strengths and Weaknesses
Strengths:
- Originality: The framework is novel and detailed, building on previous concepts while providing a unique approach.
- Quality: The combination of theoretical analysis and empirical results enhances the paper's credibility.
- Clarity: The writing is clear and accessible, making complex ideas easy to follow.
- Significance: The method shows substantial improvements in training efficiency for tasks involving eigendecomposition.

Weaknesses:
- Stability and convergence rates of the proposed method are not adequately discussed.
- The design of the eigen loss appears handcrafted, suggesting a need for more flexible methods.
- The experiments are limited to small-to-moderate matrix dimensions, raising questions about scalability and performance in larger networks.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the stability and convergence rates of the amortized eigendecomposition method. Additionally, consider exploring more flexible designs for the eigen loss to enhance adaptability. It would be beneficial to include a comprehensive comparison of loss values and task performance in the experimental section to verify that the accelerated method maintains performance. Finally, we suggest addressing scalability concerns and comparing the proposed method with alternative fast SVD techniques, particularly in larger-scale settings.