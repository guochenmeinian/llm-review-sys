ID: 2EDqbSCnmF
Title: Any-to-Any Generation via Composable Diffusion
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 5, 6, 6, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. CoDi can generate multiple modalities in parallel and is not limited to a subset of modalities like text or image. The authors propose a modality alignment approach in both the input and output space to address the challenge of lacking training datasets for many combinations of modalities. CoDi employs a unique composable generation strategy that establishes a shared multimodal space through alignment in the diffusion process, enabling synchronized generation of intertwined modalities.

### Strengths and Weaknesses
Strengths:
1. The model's ability to process and generate modalities across text, image, video, and audio simultaneously is a significant and original contribution.
2. The proposed bridging alignment method is novel and allows for high customization and flexibility.
3. CoDi achieves competitive performance with state-of-the-art models in tasks such as image and video generation.

Weaknesses:
1. The method's clarity is lacking, particularly regarding the relationships among the different diffusion models and encoders.
2. The evaluation metrics used, such as FID and CLIPSIM, may not fully capture the perceptual quality of the generated outputs, and the evaluation of video quality (FVD) is insufficient.
3. The model does not consistently preserve the identity of the input modality in the generated outputs, which could limit its practical utility.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the method section by explicitly detailing the relationships among the image diffusion model, video diffusion model, vision encoder, and vision UNet. Additionally, we suggest incorporating qualitative evaluations or user studies to assess the perceptual quality and coherence of the generated outputs. The authors should also address the preservation of input modality identity more effectively and provide clearer evidence of the benefits of cross-modality conditions in their results. Finally, including the StableDiffusion v1.5 baseline in comparisons could enhance the comprehensiveness of the performance evaluation.