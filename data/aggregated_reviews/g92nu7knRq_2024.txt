ID: g92nu7knRq
Title: DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via Adaptive Heads Fusion
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Decoupled-Head Attention (DHA), a novel method to optimize large language models (LLMs) by transforming the Multi-Head Attention (MHA) mechanism. DHA reduces computational and memory costs by adaptively configuring key and value heads across layers, achieving significant efficiency gains while maintaining performance. The conversion from MHA to DHA is executed through a parameter-level process involving head clustering and linear fusion, resulting in a substantial reduction in KV cache size and pre-training budget.

### Strengths and Weaknesses
Strengths:  
- The paper identifies redundancies in attention KV heads and offers a practical solution, supported by empirical evidence.  
- The learning-based fusion optimization and adaptive budget allocation provide a systematic approach to optimal mappings, adding significant value.  
- The method demonstrates a remarkable balance between performance and efficiency, applicable to various existing MHA Transformer models.  

Weaknesses:  
- It is unclear whether the paper employs inter-layer or intra-layer grouping of heads, which affects the complexity and clarity of the method.  
- The effectiveness of DHA on a broader range of model architectures remains unexplored, particularly its compatibility with GQA.  
- The reliance on linear methods for parameter fusion may limit optimization potential; exploring non-linear techniques could enhance results.  
- A noticeable accuracy gap exists between DHA and MHA models on academic datasets, raising concerns about the practical utility of DHA.

### Suggestions for Improvement
We recommend that the authors clarify whether head grouping is performed inter-layer or intra-layer, as this impacts the method's complexity and inference efficiency. Additionally, providing a visualization similar to Figure 2 across layers could strengthen claims regarding inter-layer grouping. We suggest that the authors explore non-linear fusion techniques to potentially improve optimization outcomes. Furthermore, including performance comparisons of DHA with GQA and MHA on larger datasets would provide a more comprehensive evaluation of its effectiveness. Lastly, we advise defining "head similarity" more clearly to avoid ambiguity and specifying "parameter" and "attention probability" instead of solely using "weight."