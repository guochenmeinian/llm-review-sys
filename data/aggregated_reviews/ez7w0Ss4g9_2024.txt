ID: ez7w0Ss4g9
Title: How JEPA Avoids Noisy Features: The Implicit Bias of Deep Linear Self Distillation Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 8, 7, 7, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 4, 2, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the Joint-Embedding Predictive Architecture (JEPA) and Masked Autoencoders (MAE) in self-supervised learning (SSL) within deep linear networks. The authors demonstrate that the critical time for learning features is influenced by input variance for MAE, while JEPA focuses on features with high regression coefficients and low noise variance. The study includes numerical simulations that validate the theoretical findings, indicating robustness across different initialization schemes.

### Strengths and Weaknesses
Strengths:
- The work provides the first theoretical insights into why JEPA-based approaches learn 'abstract' features more efficiently than MAE, calling for further research to generalize these findings.
- The numerical simulations effectively support the theory and highlight qualitative differences between MAE and JEPA.

Weaknesses:
- The presentation of theoretical results, particularly Theorems 4.4 and 4.6, could be improved for clarity, as their validity is not immediately clear without appendix reference.
- The focus on linear predictors/decoders is limiting; a deep predictor/decoder should be considered for practical applications of both JEPA and MAE.
- More intuition is needed regarding covariance_{x,x} and covariance_{x,y} in typical pretraining tasks to enhance reader understanding.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Theorems 4.4 and 4.6 by providing more context in the main text. Additionally, the authors should explore the implications of using a deep predictor/decoder for both JEPA and MAE methods. It would also be beneficial to elaborate on the significance of covariance_{x,x} and covariance_{x,y} in pretraining tasks to aid comprehension of the theoretical results. Expanding the experimental scope to include diverse datasets and model architectures would further validate the findings and enhance the paper's impact.