ID: LuqrIkGuru
Title: Are Your Models Still Fair? Fairness Attacks on Graph Neural Networks via Node Injections
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 4, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel fairness attack method for Graph Neural Networks (GNNs) called Node Injection-based Fairness Attack (NIFA). The authors propose two principles: uncertainty maximization and homophily increase, to guide the injection of nodes that compromise fairness. The method is evaluated on multiple benchmark datasets, demonstrating its effectiveness in increasing bias while also impacting accuracy.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- The proposed method is technically sound and addresses an important problem in GNN fairness.
- The experiments are extensive, including comprehensive results and analyses.

Weaknesses:
- The distinction between fairness-targeted and accuracy-targeted attacks is not discussed, raising concerns about the potential for existing defenses against accuracy attacks to mitigate the proposed method.
- The connection between the homophily ratio and fairness metrics (DP/EO) is unclear, and theoretical guarantees are lacking.
- The motivation for attackers to undermine fairness is not adequately explored, and real-world examples would enhance the discussion.
- The high perturbation rate of 1% may not be realistic for large datasets, and smaller rates should be tested.
- The evaluation against fairness attack methods is limited to a single dataset, necessitating additional datasets for a comprehensive assessment.
- The authors should clarify how they ensure a fair budget across different attack methods and discuss performance with limited labeled training nodes.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the distinction between fairness-targeted and accuracy-targeted attacks, including a review of existing defenses. Additionally, providing a theoretical guarantee linking the homophily ratio to fairness metrics would strengthen the paper. The authors should also include real-world examples to illustrate the motivation for fairness attacks. Furthermore, we suggest experimenting with smaller perturbation rates and including additional datasets to evaluate the proposed method comprehensively. Lastly, clarifying the budget allocation across different attack methods and assessing performance with limited labeled training nodes would enhance the robustness of the findings.