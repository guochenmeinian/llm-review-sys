ID: MvjLRFntW6
Title: A Concept-Based Explainability Framework for Large Multimodal Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 6, 4, 4, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an explainability approach for interpreting the internal representations of large multimodal models (LMMs) through a dictionary learning-based method. The authors train an image captioning model that combines a pretrained image encoder and language model with a connector model. They utilize semi non-negative matrix factorization to decompose representations into lower-dimensional matrices, allowing for the extraction of interpretable concepts. The authors demonstrate that their method aligns well with input images and ground-truth captions, producing distinct concepts with minimal overlap.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel approach to mechanistic interpretability for multimodal models, addressing a clear need in the field.
2. The evaluation is comprehensive, incorporating both qualitative and quantitative results, and considers multiple baseline models, enhancing confidence in the chosen method.
3. The writing is clear, with a well-defined rationale and succinctly summarized results.

Weaknesses:
1. While the authors show minimal overlap among extracted concepts, they do not address the possibility of feature superposition, which could arise due to the lower dimensionality of the concept dictionary compared to internal representations.
2. Some claims made in the paper, such as the generalization of the framework to all LMMs, are overstated, as the approach primarily applies to image captioning models.
3. The representation analysis is limited to a small number of tokens, which may not capture the full range of multimodal concepts.

### Suggestions for Improvement
We recommend that the authors improve the discussion of feature superposition and consider conducting a qualitative analysis to assess its presence in the extracted concepts. Additionally, we suggest that the authors clarify the scope of their claims regarding the generalization of their framework to other LMMs. Expanding the token analysis to include a broader range of words would enhance the robustness of their findings. Lastly, we encourage the authors to explore gradient-based feature visualization approaches as a potential avenue for future research.