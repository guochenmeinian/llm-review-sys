ID: TScjG5zoB0
Title: Unveiling the Secrets of $^1$H-NMR Spectroscopy: A Novel Approach Utilizing Attention Mechanisms
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: 8, 4, -1
Original Confidences: 3, 3, 4

Aggregated Review:
### Key Points
This paper presents a method for training BERT-based models in an unsupervised manner using masked language modeling (MLM) on tokenized SMILES strings and corresponding 1H-NMR spectra. The authors achieve a commendable accuracy of 71% for peak-to-atom mapping and over 89% for mapping within ±0.59 ppm, with over 95% accuracy in identifying the correct molecule from candidates. The work is significant as it utilizes both simulated and experimental data for analysis.

### Strengths and Weaknesses
Strengths:  
- The integration of real experimental data alongside synthetic data enhances the model's applicability.  
- The proposed approach using Transformer's attention is novel and addresses a pressing issue in automatic processing of H-NMR spectra.  

Weaknesses:  
- The selection of specific layers and attention heads appears to be manual and based on visual inspection, which may hinder broader application.  
- The paper suffers from readability issues, particularly in the presentation of results and accuracy metrics, which could confuse readers.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing enhanced guidance for readers, such as introducing figures and their relevance before they appear. Additionally, it is crucial to clarify the context of reported accuracies—whether they pertain to simulated, experimental, or mapped datasets—potentially through a comprehensive table. Furthermore, we suggest optimizing the selection of attention layers on a held-out validation set to address the identified bottleneck in the algorithm's application.