ID: iWc0qE116u
Title: APEBench: A Benchmark for Autoregressive Neural Emulators of PDEs
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 9, 7, 7, -1, -1, -1
Original Confidences: 2, 3, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel code base in JAX for generating data for 46 semi-linear PDEs in 1D, 2D, and 3D using an ETDRK solver, allowing backpropagation through the solver. The authors evaluate the dataset by assessing the impact of multi-step finetuning and various deep learning architectures. APEBench offers four key contributions: a diverse set of PDE dynamics, unique identifiers for each dynamics type, a framework supporting differentiable physics training, and an examination of training paradigms on emulator performance. However, the paper's clarity and presentation of key concepts, including the research gap and definitions, require improvement.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and clearly written, guiding readers through APEBench's functionality and advantages.
- The ETDRK solver is well described, and the dataset's design facilitates fair comparisons among architectures.
- The extensive support for various dynamics and neural architectures enhances the benchmark's utility.

Weaknesses:
- Key concepts, such as the research gap and multi-step training, are not clearly articulated, leading to confusion.
- The evaluation relies heavily on nRMSE, which has known limitations, and lacks a broader set of evaluation metrics.
- The paper is limited to periodic boundary conditions and uniform Cartesian grids, which may restrict its applicability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by explicitly defining the research gap and multi-step training. Engaging in multiple rounds of internal reviews could help refine hard-to-understand sections. Additionally, we suggest modifying unreferenced statements to ensure they are backed by scientific evidence or presented as hypotheses. For example, rephrase "The FNO’s global nature and low-frequency bias makes it less suitable for this class of problems" to "The FNO’s global nature and low-frequency bias may make it less suitable for this class of problems [reference on low-frequency bias]." 

We also recommend expanding the evaluation metrics beyond nRMSE to include a broader set of performance indicators, as seen in benchmarks like WeatherBench 2. Furthermore, clarifying the purpose of Section 2 and enhancing the explanation of PDE identifiers and architecture comparisons would improve comprehension. Lastly, addressing the limitations regarding boundary conditions and geometries in future work could broaden the benchmark's applicability.