ID: wRwbv3aWzN
Title: VLIS: Unimodal Language Models Guide Multimodal Language Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VLIS, a novel framework that integrates the visual conditioning capabilities of vision-language models (VLMs) with the language understanding of unimodal text-only language models without additional training. The authors propose a method that enhances linguistic understanding by utilizing token likelihoods from text-only models and importance sampling weights from VLMs for visual alignment. VLIS demonstrates effectiveness across various multimodal tasks, including commonsense understanding and complex text generation, and shows promise in improving vision-language models.

### Strengths and Weaknesses
Strengths:
- The innovative VLIS method effectively combines visual conditioning and language understanding without further training, offering a new direction for multimodal language generation.
- Thorough experiments across diverse tasks showcase the effectiveness and versatility of the VLIS framework.
- Comparisons against several baselines provide valuable insights into VLIS's performance and adaptability.
- The framework's potential applicability to other modalities is noted, enhancing its relevance.

Weaknesses:
- The inference time increases due to multiple forward passes, potentially affecting efficiency.
- VLIS does not achieve the best outcomes on all datasets, including ROCStories.
- The exploration of model combinations is limited, and the method's reliance on last-stage token likelihood adjustments may lead to misleading results.
- Some parameter settings are underspecified, complicating reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by including state-of-the-art multimodal LLMs as baselines to provide a more comprehensive evaluation. Additionally, we suggest conducting further evaluations to assess the impact of directly adjusting token generation probabilities on response quality. It would also be beneficial for the authors to provide results on the image captioning task and analyze hallucinations, including CHAIR scores. Finally, clarifying the sensitivity of VLIS performance to different fluency threshold values would enhance the understanding of the method's robustness.