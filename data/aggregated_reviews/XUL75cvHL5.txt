ID: XUL75cvHL5
Title: The Collusion of Memory and Nonlinearity in Stochastic Approximation With Constant Stepsize
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 7, 5, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of constant step-size stochastic approximation algorithms with Markovian noise, focusing on the asymptotic bias in the context of non-linear stochastic approximation. The authors establish that the bias is of order O(Î±) and discuss the implications of their findings. They also provide a representation for the asymptotic bias, highlight the effects of memory and nonlinearity, and propose a bias attenuation technique using Richardson-Romberg extrapolation.

### Strengths and Weaknesses
Strengths:  
- The paper offers a novel characterization of bias in a challenging setting involving Markovian noise and non-linear drifts.  
- The analysis is asymptotically tight, providing an exact expression for the bias rather than a mere bound.  
- The contributions and assumptions are clearly articulated, and the implications of the results are discussed.  

Weaknesses:  
- Some assumptions, particularly strong monotonicity (A3) and smoothness (A2), are quite stringent and limit the applicability of the results.  
- The assumption regarding bounded iterates is strong and may not hold in many practical scenarios, such as stochastic gradient descent with finite support noise.  
- The paper is lengthy (54 pages), making it technical and difficult to read; it could benefit from a more concise presentation.  
- There are no numerical experiments to validate the theoretical results, which diminishes practical applicability.  

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the strong assumptions, particularly A2 and A3, to clarify their necessity and limitations. Additionally, consider including numerical experiments or toy examples to illustrate key results, such as the central limit theorem and bias attenuation technique. We suggest streamlining the paper's content to enhance readability, possibly by avoiding overly general settings or sub-cases that diverge from the main focus. Lastly, addressing the relationship between the step size and the number of iterates in the Polyak-Ruppert averaging could provide further clarity.