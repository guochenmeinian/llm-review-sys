ID: IfZwSRpqHl
Title: Dynamic Rescaling for Training GNNs
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 4, 3, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a dynamics rescaling approach to enhance the trainability of Graph Attention Networks (GATs) by leveraging their rescale invariance property. The authors propose a criterion for dynamically rescaling weights based on relative gradients, which allows for larger learning rates and mitigates imbalance issues during training, thereby improving generalization. The paper also explores layer-wise training to accelerate grokking and discusses the implications of their findings on training dynamics in GNNs. Additionally, the authors conduct a systematic ablation study examining the relationship between dynamic rebalancing (DR), gradient clipping (GC), and gradient l2-norm penalty (GP) with regularization strength. The results indicate that GP is a better alternative to GC under regular training conditions, while GC becomes more critical when training with DR. The findings suggest that frequent DR combined with both GC and GP yields optimal performance.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important yet underexplored topic in graph learning, focusing on the training dynamics of GNNs.
- The proposed method is well-motivated from a symmetry perspective and effectively enhances the trainability and generalizability of GATs.
- Empirical observations regarding the order of layer training are intriguing and may have broader implications.
- The authors provide a thorough examination of the interplay between dynamic rebalancing and gradient stabilizers, addressing core concerns effectively.
- The systematic ablation study offers valuable insights into the performance implications of different configurations.

Weaknesses:
- The integration of preliminary concepts makes it difficult to discern the novel contributions of this work compared to prior studies on training dynamics.
- Experiments in Section 4.1 are limited due to the similarity of the datasets used, raising questions about the method's applicability to GNNs lacking the invariance property.
- The relationship between the proposed criterion and the conservation law associated with GATs is not clearly articulated.
- The differences in performance when using both GC and GP versus only GC are noted to be minimal, which may raise questions about the necessity of including both methods.

### Suggestions for Improvement
We recommend that the authors improve clarity by explicitly comparing their contributions with previous works on training dynamics, particularly addressing the novelty of their approach. Additionally, conducting experiments on diverse real-world datasets to validate the effectiveness of their method in practical scenarios would strengthen their claims. We also suggest providing a more detailed explanation of how the proposed criterion relates to the conservation law and enhancing the discussion on the limitations of their approach. Furthermore, clarifying the minimal performance differences when using both GC and GP could strengthen the argument for their combined use. Finally, a more thorough exploration of the interactions between their rescale mechanism and gradient stabilization methods would be beneficial.