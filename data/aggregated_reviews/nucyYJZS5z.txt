ID: nucyYJZS5z
Title: Lion: Adversarial Distillation of Proprietary Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for knowledge transfer from a large proprietary language model, such as ChatGPT, to a smaller open-source model through model imitation. The authors propose an adversarial distillation framework that identifies "difficult" instructions and generates new ones for the student model, creating a three-stage adversarial loop. They demonstrate that Lion-13B achieves performance comparable to ChatGPT and outperforms state-of-the-art models like Vicuna on various tasks and datasets.

### Strengths and Weaknesses
Strengths:
- The innovative adversarial distillation framework facilitates efficient knowledge transfer by modeling differences between teacher and student models.
- Extensive experiments show superior performance of the Lion model across multiple benchmarks.
- The method requires significantly less training data and no additional human annotation.

Weaknesses:
- The lack of an ablation study limits understanding of hyperparameter sensitivity and the value of augmentation versus feedback.
- The selection of parameters, such as the threshold for discriminating difficult commands, is not thoroughly analyzed.
- The discussion of the model's capabilities and evaluation metrics is brief and lacks depth.

### Suggestions for Improvement
We recommend that the authors improve the paper by conducting an ablation study to demonstrate the sensitivity of hyperparameters and the impact of augmentation versus feedback. Additionally, a more detailed analysis of the parameter selection process is necessary. We suggest revising the final paragraphs of the introduction for clarity and addressing the potential drawbacks of using Lion as a substitute for ChatGPT or LLaMA. A thorough description of the pros and cons of the method, along with clearer definitions in figures, will enhance understanding. Finally, we encourage the authors to reconsider the terminology used, particularly regarding the relationship to adversarial knowledge distillation, to avoid confusion.