ID: AZ8sFZtLHD
Title: Difference-Masking: Choosing What to Mask in Continued Pretraining
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the continued pretraining setting, where pretrained models adapt to domain-specific data before downstream tasks. The authors propose a masking strategy that selectively masks words based on their relevance to the target domain, demonstrating improved performance across various language-only and multimodal tasks. The paper emphasizes the importance of domain-adaptive pretraining and critiques conventional random masking as overly simplistic.

### Strengths and Weaknesses
Strengths:
- The proposed method, Difference-Masking, is clear and straightforward, with detailed experimental results showing superior performance on multiple benchmarks.
- The paper addresses an important problem in domain-adaptive pretraining and evaluates both language-only and multimodal tasks.

Weaknesses:
- The novelty of the masking strategy is limited, and the method's generalizability to larger models or other tasks remains uncertain.
- The need to store the entire pretraining corpus for anchor selection is impractical, and the extension to multimodal tasks is constrained by reliance on specific labels.
- Some hyperparameters are sensitive, leading to significant performance variations.

### Suggestions for Improvement
We recommend that the authors improve the exploration of the distribution used for approximating pretraining data statistics to enhance robustness. Additionally, the authors should consider addressing the fairness of masking in video/image patching by either applying their strategy to patch selection or comparing it against random masking methods. Finally, a broader evaluation of the proposed method against other domain-adaptive pretraining techniques would strengthen the paper's contributions.