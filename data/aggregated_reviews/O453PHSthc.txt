ID: O453PHSthc
Title: Training biologically plausible recurrent neural networks on cognitive tasks with long-term dependencies
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 6, 4, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for extracting long-term dependencies in biologically plausible leaky RNNs, motivated by the lack of cortical evidence for gating mechanisms found in high-performing RNNs like GRUs and LSTMs. The authors propose architectural innovations, including skip connections through time and dynamics-aligned skip connections (DASC), to enhance training stability and performance on cognitive tasks requiring long-range temporal dependencies. The results indicate that DASC performs effectively, particularly for tasks with such requirements.

### Strengths and Weaknesses
Strengths:
* The paper demonstrates significant performance improvements compared to baseline methods.
* It is well-structured, with a clear flow and commendable figures that aid understanding.
* The incorporation of biological domain knowledge into the algorithm is insightful.

Weaknesses:
* The main claim of faster training is not adequately verified, and the computational cost of networks with skip connections is unclear.
* Comparisons with established methods like LSTMs and GRUs are missing, which limits the demonstration of the proposed approach's superiority.
* The presentation could be improved by including dataset properties and clarifying the equations and figures, as some aspects are difficult to understand.

### Suggestions for Improvement
We recommend that the authors improve the verification of their main claim regarding faster training by providing clearer evidence and comparisons with LSTMs and GRUs. Additionally, it would be beneficial to list the properties of the datasets used in the experiments and clarify the equations, particularly those labeled "before" and "after." We also suggest simplifying or enhancing the clarity of figures, especially those summarizing earlier work and illustrating modified model dynamics. Finally, strengthening the motivation for using biological RNNs over gated networks would enhance the paper's argument.