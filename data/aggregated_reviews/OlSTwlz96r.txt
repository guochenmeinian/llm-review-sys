ID: OlSTwlz96r
Title: Federated Multi-Objective Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 4, 3, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a federated multi-objective learning (FMOL) framework that enables multiple clients to collaboratively address a multi-objective optimization (MOO) problem while maintaining data privacy. The authors propose two new federated multi-objective optimization (FMOO) algorithms: federated multi-gradient descent averaging (FMGDA) and federated stochastic multi-gradient descent averaging (FSMGDA). These algorithms facilitate local updates to minimize communication costs while achieving convergence rates comparable to single-objective federated learning counterparts. The authors assert that their work is the first systematic effort to integrate federated learning with MOO.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant and timely issue of extending multi-objective optimization to the federated learning context, addressing both objective and data heterogeneity.
- The proposed FMOO algorithms, FMGDA and FSMGDA, come with provable Pareto-stationary convergence rate guarantees, supported by a reasonable theoretical analysis.
- Comprehensive experimental results validate the effectiveness of the algorithms.

Weaknesses:
- There are concerns regarding the handling of data heterogeneity, as Algorithm 1 lacks specific designs to address this issue.
- Experimental comparisons with FedMGDA+ are insufficient, particularly given that FedMGDA+ is relevant to the experimental context.
- The experiments do not include diverse tasks across clients, which is critical since the authors claim their algorithms can handle varied settings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how their algorithms address data heterogeneity, particularly in Algorithm 1. Additionally, we suggest including more comprehensive experimental comparisons with FedMGDA+ to strengthen the validation of their approach. It would also be beneficial to conduct experiments involving different tasks for each client to better demonstrate the versatility of their algorithms. Lastly, we advise the authors to explicitly discuss the limitations of their study to provide a clearer understanding of the scope and applicability of their findings.