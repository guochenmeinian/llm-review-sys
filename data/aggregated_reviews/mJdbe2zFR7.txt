ID: mJdbe2zFR7
Title: Rethinking and Accelerating Graph Condensation: A Training-Free Approach with Class Partition
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Class-partitioned Graph Condensation (CGC), a training-free approach aimed at enhancing the efficiency and effectiveness of graph neural networks (GNNs) on large-scale graphs. The authors propose a novel class-to-node matching paradigm that reformulates graph condensation as a class partition problem, eliminating the need for time-consuming gradient-based optimization. The paper is well-structured, providing clear motivation and comparisons to existing methods, with visual aids that enhance understanding. CGC demonstrates competitive accuracy and achieves speedup factors up to 10^4 compared to traditional methods, making it relevant for real-world applications.

### Strengths and Weaknesses
Strengths:
- CGC introduces a fresh, training-free approach to graph condensation, avoiding gradient-based optimization.
- The method is theoretically sound and achieves significant speed improvements without compromising accuracy.
- The paper is well-organized and presents a clear explanation of the proposed method and its motivation.

Weaknesses:
- The scalability of the method concerning memory costs needs further discussion.
- Generalization to other GNN tasks, such as link prediction or graph classification, requires more analysis.
- Some technical sections, particularly the mathematical formulation, lack detailed explanations for broader accessibility.
- The ablation study does not adequately address the impact of different components on performance.

### Suggestions for Improvement
We recommend that the authors improve the discussion on memory requirements and scalability of CGC. Additionally, the authors should analyze the potential for CGC to adapt to other graph-based tasks beyond node classification, detailing necessary modifications. Clarifying the relationship between the performance of CGC and the backbone GNN, particularly SGC, would enhance the paper's depth. We also suggest including more robust baselines for comparison and ensuring consistency in reported results across datasets. Finally, addressing the clarity of notations and providing more detailed explanations in technical sections would improve accessibility for readers.