ID: BOtjMacACI
Title: Efficient Adaptation of Pre-trained Vision Transformer via Householder Transformation
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a data-driven variant of LoRA-like Parameter-Efficient Fine-Tuning (PEFT) for Vision Transformers (ViTs), which learns the optimal bottleneck dimension instead of preconfiguring it. The authors propose learning three vectors: $v_{\text{left}}$, $v_{\text{right}}$, and a diagonal parameter vector $D$ to define the rank. They utilize Householder transformations to construct orthogonal matrices, allowing for layer-wise rank variations. The method, named HTA, achieves improved performance on downstream tasks while maintaining parameter efficiency.

### Strengths and Weaknesses
Strengths:  
- The paper addresses the optimal rank determination problem for LoRA-based methods using a learning-based approach, demonstrating effectiveness through experiments and ablation studies that surpass state-of-the-art baselines.  
- The method allows for layer-wise rank variations and is validated on classification tasks, showcasing extensive empirical evaluations.  
- The work is well-written and organized, with clear visualizations and a solid foundation of experimental results.

Weaknesses:  
- The discussion lacks consideration of actual training costs and latency, with a need for theoretical or empirical memory and computational cost analysis.  
- The novelty and significance of the proposed method are somewhat limited, as the performance improvements over existing methods are marginal.  
- Validation is primarily on classification tasks, which may restrict the applicability of the findings to more complex tasks like object detection and semantic segmentation.

### Suggestions for Improvement
We recommend that the authors improve the discussion on theoretical and actual memory computation costs, such as FLOPS. Additionally, it would be beneficial to include the rank applied to the LoRA-based baselines in Table 1 and clarify the settings used in Table 5. A scatter plot illustrating the accuracy versus the number of parameters tradeoff would enhance clarity regarding HTA's performance. Finally, exploring the feasibility of extending the method to Large Language Models and other modalities for PEFT could broaden the impact of the work.