ID: Fp5uC6YHwe
Title: 3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive Physics under Challenging Scenes
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 5, 6, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 2, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method that combines a NeRF-style perception module with a 3D point-based dynamics module to model complex physical phenomena involving fluids, granular materials, and rigid objects. The perception module learns spatial-equivariant representations, transforming the environment into points, while the dynamics module utilizes GNNs to exploit the compositional structure of these point systems. The proposed approach demonstrates superior performance over existing models lacking explicit 3D representations, achieving strong long-horizon predictions and generalization to complex scenarios.

### Strengths and Weaknesses
Strengths:
- The method effectively models complex dynamics without requiring strong supervision from ground truth 3D point trajectories, which are typically only available in simulated environments.
- It generalizes well to unseen environments due to its explicit point-based representation.
- The approach achieves state-of-the-art performance across multiple simulation tasks, improving visual reconstruction quality and dynamic prediction accuracy.

Weaknesses:
- The paper lacks details on dataset construction, module design, evaluation metrics, and particle velocity computation. A pointer to supplementary material for these details would enhance clarity.
- There is no real-world demonstration of the method, which would strengthen claims regarding learning complex physical dynamics from visual supervision.
- The reliance on point colors for segmentation necessitates a one-to-one mapping between color and instance, limiting the method's applicability, particularly for transparent fluids.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing more details on dataset construction, module design, and evaluation metrics. Including a real-world demonstration of the proposed method would significantly enhance the paper's impact. Additionally, addressing the limitations related to the reliance on point colors for segmentation and providing a more in-depth analysis of the method's generalization to real-world scenarios would be beneficial. Lastly, we suggest clarifying the encoding of object properties and segmentation in the introduction to the method section to distinguish this work from prior research.