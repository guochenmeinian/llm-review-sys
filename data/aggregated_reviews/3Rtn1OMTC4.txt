ID: 3Rtn1OMTC4
Title: Spatiotemporal Predictive Pre-training for Robotic Motor Control
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents STP, a novel self-supervised learning method aimed at enhancing robotic visual-motor control by extracting useful visual features from out-of-domain, action-free human videos. The authors argue that simply extracting spatial features via masked autoencoding (MAE) is inadequate; instead, they propose a method that jointly captures spatial control and temporal movement. STP employs dual decoders for spatial and temporal predictions, evaluating its effectiveness across 21 diverse tasks, from simulation to real-world applications.

### Strengths and Weaknesses
Strengths:
1. The paper is well-motivated, emphasizing the significance of pre-training visual features for robotic foundation models.
2. The method is clearly articulated, and the experimental evaluations are extensive, covering various simulated and real-world settings.
3. The ablation studies provide valuable insights into the design choices made.

Weaknesses:
1. The performance improvements of STP over baselines are marginal, raising concerns about the robustness of the results due to the stochastic nature of imitation learning and the lack of variance reporting across multiple random seeds.
2. The paper does not sufficiently address existing methods that also consider temporal movements, such as R3M, Voltron, and DecisionNCE, which could strengthen the authors' contributions by situating STP within the broader context of related work.
3. The real-world experiments are limited, with only two tasks where MAE also performs reasonably well, and the source of diverse image data for STP is unclear.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their evaluations by assessing the impact of multiple random seeds in simulated tasks to provide more reliable statistical insights. Additionally, the authors should enhance the discussion of related works that consider temporal information in visual representation learning, clearly highlighting the differences between STP and these methods. Finally, providing more comprehensive real-world experimental results and clarifying the data sources for STP would strengthen the paper's contributions.