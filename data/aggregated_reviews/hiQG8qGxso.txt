ID: hiQG8qGxso
Title: Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 4, 6, 5, 5, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called Hierarchical Ensembles (HiE) aimed at enhancing fine-grained classification by leveraging label hierarchy information at test time. The authors propose a post-hoc correction technique that involves training both fine-grained and coarse-grained classifiers, using the latter's predictions to adjust the probabilities of the former. The method claims to achieve state-of-the-art results on benchmark datasets iNaturalist-19 and tieredImageNet-H, while also demonstrating effectiveness in a semi-supervised learning context. The HiE loss is introduced to penalize mistakes in both coarse and fine-grained predictions, thereby addressing the issue of mistake severity.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- It builds on prior work and shows improved results.
- The semi-supervised learning setup is intriguing and well-explored.
- The experiments and ablations conducted are sensible and comprehensive.

Weaknesses:
- The methodology, particularly in Section 3, is confusing, especially regarding the derivation and motivation for the decision rule in Eqs. (2) and (3).
- The assumption of conditional independence in Eq. (1) is questionable and may undermine the derivations in the section.
- There are inconsistencies in terminology, such as referring to semi-supervised models as self-supervised in different sections.
- The proof of Theorem 3.1 lacks convincing support, and the experimental setup for semi-supervised learning is not clearly presented.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology in Section 3, particularly by providing a more detailed derivation and motivation for the decision rule in Eqs. (2) and (3). Additionally, addressing the conditional independence assumption in Eq. (1) is crucial, as it may affect the validity of the results. We also suggest ensuring consistent terminology throughout the paper to avoid confusion. Furthermore, a more robust proof of Theorem 3.1 and a clearer presentation of the semi-supervised learning experimental setup would strengthen the paper. Lastly, including a discussion on the implications of using logits directly instead of softmaxed probabilities could enhance the analysis.