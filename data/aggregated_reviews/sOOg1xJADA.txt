ID: sOOg1xJADA
Title: Projection-Free Online Convex Optimization via Efficient Newton Iterations
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 6, 4, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents new projection-free algorithms for online convex optimization that utilize efficient Newton iterations with a self-concordant barrier for the target set. The authors establish a state-of-the-art regret bound, achieving optimal regret with improved efficiency by calculating Hessians only $O(\sqrt{T})$ times. The algorithm requires access to a self-concordant function via gradient and Hessian oracles.

### Strengths and Weaknesses
Strengths:  
- The paper makes a solid contribution to more efficient projection-free online convex optimization, achieving optimal regret and better efficiency simultaneously.  
- The proposed algorithm requires computing a full inverse of the Hessian in a vanishing $O(1/\sqrt{T})$ fraction of each round, resulting in lower per-iteration computational costs compared to linear optimization.  
- It opens a new avenue for using IPM-like methods in projection-free OCO, enhancing the existing literature.

Weaknesses:  
- There is a potential violation of the double-blind rule due to an author's comment in line 160.  
- The paper lacks a detailed comparison with the ONS algorithm in HAK07 and does not sufficiently analyze the gradient complexity relative to other works.  
- The assumptions made in Theorem 6 regarding the local norm of $g_t$ are not trivial, as the Hessian of a self-concordant function can be unbounded.  
- Several typos and inconsistencies in terminology and notation were noted, which detract from the overall presentation.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing a detailed comparison with the ONS algorithm in HAK07 to clarify the contributions of their work. Additionally, further analysis of the computational complexity mentioned in line 208 should be included, specifically addressing the conditions under which gradient complexity dominates Hessian complexity. The authors should also clarify the error tolerances in the definitions of the approximate gradient and Hessian, ensuring they are reasonable given the potential for small eigenvalues. Finally, we urge the authors to correct the identified typos and inconsistencies in terminology throughout the manuscript.