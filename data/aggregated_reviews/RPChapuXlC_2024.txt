ID: RPChapuXlC
Title: Lisa: Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning Attack
Conference: NeurIPS
Year: 2024
Number of Reviews: 28
Original Ratings: 8, 4, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to mitigate the risk of harmful fine-tuning in large language models (LLMs) by introducing the Lisa algorithm, which incorporates a proximal term to address the excess drift observed in the baseline BSO method. The authors argue that their approach provides better protection against fine-tuning datasets containing poisoned samples, supported by both theoretical analysis and empirical results across multiple datasets. Additionally, the authors propose a method for fine-tuning LLMs using a safety alignment dataset, specifically BeaverTails, which contains 330k alignment samples. They assert that their experiments are based entirely on this dataset, addressing concerns about generalization, and provide results demonstrating that Lisa effectively handles remnant toxicity left by filtering harmful data.

### Strengths and Weaknesses
Strengths:
1. The authors demonstrate theoretically that their proximal term induces convergence guarantees for the gradient magnitudes of both subloss objectives.
2. The experimental results are comprehensive, including various controls, baselines, and ablations that clarify the contribution of the Lisa algorithm.
3. The proposed approach is technically solid and demonstrates effective handling of remnant toxicity.
4. The authors provide detailed experimental results and additional experiments in response to reviewer feedback, enhancing the robustness of their findings.
5. The use of an open-source safety alignment dataset (BeaverTails) supports the generalization of their method.

Weaknesses:
1. There is a lack of detail regarding the initial jail-broken effect from harmful fine-tuning experiments, including the specific model and data used.
2. The operationalization of alignment is unclear, raising concerns about the appropriateness of the term without sufficient clarity.
3. Section 4 lacks definitions for critical variables and terms, such as w, x, y, and the optimizer_step, which hampers understanding.
4. Key terms like Proximal, Convergence, Consensus, and Drift are not well-defined, leading to potential misunderstandings regarding the authors' motivations and claims.
5. There are concerns regarding the availability of safety alignment datasets for users who wish to fine-tune models independently.
6. Some reviewers noted that parts of the paper were difficult to understand due to missing descriptions.

### Suggestions for Improvement
We recommend that the authors improve clarity by defining key terms such as Proximal, Convergence, Consensus, and Drift early in the paper to avoid confusion. Additionally, the authors should provide more details on the harmful dataset and fine-tuning task prior to presenting experimental results. Linking appendices for missing details where appropriate would enhance the paper's coherence. Furthermore, we suggest conducting experiments with higher mixtures of harmful data to better assess the viability of the proposed defense against harmful fine-tuning attacks. We also recommend including comparisons between Lisa and other defense mechanisms, highlighting effectiveness and trade-offs. Lastly, the authors should fix citation styles to use "citet" where appropriate and ensure that all variables and functions are clearly defined throughout the paper.