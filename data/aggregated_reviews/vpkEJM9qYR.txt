ID: vpkEJM9qYR
Title: Unveiling the Multi-Annotation Process: Examining the Influence of Annotation Quantity and Instance Difficulty on Model Performance
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on annotation design choices in dataset collection, specifically examining how varying the number of annotations per instance affects model performance in the textual entailment task using the ChaosNLI dataset. The authors simulate datasets with K annotations per instance, varying K from 1 to 100, and find that model performance saturates quickly with increasing annotations. The work addresses a critical issue in dataset creation, providing insights into the necessary number of annotations per prompt within budget constraints.

### Strengths and Weaknesses
Strengths:
- The paper tackles an important problem in annotated dataset collection and employs non-trivial analysis methods, such as dataset cartography techniques.
- It is well-organized and clearly written, making complex details accessible.
- The findings have practical implications for decisions regarding corpus creation and budget considerations.

Weaknesses:
- The study does not explore the tradeoff between the number of annotators (K) and the number of instances (N) annotated under a fixed budget, which is crucial for understanding annotation design choices.
- The analysis overlooks the overall quality of datasets at different K values, failing to measure accuracy against the "absolute" ground truth from 100 workers.
- The title and introduction may mislead readers into thinking the study is general-purpose, while it is limited to textual entailment tasks.

### Suggestions for Improvement
We recommend that the authors improve the exploration of the tradeoff between K and N under a fixed budget to enhance the contribution of the work. Additionally, the authors should measure dataset quality at different K values against the "absolute" ground truth to provide a more comprehensive analysis. Clarifying the title and introductory sections to accurately reflect the specific NLP task studied would also help prevent misinterpretation. Finally, adding standard deviation to accuracy results in charts would provide insight into variability, particularly at lower K values.