ID: toEGuA9Qfn
Title: SafeDICE: Offline Safe Imitation Learning with Non-Preferred Demonstrations
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SafeDICE, an offline safe imitation learning algorithm that learns a safe policy using non-preferred and unlabeled demonstrations. It formulates the safe imitation learning problem as a stationary distribution matching problem, solving it through a single convex minimization without requiring hyper-parameter search. The authors clarify that their approach aims to learn a policy that negates non-preferred demonstrations, serving as safety guidelines. They argue that constrained reinforcement learning (RL) algorithms cannot be fairly compared as baselines due to their requirement for extensive reward and cost annotations, whereas SafeDICE operates with minimal annotations. The empirical evaluation demonstrates SafeDICE's effectiveness across RWRL and Safety-Gym benchmarks, showing competitive performance against COptiDICE, a state-of-the-art offline constrained RL algorithm, even outperforming it in some domains.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to understand, with clear and concise presentation of ideas.
- The problem setting is well-motivated, addressing an important and unexplored area in safe imitation learning.
- The authors provide a clear distinction between safe imitation learning and CMDP, emphasizing the broader context of safety in RL.
- The empirical evaluation is robust, effectively demonstrating the proposed algorithm's performance against reasonable baselines, including experiments on noisy datasets that enhance the robustness of the findings.

Weaknesses:
- The mathematical contributions appear minor, primarily building on existing DICE family formulations without significant innovation.
- The performance metrics seem low, and the normalization of returns raises questions about whether tasks are successfully solved. The inclusion of a typical Safe RL algorithm (e.g., CPO, TRPO-Lagrangian) as an oracle agent is recommended.
- The choice of baselines is limited, lacking comparisons with established constrained optimization methods, which are crucial for assessing safety in autonomous systems.
- The dataset design is biased, with a significant imbalance between preferred and non-preferred demonstrations, which may not reflect real-world applications.
- The paper lacks a comparison with existing oracle online baselines, such as PPO-Lagrangian, which could provide a more comprehensive performance benchmark.
- The empirical evaluation does not adequately assess the utility of the proposed objective, particularly regarding the impact of non-preferred demonstrations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the mathematical contributions by explicitly stating the novel aspects of their work compared to existing methods. Additionally, the authors should include comparisons with relevant constrained optimization baselines such as CPO, PPO-Lagrangian, TRPO, and LAMBDA to strengthen their claims. A more detailed breakdown of the dataset, including the number of safe and unsafe actions, is necessary to address concerns about bias. We also suggest conducting ablation studies to evaluate the impact of the mix ratio alpha and the presence of non-preferred demonstrations on performance. Finally, we recommend incorporating a discussion on the implications of noisy datasets and the limitations of the proposed approach, particularly regarding the efficiency of learning from non-preferred data, in the final version.