ID: ZrG8kTbt70
Title: WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 8, 8, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents WalkLM, a universal language model for generic graph representation learning that integrates random walks (RWs) and language models (LMs) to create meaningful textual sequences from attributed graphs. The authors propose a fine-tuning strategy to extract embedding vectors from the LM, demonstrating the method's superiority over state-of-the-art approaches through extensive experiments on real-world datasets. The paper addresses the limitations of existing graph representations, particularly in unsupervised settings, and showcases promising results in various downstream tasks.

### Strengths and Weaknesses
Strengths:  
1. The paper is well-motivated, addressing the limitations of traditional graph representations and integrating LMs and RWs for improved performance.  
2. It is well-written and organized, facilitating reader comprehension and showcasing promising results across different tasks.  
3. The problem setting is realistic, and the experimental results are encouraging, indicating potential applications in real-world scenarios.  
4. The use of random walks is a highlight, effectively capturing flexible graph structures without supervision.

Weaknesses:  
1. There is a lack of deeper analysis on why the proposed framework performs well with limited training data in few-shot settings.  
2. The impact of hyperparameters, such as latent dimension and the number of masked samples, on downstream task performance needs further exploration.  
3. The evaluation is limited to two datasets, raising questions about the method's applicability to other scenarios and datasets.  
4. The novelty of the approach is questioned, as similar concepts have been explored in prior works.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the framework's performance with small training datasets, particularly in few-shot settings. Additionally, a thorough examination of hyperparameter effects on performance should be included. To strengthen the evaluation, we suggest expanding the dataset coverage to demonstrate the method's applicability across various contexts. Finally, we encourage the authors to clarify the distinctions between their approach and existing literature on knowledge graph embedding to address concerns regarding novelty.