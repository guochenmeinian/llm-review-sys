ID: 0G0VpMjKyV
Title: Sketching for Distributed Deep Learning: A Sharper Analysis
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 4, 6, -1, -1, -1, -1
Original Confidences: 5, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel analysis of sketching-based distributed learning algorithms for deep neural networks, achieving dimension-independent convergence by leveraging the restricted strong smoothness (RSS) property of loss functions. The authors address a gap between theoretical analyses that suggest dimension dependence and empirical results showing competitive performance without such dependence. The work provides theoretical guarantees, improved communication complexity bounds, and empirical validation of their approach.

### Strengths and Weaknesses
Strengths:
1. The paper offers the first dimension-independent convergence analysis for sketching in distributed deep learning without requiring restrictive assumptions like heavy-tailed distributions or top-r sparsification.
2. It bridges the gap between theory and practice, explaining why sketching-based methods perform well empirically despite previous pessimistic theoretical bounds, supported by experimental results.
3. The analysis covers both single-step and multi-step local update scenarios, providing a thorough theoretical treatment.

Weaknesses:
1. The paper relies on certain assumptions about the loss function (e.g., PL condition) and the eigenvalue distribution of the Hessian, with insufficient discussion on the robustness of results to violations of these assumptions.
2. The experimental comparisons are not reasonable, particularly the unfair comparison with flawed algorithms, and the presentation of results lacks clarity.

### Suggestions for Improvement
We recommend that the authors improve the comparison with Song et al. by providing a more comprehensive analysis of the assumptions and results, particularly clarifying the role of the PL condition and the eigenvalue distribution. Additionally, we suggest including a table that compares the assumptions and convergence results with those of Song et al. to enhance clarity. For experimental validation, we advise that the authors present a fair comparison with FetchSGD, ensuring that only appropriate versions of the algorithms are compared. Furthermore, we encourage the authors to elaborate on the practical aspects of implementing their approach in large-scale distributed learning systems and to clarify the notation used in Assumption 3.3. Lastly, we recommend improving the formatting of references to ensure completeness and accuracy.