ID: QGmNMtK3pQ
Title: Learning Large-scale Neural Fields via Context Pruned Meta-Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel optimization-based meta-learning technique for large-scale neural field training, introducing Gradient Norm-based Context Pruning (GradNCP) to enhance memory efficiency and model quality. The authors propose a bootstrap correction to mitigate myopia in optimization-based meta-learning and implement gradient scaling during meta-testing. Experimental results across multiple datasets demonstrate significant improvements in performance compared to baseline methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clearly structured, making it accessible.
- The proposed method is straightforward and applicable within existing meta-learning frameworks, yielding substantial performance improvements.
- Comprehensive experimental results effectively illustrate the contributions of each component of the method.

Weaknesses:
- The experimental setup does not adequately showcase the effects of the proposed components, particularly the norm-based context pruning, which lacks comparisons with other data pruning or importance sampling methods.
- The necessity and effectiveness of meta-training with GradNCP are insufficiently validated; a more rigorous comparison with alternative setups is recommended.
- The focus on reconstruction tasks may overlook the broader applications of neural fields, such as generalization in novel view synthesis.
- The motivation for the choice of optimization-based meta-learning is not clearly articulated, and the paper lacks comparative experiments with other methods like MAML.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind using optimization-based meta-learning, providing a detailed explanation of its advantages. Additionally, the authors should conduct comparative experiments with other data pruning methods, such as loss-based top-k sampling, to validate the effectiveness of GradNCP. It would also be beneficial to include a more thorough evaluation of the necessity of meta-training by comparing GradNCP against random initialization with context pruning. Lastly, the authors should expand the discussion of related works, particularly in the context of generalization tasks and the practical applications of neural fields beyond reconstruction.