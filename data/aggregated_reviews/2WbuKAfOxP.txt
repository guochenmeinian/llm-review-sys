ID: 2WbuKAfOxP
Title: The Scandinavian Embedding Benchmarks: Comprehensive Assessment of Multilingual and Monolingual Text Embedding
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 9, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Scandinavian Embedding Benchmark (SEB), a comprehensive framework for evaluating text embeddings in Scandinavian languages, encompassing 24 tasks across 12 domains. The SEB serves as an extension of the Massive Text Embedding Benchmark (MTEB), focusing on the unique characteristics of Scandinavian languages while facilitating comparisons with existing benchmarks. The authors conducted thorough evaluations of over 26 models, highlighting the performance differences between monolingual and multilingual models. Additionally, the paper includes a comprehensive analysis of benchmarks in natural language processing, specifically focusing on GLUE and ScandEval, proposing a framework for evaluating multi-task performance and emphasizing the significance of these benchmarks in advancing the field.

### Strengths and Weaknesses
Strengths:  
- The SEB contributes significantly to the literature on sentence/document embedding benchmarks, enabling evaluation across additional languages.  
- The detailed analysis of model performance provides valuable insights into the trade-offs between self-supervised and supervised models.  
- The benchmark is well-structured, with clear task definitions and evaluation metrics, enhancing reproducibility.  
- The authors provide a thorough examination of existing benchmarks, contributing valuable insights into their effectiveness and applicability in natural language understanding tasks.

Weaknesses:  
- The originality of the benchmark is limited, as many components have been previously published, with only a few tasks being novel.  
- The analysis lacks depth regarding individual task performance, which restricts understanding of model strengths and weaknesses.  
- The inclusion of only the Norwegian parliament, while omitting Danish and Swedish parliaments, raises questions about comprehensiveness.  
- Some reviewers noted that while the response to feedback was appreciated, there remains a need for further clarity in certain sections to enhance understanding.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by including additional datasets from the Danish and Swedish parliaments to enhance its comprehensiveness. Furthermore, the authors should clarify the inclusion of Faroese and Icelandic, as their current representation is minimal. A deeper analysis of model behavior on individual tasks would provide better insights into performance variations. Additionally, we suggest that the authors include details on the bootstrapping method used for confidence intervals and consider using F1 scores consistently across all tasks. Lastly, providing MTEB results for the multilingual models tested would contextualize the findings and strengthen the benchmark's relevance. We also recommend that the authors improve the clarity of the sections that reviewers found confusing and consider incorporating more detailed explanations of the benchmarks' implications for future research.