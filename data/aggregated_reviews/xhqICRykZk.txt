ID: xhqICRykZk
Title: Text Augmented Spatial Aware Zero-shot Referring Image Segmentation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel zero-shot referring expression segmentation method utilizing the SAM model, pretrained CLIP, and BLIP2. The authors propose a Text Augmented Spatial-aware (TAS) framework that includes a mask proposal network, a text-augmented visual-text matching score, and a spatial rectifier for mask post-processing. The framework demonstrates strong performance on various datasets, outperforming state-of-the-art methods in zero-shot referring expression segmentation.

### Strengths and Weaknesses
Strengths:
- The proposed method shows strong performance on widely-used benchmarks, effectively handling challenging scenarios.
- The paper includes comprehensive ablation studies, demonstrating the contribution of each module in the TAS framework.
- The innovative combination of visual-text matching and alignment quality scores offers new avenues for leveraging text information in segmentation tasks.

Weaknesses:
- The method's reliance on large models like BLIP2 raises concerns about practical applicability due to high inference costs and hardware requirements.
- Some components of the proposed method exhibit similarities to existing approaches, leading to questions about its novelty.
- Clarity is lacking regarding the Negative Text Miner's effectiveness in managing diverse image descriptions and semantically varied phrases.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the real-world applicability of the Negative Text Miner, particularly in handling diverse image descriptions. Additionally, it would be beneficial to provide further results from multiple datasets to validate the method's effectiveness and generalizability. We also suggest addressing the inference cost associated with the combined use of large models and considering alternative training methods for a fair comparison. Lastly, refining sections of the paper for better readability and logical flow would enhance the overall presentation.