ID: owHj0G15cd
Title: Direct Preference-Based Evolutionary Multi-Objective Optimization with Dueling Bandits
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a preference-based evolutionary multi-objective optimization (PBEMO) framework that leverages human feedback through a clustering-based stochastic dueling bandits algorithm. The authors aim to address inefficiencies in existing PBEMO methods, which often fail to accurately interpret decision-makers' intentions. The proposed method is validated through experiments, showcasing its application in real-world problems, such as protein structure prediction.

### Strengths and Weaknesses
Strengths:
- The paper tackles an important problem with a novel method that directly incorporates human feedback, which is crucial for preference outputs.
- The authors prove a regret bound for their algorithm that surpasses that of existing Thompson sampling-based dueling bandit algorithms, and empirical evaluations demonstrate the effectiveness of the approach.

Weaknesses:
- The selection criteria for benchmark problems in Section 4.1 are unclear, particularly why certain benchmarks like DTLZ7 were not chosen.
- The comparison algorithm in Section 4.2 may not represent the state-of-the-art (SOTA) in the domain.
- The discussion of the two variants, DPNSGA-II and DP MOEA/D, lacks depth regarding their respective strengths and weaknesses.
- The experimental section tends to restate results without sufficient analysis of underlying reasons.
- Key parameters such as population size and number of search iterations are not specified.
- The writing quality needs improvement, with many mathematical notations inadequately defined and missing details.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the benchmark problem selection criteria in Section 4.1 and consider including a broader range of benchmarks. In Section 4.2, ensure that the comparison algorithm reflects the current SOTA. We suggest enhancing the discussion on the strengths and weaknesses of the two variants, DPNSGA-II and DP MOEA/D, to provide a more comprehensive understanding. Additionally, we encourage the authors to analyze the experimental results more deeply rather than merely restating them. It would be beneficial to include specific parameter settings, such as population size and number of search iterations, to enhance reproducibility. Lastly, we recommend improving the overall writing quality by rigorously defining mathematical notations and providing necessary details.