ID: JD3NYpeQ3R
Title: Large language model validity via enhanced conformal prediction methods
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two new conformal prediction methods aimed at enhancing the validity of large language models (LLMs) and generalizing split conformal prediction to achieve marginal control of a monotone loss. The authors propose a level-adaptive conformal prediction algorithm that adjusts the filtering threshold based on prompt rarity, and a conditional boosting method that improves scoring functions to retain more valid claims while ensuring validity. Additionally, they introduce a method that computes a conditionally valid and calibrated estimate of quantiles, utilizing a data-dependent guarantee rather than a fixed one. The methods are validated through experiments on synthetic and real-world datasets, addressing existing deficiencies by ensuring conditional validity and improving claim accuracy.

### Strengths and Weaknesses
Strengths:
- The introduction of generalized conditional conformal procedures and improved scoring functions addresses critical gaps in current approaches.
- The paper is technically sound, with well-supported claims and robust experimental validation, particularly Theorems 3.1 and 3.2, and Proposition 3.1.
- The methods effectively showcase practical utility and robustness through evaluations on both synthetic and real-world datasets.
- The introduction of a data-dependent guarantee represents a significant theoretical advancement in conformal prediction.
- The authors effectively contextualize their work within existing literature, highlighting its contributions to prediction set efficiency.

Weaknesses:
- The methodology assumes the existence of an annotated calibration set, raising questions about generalizability to different datasets.
- Some reviewers express confusion regarding the novelty of the contributions, particularly in relation to existing methods.
- The experiments lack sufficient ablation studies and comparisons with other baseline methods, particularly the fixed-level conditional conformal procedure.
- There are concerns about the practical utility of guarantees, especially when the error is set to zero, potentially leading to unhelpful outputs.
- The choice of three wrong claims in experiments is arbitrary and lacks justification, leading to concerns about practical applicability.
- The paper does not adequately address the limitations of the exchangeable assumption and the implications of using different scoring functions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their methodology by providing additional explanations for the metrics used in evaluation. Furthermore, we suggest that the authors include comparisons with other baseline methods, particularly the fixed-level conditional conformal procedure, to better illustrate the contribution of their level-adaptive method. We also encourage the authors to clarify Section 3.1 by clearly delineating the similarities and differences between their work and previous methods, particularly addressing the novelty of their contributions. Additionally, we suggest adding ablation studies to justify the choice of three wrong claims in experiments and to explore the implications of using different scoring functions. Lastly, we encourage the authors to elaborate on the generalizability of their approach beyond the annotated calibration set and to provide a thorough discussion on the practical utility of their guarantees, especially when the error is set to zero.