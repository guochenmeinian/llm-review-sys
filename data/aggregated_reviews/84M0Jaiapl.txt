ID: 84M0Jaiapl
Title: LLM-based SQL Generation with Reinforcement Learning
Conference: AAAI
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 6, 6
Original Confidences: 5, 3, 4

Aggregated Review:
### Key Points
This paper presents a reinforcement learning (RL)-based approach for fine-tuning a small language model (LM) to outperform state-of-the-art (SOTA) methods in text-to-SQL generation using a limited dataset of approximately 1,000 samples. The authors introduce two models: SQL-RL-Gen, which generates a reward function to enhance the training process, and SQL-RL-Gen*, which utilizes this reward function for fine-tuning. The results indicate improved performance, although the enhancements over existing methods are modest.

### Strengths and Weaknesses
Strengths:
- The topic is highly relevant, addressing the challenging text-to-SQL problem with significant implications for industry practitioners.
- The application of a modified EUREKA PPO algorithm for reward function generation is innovative, leveraging a gradient-free, evolution search-based method that performs well without reward templates.
- The experimental evaluation is robust, employing one LM for reward generation and another for fine-tuning, iterating until reaching stopping criteria, with solid results.

Weaknesses:
- The clarity of presentation could be improved; the paper introduces EUREKA but fails to elaborate on its differences from the PPO method in the experiments section.
- The choice of LLMs raises concerns about the generalizability of the approach, as only one model is used for reward generation and another for fine-tuning, without evaluating alternatives like OpenAI GPT-4o.
- The justification for the sample size of 1,000 for fine-tuning is unclear; scaling experiments with varying sample sizes would provide insights into the impact on performance.
- The paper does not explore the applicability of DPO for optimizing human preferences, which could enhance the discussion in the background section.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by providing a more detailed comparison of EUREKA and PPO in the background section. Additionally, the authors should justify their choice of LLMs and consider evaluating other models to assess the robustness of their approach. Conducting scaling experiments with varying sample sizes would also strengthen the findings. Finally, addressing the potential applicability of DPO in this context would enrich the paper's contribution.