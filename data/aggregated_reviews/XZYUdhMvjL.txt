ID: XZYUdhMvjL
Title: MultiOrg: A Multi-rater Organoid-detection Dataset
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 8, 3, 6, -1, -1
Original Confidences: 4, 3, 5, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents MultiOrg, a large-scale organoid imaging dataset designed for object detection in 2D microscopy images, comprising over 400 images and more than 60,000 annotations by two expert annotators. The dataset includes three label sets that allow for the assessment of inter- and intra-rater variability, and the authors benchmark four popular object detection models, demonstrating their robustness to label noise. The best-performing model is made publicly available via a Napari plugin.

### Strengths and Weaknesses
Strengths:  
- The dataset is large and publicly available, promoting transparency and encouraging further research.  
- The design effectively quantifies annotation uncertainty and assesses inter-rater performance.  
- The integration of a Napari plugin enhances practical utility for researchers.

Weaknesses:  
- The dataset's coverage is limited to lung organoids, restricting generalizability.  
- The use of only two annotators limits the ability to generalize findings regarding inter-rater variability.  
- The absence of multi-class labels and instance masks reduces the dataset's applicability for certain analyses.  
- The benchmarking is limited to convolutional methods, lacking more recent techniques.

### Suggestions for Improvement
We recommend that the authors improve the dataset by including a larger and more diverse pool of annotators to better represent inter-rater variability. Additionally, incorporating multi-class labels and instance masks for organoids would enhance the dataset's utility. We suggest extending the benchmarking to include more recent methods such as DETR, MedSAM, or Cellpose. Furthermore, clarifying how quantifying annotation uncertainty can improve model benchmarking and visually differentiating the proposed design from previous datasets would strengthen the paper. Lastly, we encourage the authors to address the potential use of noise annotation filtering and sentinel tests to enhance the credibility of the annotations.