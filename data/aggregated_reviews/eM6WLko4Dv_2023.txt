ID: eM6WLko4Dv
Title: LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 8, 5, 5, 7, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the LAMM-Dataset and LAMM-Benchmark, which extend the research of multi-modal large language models (MLLMs) to point clouds and various visual tasks, including object detection and classification. The dataset comprises instruction-response pairs generated using the GPT-4 API, while the benchmark provides a framework for evaluating MLLMs on these tasks. The authors emphasize the significance of data quality over quantity, demonstrating through experiments that smaller, high-quality datasets yield better performance than larger, lower-quality ones. The dataset includes various data types, such as daily dialogue, factual knowledge dialogue, and visual task dialogues, all sourced from publicly available datasets. Additionally, the authors introduce the Binary Location Metric for evaluating localization abilities in LLM models. They also propose a training framework that optimizes feature projection layers and LoRA during training.

### Strengths and Weaknesses
**Strengths:**
- The LAMM-Dataset is the first instruction-tuning dataset for point clouds, and the LAMM-Benchmark offers a comprehensive evaluation of existing models across multiple computer vision tasks.
- The paper provides high-quality human-GPT dialogues that enhance cross-modality performance in human-machine dialogue systems.
- The emphasis on ethical data practices and the manual curation of the dataset enhances its trustworthiness.
- The innovative multimodal approach of the LAMM dataset distinguishes it from existing datasets, providing a valuable resource for multimodal instruction fine-tuning.
- Experimental results support the claim that data quality is more critical than sheer quantity, aligning with findings from related work.

**Weaknesses:**
- The dataset size is relatively small, with 186,098 image-language pairs and 10,262 point cloud-language pairs, which may limit its performance across tasks and raise concerns about its robustness.
- The authors do not clearly explain the selection process for data from other datasets, which is crucial for understanding dataset construction.
- The evaluation pipeline lacks task-specific tokens for fine-grained tasks, which may hinder performance.
- The rationale behind the data selection process could be more clearly articulated in the manuscript.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset's construction process by detailing how data was selected from existing datasets. Additionally, we suggest enlarging the scale of the LAMM-Dataset to enhance its applicability and performance. It would also be beneficial to explore deploying a local LLM and fine-tuning it instead of relying solely on the GPT-API, which could enhance transparency and independence in the dataset generation process. Furthermore, we encourage the authors to address the limitations associated with using GPT-generated data, particularly regarding potential biases and the implications of using such data for evaluation. Exploring automated methods for data alignment could enhance ethical considerations in dataset creation. Finally, addressing any ambiguities in the naming convention will ensure clarity and accuracy in the submission.