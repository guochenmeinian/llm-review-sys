ID: djyn8Q0anK
Title: Scalable Transformer for PDE Surrogate Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 5, 7, 6, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 5, 5, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a high-dimensional function decomposition technique, termed FactFormer, aimed at reducing the complexity of Transformers in handling high-dimensional data for PDE surrogate modeling. The authors provide a logical argument and a thorough literature review, conducting extensive experiments on benchmark 3D problems governed by Navier-Stokes equations. The method employs latent marching, demonstrating better empirical performance than autoregressive models. However, the novelty is questioned due to similar concepts introduced in recent works like Factorized Fourier Neural Operator (FFNO) and Tensorized FNO. While the paper shows clear motivation and technical soundness, it suffers from limited experimental comparisons and confusing notations.

### Strengths and Weaknesses
Strengths:
1. The motivation is clear, and the Introduction is well-written and accessible.
2. The methodology is technically sound and replicable, especially if the authors open-source their code and datasets.
3. The proposed method exhibits empirical advantages in certain scenarios.

Weaknesses:
1. The experimental section lacks depth, only considering 3D datasets within regular geometric areas and not comparing with various relevant baselines like U-FNO, FFNO, HT-Net, and Tensorized FNO.
2. The notation used is somewhat confusing, particularly regarding the representation of dimensions and selection criteria.
3. The paper does not adequately address the exponential growth of the 'v' kernel in equation (7) concerning the curse of dimensionality.
4. The theoretical relationship between tensor decomposition processes and model capacity is not explored in detail.
5. Important related works, including FFNO, HT-Net, and Tensorized FNO, are not sufficiently referenced.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by incorporating additional representative baselines, at least one or two, during the rebuttal period to enhance the comparative analysis. Clarifying the notation, particularly regarding the dimension of the hidden layer and intermediate variables, would benefit readers. Additionally, the authors should address the exponential growth of the 'v' kernel in equation (7) and explore the theoretical implications of tensor decomposition on model capacity. Finally, we suggest including references to closely related works and relevant surveys on neural operators to strengthen the literature context.