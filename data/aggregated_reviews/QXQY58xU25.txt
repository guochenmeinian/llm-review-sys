ID: QXQY58xU25
Title: Data-Efficient Learning with Neural Programs
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 5, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ISED (Infer-Sample-Estimate-Descend), an algorithm designed for learning neural programs that integrate a deep neural network (DNN) with traditional programming components, such as API calls to large language models (LLMs). ISED is evaluated on various benchmarks, demonstrating comparable performance to state-of-the-art neurosymbolic frameworks while achieving greater data and sample efficiency. The authors address challenges in propagating gradients through black-box programs and propose a novel approach to approximate these programs using input-output samples.

### Strengths and Weaknesses
Strengths:
- ISED significantly outperforms prior work in terms of accuracy and data efficiency based on comprehensive evaluations.
- The approach is novel, estimating gradients of black-box programs from inductive samples.
- The paper provides a thorough evaluation across multiple benchmark tasks, showing promising results.

Weaknesses:
- Clarity issues arise in Section 3 due to over-formalism and a lack of end-to-end examples, making it difficult for readers.
- Inconsistencies in symbolism, particularly regarding the definitions of $x$ and $y$ across sections.
- The motivation for composing neural programs is weak, and the advantages of ISED over traditional programming methods are not sufficiently explored.
- The method struggles with high-dimensional input spaces, limiting its applicability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 3 by including more end-to-end running examples to aid understanding. Additionally, addressing the inconsistencies in symbolism would enhance readability. Strengthening the motivation for the composite approach and investigating the advantages of ISED over individual programming would provide a clearer rationale for its use. Furthermore, we suggest exploring how the capability of traditional programming affects ISED's performance and considering the implications of low-probability outputs on learning signals. Finally, presenting results for all benchmarks in the main paper rather than relegating negative outcomes to the appendix would provide a more balanced view of ISED's performance.