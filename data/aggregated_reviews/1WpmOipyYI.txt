ID: 1WpmOipyYI
Title: Tanh Works Better with Asymmetry
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 4, 6, 6, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the impact of the order of Batch Normalization (BN) and activation functions on network performance, specifically focusing on bounded activation functions like Tanh. The authors demonstrate that placing BN after the activation function (the swapped order) significantly enhances performance due to increased asymmetric saturation and sparsity. They also introduce a modified activation function that promotes asymmetric saturation, showing improvements even in the conventional order. However, the relevance of the BN and Tanh combination in modern networks is questioned, limiting the paper's contribution.

### Strengths and Weaknesses
Strengths:
- The paper presents a novel exploration of activation function order, challenging conventional wisdom and providing new insights into deep learning model design.
- Comprehensive experiments validate the hypothesis that asymmetric saturation benefits performance, with clear empirical evidence supporting the findings.
- The writing is clear and well-structured, making the ideas accessible.

Weaknesses:
- The limited scope of network configurations in experiments raises concerns about the generalizability of the findings, particularly as results with ReLU models appear to overshadow those with Tanh.
- The paper lacks a deeper theoretical analysis, which could enhance understanding of the mechanisms behind performance improvements.
- Figures and tables do not include confidence intervals or standard deviations, which are essential for assessing the reliability of results.

### Suggestions for Improvement
We recommend that the authors improve the scope of their experiments by including a wider variety of network architectures, such as ResNet and transformer-based models, to strengthen the generalizability of their conclusions. Additionally, addressing the differential performance between ReLU and Tanh in the context of asymmetric saturation could provide valuable insights. Incorporating a more thorough theoretical analysis would enhance the paper's contribution, and including confidence intervals in figures and tables would improve the presentation of results. Lastly, we suggest introducing the definition of the sparsity metric earlier in the paper to clarify its distinction from traditional definitions.