ID: Y3FjKSsfmy
Title: Preventing Dimensional Collapse in Self-Supervised Learning via Orthogonality Regularization
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 6, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to mitigate dimensional collapse in self-supervised learning (SSL) by employing orthogonal regularization (OR) on weight matrices during pretraining. The authors demonstrate that OR leads to a smoother decay of eigenvalues in both intermediate representations and weight matrices, thereby enhancing the meaningfulness of features and improving performance across various SSL methods and architectures. Additionally, the paper analyzes the DINO framework's performance on the LIGHTLY platform, highlighting discrepancies in results compared to the original DINO article. The authors propose that the lack of multi-crop functionality in LIGHTLY significantly impacts performance outcomes and acknowledge the need to adjust the form of OR to prevent hyperparameter adjustments due to model size increases. However, the novelty of the approach is questioned, as it largely applies existing loss formulations from prior work, particularly Barlow Twins, without significant modification.

### Strengths and Weaknesses
Strengths:
- The proposed method integrates well with existing SSL techniques and shows consistent benefits across different datasets and architectures.
- The paper is well-structured and effectively communicates the problem and solution, with informative observations regarding eigenvalues.
- The authors effectively address previous concerns, leading to improved work quality.
- The analysis of DINO's performance on LIGHTLY is thorough, supported by comparative results.

Weaknesses:
- The method lacks novelty, primarily replicating existing formulations without substantial innovation.
- Experimental results are insufficient, with many SSL methodologies used for evaluation being outdated and showing considerable variability in effectiveness.
- The improvements observed with OR are marginal, particularly in well-tuned baselines, raising questions about its overall efficacy.
- The absence of multi-crop in LIGHTLY is a critical limitation that undermines the validity of the results.
- Future experiments are necessary to enhance the credibility of findings.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contribution by incorporating a more detailed analysis of the impact of whitening on dimensional collapse, as suggested in the reviews. Additionally, it would be beneficial to evaluate the OR method using state-of-the-art SSL techniques and to include results for a broader range of datasets, particularly larger ones. Addressing the variability in results, especially in Tables 1, 3, and 5, would strengthen the paper's claims. Furthermore, we suggest that the authors clarify the advantages of regularizing weights over representations and provide a comparison of the performance of OR with well-tuned baselines, particularly in longer training scenarios. To enhance performance, we also recommend that the authors improve the LIGHTLY framework by incorporating multi-crop functionality. Conducting experiments directly within the DINO repository would provide more robust validation of results.