ID: dU73Mgx7xm
Title: Graph-Skeleton: Less than 2% Nodes are Sufficient to Represent Billion-Scale Graph
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on compressing large graphs to enhance machine learning tasks, specifically focusing on node classification. The authors propose a method called Skeleton, which retains target nodes while compressing background nodes to eliminate redundancy. An empirical analysis identifies useful edges and nodes, leading to three compression strategies: \alpha, \beta, and \gamma. The experiments demonstrate that Skeleton outperforms nine competitive methods across six datasets in terms of AUC and ACC scores, while also exploring the importance of background nodes for target node predictions.

### Strengths and Weaknesses
Strengths:
- The problem is well-motivated and relevant to node classification tasks.
- The empirical study provides solid support for the Skeleton method.
- The method is clearly presented with illustrative examples.
- Positive results from empirical evaluations and availability of source code enhance reproducibility.

Weaknesses:
- The paper lacks a quantitative study of execution time for Skeleton compression compared to existing methods.
- It does not provide the time required to train GNN models on original versus compressed graphs.
- Table 3 presents data storage for Skeleton but not for competitive approaches.
- There are no guarantees regarding the percentage of information retained in the compressed graph, which could strengthen the method's applicability.

### Suggestions for Improvement
We recommend that the authors improve the paper by including a quantitative analysis of the execution time required for Skeleton compression and comparing it with existing methods. Additionally, providing the time needed to train GNN models on both original and compressed graphs would enhance the evaluation. We suggest including storage data for competitive approaches in Table 3 and offering guarantees on the information retention in the compressed graph to bolster confidence in the method's effectiveness. Furthermore, clarifying the empirical analysis in Section 2 and addressing the hardware settings used in experiments would improve the overall clarity and rigor of the study.