ID: XWYv4BNShP
Title: On the Size and Approximation Error of Distilled Datasets
Conference: NeurIPS
Year: 2023
Number of Reviews: 30
Original Ratings: 5, 6, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of dataset distillation methods based on kernel ridge regression (KRR), focusing on the existence of distilled datasets and the relationship between generalization error and the "number of effective degrees of freedom" in the random Fourier features (RFF) regime. The authors derive error bounds for KRR-based dataset distillation and validate these bounds through simple experiments. Additionally, the authors outline a method for generating a distilled set $(S, y_S)$ from an input space, ensuring that the characteristics of the original data are preserved. They emphasize the importance of constructing the dataset \( S \) such that its RFF representation maintains necessary rank conditions to ensure provable guarantees. The authors clarify that while their approach ensures consistency in the RFF space, it does not guarantee the same KRR solution due to the need for an in-sample function that approximates the desired predictions.

### Strengths and Weaknesses
Strengths:
1. The paper provides valid error bounds for KRR-based dataset distillation methods, offering guidance on the size of synthetic datasets needed for acceptable errors.
2. It is the first theoretical work in the field, establishing a correlation between distilled dataset size and kernel characteristics in the RFF regime.
3. The authors provide a clear methodological framework for generating the distilled set, including detailed mathematical formulations.
4. The writing is logical and consistent, with clear contributions and a solid theoretical foundation.
5. The authors address reviewer concerns thoroughly, demonstrating a commitment to clarity and engagement with the academic community.

Weaknesses:
1. The analysis does not align with typical dataset distillation settings, as it assumes a distilled dataset size greater than the feature dimension, which simplifies the problem and may render results trivial.
2. The technical contribution is insufficient for a NeurIPS paper, as it largely applies previous results without significant innovation.
3. The derived error bounds are not tight, limiting their practical utility, and the experiments are overly simplistic, lacking benchmarks on larger datasets like CIFAR10.
4. The clarity of the writing and the logical flow of the arguments require significant improvement, as some reviewers found the explanations convoluted and difficult to follow.
5. There are gaps in the explanation of how the constructed dataset \( S \) relates to the solved vector \( \alpha \), leading to potential confusion regarding their functionalities.
6. The brute-force approach to constructing \( S \) may not align with traditional dataset distillation practices, which could affect the practical applicability of the proposed methods.

### Suggestions for Improvement
We recommend that the authors improve the consistency of their analysis with typical dataset distillation settings, particularly by addressing cases where the number of synthetic samples is smaller than the feature dimension. Additionally, the authors should provide a clearer explanation of the proof for the existence of the distilled dataset and enhance the clarity of their theoretical results. We suggest that the authors directly prove the full rank condition for $\tilde{S}$ for any chosen set \( S \) to eliminate confusion. Expanding experimental analysis to include larger datasets and more complex scenarios, such as multi-class cases, would strengthen the paper significantly. Furthermore, we recommend that the authors clarify the relationship between the constructed dataset \( S \) and the vector \( \alpha \) to eliminate any confusion regarding their roles. Lastly, the authors should address the gap between the brute-force construction method and conventional dataset distillation approaches, possibly by discussing the implications of using synthetic versus original dataset samples in their analysis.