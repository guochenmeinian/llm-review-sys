ID: TZW4nzgtQ8
Title: Locally Differentially Private Document Generation Using  Zero Shot Prompting
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 6
Original Ratings: -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a locally differentially private text sanitization method called DP-Prompt, which leverages large language models (LLMs) to paraphrase text while mitigating privacy risks, specifically against author de-anonymization attacks. The authors propose a zero-shot prompting approach that generates sanitized documents, balancing utility and privacy. Extensive experiments demonstrate the effectiveness of DP-Prompt across various models, including both commercial and open-source LLMs.

### Strengths and Weaknesses
Strengths:  
- The proposed method is simple to implement and shows superior performance compared to baseline methods, effectively addressing the critical issue of privacy in text data.  
- The paper provides thorough evaluations and insights into the privacy-utility tradeoff, including the impact of sampling temperature.  
- The use of LLMs for authorship obfuscation is a novel contribution that enhances practical applications in privacy preservation.

Weaknesses:  
- The privacy analysis relies heavily on empirical evaluations without sufficient theoretical guarantees, focusing narrowly on authorship obfuscation.  
- There is a lack of evaluation regarding the preservation of original meaning, hallucinations, and factual accuracy, which are crucial for effective paraphrasing.  
- Comparisons with baseline methods are not adequately justified, particularly regarding the use of smaller models against powerful LLMs like ChatGPT.

### Suggestions for Improvement
We recommend that the authors improve the theoretical privacy guarantees of DP-Prompt and broaden the scope of privacy risks addressed beyond authorship obfuscation. Additionally, we suggest incorporating evaluations of meaning preservation, hallucination rates, and factual accuracy to ensure the integrity of the paraphrased text. It would also be beneficial to test stronger baselines, including state-of-the-art open LLMs, to provide a clearer understanding of performance improvements. Lastly, addressing the philosophical implications of using LLMs with inherent privacy concerns in the conclusion would enhance the depth of the discussion.