ID: MEByW1upLk
Title: Learning from Mistakes via Cooperative Study Assistant for Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework, SALAM, designed to enhance the reasoning capabilities of large language models (LLMs) by providing feedback based on previous mistakes. SALAM functions as a study assistant, grading responses against ground truth and collecting common errors during training to guide LLMs in avoiding similar mistakes during inference. The authors empirically validate SALAM's effectiveness through comprehensive experiments on the BBH and BBQ datasets, demonstrating performance improvements across multiple tasks.

### Strengths and Weaknesses
Strengths:
1. The framework introduces an innovative approach to improving LLM performance by systematically learning from mistakes.
2. The paper is well-structured, with clear mathematical formulations and a thorough analysis supported by ablation studies.
3. The experimental results provide solid evidence for the proposed method's effectiveness, and the authors share code and plans to release the feedback dataset.

Weaknesses:
1. The description of section 3.4, **Imitation Learning for Study Assistant**, lacks clarity regarding the sampling of trajectories and the selection of actions.
2. The experimental setup is insufficiently detailed, making replication challenging; specific comparisons with state-of-the-art methods like Self-Refine are missing.
3. The framework's reliance on high-quality ground truth labels may limit its applicability in domains with noisy or subjective outputs.

### Suggestions for Improvement
1. We recommend that the authors improve the clarity of section 3.4 by explicitly detailing how trajectories are sampled and how actions are selected.
2. The authors should compare SALAM against established baselines, such as Self-Refine and Self-Correct, to contextualize its effectiveness.
3. We suggest including experiments with larger LLMs, such as GPT-4, to evaluate the framework's performance and potential benefits.
4. The authors should provide more details about the experimental setup in the appendix to facilitate reproducibility.
5. We recommend addressing the computational overhead of imitation learning by exploring different study assistant sizes and their impacts.