ID: 232VcN8tSx
Title: GREATS: Online Selection of High-Quality Data for LLM Training in Every Iteration
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 7, 7, 7, 5, -1, -1, -1
Original Confidences: 4, 4, 3, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GREedy Approximation Taylor Selection (GREATS), a novel online batch selection algorithm designed to enhance training efficiency for large language models (LLMs). The algorithm aims to improve training convergence speed and generalization performance by selecting informative and diverse examples for model updates. GREATS formulates the online batch selection problem as set function optimization and employs Taylor expansions to approximate the utility of data points. The authors conduct extensive experiments across various LLM tasks, demonstrating GREATS's effectiveness in improving training performance.

### Strengths and Weaknesses
Strengths:
- GREATS is grounded in a principled set function optimization framework, directly optimizing model performance on a held-out validation set, contrasting with heuristic-based methods.
- The use of Taylor expansions for approximating marginal gains of data points significantly reduces computational complexity, making GREATS practical for large-scale LLM training.
- The innovative "ghost inner-product" technique addresses a major computational bottleneck in online batch selection.

Weaknesses:
- The evaluation is limited, as GREATS only compares against simple baselines (MaxLoss, GradNorm) and lacks comparisons with other state-of-the-art online batch selection methods.
- The absence of confidence intervals in tables makes it difficult to assess the significance of accuracy gains.
- The paper does not adequately address the sensitivity of GREATS to hyperparameters like learning rate and batch size, nor does it clarify the runtime complexity of the algorithm.
- The reliance on a small set of clean validation data may not be practical in real-world scenarios, and the approximation accuracy of Taylor expansions could be affected by model non-linearity.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including comparisons against more robust baselines, such as submodular subset selection techniques and reference-model-based methods. Additionally, incorporating confidence intervals in the results would enhance the clarity of the findings. We suggest providing a systematic study of hyperparameter sensitivities and clarifying the runtime complexity of GREATS. Finally, addressing the potential limitations of using a small set of clean validation data and discussing the implications of approximation accuracy would strengthen the paper's contributions.