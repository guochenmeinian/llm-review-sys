ID: fhQIsEbvlN
Title: Embarrassingly Simple Dataset Distillation
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3

Aggregated Review:
### Key Points
This paper presents a new backpropagation algorithm, "Random Truncated Backpropagation Through Time" (RaT-BPTT), aimed at enhancing the dataset distillation process. The authors claim to achieve state-of-the-art results in dataset distillation by addressing the complex nature of backpropagation and its substantial optimization and memory demands. They introduce randomization and truncation in BPTT, which is framed as a bilevel optimization problem, and utilize a window mechanism to reduce the storage requirements for meta gradients. The method is evaluated on datasets such as CIFAR-10, CIFAR-100, CUB200, and Tiny ImageNet.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel method that improves inner loop optimization in dataset distillation.
2. It demonstrates performance improvements over previous methods.
3. The method is robustly tested across multiple datasets, indicating its effectiveness.

Weaknesses:
1. The paper does not evaluate the proposed optimization method on larger networks, such as ResNets or Vision Transformers, which are crucial for assessing long-term dependencies.
2. There is insufficient discussion on the method's applicability across different domains, such as vision and NLP, and its scalability for larger networks.
3. The paper lacks a comprehensive review of related work, particularly regarding other approaches that also formulate dataset distillation as a bilevel optimization problem.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of their method by testing it on larger networks, such as ResNets and Vision Transformers, to better assess its performance on long-term dependencies. Additionally, the authors should discuss the applicability of their method in various domains, including vision and NLP, and how it can scale for larger networks. Furthermore, we suggest that the authors enhance the related work section to include discussions on other relevant papers that address similar problems in dataset distillation. Finally, we recommend providing more details in the setup section to ensure full reproducibility, including missing hyper-parameters like the degree of rotation and amount of flipping.