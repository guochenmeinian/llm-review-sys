ID: 7j6xgGj5lF
Title: Initializing Variable-sized Vision Transformers from Learngene with Learnable Transformation
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 5, 4, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Learnable Transformations (LeTra) aimed at enhancing learngene-based model initialization. The authors propose learning width and depth transformations to generate weight matrices of varying dimensions. An auxiliary model is distilled from a large model during the learngene learning stage, allowing for the optimization of both the learngene and the transformations. The resulting variable-sized models serve as effective starting points for fine-tuning. Experiments on image classification datasets, including ImageNet and CIFAR with the Vision Transformer (ViT) architecture, demonstrate the method's efficacy.

### Strengths and Weaknesses
Strengths:
- The application of both depth and width transformations to learngene is intuitive and shows the benefits of their combination.
- Empirical results indicate that LeTra provides strong model initialization weights, significantly outperforming prior methods like TLEG and Grad-LG.

Weaknesses:
- The presentation of the proposed approach lacks clarity, particularly in Section 3, where the core ideas and notations could be simplified. Section 3.3 is vague, lacking specifics on "step-wise selection" and the differences between selection methods. Additionally, Figure 3's caption could be more detailed.
- The transformations learned during Stage 1 cannot extrapolate to create models larger than the Aux-Net, limiting application scenarios.
- The training costs of Stage 1 should be included in comparisons with baselines to provide a clearer context for LeTra's efficiency.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 3 by simplifying notations and explicitly stating the core ideas behind the transformations. Additionally, clarifying the selection process in Section 3.3 and providing more detail in Figure 3's caption would enhance understanding. To address the limitation of not extrapolating beyond Aux-Net, consider exploring methods to combine LeTra with other Learngene strategies. Including training costs in comparisons with baselines would also provide a more comprehensive evaluation of LeTra's efficiency. Lastly, we encourage the authors to investigate the potential of applying LeTra to more complex downstream tasks, such as semantic segmentation or object detection.