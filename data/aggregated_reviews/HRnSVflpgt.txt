ID: HRnSVflpgt
Title: Schur Nets: exploiting local structure for equivariance in higher order graph neural networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Schur layers, a novel architecture for higher-order message passing neural networks (MPNNs) that enhances expressive power by leveraging spectral graph theory. Schur layers address the limitations of traditional GNNs, which struggle with complex local graph structures due to full permutation equivariance. The authors demonstrate that Schur layers can compute basis functions directly from the graph Laplacian, achieving state-of-the-art results on the ZINC dataset. Additionally, the paper analyzes the expressivity gap between group theoretical and spectral approaches in graph representation, proposing that their method, utilizing eigendecomposition, simplifies the process compared to previous complex methods involving irreducible representations. The authors acknowledge the importance of the [Babai et al., 1982] paper in potentially bounding the expressivity gap and plan to include a discussion in the Appendix.

### Strengths and Weaknesses
Strengths:
- **(S1 - Significance)** Schur layers provide a general architecture applicable to various higher-order networks, potentially improving many existing models.
- **(S2 - Performance)** The method consistently outperforms other approaches on the ZINC dataset.
- **(S3 - Novelty)** The approach is novel, rooted in established mathematical principles.
- **(S4 - Clarity)** Sections 1 and 2 are well-written, particularly the introduction of MPNNs and equivariance.
- The authors effectively simplify complex concepts using eigendecomposition, enhancing accessibility.
- They demonstrate a willingness to improve the paper based on feedback, indicating a collaborative approach to research.

Weaknesses:
- **(W1 - Clarity)** Theorems lack intuitive explanations, making theoretical contributions unclear.
- **(W2 - Significance)** The theoretical contributions appear limited, though verification is hindered by clarity issues.
- **(W3)** The absence of code impedes reproducibility.
- **(W4 - Experiments)** Experiments are limited to a single dataset (ZINC12k), risking overfitting, and the performance improvements are often marginal. The authors only test Schur layers with one model ($P$-tensors).
- Sections 3 and 4 may be too dense for readers unfamiliar with group theoretic approaches, potentially hindering comprehension.
- The initial presentation of Lemma 1 and Corollary 1 may appear overly simplistic, which could detract from the perceived depth of the analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical sections by providing intuitive explanations for the theorems and focusing on essential results. Additionally, we suggest reworking Sections 3 and 4 to enhance readability, possibly by adding more background information in the Appendix and removing unnecessary details from the main text. Expanding the experimental evaluation to include diverse datasets and baseline models would enhance the paper's applicability. Including a description of how Linamps (from [Maron et al., 2019]) function, along with illustrative examples, would further aid understanding. Finally, we encourage the authors to make the code publicly available to facilitate reproducibility.