ID: E8vGACczsQ
Title: (Out-of-context) Meta-learning in Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 6, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into out-of-context meta-learning in large language models (LLMs), demonstrating through experiments that LLMs can internalize semantic content from reliable sources rather than merely imitating shallow co-occurrence statistics. The authors explore weak and strong internalization, examining how models store knowledge and the implicit gradient alignment bias of gradient-based methods. They address concerns regarding the informal use of the term "definition" and clarify their experimental results, asserting that the internalization of identity statements is a significant finding. The implications for future AI systems, including potential risks associated with internalization, are discussed, along with a recognition of the need to moderate claims about the generality of their results based on the limited model evaluation.

### Strengths and Weaknesses
Strengths:
1. Innovative Methodology: The paper introduces out-of-context meta-learning in LLMs, supported by carefully designed synthetic experiments that provide valuable insights into LLMs' internalization processes.
2. Comprehensive Experimental Design: The experiments evaluate out-of-context meta-learning from multiple perspectives, considering various variables and defining tags and questions.
3. Novel Contribution: The exploration of internalization in language models enhances understanding of their synthesizing capabilities.
4. Responsiveness to Feedback: The authors have shown a willingness to refine their claims and improve clarity based on reviewer feedback.
5. Implications and Risks: The discussion on the implications of findings for future AI systems adds depth, emphasizing the need to understand and mitigate challenges posed by internalization.
6. Reproducibility: Detailed information about the experimental setup, including hyperparameters and performance metrics, enhances reproducibility.

Weaknesses:
1. Complexity: The paper is difficult to penetrate, with unclear definitions and intentions behind certain phrases, particularly regarding weak and strong internalization.
2. Confusing Annotations: Inconsistencies in entity representation and the introduction of definitions after their usage hinder comprehension.
3. Lack of Clarity in Results Interpretation: Ambiguities in the description of experimental results and the lack of conclusions regarding pretraining and internalization complicate understanding.
4. Title Misalignment: The title does not accurately reflect the focus on LLMs and the exploration of the phenomenon in computer vision models.
5. Limited Model Evaluation: The reliance on only two classes of models (Pythia and T5) raises concerns about the generalizability of the results.
6. Informal Language: The informal language used in some claims may lead to confusion and misinterpretation of the findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of definitions, particularly regarding weak and strong internalization, to enhance reader understanding. It would be beneficial to clarify the annotations used in the experiments and ensure consistency in entity representation. Additionally, we suggest providing a more thorough interpretation of experimental results, including explicit conclusions about pretraining and internalization. The title should be revised to accurately reflect the scope of the work, focusing on LLMs. We also recommend that the authors moderate their claims about the generality of results throughout the paper, specifically changing phrases like "Our results indicate that internalization is a general property" to "might be a general property." Finally, we encourage the authors to explore additional models, such as LLaMa and T5Flan, to strengthen their analysis and address concerns about the robustness of their findings.