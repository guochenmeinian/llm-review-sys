ID: Hz6cSigMyU
Title: Reinforcing LLM Agents via Policy Optimization with Action Decomposition
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Policy Optimization with Action Decomposition (POAD), a method for optimizing language agents by operating at the token level rather than the action level. The authors derive a theoretical framework called Bellman backup with Action Decomposition (BAD) to reconcile discrepancies between action-level and token-level optimization. By integrating BAD with the PPO algorithm, POAD facilitates finer-grained credit assignment and reduces optimization complexity. Empirical results across various environments demonstrate POAD's enhanced learning efficiency and generalization compared to baseline methods.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel token-level action decomposition, providing finer-grained supervision and addressing optimization complexity.
- The theoretical foundation of BAD ensures consistency and fills gaps in prior research.
- Empirical validations across diverse testbeds illustrate POAD's effectiveness and efficiency compared to baseline methods.

Weaknesses:
- The paper lacks a comprehensive comparison with significant pioneering works in the field, such as LLM decoding and hierarchical reinforcement learning.
- There is insufficient analysis in the experimental section, particularly regarding case studies that demonstrate token-level credit assignment.
- The finetuning approach is limited to using Lora, leaving the effects of full-capacity finetuning unexplored.

### Suggestions for Improvement
We recommend that the authors improve the comparison and analysis of their method by including a thorough examination of important prior works, particularly in hierarchical reinforcement learning. Additionally, conducting an ablation study to validate the intuition presented in Figure 2 is essential; specifically, demonstrating that the Q-value changes significantly for key tokens in sentences like "turn on/off TV." Furthermore, we suggest comparing POAD with ArCHer to substantiate the advantages of token-level Bellman updates. Lastly, exploring the effects of full-capacity finetuning would provide valuable insights into the method's performance.