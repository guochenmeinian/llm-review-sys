ID: FExX8pMrdT
Title: AutoSurvey: Large Language Models Can Automatically Write Surveys
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 8, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AutoSurvey, a framework that utilizes Large Language Models (LLMs) to automate the writing of scientific literature surveys. The process involves four main steps: retrieval of relevant papers, outline generation, subsection drafting, and evaluation. The authors compare AutoSurvey's performance against human-written surveys and naive RAG-based LLMs, focusing on metrics such as speed, citation quality, and content quality. Empirical results indicate that AutoSurvey is faster and achieves better citation and content quality in certain contexts. The authors also conduct ablation studies to assess the framework's robustness and sensitivity to its components.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written, with clear scopes that are easy to follow.
2. Solid experimental design, including valuable ablation studies.
3. The application of LLMs for literature surveys is an interesting and novel use case.

Weaknesses:
1. The framework primarily relies on the Claude-3-Haiku model, which may bias findings since other models are only used for evaluation.
2. The evaluation of citation quality uses an NLI model without detailed performance reporting.
3. The database for retrieval is not disclosed, and its focus on computer science may limit applicability to other fields.
4. The evaluation metrics depend heavily on LLMs, which may not capture the nuanced quality of human-written surveys.

### Suggestions for Improvement
We recommend that the authors improve the diversity of models used in the framework to mitigate potential bias. Additionally, please provide detailed performance metrics for the NLI model used in citation quality evaluation. Clarifying the retrieval database and considering publications from various domains could enhance the framework's applicability. We also suggest incorporating more advanced baseline techniques for comparison to strengthen the experimental analysis. Lastly, a more thorough examination of the evaluation criteria and the nuances of human surveys would be beneficial.