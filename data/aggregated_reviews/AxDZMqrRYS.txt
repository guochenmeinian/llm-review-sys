ID: AxDZMqrRYS
Title: DriveLM: Driving with Graph Visual Question Answering
Conference: thecvf
Year: 2024
Number of Reviews: 1
Original Ratings: 8
Original Confidences: 4

Aggregated Review:
**Key Points:**

This work presents a new framework that integrates vision-language models into end-to-end autonomous driving systems through a task called Graph Visual Question Answering (GVQA). This task simulates human decision-making in driving using interconnected question-answer pairs across various levels, including perception, prediction, planning, behavior, and motion.

**Strengths and Weaknesses:**

We find the introduction of GVQA as an innovative task for autonomous driving systems to be a significant strength. The graph decomposition enhances explainability within the context of end-to-end systems, and the DriveLM-Data offers rich annotations, providing a solid foundation for training and validation. Additionally, the proposed framework demonstrates improved zero-shot generalization across different sensor configurations. However, we note that the framework is single-framed, while many state-of-the-art methods on NuScene datasets, such as UniAD, typically perform better with multi-frame approaches that utilize historical information. The complexity of the graph-based VQA may increase exponentially in a multi-frame context, posing challenges for scalability to video-based frameworks. Furthermore, we observe that in the experimental setting of Table 1, the Chain and Graph versions of DriveLM-Agent do not show any advantage over no context, raising questions about the paper's explanation for this outcome.

**Suggestions for Improvement:**

We suggest including a discussion on how the proposed method differs from existing LLM-powered driving models, such as DriveGPT4 and GPT-Driver.