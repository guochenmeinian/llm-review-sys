ID: 8JMexYVcXB
Title: DAC-DETR: Divide the Attention Layers and Conquer
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 7, 4, 7, 7, -1, -1
Original Confidences: 5, 4, 4, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents DAC-DETR, a method designed to enhance the training efficacy of DEtection Transformers (DETR) by addressing the conflicting impacts of cross-attention and self-attention layers in the DETR decoder. The authors propose dividing the cross-attention layers into an auxiliary decoder that focuses on learning cross-attention, while the original decoder manages non-duplicate detection. By implementing one-to-many label assignment in the auxiliary decoder, DAC-DETR significantly improves query gathering, leading to enhanced detection accuracy compared to existing DETR models.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized, clearly presenting its ideas and findings.
- The motivation for the proposed methods is clear, and they are logical and easy to implement.
- Comprehensive experiments support the effectiveness of the proposed methods, demonstrating significant improvements over existing DETR models.
- The findings about the competition between self-attention and cross-attention are novel and provide valuable insights.

Weaknesses:
- The design appears somewhat complex, raising concerns about its integration with other DETR-like models.
- The originality of the work is questioned, as it combines existing methods without sufficient innovation.
- The paper lacks a thorough analysis of the contradictions between cross-attention and self-attention.
- Clarity issues exist, including grammatical errors and organizational weaknesses.

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of the paper, addressing grammatical errors and enhancing the overall presentation. Additionally, providing a more detailed analysis of the contradictions between cross-attention and self-attention would strengthen the theoretical foundation of the work. We suggest including qualitative examples to illustrate the differences between the proposed approach and the baselines. Furthermore, the authors should clarify the applicability of the "gather â†” disperse" effect beyond the examples provided and explore potential combinations with other techniques, such as stable matching and memory fusion, to further enhance performance.