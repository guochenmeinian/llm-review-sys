ID: uvFDaeFR9X
Title: Exploring Jacobian Inexactness in Second-Order Methods for Variational Inequalities: Lower Bounds, Optimal Algorithms and Quasi-Newton Approximations
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 8, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents second-order methods for variational inequalities that utilize inexact Jacobian information, addressing the limitations of existing methods that require exact derivatives, which can be costly for large problems. The authors analyze the effect of inexact derivatives on convergence rates, establishing a lower bound that depends on the level of inexactness, and propose an optimal algorithm that achieves this bound. The paper also introduces several inexact schemes based on quasi-Newton approximations that attain global sublinear convergence rates.

### Strengths and Weaknesses
Strengths:  
1. The paper is the first to systematically investigate the impact of inexact Jacobian information on the global convergence rate of second-order methods for variational inequalities, demonstrating the precise influence of inexactness on convergence speed.  
2. In the case where F is smooth and monotone, the authors provide a lower bound on the convergence rate that explicitly depends on inexactness, along with an algorithm that achieves this bound, indicating a tight analysis.  
3. The authors develop practical inexact second-order schemes achieving global sublinear convergence based on quasi-Newton methods.  
4. The theoretical findings are supported by numerical simulations and comparisons with existing methods.  
5. The paper is clear and well-written.  

Weaknesses:  
The primary weakness is the focus on deterministic inaccuracy in the Jacobian, while most practical machine learning algorithms utilize stochastic derivative information. This raises questions about the relevance of the algorithms and analysis to real-world settings. Additionally, there is a lack of discussion on related works, particularly regarding inexact Jacobians in convex optimization and quasi-Newton methods for saddle point problems.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the relevance of their algorithms when only stochastic Jacobian information is available, particularly regarding convergence up to the noise level. Additionally, we suggest including a table in the experimental results to compare the proposed methods' performance in terms of wall-clock time against existing methods like Perseus. Furthermore, we encourage the authors to enhance the related work section by discussing the relationship with existing literature on inexact Jacobians in convex optimization and addressing the technical difficulties compared to optimal second-order methods for solving variational inequalities.