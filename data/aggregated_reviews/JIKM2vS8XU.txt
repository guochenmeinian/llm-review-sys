ID: JIKM2vS8XU
Title: DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 30
Original Ratings: 5, 4, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called DatasetDM that utilizes diffusion models to generate synthetic data and corresponding annotations for various perception tasks. The core component, the P-Decoder, generates perception annotations by extracting feature maps and cross-attention maps from the pre-trained Stable Diffusion UNet during training. The generation process involves random sampling from prompts provided by GPT-4 to enhance diversity, followed by image generation through the VAE decoder of Stable Diffusion and annotation generation via the P-Decoder. The authors clarify that their work is distinct from VPD, which focuses on applying pre-trained Stable Diffusion for perception models, and emphasize that their architecture is optimized for open-vocabulary segmentation, pose estimation, and depth estimation. They provide comparative results demonstrating their method's effectiveness against existing semi-supervised approaches, highlighting significant improvements in mIoU metrics across various datasets. Additionally, the paper utilizes COCO's 118K unlabeled images, leveraging a model trained on Pascal to generate pseudo-labels, aiming to enhance performance through a combination of real and synthetic data.

### Strengths and Weaknesses
Strengths:
- The paper proposes a novel approach to leveraging visual representations from pre-trained Stable Diffusion for multiple perception tasks.
- The methodology is well-organized and clearly illustrated, addressing a relevant problem in the field.
- The framework is adaptable to various tasks and demonstrates competitive performance with limited labeled data.
- The authors provide a clear distinction between their work and VPD, emphasizing unique architectural and methodological differences.
- Comparative results show substantial improvements in performance metrics, particularly in semantic segmentation tasks.
- The integration of synthetic data augmentation demonstrates potential for enhancing perception tasks and supports a diverse range of tasks beyond semantic segmentation.

Weaknesses:
- The methodology lacks novelty, closely resembling the VPD paper, particularly in extracting visual representations and employing task-specific decoders.
- Experimental validation is insufficient, lacking ablation studies on the number of synthetic images and comparisons with more state-of-the-art (SOTA) methods.
- The paper does not adequately clarify the correspondence between generated images and annotations or the rationale for requiring less than 1% of manually labeled images for training the P-Decoder.
- Limitations of the proposed method are not explicitly discussed.
- The paper lacks thorough comparisons with other semi-supervised methods, which could provide a more comprehensive evaluation of its effectiveness.
- Some reviewers noted that the dataset choices may not be optimal for the proposed methods, potentially limiting the generalizability of the results.
- Some reviewers express skepticism regarding the efficacy of synthetic data compared to real unlabeled images and raise concerns about the potential domain gap when applying synthetic data to specialized fields like medical imaging.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by providing a detailed comparison with the VPD paper, highlighting differences and similarities in methodology. Additionally, conducting ablation studies to evaluate the contribution of cross-attention maps and multi-scale feature maps is essential. The authors should include more SOTA methods as baselines for a comprehensive evaluation and clarify how the correspondence between generated images and annotations is maintained. Furthermore, we suggest discussing the limitations of the method in the main paper rather than in supplementary materials. We also recommend improving comparisons with state-of-the-art semi-supervised segmentation methods, particularly those that align closely with their labeled image count. Considering the potential impact of using real unlabeled datasets, such as COCO, to enhance the model's performance without relying solely on synthetic data would be beneficial. Clarifying the training image count in relation to validation images would strengthen the paper's clarity and accuracy. Lastly, addressing the concerns about domain gaps in specific applications, such as medical images, and providing more robust evidence to support claims about the superiority of synthetic data in various contexts would enhance the overall argument for the utility of synthetic data.