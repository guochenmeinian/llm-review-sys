ID: z2739hYuR3
Title: Provably Efficient Reinforcement Learning with Multinomial Logit Function Approximation
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 5, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of Markov Decision Processes (MDPs) utilizing multinomial logit (MNL) function approximation for transition probabilities, building on the work of Hwang and Oh [2023]. The authors propose efficient algorithms inspired by online Newton steps and local learning with mirror descent, achieving improved computational efficiency and a reduced dependency on the parameter $\kappa$. The algorithms demonstrate a regret bound of $\tilde{O}(\kappa^{-1} dH^2\sqrt{K})$, with enhancements that detach $\kappa$ from the leading term.

### Strengths and Weaknesses
Strengths:
- The algorithms are computationally efficient and improve $\kappa$ dependency compared to previous work.
- The paper is well-structured, with clear logic and useful tables demonstrating advancements.
- The establishment of a lower bound on $d$ and $K$ confirms the optimality of the proposed algorithms.

Weaknesses:
- The novelty of the algorithms is limited, as they are based on existing methods for logistic or MNL bandits, particularly the online Newton update.
- The improvement on $\kappa$ relies heavily on previously established mirror descent algorithms, with proofs closely following earlier works.
- The MNL model's requirement for a finite number of achievable states for each $(k,n)$ may pose inherent limitations, and the necessity of knowing the state space $S_{k,n}$ is a concern.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the novelty of their algorithms, particularly in relation to existing methods in the literature. Additionally, including experiments on synthetic and real-world datasets would enhance the paper's practical relevance. It would be beneficial to address the implications of the support of the next state distribution in the analysis and consider providing examples that illustrate the assumptions in more relatable contexts. Finally, we suggest incorporating a discussion on sample complexity to further enrich the paper's contributions.