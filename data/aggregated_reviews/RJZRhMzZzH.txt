ID: RJZRhMzZzH
Title: A Careful Examination of Large Language Model Performance on Grade School Arithmetic
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 8, 9, -1, -1
Original Confidences: 4, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the data contamination of large language models (LLMs) through the creation of a new dataset, GSM1k, designed to assess potential overfitting on the existing GSM8k benchmark. GSM1k replicates the style and complexity of GSM8k while avoiding data leakage. The study reveals significant accuracy drops for leading LLMs on GSM1k, indicating systematic overfitting in some model families, while frontier models show minimal signs of such issues. The paper emphasizes the necessity for robust benchmarks to accurately evaluate LLM reasoning capabilities.

### Strengths and Weaknesses
Strengths:
- The novel benchmark design of GSM1k serves as a valuable control dataset for evaluating LLM reasoning abilities and identifying overfitting.
- Rigorous dataset construction ensures GSM1k's similarity to GSM8k, allowing for fair comparisons.
- The paper provides a nuanced analysis of results, identifying patterns of overfitting and exploring implications for LLM development.

Weaknesses:
- Generalizability concerns arise as findings are primarily focused on GSM8k and GSM1k, leaving unclear how they apply to other reasoning tasks.
- The limited scope of rephrasing techniques focuses mainly on costly human annotation, neglecting other potential methods like paraphrasing and translation.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by addressing other potential benchmarks or datasets where overfitting might occur. Additionally, we suggest discussing the implications of training LLMs on extensive rephrased samples of GSM8k, particularly regarding the necessity of benchmark decontamination with GSM1k. Furthermore, consider providing a testing interface for the research community to facilitate evaluation on GSM1k, and imposing a limit on submissions to mitigate overfitting risks. Lastly, introducing an estimate of time and cost involved in building the benchmark could enhance transparency.