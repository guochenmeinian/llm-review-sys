ID: Isy7gl1Hqc
Title: Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 4, 6, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel attack in machine unlearning, specifically a camouflaged data poisoning attack. The attack involves injecting poisoned and camouflaged samples into the training data, which allows the model to misclassify a target sample after the camouflaged samples are removed. The authors conduct experiments demonstrating the effectiveness of their approach across various model architectures and datasets.

### Strengths and Weaknesses
Strengths:
- The authors propose a novel attack in machine unlearning.
- Evaluations are conducted on label flipping and gradient matching for camouflaged data generation, utilizing multiple model types (SVM and neural networks) and datasets.
- The paper is well-structured and easy to follow.

Weaknesses:
- The paper makes impractical assumptions regarding the unlearning process and the effectiveness of the initial training to identify poisoning effects.
- Some claims, such as the characterization of the attack as "clean-label," are questionable given the nature of the poisoned samples.
- The experiments are limited to simpler datasets, and the performance on more complex datasets like ImageNet is not adequately addressed.
- The attack's effectiveness varies significantly across model architectures, and the rationale behind this variation is not discussed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their assumptions, particularly regarding the retraining process and its implications for identifying poisoning effects. Additionally, we suggest that the authors provide a more robust justification for the "clean-label" classification of their attack. Expanding the experimental evaluation to include more complex datasets, such as CIFAR-100 and ImageNet, would enhance the paper's contribution. Furthermore, we encourage the authors to explore the performance of their attack on multiple test points and provide insights into the variability of attack effectiveness across different model architectures. Finally, a discussion on the implications of their findings for real-world applications of machine unlearning should be included.