ID: N8TTwaIBId
Title: CCEval: A Representative Evaluation Benchmark for the Chinese-centric Multilingual Machine Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Chinese-centric multilingual machine translation (MMT) evaluation benchmark dataset called CCEval, consisting of 2,500 Chinese source sentences translated into 11 target languages. The authors propose methods for selecting a diverse set of Chinese sentences based on linguistic features such as topic, word frequency, grammar complexity, and translation difficulty. The dataset spans various domains and includes quality control measures to ensure high translation quality. Experiments indicate that their translation difficulty metric effectively correlates with human assessments for system ranking.

### Strengths and Weaknesses
Strengths:
- CCEval fills a significant gap as the first open benchmark tailored for Chinese-centric MMT, providing valuable resources for the research community.
- The paper outlines sensible methods for constructing a high-quality, diverse dataset, with empirical evidence supporting the effectiveness of the difficulty-based sample selection.
- The release of the dataset and an online evaluation platform enhances accessibility for researchers.

Weaknesses:
- The dataset's scale (2,500 sentences) is relatively small for comprehensive MMT evaluation, and expanding it would be beneficial.
- The authors did not utilize the evaluation benchmark to test a machine translation model, which limits its practical application.
- The empirical verification of the difficulty metric's effectiveness is indirect, lacking sufficient evidence from CCEval itself.
- The paper does not provide data examples or detailed information about data annotation, including the number of annotators and their qualifications.

### Suggestions for Improvement
We recommend that the authors improve the dataset's scale by expanding the number of sentences and target languages. It is essential to validate the effectiveness of this benchmark on actual machine translation models and provide sample data as evidence. Additionally, please include more detailed annotation information, such as the number of annotators, their educational backgrounds, and relevant details. More analysis characterizing the dataset's diversity and its comparability to existing benchmarks should be provided, clarifying its representativeness for intended use cases. Furthermore, addressing the linguistic diversity of target sentences and discussing the rationale behind the removal of long sentences would enhance the paper's clarity. Lastly, we suggest correcting grammatical errors throughout the article.