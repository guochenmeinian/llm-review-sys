ID: GcZgo9ffGt
Title: Instruction Tuning With Loss Over Instructions
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 6, 7, 7, 7
Original Confidences: 5, 4, 4, 4

Aggregated Review:
### Key Points
This paper presents a novel method called Instruction Modelling (IM) for training language models, which applies a loss function to both the instruction and output components of training data, rather than solely to the output. The authors demonstrate that IM enhances model performance on various NLP tasks and open-ended generation benchmarks compared to standard Instruction Tuning (IT). Two critical factors influencing IM's effectiveness are identified: the ratio of instruction length to output length in the training data and the quantity of training examples. The authors suggest that IM's improvements may arise from reduced overfitting during instruction tuning.

### Strengths and Weaknesses
Strengths:
- The paper challenges the conventional approach to instruction tuning by proposing that loss be applied to both instructions and outputs, offering a fresh perspective on a widely-used technique.
- Extensive experiments across diverse benchmarks showcase the broad applicability and effectiveness of IM.
- The analysis of key factors affecting IM's performance provides valuable insights for practitioners, particularly in low-resource scenarios.

Weaknesses:
- Previous works have proposed similar ideas and conclusions.
- The paper lacks a robust theoretical foundation explaining why applying loss to instructions is effective; while the authors hypothesize that IM reduces overfitting, a more rigorous analysis is needed.
- The experiments are limited to LLaMA-2 and OPT models, leaving unexplored how IM performs across a wider range of model architectures or sizes.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of IM to elucidate why applying loss to instructions is effective. Additionally, providing clear definitions and examples of what constitutes an "instruction" would enhance clarity. The authors should also consider expanding their experiments to include a broader variety of model architectures, particularly larger models (e.g., 34B or 70B), to assess the generalizability of their findings. Furthermore, an analysis of the quality of the instruction part and its impact on model performance should be included, as well as a discussion of scenarios where IM may underperform compared to baseline methods.