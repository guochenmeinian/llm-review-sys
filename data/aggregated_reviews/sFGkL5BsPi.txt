ID: sFGkL5BsPi
Title: Q-DM: An Efficient Low-bit Quantized Diffusion Model
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 4, 7, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a quantization-aware training framework for diffusion models, addressing activation distribution oscillation and quantization error accumulation as primary performance detractors. The authors propose two methods: Timestep-aware Quantization (TaQ) for data normalization and Noise-estimating Mimicking (NeM) for error reduction. Experimental results indicate that Q-DMs achieve performance comparable to full-precision models.

### Strengths and Weaknesses
Strengths:
1. The motivation behind the paper is valid, and the proposed methods TaQ and NeM are reasonable solutions to the identified problems.
2. The paper is well-organized, with fluent writing that aids reader comprehension, and detailed experimental setups that facilitate result replication.
3. Experimental results are promising, showing that Q-DMs perform on par with full-precision models.

Weaknesses:
1. The novelty of TaQ and NeM is questioned, as they appear similar to existing normalization and knowledge distillation techniques.
2. There is a lack of numerical descriptions for the quantization-aware training (QAT) process, including training time, calibration dataset size, and epochs.
3. The models and datasets used for performance evaluation are somewhat outdated, necessitating more comprehensive experimental results with recent models and datasets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of technical details, particularly regarding the meaning of "PTQ4DM" in Table 2 and the statistical mean and variance calculations during training. Additionally, we suggest providing more information on the inference process, including how TaQ affects inference speed and the calculation of "distance" in Figure 4. It would also be beneficial to include comparisons with recent literature on normalization techniques and to conduct experiments on larger models and datasets to strengthen the evaluation of Q-DM's performance. Finally, addressing the limitations and potential societal impacts of the proposed methods would enhance the paper's depth.