ID: IGn0ktYDwV
Title: SAMPa: Sharpness-aware Minimization Parallelized
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a parallelized variant of Sharpness Aware Minimization (SAM) called SAMPa, which aims to enhance optimization efficiency by parallelizing the computation of two gradient calculations required in SAM. The authors introduce an auxiliary sequence to approximate the gradient for perturbation, allowing for simultaneous gradient calculations. Theoretical analysis guarantees convergence for convex problems, and empirical results demonstrate that SAMPa performs comparably or better than SAM across various benchmark tasks.

### Strengths and Weaknesses
Strengths:
- The methodology of breaking the sequential dependence of SAM gradients is novel.
- The proposed approach integrates well with different SAM variants and shows improved test performance on downstream tasks.
- The writing is clear, and the theoretical analysis supports the proposed method.

Weaknesses:
- The auxiliary sequence's approximate error needs a bound for theoretical support; without it, the method lacks robustness.
- The theoretical analysis assumes convex loss, which is often not the case in practical applications, rendering some results less meaningful.
- The runtime comparison may be unfair due to the use of two GPUs for SAMPa without a similar setup for SAM.
- More discussion on memory consumption, communication efficiency, and the ability to overlap computation with communication in SAMPa is necessary.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework by establishing a bound for the approximate error $\nabla f(x_t, B_{t+1}) - \nabla f(y_{t+1}, B_{t+1})$ to strengthen the method's theoretical foundation. Additionally, the authors should consider discussing the implications of non-convex losses more thoroughly, as well as providing a fair runtime comparison by reporting data for SAM using two GPUs. Further exploration of memory consumption and communication efficiency in SAMPa, along with a detailed analysis of the reasons behind SAMPa's performance relative to SAM, would enhance the paper's depth.