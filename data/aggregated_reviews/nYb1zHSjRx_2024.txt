ID: nYb1zHSjRx
Title: Do LLMs estimate uncertainty well in instruction-following?
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 5, 7
Original Confidences: 4, 4

Aggregated Review:
### Key Points
This paper presents a systematic evaluation of uncertainty estimation methods in instruction-following tasks, addressing a significant gap in existing research. The authors propose the IFEval dataset, which includes Controlled and Realistic versions of LLM-generated outputs, to facilitate a holistic evaluation of LLM performance. The paper emphasizes the importance of evaluating instruction-following capabilities beyond mere response accuracy, highlighting challenges such as task execution quality and the severity of mistakes.

### Strengths and Weaknesses
Strengths:
- The authors conducted a comprehensive literature review, effectively illustrating the need for improved evaluation of instruction-following capabilities in LLMs.
- The depth of analysis is commendable, with diverse metrics identified for evaluating uncertainty estimation across various characteristics.
- The IFEval dataset's design, incorporating Controlled-Easy, Controlled-Hard, and Realistic outputs, enhances its applicability for users selecting LLMs based on specific use cases.

Weaknesses:
- The choice of LLMs is questionable; the inclusion of more recent models, such as Llama 3.1, would have been preferable.
- The paper lacks a comparative analysis of the strengths and weaknesses of the baseline methods, leaving readers without clear guidance on model suitability.
- The abstract is unclear, and the motivation for using AUROC is not adequately explained, leading to confusion about its application in this context.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the abstract by explicitly distinguishing between Instruction Following and Question Answering, providing more experimental detail, and summarizing the differences between the benchmark versions. Additionally, we suggest including a comparison of the relative strengths and weaknesses of the baseline methods to guide readers in model selection. Clarifying the use of AUROC and addressing the choice of LLMs by incorporating frontier models would enhance the paper's rigor. Lastly, we encourage the authors to consider a comparative study across different domains to determine if LLMs perform better at following instructions in specific areas.