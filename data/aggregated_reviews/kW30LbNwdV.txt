ID: kW30LbNwdV
Title: Improving Adversarial Robust Fairness via Anti-Bias Soft Label  Distillation
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 5, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two methods aimed at enhancing fairness and robustness in machine learning models: Anti-Bias Soft Label Distillation (ABSLD) and a comparative analysis of re-temperating versus re-weighting methods. ABSLD focuses on adjusting the smoothness degree of soft labels during knowledge distillation, proposing that sharper soft labels for harder classes and smoother ones for easier classes can reduce the error risk gap between classes. Experiments on CIFAR-10 and CIFAR-100 demonstrate that ABSLD outperforms existing methods in achieving both robustness and fairness, although it reveals a trade-off between worst-class performance and overall robustness. In contrast, the authors argue that re-temperating fundamentally resolves fairness issues, while re-weighting merely alleviates them. They provide experimental evidence showing that re-temperating reduces overfitting in easy classes and underfitting in hard classes, thereby enhancing model performance, and explore the theoretical underpinnings of their approach, emphasizing the role of soft labels in improving information quality for model learning.

### Strengths and Weaknesses
Strengths:  
1. The ABSLD method is innovative, focusing on the smoothness degree of soft labels, which distinguishes it from existing re-weighting approaches.  
2. The paper includes a theoretical analysis supporting both ABSLD and re-temperating, enhancing the validity of its claims.  
3. Extensive experiments across various datasets demonstrate the effectiveness of ABSLD and provide rigorous comparisons between re-temperating and re-weighting methods.

Weaknesses:  
1. The relationship between ABSLD and re-weighting is unclear; the authors should clarify how ABSLD fundamentally differs from re-weighting and why it achieves better performance.  
2. The overall robustness degrades despite improvements in worst-class performance, raising concerns about the trade-off.  
3. The proof of theorem 1 lacks clarity, particularly regarding the mathematical relationship between gradients and error risk.  
4. Implementation details, such as hyperparameter choices, need more thorough discussion to aid replication and practical understanding.  
5. The conclusion regarding the superiority of re-temperating over re-weighting lacks a detailed analysis of the underlying mechanisms driving these benefits.  
6. There is ambiguity in the derivation of results related to Taylor's Formula, which may affect the clarity of the conclusions drawn.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the differences between ABSLD and re-weighting, providing rigorous theoretical analysis to substantiate the advantages of their approach. Additionally, the authors should address the trade-off between overall robustness and worst-class performance more comprehensively. Clarifying the proof of theorem 1, particularly the mathematical relationship between gradients and error risk, is essential. A more detailed discussion on hyperparameter choices and their implications for implementation would enhance the paper's practical relevance. Furthermore, we suggest that the authors provide a more rigorous examination of why re-temperating is superior to re-weighting, including a clearer explanation of the mechanisms involved, and clarify the derivation related to Taylor's Formula to ensure that the conclusions are robust and well-supported.