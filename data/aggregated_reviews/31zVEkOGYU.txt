ID: 31zVEkOGYU
Title: Enemy is Inside: Alleviating VAE's Overestimation in Unsupervised OOD Detection
Conference: NeurIPS
Year: 2023
Number of Reviews: 26
Original Ratings: 5, 4, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a mathematical examination of unsupervised out-of-distribution (OOD) detection performance using Variational Autoencoders (VAEs), focusing on the overestimation issue in the evidence lower bound (ELBO). The authors decompose the expected ELBO into the entropy $\mathcal{H}(x)$ of a dataset and the KL divergence $D_{KL}(q(z)||p(z))$ between the estimated latent variable $z$ and the prior. They theoretically demonstrate that the data distribution's entropy is self-defined, which may not benefit OOD detection. To address the overestimation problem, the authors propose the AVOID method, which utilizes a post-hoc prior to improve calibration and OOD detection performance. Additionally, they introduce a dataset entropy calibration (DEC) method, clarifying the structure of their findings regarding the design of priors in VAEs. The paper includes a comprehensive experimental setup that demonstrates the effectiveness of the proposed methods across various OOD detection scenarios.

### Strengths and Weaknesses
Strengths:
1. The mathematical decomposition of the ELBO is crucial for understanding the benefits and drawbacks of using ELBO as an OOD score.
2. The demonstrations highlight the mismatch between prior and post-hoc prior, effectively inspiring the proposed method.
3. The paper provides a clear framework for understanding the overestimation issue in VAEs and proposes methods to address it, including AVOID and DEC.
4. The experiments are well-designed and executed, showing that AVOID consistently outperforms baselines and supporting the analysis with comprehensive results.

Weaknesses:
1. Notation inconsistencies, such as $p$ and $p_\theta$ in Figure 3, detract from clarity.
2. The motivation for Eq. 8 is not well-founded, as it focuses on the ID distribution's diversity rather than the overall OOD distribution.
3. The use of a 3-layer neural network for $q_\phi$ and $p_\theta$ raises questions about the adequacy of training samples and network capacity in estimating $p_\theta(x)$.
4. Some citations are missing, which detracts from the paper's credibility.
5. The connection between image complexity and dataset entropy requires further clarification, particularly how the DEC method derives from this relationship.

### Suggestions for Improvement
We recommend that the authors improve the consistency of notation throughout the paper, particularly in Figure 3. Additionally, we suggest clarifying the motivation behind Eq. 8 by considering the overall OOD distribution rather than solely the ID distribution. It would be beneficial to explore whether increasing the number of training samples or enhancing the neural network's capacity could mitigate the overestimation issue. We also encourage the authors to provide clearer explanations regarding the design of the calibration term and its relationship to entropy and complexity. Furthermore, we recommend improving the organization of section 3.2 to clarify the pipeline and findings regarding the design of priors, ensuring that all relevant citations are included to support their claims. Including detailed error bars for as many methods as possible and discussing the limitations of the class condition as future work would strengthen the statistical validity of their results. Finally, we suggest that the authors compare their method against more recent and stronger baselines in the supervised category and address the model mis-specification of VAEs in their discussion.