ID: TzxSrNJE0T
Title: Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 4, 7, 7, 7, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the convergence rate of general adaptive stochastic approximation methods with biased gradients, which is significant due to the prevalence of biased gradients in practical machine learning problems. The authors establish non-asymptotic bounds under various assumptions, some of which are notably strong. The theoretical results include convergence guarantees for adaptive methods applied to non-convex smooth functions, specifically addressing popular algorithms like Adagrad, RMSProp, and AMSGrad.

### Strengths and Weaknesses
Strengths:  
- The authors provide a convergence analysis for biased adaptive optimization methods and apply their findings to well-known algorithms, demonstrating the relevance of their work to practical machine learning problems.  
- The paper is well-organized, clearly written, and effectively presents the motivation and contributions compared to previous works.  
- The numerical experiments conducted are comprehensive and support the theoretical claims made in the paper.  

Weaknesses:  
- The technique lacks novelty compared to existing works on biased SGD, as the authors impose upper and lower bounds on the preconditioning matrix, which does not significantly differ from traditional SGD analysis.  
- The claim of analyzing Adam throughout the paper is misleading, as only AMSGrad is addressed, which could confuse readers due to the distinct performances of these algorithms.  
- The role of parameters related to sampling noise, such as batch size, in theorems is unclear, raising questions about their impact on training and generalization.  
- The bounded spectrum assumption of the preconditioning matrix may not hold for all optimizers, and the limitations of assumptions H1-5 are not adequately discussed.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their approach by establishing the boundedness of the preconditioning matrix along the trajectory, as suggested in existing literature. Additionally, we suggest correcting the misleading claim regarding the analysis of Adam and AMSGrad to clarify their differences. It would be beneficial to address the role of sampling noise parameters in theorems and to provide a clearer discussion on the limitations of assumptions H1-5. Lastly, we encourage the authors to explore the boundedness of gradients along the trajectory in future work.