ID: mkSDXjX6EM
Title: FIND: A Function Description Benchmark for Evaluating Interpretability Methods
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 8, 4, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FIND, a framework designed to evaluate the capability of large language models (LLMs) in providing transparent explanations for black-box models. The framework categorizes black-box models into four types: numeric, strings, entities, and relations, utilizing atomic functions to create complex functions through various operations. The LLM is tasked with querying the model to understand its behavior and subsequently generating domain-specific explanations. The paper introduces metrics for evaluating these explanations, such as MSE for numeric functions. Additionally, it presents a benchmark suite for evaluating the efficacy of different interpretation systems on functions resembling sub-computations in larger neural networks. The authors propose using a fine-tuned open-source LM evaluator, vicuna-evaluator, to replace GPT-4, addressing reproducibility challenges due to model deprecation. The revised paper includes additional benchmarking of Llama-2-13b-chat and Llama-2-70b-chat, revealing that these models often hallucinate outputs rather than accurately interpreting function outputs. The authors demonstrate that vicuna-evaluator achieves higher accuracy than GPT-4 and GPT-3.5 in matching function descriptions to inputs and outputs. Furthermore, the paper enhances synthetic neural modules that operate at the sentence level, implementing syntactic operations such as dependency relations and parts of speech identification, focusing attention on specific tokens within input sequences.

### Strengths and Weaknesses
**Strengths:**
- The framework offers a novel approach to explaining black-box models using LLMs, filling a gap in existing literature.
- The paper is well-written, clear, and easy to follow, with comprehensive documentation.
- It presents a robust benchmark dataset of over 2,000 functions, facilitating the evaluation of LLMs in experimental planning and function interpretation.
- The effective benchmarking of additional models enhances the robustness of the evaluation.
- The use of vicuna-evaluator shows a strong correlation with human judgments, validating its role as a surrogate for human evaluations.
- The incorporation of additional synthetic neural modules and clear explanations of dependency relations and attention mechanisms demonstrate the authors' responsiveness to feedback.

**Weaknesses:**
- The prompting for LLMs requires precise descriptions, which may limit usability.
- The evaluation lacks a strong baseline for comparison, particularly against human performance, which is essential for contextualizing results.
- The complexity and diversity of functions could be expanded to better reflect real-world AI systems.
- The evaluation framework primarily relies on NMSE and distractor-based unit testing, which may not capture all nuances of function interpretation.
- The potential for hallucination in Llama-2 models raises concerns about their reliability in practical applications.
- The current framework may not adequately address more complex functions, raising questions about its broader applicability.

### Suggestions for Improvement
We recommend that the authors improve the baseline comparisons by including human performance metrics to provide a reference point for LLM capabilities. Additionally, the evaluation methods should incorporate more complex assessments beyond MSE and NMSE to better capture the nuances of LLM explanations and interpreter performance. Expanding the range of evaluated LLMs, such as Vicuna and Alpaca, and enhancing the diversity and creativity of functions represented in the benchmark would strengthen the framework and its relevance to real-world scenarios. Finally, we suggest adding a detailed description of the FIND API in the appendix to clarify its functionality and providing more detailed examples of how the proposed functions operate in various contexts to clarify their utility and effectiveness.