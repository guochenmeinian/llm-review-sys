ID: YwpL0BVxts
Title: United We Stand, Divided We Fall: Fingerprinting Deep Neural Networks via Adversarial Trajectories
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 7, 6, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ADV-TRA, a robust model fingerprinting method for protecting the intellectual property (IP) of deep neural networks (DNNs) using adversarial trajectories. Unlike traditional single-point fingerprinting, ADV-TRA generates a series of fixed-length trajectory samples that capture a comprehensive representation of the decision boundary, enhancing resistance to removal attacks. The authors introduce an adaptive step-size algorithm for trajectory generation, which improves precision in capturing decision boundary changes. The verification process involves two thresholds: the first threshold \(Thr_{\textit{mut}}=0.5\) assesses trajectory matching with the source model, while the second evaluates potential infringement based on detection rates. The Fingerprint Detection Rate is derived from surface trajectories \(T_{\textit{surf}}\), contrasting with baseline methods that utilize samples. The T1%F metric is defined as the True Positive Rate at a 1% False Positive Rate, emphasizing the use of non-threshold-based metrics like T1%F, T10%F, and AUC to ensure fair comparisons. Extensive experiments validate ADV-TRA's effectiveness in distinguishing between infringing and non-infringing models with a low false positive rate, although the authors acknowledge a lack of clarity regarding metric definitions and the second threshold in their algorithm.

### Strengths and Weaknesses
Strengths:  
1. **Robustness Against Attacks**: ADV-TRA shows superior resistance to removal attacks, crucial for practical deployment against sophisticated threats.  
2. **Low False Positive Rate**: The method achieves a low false positive rate through a comprehensive analysis of the model's decision boundary, reducing misidentification of legitimate models.  
3. **Comprehensive Verification Framework**: The paper introduces clear definitions of thresholds and metrics, allowing for unbiased performance evaluation.  
4. **Extensive Validation**: The paper conducts thorough experiments across multiple datasets, providing strong empirical evidence of ADV-TRA's effectiveness.  
5. **Commitment to Improvement**: The authors demonstrate a commitment to addressing reviewer feedback and improving clarity in future versions.  

Weaknesses:  
1. **Complexity and Computational Costs**: The generation and verification of adversarial trajectories can be computationally intensive, potentially limiting scalability in resource-constrained environments.  
2. **Limited Applicability**: The proposed method may not apply to models with different tasks and classes, raising concerns about robustness in transfer learning scenarios.  
3. **Lack of Theoretical Guarantees**: The method lacks theoretical backing, particularly regarding the false positive rate, which is only empirically measured.  
4. **Ambiguity in Metric Definitions**: There is ambiguity in the definitions of certain metrics, particularly T1%F and the second threshold, which may lead to confusion.  
5. **Reliance on Specific Thresholds**: The reliance on specific thresholds for performance evaluation could be seen as a limitation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions and metrics used in the paper, particularly regarding the Fingerprint Detection Rate, T1%F, and the second threshold. Additionally, the authors should address the computational costs associated with ADV-TRA and provide a more detailed comparison with state-of-the-art model watermarking techniques. We also suggest that the authors clarify the robustness of ADV-TRA across various neural network architectures and discuss the implications of potential fingerprint collisions. Furthermore, consider integrating discussions on the robustness of verification methods against removal attacks and the practical implications of training multiple surrogate models, as highlighted in the reviews.