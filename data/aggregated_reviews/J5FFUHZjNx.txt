ID: J5FFUHZjNx
Title: SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SteerLM, a supervised fine-tuning method that enables users to control language model responses based on explicitly defined attributes, addressing limitations of reinforcement learning from human feedback (RLHF). The authors demonstrate that SteerLM outperforms state-of-the-art RLHF-trained models on the Vicuna benchmark, while being easier to train. The methodology is well-structured, with comprehensive evaluations including automatic and human assessments.

### Strengths and Weaknesses
Strengths:
- The proposed SteerLM offers a novel and user-controlled alternative to RLHF for aligning language models.
- The methodology is clearly explained, and the paper includes detailed experimental evaluations demonstrating effectiveness.
- The approach shows strong performance, surpassing existing models on the Vicuna benchmark.

Weaknesses:
- More details are needed regarding the base model's performance, compute costs, and public access.
- The selection of 7 out of 13 attributes in OASST requires further justification.
- The method's novelty is questioned, necessitating a discussion on its differentiation from similar works like RAFT and RRHF.
- Challenges in collecting alignment data and the need for additional evaluations on toxicity and hallucinations are noted.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the base model's performance and provide details on compute costs and public access. Justifying the choice of attributes in OASST is essential. Additionally, we suggest discussing how SteerLM differentiates from RAFT and RRHF. To enhance the paper, include more evaluations focusing on toxicity and hallucinations, and provide examples related to these aspects. Lastly, addressing the data collection costs for attribute-conditioned SFT compared to RLHF would be beneficial.