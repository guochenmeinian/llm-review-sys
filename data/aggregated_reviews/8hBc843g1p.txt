ID: 8hBc843g1p
Title: Improved Generation of Adversarial Examples Against Safety-aligned LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 7, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents enhancements to gradient-guided LLM attack algorithms through two modifications: skipping skip connections in transformers during gradient propagation and adjusting the optimization objective to align latent representations with those from preliminary attacks. The authors also study and improve white-box suffix-based language model jailbreaking by treating the gradient-based discrete optimization problem as a continuous surrogate model, drawing parallels with transfer-based image adversarial attacks. Furthermore, the paper evaluates generating universal adversarial suffixes using 200 standard behaviors from HarmBench, addressing previous limitations by incorporating all behaviors during generation. The authors report performance metrics, including average ASR (AASR), best ASR (BASR), and worst ASR (WASR) across various models, demonstrating that their method outperforms the GCG attack. Ongoing experiments on Phi3-Mini-Instruct and combinations with PEZ and AutoDAN are acknowledged, with results pending for the updated version.

### Strengths and Weaknesses
Strengths:  
- The insights of skipping skip connections and modifying the optimization objective are clever and stem from a critical analysis of past literature.  
- The authors provide comprehensive performance metrics (AASR, BASR, WASR) across multiple models, showcasing the effectiveness of their method.  
- The incorporation of all behaviors in generating adversarial suffixes enhances the robustness of the results.  
- The experimental results show substantial improvements in attack success rates and time efficiency, with the proposed methods outperforming the baseline GCG method.  
- Figures and tables, particularly Figures 2, 3, 4, and Table 1, provide compelling evidence of the methods' effectiveness.  

Weaknesses:  
- The writing quality is poor, with vague and unexplained phrases that hinder understanding, particularly in the abstract and early sections.  
- The paper lacks qualitative analysis, specifically side-by-side comparisons of attacks in Figure 8.  
- Empirical evaluations are insufficient, with missing error bars in several figures and a need for more diverse model evaluations.  
- The experiments on Phi3-Mini-Instruct are incomplete, limiting the current findings.  
- The potential bias in evaluations between AdvBench and HarmBench is not fully addressed, particularly regarding the universality of adversarial suffixes.  
- There is no discussion on the impact of the $\beta$ hyperparameter for (LSGM-)LILA$^\dagger$, and the paper does not adequately address continuous attacks or recent works on circuit breaking and refusal directions.

### Suggestions for Improvement
We recommend that the authors improve the clarity and conciseness of the writing, starting with the abstract, to enhance reader comprehension. Additionally, including qualitative analysis of attacks in Figure 8 would provide valuable insights. We suggest conducting further experiments with diverse LLM architectures to validate the generalizability of the proposed methods. It would also be beneficial to discuss the impact of the $\beta$ hyperparameter and to include error bars in all relevant figures. Furthermore, we recommend emphasizing the experimental settings for the Top-$k$ of 4 and a candidate set size of 20 in the updated version of the paper. Finally, we encourage the authors to explore the relationship between their methods and recent studies on refusal directions and circuit breaking in the related work section, as well as including the results of attacking closed models in the revision to provide a more comprehensive evaluation.