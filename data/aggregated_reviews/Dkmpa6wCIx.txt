ID: Dkmpa6wCIx
Title: Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 8, 5, 5, 7, -1, -1, -1
Original Confidences: 4, 4, 2, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of the relationship between generalization, flatness, and the explicit induction of flatness via Sharpness-Aware Minimization (SAM). The authors identify three regimes based on architectural properties of two-layer ReLU networks: (1) all flat minimizers generalize well and are found by SAM, (2) some flat minimizers generalize poorly and are not found by SAM, and (3) some flat minimizers generalize poorly but are found by SAM. The theoretical existence of flat solutions that generalize or memorize is established, while the effects of SAM are demonstrated empirically.

### Strengths and Weaknesses
Strengths:
1. The questions formalized in the paper are novel and well-formulated.
2. The authors provide a clearer understanding of flatness, generalization, and explicitly inducing flatness, handled with care and rigor.
3. The introduction is well-written, with clear explanations of the arguments.
4. The authors utilize a minimal setting of a 2-layer non-linear model, supporting their arguments with formal and empirical results.
5. The finding that flattest solutions may not generalize well is a surprising theoretical insight.

Weaknesses:
1. The model setup is considered too simplistic, lacking noise and broader applicability.
2. The experiments on SAM lack clarity regarding implementation, hyper-parameters, and initialization.
3. The authors do not adequately compare their results with existing literature, particularly regarding noisy setups.
4. Some statements in the paper are unclear or misleading, particularly regarding generalization guarantees.
5. The paper contains grammatical and formatting issues that detract from clarity.

### Suggestions for Improvement
We recommend that the authors improve the model setup to include noise and explore broader applicability to enhance generalization claims. Additionally, we suggest providing more detailed explanations of SAM implementation, hyper-parameters, and initialization to strengthen the experimental section. It would be beneficial to include comparisons with relevant literature, particularly regarding noisy setups, to contextualize the findings. Clarifying ambiguous statements and ensuring consistency in terminology and formatting throughout the paper will enhance readability. Lastly, we encourage the authors to consider visualizations to aid in understanding the SAM trajectories and their implications.