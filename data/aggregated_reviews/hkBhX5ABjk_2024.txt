ID: hkBhX5ABjk
Title: Multi-Agent Domain Calibration with a Handful of Offline Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 5, -1, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, -1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Madoc, a domain transfer method that calibrates a source domain using a small amount of offline data from the target domain via cooperative Multi-Agent Reinforcement Learning (MARL). The authors propose this approach to adjust physics parameters in the source domain to better align with those in the target domain, demonstrating its effectiveness on D4RL and NeoRL benchmark tasks. However, the empirical analysis raises concerns regarding the claims that Madoc reduces the dynamics gap and outperforms existing methods.

### Strengths and Weaknesses
Strengths:
- Strong empirical performance, with significant improvements over baselines on D4RL and NeoRL benchmarks.
- The motivation for the method is clearly articulated, addressing a critical research topic in domain calibration.

Weaknesses:
- The paper is difficult to follow, particularly in sections 4.1 and 4.2, where explanations and derivations are insufficient or incorrect.
- Weak empirical results, as confidence regions for Madoc overlap with those of other baselines, raising doubts about its effectiveness in reducing the dynamics gap.
- The complexity of the method compared to offline RL methods may affect the fairness of comparisons.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper, especially in sections 4.1 and 4.2, by providing more detailed explanations and correcting any inaccuracies in the derivations. Additionally, the authors should include additional experiments to explicitly demonstrate that Madoc reduces the dynamics gap, such as reporting the difference in physics parameters or KL divergence between source and target domains. It would also be beneficial to discuss the trade-off between the complexity of their method and performance, particularly in comparison to offline RL methods. Finally, consider exploring the use of simpler neural architectures to address optimization challenges, as the necessity of a multi-agent approach in small-sized experiments remains questionable.