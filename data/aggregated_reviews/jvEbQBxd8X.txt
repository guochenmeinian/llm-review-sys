ID: jvEbQBxd8X
Title: Improving Language Plasticity via Pretraining with Active Forgetting
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 6, 7, 6, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an active forgetting mechanism for pre-training PLMs, specifically by resetting the embedding layer every K updates, aimed at enhancing cross-lingual transfer and adaptation. The authors propose a multi-stage adaptation framework that separates language-specific and task-specific knowledge, demonstrating that this method leads to faster convergence and improved performance on downstream tasks across various languages, particularly distant ones from English.

### Strengths and Weaknesses
Strengths:
- The paper introduces a simple yet innovative approach that shows consistent improvement in cross-lingual transfer.
- It is well-written, with clear motivation and insightful analysis.
- The empirical findings are compelling, particularly the effectiveness of resetting embeddings during pre-training.

Weaknesses:
- The paper lacks comprehensive ablation studies to isolate the effects of resetting during pre-training from those of adapting parameters separately.
- Some figures are visually inconsistent, and the experimental setup does not adequately address the implications of using simulated low-resource data.
- The method's sensitivity to the choice of forgetting frequency remains unexplored, and the justification for the hyperparameter K=1000 is insufficient.

### Suggestions for Improvement
We recommend that the authors improve the visual quality of figures, particularly addressing axis scales and rounding inconsistencies. Additionally, the authors should conduct ablation experiments to clarify the contributions of different components of their method. It would be beneficial to explore the influence of script differences on transfer performance and to provide justification for the choice of forgetting frequency. Including loss curves during language adaptation would enhance understanding of the method's dynamics. Finally, consider investigating the application of active forgetting beyond the embedding layer to other model components.