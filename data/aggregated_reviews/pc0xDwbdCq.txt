ID: pc0xDwbdCq
Title: The Structural Safety Generalization Problem
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 8
Original Confidences: 4, 3

Aggregated Review:
### Key Points
This paper presents a novel perspective on AI vulnerabilities by proposing that AI models exhibit structural vulnerabilities alongside semantic ones, particularly through multi-turn and multi-image attacks. The authors frame structural safety generalization as a distinct class of AI vulnerability, highlighting its relevance across various AI systems, including language and vision models. The paper combines insights from adversarial attacks across different domains, illustrating how these vulnerabilities manifest in diverse AI contexts.

### Strengths and Weaknesses
Strengths:  
- The introduction of structural safety generalization offers a fresh viewpoint on AI adversarial robustness, expanding the focus beyond single-turn or single-modality vulnerabilities.  
- The experiments are well-executed, providing systematic evaluations of vulnerabilities in multi-turn and multi-modal attacks, with evidence of gaps in current AI modelsâ€™ safety generalization.

Weaknesses:  
- The core discussion primarily addresses general AI model vulnerabilities, with insufficient exploration of their relevance to generative models, particularly in the context of workshops like Safe Generative AI.  
- The framing of structural safety may be less suitable in multimodal contexts, as perturbations introduce variability that could align more with bijective attacks rather than structural generalization failures.  
- The nature of attacks on multimodal language models is relegated to Appendix N.2, which detracts from the main text's clarity.  
- The paper's findings on multi-turn and multi-modal jailbreaks overlap with concurrent research, although the introduction of new datasets remains a valuable contribution.

### Suggestions for Improvement
We recommend that the authors improve the articulation of the connection between structural vulnerabilities and generative AI applications, ensuring that the relevance to models like LLMs and vision-language models is clearly established. Additionally, we suggest that the authors reconsider the framing of structural safety in the context of multimodal attacks, potentially addressing the role of bijective attacks more explicitly. Furthermore, we advise that the discussion of multimodal language model attacks be integrated into the main text for better accessibility.