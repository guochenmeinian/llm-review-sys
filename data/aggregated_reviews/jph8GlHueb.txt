ID: jph8GlHueb
Title: MUX-PLMs: Data Multiplexing for High-throughput Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MUX-PLMs, a novel pre-trained language model designed to enhance throughput in multi-input multi-output scenarios through data multiplexing and demultiplexing. The authors introduce a multiplexing layer at the input and a demultiplexing layer at the output, allowing the model to process multiple inputs simultaneously. Experimental results indicate that MUX-PLMs, particularly under BERT and ELECTRA architectures, achieve comparable performance to traditional PLMs while significantly reducing computational overhead. However, validation is primarily limited to GLUE, NER, and POS tagging tasks.

### Strengths and Weaknesses
Strengths:
1. The introduction of MUX and DeMUX architectures addresses computational cost challenges in large language models.
2. Experimental results demonstrate that MUX-PLMs maintain performance levels comparable to traditional PLMs while reducing computational overhead.
3. The paper is well-structured and accessible, enhancing readability.
4. The method is applicable across different architectures and can be combined with other compression techniques.

Weaknesses:
1. The validation of model performance is restricted to a limited set of tasks, lacking exploration of more challenging benchmarks.
2. The Contextual MUX module does not outperform the original MUX module and is absent from primary experiments, raising questions about its utility.
3. MUX-PLMs do not perform competitively against state-of-the-art model compression methods like MobileBERT and TinyBERT.
4. Several ambiguities exist in the paper, particularly regarding the method's uniqueness compared to TMUX and the impact of different data within a batch.
5. Increased training costs and the need for pre-training different MUX-PLMs for various acceleration ratios may limit practical adoption.

### Suggestions for Improvement
We recommend that the authors improve the validation of MUX-PLMs by including a broader range of challenging tasks, such as SQuAD for question-answering. Additionally, conducting an ablation study for the pre-training steps would strengthen the experimental results. The authors should analyze and discuss the impact of different data within a batch in the multi-input multi-output setting. Clarifying the performance of the Contextual MUX module and addressing ambiguities regarding its effectiveness compared to the original MUX module is essential. Finally, a comparison of training computational times between MUX-PLM and standard PLM training should be provided to better assess the training costs involved.