ID: 5atraF1tbg
Title: PANORAMIA: Privacy Auditing of Machine Learning Models without Retraining
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 4, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for estimating a lower bound on the (pure) Differential Privacy parameter $\epsilon$ for trained machine learning models through a post-hoc empirical evaluation using Membership Inference Attacks (MIAs). The authors propose the PANORAMIA framework, which utilizes synthetic non-member samples generated from a trained generative model on a subset of the training data. This approach allows for auditing without altering the training process and aims to provide a valid lower bound on $\epsilon$, contingent on the synthetic data being sufficiently close to the true data distribution. The method is evaluated across various machine learning models and data types, demonstrating reasonable approximations of DP lower bounds. Additionally, the paper introduces a privacy auditing technique that leverages partial access to member data to generate synthetic non-member data, which is then used to train a meta-classifier for measuring privacy leakage related to record membership. The authors evaluate their technique on models as large as GPT2, finding a correlation between audit scores and expected leakage.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant problem in privacy auditing by allowing post-hoc evaluations without modifying the training process.
- The use of synthetic data that closely resembles the training data mitigates performance degradation while providing useful DP estimations.
- Empirical evaluations indicate that the proposed estimator correlates with DP upper bounds and identifies high privacy leakage scenarios.
- The inclusion of negative results is commendable.
- The proposed methods address the challenge of obtaining high-quality non-member data, which is often difficult in third-party scenarios.
- The paper is well-structured, and the experiments are comprehensive, covering various models and modalities.

Weaknesses:
- The correctness of the analysis in Proposition 2 is questionable, particularly regarding the independence of the model and the sampled Bernoulli random variables.
- The sweeping over recall values in Algorithm 1 may lead to inflated DP estimates, and the implications of using synthetic data that is $c$-close to the true distribution are not fully explored.
- The rationale for generating synthetic non-member data instead of using standard validation/test data is unclear.
- Contributions (1) and (2) have been previously explored, and the authors should clarify how their approach differs from existing methods.
- Concerns exist regarding the dependency on the effectiveness of the baseline distribution detection system, particularly in the evaluation of generated images.
- The accuracy of models in Table 1 indicates potential overfitting, especially when using the entire dataset.
- The utility of the audit could be better demonstrated by comparing models with similar test performance but differing leakage.
- The clarity of figures and their explanations is lacking, particularly regarding theoretical and empirical maximum values.
- The paper does not provide an operationalizable algorithm for $(\epsilon, \delta)$-DP, and the limitations of the method in providing a true lower bound for $\epsilon$ are not sufficiently discussed.
- The justification for accessing partial member data lacks convincing support, particularly in the context of federated learning.

### Suggestions for Improvement
We recommend that the authors improve the clarity and correctness of Proposition 2 by addressing the independence of the model and the sampled variables. Additionally, we suggest revisiting the methodology in Algorithm 1 to ensure that the $p$-value adjustments are accurately reflected in the experiments. The authors should also provide a more thorough discussion on the implications of using synthetic data that is $c$-close to the true distribution, particularly regarding the potential impact on MIA performance. Furthermore, we encourage the authors to enhance the presentation of figures and clarify the definitions and implications of the privacy semantics used in the paper. We recommend improving the clarity of the benefits of generating synthetic non-member data compared to using standard validation/test data and highlighting the differences between their contributions and existing literature more explicitly. It would be beneficial to address the concerns regarding the baseline distribution detection system's effectiveness and provide a more robust evaluation of model performance to mitigate overfitting. The authors should also consider demonstrating the utility of their audit by comparing models with comparable test performance but varying leakage. Lastly, we suggest providing a more convincing justification for the assumption of accessing partial member data, particularly in relation to federated learning scenarios.