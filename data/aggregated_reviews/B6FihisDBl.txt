ID: B6FihisDBl
Title: Universal Gradient Descent Ascent Method for Nonconvex-Nonconcave Minimax Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Doubly Smoothed Gradient Descent Ascent (DSGDA) method, which addresses nonconvex-nonconcave and nonconvex-concave minimax problems. The authors claim that DSGDA effectively eliminates limiting cycles and achieves convergence under a one-sided Kurdyka-≈Åojasiewicz condition, with complexity results matching the best-known outcomes for similar algorithms. Furthermore, the authors introduce a universal version of DSGDA that achieves a suboptimal rate of $\mathcal{O}(\epsilon^{-4})$ across NC-C, NC-KL, C-NC, and KL-NC minimax problems. They propose a formal corollary and additional theorems that provide symmetric counterparts to previously presented results, including iteration complexity and proximal error bounds, while clarifying that the adjustments made do not alter the scientific content.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel algorithm that empirically converges to stationary points of difficult minimax problems, outperforming existing methods.
- The theoretical analysis aligns with the best-known results for single-loop algorithms, and the writing quality is high, making the paper easy to follow.
- The authors effectively address the universality of the method, maintaining a suboptimal rate while providing detailed formal statements.
- The inclusion of MATLAB code to visualize feasible parameter pairs enhances the practical applicability of the results.

Weaknesses:
- The theoretical guarantees of DSGDA do not demonstrate superiority over existing methods like Smoothed GDA or GDA-AM, raising questions about its effectiveness in general minimax problems.
- The convergence results are based on one-sided KL conditions, which may limit the algorithm's applicability.
- The choice of hyperparameters for convergence is complex, making it challenging for users to implement the method effectively.
- The paper lacks formal statements and proofs for certain settings, particularly for C-NC/KL-NC, which reviewers found necessary for a comprehensive understanding.
- Some reviewers expressed that the paper requires major revisions and is not ready for publication in its current form.

### Suggestions for Improvement
We recommend that the authors improve the theoretical guarantees of DSGDA by demonstrating its superiority over existing methods in various settings. Additionally, the authors should clarify the differences between their work and prior contributions, such as GDA-AM and fast extragradient methods. To enhance the empirical validation, we suggest including comparisons with a broader range of algorithms beyond SGDA. Furthermore, we encourage the authors to elaborate on the relationship between the existing theory and the role of DSGDA in avoiding limiting cycles, as well as to simplify the hyperparameter selection process to facilitate user implementation. We also recommend improving the clarity and completeness of their formal statements, particularly for the C-NC/KL-NC settings, to ensure that all necessary proofs and details are included. Lastly, we suggest that the authors consider expanding their explanations regarding the implications of using symmetric versus asymmetric parameters to enhance the understanding of their results. We advise against using "nonconvex-nonconcave" in the title, opting instead for "nonconvex-KL" to better reflect the paper's focus.