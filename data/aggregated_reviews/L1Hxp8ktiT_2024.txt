ID: L1Hxp8ktiT
Title: Mix Data or Merge Models? Optimizing for Performance and Safety in Multilingual Contexts
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 7, 5
Original Confidences: 4, 3

Aggregated Review:
### Key Points
This paper presents an investigation into the safety of merging pretrained models, demonstrating that merging is preferable to data mixing. The authors conduct extensive experiments across six languages, showing improvements in safety and general performance in multilingual contexts. The paper includes a comprehensive overview of merging methods and their effectiveness compared to data mixing.

### Strengths and Weaknesses
Strengths:  
- The authors provide a thorough comparison of various language models and merging techniques.  
- Extensive experiments illustrate the effectiveness of model merging in enhancing safety and general performance.

Weaknesses:  
- The abstract lacks specific metrics for the reported quantitative improvements.  
- There is no new development of methodology; the authors should elaborate on the mathematical formulations of merging methods.  
- No single method consistently outperforms others in safety and general performance, leaving the effectiveness of merging methods unclear.  
- The inclusion of additional merging and ensembling methods would enhance the paper's depth.

### Suggestions for Improvement
We recommend that the authors improve the abstract by including specific metrics for the quantitative improvements. Additionally, we suggest moving the related work section to section 2 to better serve as background information. The authors should elaborate on the mathematical formulations of merging methods and consider including other merging and ensembling methods to provide a more comprehensive analysis.