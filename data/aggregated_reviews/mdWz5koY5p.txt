ID: mdWz5koY5p
Title: RGMDT: Return-Gap-Minimizing Decision Tree Extraction in Non-Euclidean Metric Space
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Return-Gap-Minimization Decision Tree (RGMDT) algorithm, which aims to extract interpretable decision tree policies from deep reinforcement learning (DRL) policies. The authors quantify the return gap between an oracle RL policy and its extracted decision tree policy, providing a theoretical guarantee for the performance of the DT policy. The RGMDT algorithm reformulates the DT extraction problem as an iterative non-Euclidean clustering problem and extends to multi-agent settings. Empirical results demonstrate that RGMDT outperforms other decision tree-based algorithms in D4RL tasks.

### Strengths and Weaknesses
Strengths:
- The methodology of using clustering to reformulate DT extraction is innovative.
- The paper provides a formal theoretical guarantee for the proposed algorithm.
- RGMDT is applicable in both single-agent and multi-agent contexts.
- Empirical results show significant performance improvements over selected baselines.

Weaknesses:
- The proposed method does not clearly enhance the interpretability of DRL policies, as it primarily showcases superior performance without addressing interpretability.
- The comparison with a baseline of training a simple decision tree on the RL policy's actions and observations is missing.
- There is a lack of ablation studies to assess the impact of non-Euclidean clustering errors and return gaps on performance.
- The clarity of presentation could be improved, with more intuitive illustrations and better organization of figures.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing and include more intuitive illustrations to enhance the paper's impact. Additionally, we suggest adding a section discussing existing works on interpretable RL, including relevant early studies. The authors should also consider including comparisons with a baseline of training a simple decision tree on the RL policy's actions and observations, as well as conducting ablation studies to evaluate the sensitivity of the algorithm to clustering errors and return gaps. Finally, addressing the computational complexity and scalability of RGMDT in real-world applications would strengthen the paper.