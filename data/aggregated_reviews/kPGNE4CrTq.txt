ID: kPGNE4CrTq
Title: Solving Sparse \& High-Dimensional-Output Regression via Compression
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Sparse & High-dimensional-Output REgression (SHORE) model to tackle challenges in Multi-Output Regression (MOR). The authors propose a two-stage framework that incorporates output compression to enhance computational efficiency while maintaining accuracy. Theoretical analysis demonstrates the framework's scalability and error bounds, and empirical results validate its performance in modern MOR applications.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, with a clear definition of the MOR problem and proposed methods aimed at achieving good performance.
- The two-stage algorithm is easy to understand and implement, effectively reducing the dimension of $Y$ and relaxing the RIP-type condition in sparsity-constrained optimization.
- The theoretical analysis is robust, providing comprehensive training loss bounds and convergence guarantees.

Weaknesses:
- The prediction stage diverges significantly from classical methods, involving sparsity-constrained optimization (SCO) that is more computationally intensive. More discussion on this difference is needed.
- The comparison methods used, such as Orthogonal Matching Pursuit and Elastic Net, are outdated; the authors should consider including more recent sparse regression methods.
- The paper lacks information on the sparsity level in synthetic datasets, which is crucial for evaluating model performance.
- Algorithm 1's steps 3-4 resemble iterative hard thresholding, which should be discussed appropriately.
- The selection of hyperparameter $\eta$ could benefit from default values and a more data-driven selection approach.
- Some sentences are incomplete, and the authors should revise them for clarity.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational intensity of the prediction stage compared to classical methods. Additionally, consider including comparisons with more recent sparse regression techniques to strengthen the contribution. It is essential to specify the sparsity level in synthetic datasets and to clarify the relationship between the hyperparameter $m$ and training time. Furthermore, we suggest revising incomplete sentences for clarity and discussing the resemblance of Algorithm 1's steps to iterative hard thresholding in the appropriate context. Lastly, providing default values for $\eta$ and exploring a data-driven selection method would enhance practical applicability.