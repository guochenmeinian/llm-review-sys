ID: qegD54EWAl
Title: Hiding in Plain Sight: Tweets with Hate Speech Masked by Homoglyphs
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Offensive Tweets with Homoglyphs (OTH) dataset, a benchmark for Hate Speech Detection obscured by homoglyphs in code-mixed language. The authors evaluated seven transformer-based models, observing significant performance improvements through data normalization (F1 scores from 0.64 to 0.74). The study highlights the feasibility of collecting a dataset with both known and unknown homoglyphs and demonstrates the efficacy of training neural models to identify camouflaged hate speech.

### Strengths and Weaknesses
Strengths:
- The introduction of the OTH dataset could significantly contribute to hate speech detection research.
- The paper includes compelling experiments and analyses, providing insights for future research directions.

Weaknesses:
- The rationale for selecting query terms derived from the 41 most offensive American English words is unclear.
- The claim regarding the absence of libraries for homoglyph conversion is inaccurate, as existing libraries like unidecode provide such functionality.
- The paper lacks detailed information on the dataset configurations and the decision-making process behind baseline models.
- The small size of the labeled dataset (200 tweets) raises concerns about the robustness of the findings.

### Suggestions for Improvement
We recommend that the authors clarify the rationale behind the selection of offensive query terms. Additionally, the authors should address the discrepancies regarding existing libraries for homoglyph conversion and provide a comparison with these resources. Detailed information about the dataset configurations and the annotation process should be included to enhance transparency and reproducibility. To demonstrate the dataset's utility, comparisons with popular datasets are necessary. Furthermore, the authors should explain their choice of baseline models to aid understanding of their benefits and limitations. Lastly, we suggest including annotation guidelines to assist other researchers in developing similar datasets for hate speech masked by homoglyphs in various languages.