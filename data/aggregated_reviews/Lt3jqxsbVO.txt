ID: Lt3jqxsbVO
Title: Sharp Spectral Rates for Koopman Operator Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 9, 6, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper develops bounds on the approximated Koopman modes and eigenvalues for two methods: Principal Component Regression (PCR) and Reduced Rank Regression (RRR). The authors find that PCR, which includes extended dynamic mode decomposition (EDMD), can exhibit larger bias than RRR due to poorly chosen kernels. They also introduce an empirical method for identifying spurious eigenvalues, which aids in model selection. The study focuses on the approximation and learning of the Koopman operator, emphasizing the significance of these bounds in the context of Markovian dynamical systems.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow, presenting new techniques for bounding approximated Koopman spectral objects.
2. The discovery that PCR can have larger bias than RRR is an important contribution to the field.
3. The introduction of an empirical method for identifying spurious eigenvalues is a valuable addition for model selection.
4. Numerical examples in Figs. 1-3 effectively illustrate the developed theory and support the claims made.

Weaknesses:
1. The paper lacks discussion on how it differs from previous works by Klus et al. (2016) and Korda and Mezic (2018), which proved the convergence of EDMD to the true Koopman operator. Clarification on this point is necessary.
2. Fig. 3 is confusing; a secondary panel explaining the best estimator's application to the validation data would enhance clarity.
3. Some technical details, such as the derivation of certain inequalities and the connection between DMD and KMD, require further elaboration.

### Suggestions for Improvement
We recommend that the authors improve the discussion on how their results reconcile with the findings of Klus et al. (2016) and Korda and Mezic (2018), particularly regarding the asymptotic limits of EDMD. Additionally, we suggest adding a secondary panel to Fig. 3 to clarify the results presented. It would also be beneficial to provide more detail on the derivation of inequalities mentioned in lines 154-155 and to discuss the connection between DMD and KMD more thoroughly. Lastly, we recommend ensuring consistent terminology regarding "non-linear" and "nonlinear" throughout the paper.