ID: nrgyOGU7ZP
Title: SS1: Accelerating Inference with Fast and Expressive Sketch Structured Transform
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Sketch Structured Transform (SS1), a randomized parameter-sharing method aimed at reducing computational complexity while maintaining model quality. SS1 effectively enhances tensor multiplication efficiency in deep learning models and demonstrates significant improvements in inference speed for models like GPT-2, BERT, and Llama-3-8B. The authors provide empirical evidence of SS1's performance compared to existing methods such as Monarch and LowRank, highlighting its compatibility with quantization and its applicability to pre-trained models.

### Strengths and Weaknesses
Strengths:
- SS1 introduces a novel approach to structured yet random parameter sharing, effectively reducing computational requirements while preserving model expressivity.
- Extensive experiments validate SS1's superior performance across various models and applications, particularly in inference throughput.
- The method is designed to be hardware-efficient, allowing for low runtime overhead and latency improvements.

Weaknesses:
- The experimental section lacks a comprehensive comparison between SS1 and existing RPS methods, particularly regarding latency and accuracy on larger models.
- The presentation of results is unclear, making it difficult to assess SS1's performance relative to alternatives as a function of latency.
- Writing quality needs improvement, especially in mathematical expressions and algorithms, which are sometimes confusing or poorly explained.

### Suggestions for Improvement
We recommend that the authors improve the presentation by including a baseline comparison between SS1 and existing RPS methods to clarify its advantages. Additionally, the authors should provide more details on the performance of SS1 with larger models and ensure that the experimental results clearly illustrate the latency and accuracy trade-offs. We suggest enhancing the clarity of mathematical expressions and algorithms, particularly by adding comments to important lines in Algorithms 1 and 2, and ensuring consistent notation throughout the paper. Finally, addressing the learning rate tuning for structured models could further validate the experimental results.