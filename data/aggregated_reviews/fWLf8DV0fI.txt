ID: fWLf8DV0fI
Title: Rethinking Tokenizer and Decoder in Masked Graph Modeling for Molecules
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 5, 5, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into tokenization and decoder strategies within masked graph modeling (MGM) for molecular representation learning. The authors propose the Simple GNN-based Tokenizer (SGT) and a combination of GINE and GraphTrans (GTS) as an efficient decoder. SGT employs non-trainable linear graph aggregation for graph tokenization and introduces a remask variant for GTS. Experimental results demonstrate that the proposed method outperforms several self-supervised learning (SSL) baselines across multiple molecular property benchmarks, supported by comprehensive ablation studies.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow.
2. The systematic study of tokenization and decoder contributes significantly to MGM for molecules, addressing a less-explored design space.
3. Section 4 includes thorough investigations of MGM design choices, validating the effectiveness of SGT and GTS.
4. Technical details and complete results are provided in supplementary material, showcasing experimental soundness.

Weaknesses:
1. A major weakness is the absence of quantum-mechanics-related benchmarks, which are crucial for evaluating the proposed method's effectiveness in practical applications.
2. The motivation for the proposed method lacks clarity, and the novelty appears limited, with the contributions being somewhat incremental.
3. The performance of the proposed tokenizer and decoder shows limited improvements, and comparisons with baselines may not be entirely fair due to differences in encoder architectures.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation and novelty of their work. Specifically, they should discuss the potential impact of incorporating nonlinearity into SGT and elaborate on the observed mismatch between graph reconstruction and molecular representation learning (MRL). Additionally, the authors should benchmark their method against quantum-mechanics-related datasets (e.g., QM9, MD17) to provide a more comprehensive evaluation. Clarifying the differences between SGT and previous pretrained GNN-based tokenizers, as well as including new baselines such as S2GAE and GraphMAE2, would enhance the paper's rigor. Finally, addressing the computational time comparisons and providing a theoretical analysis of the proposed method would strengthen the overall contribution.