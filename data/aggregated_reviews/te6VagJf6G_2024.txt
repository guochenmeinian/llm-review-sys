ID: te6VagJf6G
Title: Learning to Reason via Program Generation, Emulation, and Search
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 5, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a fine-tuned LLM, CoGEX, that reasons by generating pythonic code and executing it internally rather than using an external interpreter. The authors propose generating candidate programs from a training set and selecting the top-k performing programs for evaluation (CoTACS). The model demonstrates improved performance on various datasets compared to benchmark models without pretraining, utilizing in-context learning and chain-of-thought reasoning. Additional analyses on model efficacy, including training items and sampled programs, are provided.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow, with clear presentation of analyses and results.
2. The CoGEX model offers insights into how LLMs can benefit from program-like planning without external interpreters.
3. The flexibility of using ‘primitive’ functions enhances the model's capability to query factual information.

Weaknesses:
1. The novelty of the ideas is questionable, as generating code with LLMs and simulating program execution are not new concepts; CoTACS is merely a top-k search.
2. The advantages of this method over passing code to a Python interpreter are unclear, especially given that the dataset was created using GPT-4.
3. The results often show marginal improvements, and it is not evident when this approach is preferable. The use of 0-shot CoT as a comparison model is weak, necessitating a clearer understanding of CoGEX's advantages over simpler alternatives.

### Suggestions for Improvement
We recommend that the authors improve the paper by:
A. Providing a fair comparison with CoT, controlling for the number of examples or demonstrating at what point models achieve parity, as this would be straightforward for ML practitioners.
B. Including comparisons to actual interpreters (e.g., Python) and clarifying how often the model encounters undefined functions. If predefined functions are used, the authors should quantify the benefits of using undefined functions.
C. Expanding the Qualitative Analysis section (S 3.4) to elucidate when CoGEX significantly outperforms baselines, as the current results in Table 1 require extensive prior knowledge from readers.
Additionally, the authors should address the following questions and concerns:
1. Define BM25 in Table 1 and clarify benchmarking settings.
2. Explain the term "silver-quality" in Line 91.
3. Clarify whether the model generates meaningfully different programs in Table 3 and Figure 2.
4. Provide details on ‘fuzzy’ primitives and their definitions.
5. Report actual SD or 95% intervals in Line 187.
6. Include baselines for tasks in Figure 2 to assess the linear trends accurately.
7. Investigate instances where the generated program is incorrect but the model still arrives at the correct solution.