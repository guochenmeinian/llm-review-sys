ID: diItUQ1idA
Title: Abstractive Open Information Extraction
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a dataset, a model, and evaluation metrics for Abstractive Open Information Extraction (OIE) with synthetic relations. The authors create a dataset through data augmentation via back-translation and traditional relation extraction, and they propose a model by fine-tuning the T5 model on this corpus. Additionally, they introduce two semantic-based evaluation metrics using a BERT-based textual entailment model, comparing their approach with Multi2OIE across various datasets.

### Strengths and Weaknesses
Strengths:
- The focus on extracting synthetic relations in OIE is novel and underexplored in the literature.
- The paper is well-written, clearly explaining related works, motivation, and the proposed approach.
- The proposed evaluation metrics and data augmentation methods are significant contributions that could benefit the community.

Weaknesses:
- The justification for the data augmentation method is flawed, as it may not effectively capture implicit and non-lexicalised relations.
- The reliance on translation models introduces potential errors, and the dataset creation lacks clarity regarding noise control.
- The evaluation metrics conflate structural information, which could misrepresent the semantic content of extractions.
- The limited scope of experiments raises concerns about the generalizability of the results, and there is a lack of qualitative analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset creation process and address potential noise introduced by traditional relation extraction methods. Additionally, a thorough analysis should be conducted to demonstrate that the proposed model does not negatively impact the performance of explicit or un-inferred relations. We suggest expanding the experimental scope to include more downstream tasks and qualitative analyses to better understand the strengths and limitations of the model. Furthermore, the authors should explicitly state the limitations of their work being confined to the English language, as this may affect the applicability of their results to other languages and syntactic structures.