ID: IwnINorSZ5
Title: Conformal Meta-learners for Predictive Inference of Individual Treatment Effects
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 7, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a meta-learner-based approach for conformal inference of individual treatment effects (ITEs), utilizing plug-in pseudo outcomes as an inference proxy instead of predicting missing outcomes. The validity of ITE inference is established through an analysis of statistical dominance, allowing for model-free application of conformal inference, albeit with limited coverage guarantees. The authors introduce a framework that merges conformal prediction with meta-learners, characterized by distribution-free validity and coverage guarantees, supported by stochastic ordering techniques. The method shows promise in both synthetic and real-world datasets.

### Strengths and Weaknesses
Strengths:
- The paper makes a novel contribution to uncertainty quantification and predictive inference of ITEs, differing significantly from existing PO-prediction-based approaches.
- It is well-written, with clear articulation of challenges and contributions, making it enjoyable to read.
- The innovative use of stochastic ordering to validate pseudo outcomes in conformal prediction is noteworthy.
- The proposed method is technically sound, with theoretical proofs and empirical validation across diverse benchmarks.

Weaknesses:
- The analysis is limited to the known propensity case, which somewhat marginalizes the contribution since previous methods already achieve validity in this context.
- The experimental settings, particularly regarding synthetic datasets, lack clarity, leaving questions about the scenarios being tested.
- The benefits of the proposed method in experiments are unclear, particularly regarding its performance in cases with missing potential outcomes.
- The practical application of stochastic ordering is hindered by uncheckable assumptions, and the method can be overly conservative in its predictions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental settings, explicitly stating whether they are inferring ITEs in cases with both potential outcomes missing and providing details on the calibration data used. Additionally, we suggest that the authors address the implications of unknown propensity scores more robustly, discussing how the dominance condition holds under fitted propensities. Clarifying the relationship between the choice of nuisance models and the empirical calibration distribution would enhance the understanding of the method's robustness to covariate shifts. Lastly, we encourage the authors to provide guidance on the conditions under which their method performs optimally, particularly in relation to the data generating process and the characteristics of the distributions involved.