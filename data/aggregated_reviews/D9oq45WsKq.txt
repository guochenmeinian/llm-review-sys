ID: D9oq45WsKq
Title: Ensemble-Instruct: Instruction Tuning Data Generation with a Heterogeneous Mixture of LMs
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel methodology for generating instruction-tuning data using smaller LLMs instead of the traditionally used large models. The authors propose two key techniques: categorizing and simplifying ICL prompts, and ensembling outputs from multiple LLMs to enhance accuracy and diversity. The approach, termed Ensemble-Instruct, demonstrates that a smaller model (MPT-7B) can outperform a significantly larger model (GPT-3 at 175B) across various benchmarks. The methodology includes a greedy consensus algorithm for selecting high-quality samples based on inter-model ROUGE-L scores.

### Strengths and Weaknesses
Strengths:
- The methodology is simple and facilitates easy application, achieving notable performance with smaller models.
- Comprehensive evaluation across multiple settings using various permissive LLMs adds practical importance to the study.
- The paper is well-written and presents clear gains over the Self-Instruct method.

Weaknesses:
- The proposed methodology appears disconnected from its primary motivation regarding model size and openness; experiments with larger models would strengthen the argument.
- The analysis of categorization and simplification lacks depth, and a more thorough exploration would enhance understanding.
- Sole reliance on ROUGE-L as the primary performance metric raises concerns about over-optimization and potential biases in sample selection.

### Suggestions for Improvement
We recommend that the authors improve the depth of analysis regarding the effectiveness of categorization and simplification, particularly in section 3.2. Additionally, conducting experiments with larger models would better demonstrate the methodology's applicability. Including comparisons with other datasets, such as Alpaca, and qualitative assessments against other methods would provide a more comprehensive evaluation. We also suggest incorporating human evaluations or alternative AI assessments to complement the ROUGE-L metric. Lastly, clarifying the relationship between UL2 and NEOX, and addressing the floating tables and figures for better readability would enhance the paper's presentation.