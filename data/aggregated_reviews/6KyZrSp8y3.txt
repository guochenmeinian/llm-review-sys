ID: 6KyZrSp8y3
Title: Unnatural language processing: How do language models handle machine-generated prompts?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploratory analysis of how language models (LMs) process human-generated versus machine-generated prompts. The authors propose that machine-optimized prompts exhibit distinct processing behaviors, such as higher perplexity, lower activation overlap, and different attention distributions compared to human-readable prompts. While the study does not delve into the causes of these differences, it suggests that prompt optimization may exploit unique activation pathways within the network. The paper is well-written and contributes to the understanding of prompt engineering, although it is noted that the results feel preliminary.

### Strengths and Weaknesses
Strengths:
- Clearly written and well-structured analysis.
- Provides detailed insights into the differences in processing between prompt types.
- Timely topic that situates itself well within existing literature.
- Offers potentially fundamental insights into the activation pathways of LMs.

Weaknesses:
- Lacks a causal hypothesis, which may disappoint some readers.
- Relies on quantitative approaches that do not meet scientific standards for uncertainty quantification.
- Some analyses, particularly in Section 5.1, could benefit from more rigorous statistical testing.
- The assumption that better-calibrated models have lower output entropy is not universally valid.
- Presentation issues in Table 3 and mislabeling of "human" prompts could lead to confusion.

### Suggestions for Improvement
We recommend that the authors improve the statistical rigor of their analyses by incorporating uncertainty quantification methods, such as confidence intervals or hypothesis tests, to clarify the significance of their findings. Additionally, we suggest revising the activation overlap analysis to avoid arbitrary thresholds that may distort results, potentially using simple correlation instead. The authors should also clarify their assumptions regarding output entropy and provide a more detailed explanation of the correlations presented in Table 3. Furthermore, we advise renaming "human" prompts to "rule-based" or "human-readable" to better reflect their generation method. Lastly, addressing the limitations of the study's focus on English and considering the inclusion of other languages in future work would enhance the paper's relevance.