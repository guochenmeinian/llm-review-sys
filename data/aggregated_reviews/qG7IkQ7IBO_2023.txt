ID: qG7IkQ7IBO
Title: Temporal Graph Benchmark for Machine Learning on Temporal Graphs
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 9, 7, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Temporal Graph Benchmark (TGB), a comprehensive benchmark for evaluating machine learning models on temporal graphs, featuring diverse datasets across various domains. The contributions include the public release of datasets and leaderboards, the incorporation of both historical and random negatives in evaluation, and the use of practical ranking metrics (MRR and NDCG) instead of AUC. The authors also introduce a novel approach to the node property prediction task, motivated by applications in recommendation systems that require understanding user preferences over time. They clarify the distinction between node property prediction and link prediction, emphasizing that the former focuses on node affinities towards items rather than link weights. Practical datasets, tgbl-flight and tgbn-genre, are introduced to support their task, highlighting their relevance in traffic forecasting and music recommendation systems. The authors address the necessity of evaluating models in both inductive and transductive settings and provide additional experiments to support their claims.

### Strengths and Weaknesses
**Strengths:**
1. TGB provides novel findings on GNN characteristics, including the "surprise index" and its correlation with GNN design, revealing performance variations among models based on this index.
2. The benchmark encompasses a wide range of datasets and supports both link and dynamic node property prediction tasks.
3. The automated machine learning pipeline enhances reproducibility and accessibility for temporal graph research.
4. The authors have expanded the introduction to better articulate the importance of the node property prediction task.
5. Comprehensive discussions on the datasets and their applications enhance the paper's relevance.
6. The addition of experiments comparing transductive and inductive settings strengthens the evaluation framework.

**Weaknesses:**
1. The experimental setup is limited to the Streaming Setting, potentially reducing the impact of the findings.
2. The evaluation lacks diversity in baseline models and does not include comparisons with state-of-the-art methods like Jodie and TGAT.
3. There is insufficient discussion on the necessity of certain datasets and the node property prediction task.
4. The definition of the node property prediction task may be perceived as too application-specific, potentially limiting its generalizability across different graph types.
5. Some reviewers noted the lack of certain baseline models and metrics, such as AUC and AP, which could provide a more robust evaluation.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by including training time evaluations alongside inference time. Additionally, consider incorporating the latest methods such as NAT and TGAT into the experiments. It would also be beneficial to clarify the significance of certain datasets and provide a more comprehensive discussion on the node property prediction task. We suggest including more diverse baseline models and facilitating experiments in both inductive and transductive settings to enhance the robustness of the evaluation. Furthermore, we recommend improving the generality of the node property prediction task definition to encompass broader applications beyond specific use cases. Consider including a more comprehensive set of evaluation metrics, such as AUC and AP, to enhance the robustness of the results. Finally, it would be beneficial to clarify the fixed future period for the link prediction task to align it with the dynamic node property prediction task.