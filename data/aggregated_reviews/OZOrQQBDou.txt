ID: OZOrQQBDou
Title: TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two approaches—summarization and semantic compression—to reduce token size for retrieval augmented large language models (LLMs). The authors propose a food recommendation dataset focused on safe food consumption for women during pregnancy and infancy. By applying these techniques, they achieve a significant token reduction of 65% while maintaining a 0.3% performance improvement. The study also explores the efficacy of LLMs with supplementary context and shortened input context.

### Strengths and Weaknesses
Strengths:  
1. Novel combination of token compression methods that effectively reduces input tokens for LLMs.  
2. Introduction of a new dataset for food recommendations, addressing a specific and relevant domain.  
3. Strong experimental results demonstrating significant token compression with maintained performance.  
4. The solution addresses a critical cost issue in LLM applications, making it relevant and impactful.  

Weaknesses:  
1. Limited novelty as the methods primarily combine existing techniques, with improvements largely stemming from fine-tuning.  
2. Generalization to other domains is unclear, as the approach requires fine-tuning on new datasets.  
3. Lack of detailed cost analysis and insufficient information on data collection and annotation processes.  
4. The paper suffers from numerous typos and lacks clarity in presentation, making it difficult to follow.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by including important details and hyperparameters in the main text rather than solely in supplementary materials. A comprehensive cost analysis comparing the proposed solution with traditional methods should be added. Additionally, the authors should address the generalizability of their approach by testing it on diverse datasets and domains. Finally, we suggest proofreading the manuscript to correct typos and enhance readability.