ID: w0H2xGHlkw
Title: Visual Instruction Tuning
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 8, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the first attempt to extend the instruction-tuning paradigm to the multimodal domain, introducing the LLaVA model. The authors curate the first vision-language instruction-following dataset by converting public image-text pairs into a suitable format using ChatGPT, resulting in over 100K multimodal instruction-following samples. The multimodal model, which includes a CLIP visual encoder, a linear projection layer, and a LLaMA language decoder, demonstrates robust multimodal chatting abilities. All research assets, including datasets and models, are open-source. Additionally, the paper discusses more scientific and cost-efficient evaluation metrics for MLLMs, proposing alternative methods, such as employing ChatGPT to extract answers instead of directly evaluating with GPT-4. However, it lacks specific references to recent works and studies that support these claims.

### Strengths and Weaknesses
Strengths:
- The paper introduces one of the first large-scale instruction-following multimodal datasets, leveraging public image-text pairs.
- It provides valuable resources, including training code, pre-trained models, and evaluation benchmarks, to the multimodal research community.
- The model shows advanced instruction-following capabilities for images and text, supported by a detailed evaluation using LLaVA-Bench and ScienceQA.
- The authors' discussion on evaluation metrics is appreciated and deemed valuable.
- The detailed responses to reviewers' questions have positively influenced the ratings.

Weaknesses:
- Concerns exist regarding the evaluation benchmarks and metrics, particularly the small evaluation sets used for LLaVA-Bench and the potential cherry-picking of in-the-wild images.
- The evaluation relies solely on text-based scoring with GPT-4, raising questions about its robustness for multimodal reasoning.
- The paper lacks simple baselines, such as captioning-based approaches, and omits critical implementation details like the sampling procedure.
- The paper is missing crucial references to support claims about recent works and studies related to evaluation metrics.
- There is a lack of transparency regarding the citations due to anonymity concerns, which may hinder the paper's credibility.

### Suggestions for Improvement
We recommend that the authors improve the evaluation benchmarks by using larger and standardized datasets, such as VQA2.0 and GQA, to ensure scientific robustness. Additionally, the authors should consider splitting the instruction-following datasets into train/val splits and include classic text-only scoring metrics alongside small-scale human evaluations. It would also be beneficial to provide more detailed descriptions in the data generation pipeline and to clarify the sampling procedure used in LLaVA. Furthermore, we suggest that the authors include specific references to the recent works and studies mentioned, such as those that demonstrate the effectiveness of using ChatGPT for evaluation and the consistency of GPT-4 scores with human evaluations. Lastly, addressing the concerns regarding the language prior of the ScienceQA benchmark and clarifying citation practices to enhance transparency while adhering to the review process's anonymity requirements would enhance the paper's contributions.