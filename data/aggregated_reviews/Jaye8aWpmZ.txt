ID: Jaye8aWpmZ
Title: When LLMs Meet Cunning Texts: A Fallacy Understanding Benchmark for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 8, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the FaLlacy Understanding Benchmark (FLUB), designed to evaluate the fallacy comprehension abilities of large language models (LLMs) using challenging texts from the Chinese online forum Ruozhiba. FLUB comprises three tasks: answer selection, cunning type classification, and fallacy explanation. The experiments demonstrate that LLMs exhibit significant difficulties in detecting fallacies and reasoning, underscoring the need for enhancements in their capabilities.

### Strengths and Weaknesses
Strengths:
- FLUB includes cunning texts that effectively challenge LLMs by incorporating misleading information and various fallacies.
- The empirical results provide valuable insights, revealing LLMs' poor performance in recognizing fallacy types and the non-linear relationship between model size and performance.
- The dataset construction is rigorous, and the tasks are well-designed to assess LLMs from multiple perspectives.

Weaknesses:
- The dataset's size of 834 samples may be insufficient for training and evaluating LLMs, potentially limiting the generalizability of the findings.
- The exclusive focus on Chinese text raises questions about whether the observed performance issues are specific to cunning text or the language itself.
- There is a lack of clear distinction between FLUB and general logic problems, suggesting a need for a more thorough analysis of the data.
- The ethical implications of using data from potentially offensive forums require more comprehensive discussion.

### Suggestions for Improvement
We recommend that the authors improve the dataset size to enhance the generalizability of their findings. Additionally, consider including examples in English to broaden the benchmark's applicability. Clarifying the distinction between FLUB and general logic problems would strengthen the paper's contributions. We also suggest a more detailed discussion on the ethical considerations surrounding data sourcing, particularly regarding the potential for offensive content. Finally, please ensure that the explanation of the GPT-4 scoring methodology, especially the rationale for multiplying human scores by 2, is made clearer in the final version.