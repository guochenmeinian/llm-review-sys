ID: PObXviy706
Title: DAFT: Data-Aware Fine-Tuning of Foundation Models for Efficient and Effective Medical Image Segmentation
Conference: thecvf
Year: 2024
Number of Reviews: 3
Original Ratings: 8, 8, 10
Original Confidences: 4, 4, 5

Aggregated Review:
**Key Points:**
We find the paper exceptionally well-written, with a comprehensive explanation of the entire pipeline that facilitates easy replication of results. It presents a strong motivation for data-aware fine-tuning and includes sufficient quantitative and qualitative examples demonstrating the performance of the methods on validation data. The authors propose the DAFT strategy to address the Segment Anything in Medical Images On Laptop challenge, leveraging knowledge distillation and fine-tuning across different modalities.

**Strengths and Weaknesses:**
The strengths of the paper include its thorough investigation of various settings, strong validation results outperforming baselines across many imaging domains, and clear writing that aids understanding. Additionally, the authors implement a "flood of improvements" to enhance inference speed. However, we note weaknesses such as the unclear motivation for using meta-models for image domain selection and insufficient explanation of the distillation process, particularly regarding the input data for the teacher model and the choice of distilling from LiteMedSAM over standard MedSAM.

**Suggestions for Improvement:**
We suggest improving the manuscript by including the missing number of FLOPs and CO2 equivalents in Tables 4 and 5, which could be estimated from a single forward pass or one epoch of training. Additionally, a discussion on the characteristics of the image modalities where the proposed approach excelled or failed would enhance the completeness of the paper. Lastly, we recommend clarifying the rationale behind the use of meta-models and providing a more thorough explanation of the distillation process.