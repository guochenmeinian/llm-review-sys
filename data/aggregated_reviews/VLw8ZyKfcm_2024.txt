ID: VLw8ZyKfcm
Title: Latent Neural Operator for Solving Forward and Inverse PDE Problems
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 4, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Latent Neural Operator (LNO), a novel architecture for solving forward and inverse partial differential equations (PDEs) using a sequence of transformer layers. The architecture incorporates a Physics-Cross-Attention (PhCA) module to transform data from geometric space to a learnable latent space, enhancing computational efficiency and prediction accuracy. The authors evaluate their method on standard PDE problems, reporting superior performance compared to 11 other neural operator methods. Additionally, the paper addresses reproducibility concerns related to experimental settings and statistical significance, clarifying that established protocols were followed and providing statistical significance for the Transolver model using independently reproduced code. The authors also discuss theoretical guarantees, drawing an analogy between the attention mechanism and kernel integral operators, and outline a framework for proving the universality of their proposed LNO model.

### Strengths and Weaknesses
Strengths:  
- The introduction of the PhCA module represents a simple yet innovative adaptation of the cross-attention mechanism, likely to inspire further research.  
- Impressive experimental results demonstrate that the proposed method outperforms existing approaches on multiple forward PDE problems.  
- The solution method for inverse problems is novel, effectively interpolating and extrapolating data.  
- The authors provide a thorough response to reproducibility concerns, including statistical significance and comparisons with original results.  
- The submission of supplementary materials enhances the reproducibility of their model.  
- The theoretical justification for the model design is well-articulated, with a clear framework for proving universality.

Weaknesses:  
- The originality of the paper is limited, as the use of cross-attention in latent space is not new, and key references are missing.  
- The paper lacks theoretical guarantees to support its architectural design, raising concerns about its robustness.  
- Clarity is compromised due to poor organization, grammatical errors, and undefined terms such as "geometric space" and "latent space," making it difficult to distinguish the paper's contributions from prior work.  
- The experimental details are insufficient, particularly regarding the statistical significance of results and the robustness of hyperparameters across varying input sizes.  
- There are limitations in novelty and a lack of comparisons with important baselines, which may affect the overall impact of the work.  
- The challenge posed by the weight-sharing strategy could potentially hinder the model's approximation power.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by clearly defining terms like "geometric space" and "latent space" and organizing the content more logically. Additionally, a thorough review for grammatical errors is essential to enhance readability. We suggest including a detailed statistical analysis of the experimental results to validate their significance. Furthermore, the authors should provide theoretical guarantees for their architectural choices and address the robustness of hyperparameters across different datasets. To enhance the novelty of their work, we recommend incorporating comparisons with additional important baselines. Lastly, the authors should consider addressing the limitations posed by the weight-sharing strategy, possibly by exploring alternative configurations that allow for independent weight learning between the encoder and decoder.