ID: BopG5dhH7L
Title: A Computationally Efficient Sparsified Online Newton Method
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 7, 7, 5, 5, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, 4, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SONew, a sparsified online Newton method that utilizes the LogDet matrix divergence to create a sparsified preconditioner. By matching the largest eigenvalues of the preconditioner matrix between iterations, the method imposes a sparsity constraint while remaining close to the full-matrix preconditioner. The authors demonstrate SONew's effectiveness on various models, including autoencoders, vision transformers, graph neural networks, and large language models, achieving faster convergence than first-order optimizers like Adam and AdaFactor. Additionally, the paper provides a detailed analysis of optimizer parameter counts relative to the number of parameters in various neural network benchmarks, including Autoencoder, GraphNetwork, ViT, and Language Model, offering specific counts for different optimizers such as K-FAC, Shampoo, and Adam, illustrating the scalability of optimizer parameters with respect to network size.

### Strengths and Weaknesses
Strengths:
- SONew exhibits faster convergence compared to first-order optimizers and can handle larger model sizes where other methods, like Shampoo, fail due to memory constraints.
- The method shows minimal performance degradation when using bfloat16, outperforming other optimizers in this regard.
- The theoretical foundation and numerical stability analysis of SONew are well-articulated.
- The paper offers a comprehensive breakdown of optimizer parameter counts across multiple benchmarks, enhancing understanding of optimizer efficiency.
- The inclusion of specific numerical estimates for various optimizers adds clarity and depth to the analysis.

Weaknesses:
- The experimental setup lacks clarity, particularly regarding training curves and criteria for stopping training, making it difficult to assess convergence speed.
- Comparisons with other second-order methods, such as K-FAC and M-FAC, are insufficient, limiting the evaluation of SONew's performance.
- The paper does not adequately address the robustness of SONew to hyperparameter perturbations, which is critical for practical adoption.
- The response lacks a direct engagement with potential implications of the findings on optimizer selection and performance.
- There is no discussion on the practical impact of the parameter counts on training time or resource requirements.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including training curves to clarify convergence behavior and specifying the criteria used to stop training. Additionally, we suggest expanding the comparison to include more second-order optimizers like K-FAC and M-FAC to provide a comprehensive evaluation of SONew's performance. It would also be beneficial to release the code for reproducibility and to discuss the limitations of SONew more thoroughly, particularly regarding its robustness to hyperparameter changes and its performance across various datasets and architectures. Furthermore, we recommend enhancing the discussion on the implications of the findings regarding optimizer selection and performance, as well as providing insights into the practical impact of the parameter counts on training time or resource requirements to enhance the paper's relevance.