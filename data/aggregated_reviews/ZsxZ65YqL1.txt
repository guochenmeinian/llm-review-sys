ID: ZsxZ65YqL1
Title: CriticEval: Evaluating Large-scale Language Model as Critic
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 5, 8, 9, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CriticEval, a benchmark designed to evaluate the critique ability of large language models (LLMs) across four dimensions: feedback, comparison, correction, and meta-feedback. It encompasses nine diverse task scenarios and utilizes both scalar-valued and textual critiques. The benchmark employs a human-in-the-loop approach, with critiques generated by GPT-4 and refined by human experts. Key findings indicate that GPT-4 shows high correlation with human judgments, and some open-source LLMs perform comparably to closed-source models. However, the reliance on GPT-4 raises concerns about bias and generalizability.

### Strengths and Weaknesses
Strengths:
- CriticEval offers a holistic assessment of critique abilities across multiple dimensions and diverse tasks, surpassing existing benchmarks.
- The combination of GPT-4 generated critiques with human expert refinement enhances the reliability of evaluations.
- The evaluation of 35 LLMs provides valuable insights into critique capabilities and the relationship between critique difficulty and various factors.

Weaknesses:
- The heavy reliance on GPT-4 for initial critique generation may introduce bias, potentially favoring models trained on GPT-4 distilled data.
- The scalability of CriticEval for future language models is not adequately addressed, and the benchmark's reproducibility may be hindered by its dependence on GPT-4 and human experts.
- The paper lacks a detailed analysis of failure modes and does not compare LLM performance against human performance on critique tasks.

### Suggestions for Improvement
We recommend that the authors improve the diversity of models used for critique generation to mitigate bias. Additionally, the authors should provide a clearer discussion on the scalability of CriticEval and how it can be adapted for future LLM evaluations. A more thorough analysis of failure modes would enhance understanding of LLM limitations, and including a comparison of LLM performance to human performance would strengthen the evaluation framework. Finally, we suggest addressing the critique ability of LLMs in low-resource languages to ensure a more comprehensive assessment.