ID: t5z1WcB1cz
Title: Zer0-Jack: A memory-efficient gradient-based jailbreaking method for black box Multi-modal Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 5, 5
Original Confidences: 5, 5

Aggregated Review:
### Key Points
This paper presents a black-box attack method, Zer0-Jack, against multi-modal large language models (LLMs) using zeroth-order optimization techniques to generate malicious images. The approach is significant as it addresses the challenges of accessing internal information from LLMs compared to traditional deep neural networks (DNNs), making black-box settings more practical. The method shows effective attacks with limited memory resources, which is advantageous in resource-constrained environments. However, concerns arise regarding the similarity of the proposed method to the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm, questioning the necessity of branding it as a new technique.

### Strengths and Weaknesses
Strengths:  
- The method demonstrates effectiveness in black-box settings and resource-constrained environments.  
- It proposes a novel approach that could advance the understanding of attacks on multi-modal LLMs.

Weaknesses:  
- The paper suffers from poor clarity and organization, making it difficult to follow.  
- The abstract is excessively long relative to the introduction, which lacks a clear problem statement.  
- The related works section is limited and does not adequately cover existing literature.  
- Key methodological details, such as the setup of the LLM, are not sufficiently described.  
- There is confusion regarding the classification of MiniGPT-4 as a commercial LLM.  
- The lack of supplementary material and code prevents a thorough evaluation of the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of the paper, particularly by shortening the abstract and enhancing the introduction with a clear problem statement. The related works section should be expanded to include more relevant literature. We suggest that the overview of the method and Figure 3 be moved to the main body of the paper rather than the appendix. Additionally, the authors should provide a detailed description of the LLM setup, as it is crucial for understanding alignment. Clarifying the distinction between black-box and commercial LLMs is also necessary. Finally, we urge the authors to make supplementary material and code available to enable a proper assessment of the results.