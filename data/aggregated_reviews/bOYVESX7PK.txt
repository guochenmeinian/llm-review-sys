ID: bOYVESX7PK
Title: Identifying Equivalent Training Dynamics
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 8, 6, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for identifying equivalent training dynamics of deep neural networks through the lens of dynamical systems theory, specifically utilizing the spectral analysis of Koopman operators. The authors introduce the concept of topological conjugacy to define equivalence between dynamical systems, asserting that two systems are equivalent if a smooth invertible map can transform their trajectories. They employ a variant of dynamic mode decomposition (DMD) to approximate Koopman eigenvalues and compare these eigenvalues using Wasserstein distance. The paper also includes a comparative analysis of Koopman eigenvalues associated with different optimization algorithms, specifically mirror descent, gradient descent, and the bisection method. The authors report several findings, including the identification of nonlinear topological conjugacy between optimization dynamics and the characterization of training dynamics across various neural network architectures. They clarify that time-delays in their analysis refer to a method for generating observables, not sampling frequency, and acknowledge a suggestion to sample weights from a Bayesian neural network's posterior distribution for more efficient computation of Koopman eigenvalues.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and structured, making complex concepts accessible.
- The use of topological conjugacy to define training dynamics equivalence is original and convincing.
- The method for estimating Koopman eigenvalues and comparing them is technically sound.
- The invariance of Koopman eigenvalues to neuron permutation is beneficial for analyzing training dynamics.
- Experimental results validate the proposed method and reveal new insights into training dynamics.
- The authors provide a thorough analysis of Koopman eigenvalues and their distinctiveness across different optimization algorithms.
- The response to reviewer queries demonstrates a commitment to clarity and improvement, enhancing the paper's overall impact.

Weaknesses:
- Standard assumptions in Koopman operator theory may not hold for mini-batch stochastic gradient descent, which could weaken the findings.
- The method primarily identifies non-conjugacy; demonstrating conjugacy in seemingly non-equivalent dynamics would enhance impact.
- Notations in Equation (5) may require clarification for accuracy.
- There is a need for a clearer establishment of the null hypothesis in at least one experiment, as highlighted by reviewer concerns.
- The implementation of all baselines is still in progress, which may affect the completeness of the results presented.
- Adding a non-conjugate optimization algorithm to the experimental figures could strengthen the analysis.
- The use of the Kolmogorov-Smirnov test in certain analyses may not adequately demonstrate the claimed conjugacy.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the assumptions of Koopman operator theory in the context of mini-batch stochastic gradient descent, addressing potential violations. Additionally, it would be impactful if the authors could identify conjugacy in training dynamics of differently initialized networks, as suggested by Entezari et al. (2021). We also suggest clarifying the notations in Equation (5) for precision and including a non-conjugate optimization algorithm in Figure 2 to illustrate the significance of spectral similarities. Furthermore, we recommend improving the clarity of the null hypothesis establishment for at least one experiment, as this is crucial for validating the use of Wasserstein distances in demonstrating conjugacy among differently initialized fully connected networks. Lastly, we suggest that the authors discuss the proposed method of sampling weights from a Bayesian neural network's posterior distribution in more detail in the revised manuscript and reconsider the use of the Kolmogorov-Smirnov test in Figure 3, providing a clearer metric for demonstrating conjugacy, such as a low Wasserstein distance close to 0.