ID: yloVae273c
Title: Offline Primal-Dual Reinforcement Learning for Linear MDPs
Conference: NeurIPS
Year: 2023
Number of Reviews: 28
Original Ratings: 6, 6, 5, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies offline reinforcement learning (RL) in linear Markov Decision Processes (MDPs) and explores extending the work of Zanette et al. to an infinite-horizon setting. The authors propose a gradient-based algorithm that reformulates the problem into a saddle-point problem, achieving a sample complexity of \(O(\epsilon^{-4})\) for both discounted and average-reward settings. They suggest that replacing the horizon \(H\) with the effective-horizon \(1/(1-\gamma)\) could address limitations in the critic procedure `PLSPE`, although this raises concerns about non-stationary policies and computational efficiency. An alternative approach involves using a critic procedure designed for infinite horizons, akin to PSPI, which employs a pessimistic policy evaluation but does not achieve optimal sample complexity. The authors clarify that the sample complexity of ALMIS is \(O(1/\epsilon^2)\) and discuss the nuances of partial data coverage in linear MDPs, asserting that their method offers a "most correct" notion of coverage.

### Strengths and Weaknesses
Strengths:
1. The paper presents a clear and well-structured writing style, making it easy to follow.
2. The algorithm is computationally efficient and provides solid theoretical guarantees.
3. The formulation of offline RL as a linear programming problem is innovative and well-motivated.
4. The discussion on the concentrability constant \(C\) is thorough and insightful.
5. The authors acknowledge and address feedback, demonstrating a willingness to clarify and improve their work.

Weaknesses:
1. The method lacks significant technical novelty, and the linear MDP setting is considered restrictive.
2. The convergence rate is suboptimal compared to existing methods, which may achieve \(O(\epsilon^{-2})\).
3. There are concerns regarding the comparison with previous works, particularly regarding sample complexity claims and the treatment of linear function approximation versus linear MDPs.
4. The contribution of the proposed algorithm remains unclear due to restrictions in dimensions such as linear MDPs and empirical evaluation.
5. The discussion of oracle-based algorithms may be seen as unnecessary when comparing general function approximation methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their comparisons with previous works, particularly addressing the sample complexity of algorithms like those in Cheng et al. and Xie et al. Additionally, we suggest that the authors provide a more detailed analysis of the coverage ratio \(C_{\phi,c}\) when \(c \neq \frac{1}{2}\) and clarify the implications of using different values of \(c\). Furthermore, we encourage the authors to explore the extension of their results to high-probability bounds and discuss the significance of average-reward settings more thoroughly. A more rigorous justification for the computational complexity claims would enhance the paper's robustness. We also recommend improving clarity regarding the oracle-based categorization in their comparison table and considering adding a column to indicate whether algorithms utilize the correct coverage notion. Lastly, addressing the restrictions of their method and providing empirical evaluations could strengthen the paper's contribution to the community.