ID: raABeiV71j
Title: Loki: Low-rank Keys for Efficient Sparse Attention
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 8, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for approximating attention in large language models (LLMs) using PCA to compute approximate attention scores in a reduced dimensional space, subsequently selecting the top-k tokens based on these scores. The authors demonstrate that key vectors lie in a significantly lower-dimensional space, leading to speedups of up to 40% with minor reductions in generation quality. The method is evaluated across various models and datasets, showing comparable performance to full attention while enhancing inference efficiency.

### Strengths and Weaknesses
Strengths:  
1. The discovery of low intrinsic dimensionality of attention keys is insightful and may inspire future work on Sparse Attention.  
2. Extensive evaluations across multiple models and datasets demonstrate significant speedups with minimal accuracy degradation.  
3. Theoretical support is provided through lemmas and proofs, enhancing the credibility of the approach.  
4. Optimized Triton kernels are developed for practical implementation.

Weaknesses:  
1. The method does not reduce memory usage, which may limit its applicability.  
2. Some figures are unclear, with text overlaps and sizing issues.  
3. Limited comparison with existing Sparse Attention baselines, such as SPAR-Q Attention, raises questions about the trade-offs involved.  
4. The implementation complexity may pose challenges for achieving theoretical speedups without specialized knowledge.

### Suggestions for Improvement
We recommend that the authors improve the clarity of figures, particularly addressing overlaps and sizing issues in Figures 2 and 6. Additionally, we suggest including a comparison with other Sparse Attention methods, such as SPAR-Q Attention, to elucidate the trade-off curve between acceleration ratio and effectiveness. It would be beneficial to add a plot showing latency-performance trade-offs using downstream tasks, particularly with longer sequences evaluated through LongBench. We also encourage the authors to explore the integration of PCA-TopK with high-efficiency attention mechanisms and other model compression techniques. Lastly, enhancing the readability of the Triton code by using meaningful variable names and improving documentation would be advantageous for future researchers.