ID: d7a5TpePV7
Title: How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 3, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates model shift in model-based Reinforcement Learning (RL) and proposes a two-stage model learning method called USB-PO (Unified model Shift and model Bias Policy Optimization). The first stage updates the model using a replay buffer, while the second stage aims to reduce both model shift and model bias. The authors provide a theoretical analysis and demonstrate the effectiveness of their method through experiments on MuJoCo.

### Strengths and Weaknesses
Strengths:
1. The proposed method is novel and addresses an important problem in model-based RL.
2. The authors offer a solid theoretical analysis and motivation for their approach.
3. The ablation study is comprehensive, showing that USB-PO outperforms methods that omit either model bias or shift.

Weaknesses:
1. The basis for lines 133-135 lacks clarity; conclusions drawn from Definition 1 and Theorem 1 are questionable.
2. The relationship between Theorems 2 and 3 is unclear, as both appear to prove different bounds for the same term, and there seems to be a typo in Theorem 3.
3. The performance curve of the MBRL method CMLO shows a significant drop compared to the original paper, requiring clarification.
4. The baselines STEVE and SLBO are outdated and should not be considered state-of-the-art for MBRL.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical foundations, particularly regarding the conclusions drawn in lines 133-135 and the relationship between Theorems 2 and 3. Additionally, please specify the increased time consumption due to the two-stage model learning. We suggest comparing USB-PO with more recent MBRL methods, such as PDML and ALM, rather than relying on outdated baselines. Furthermore, integrating phases 1 and 2 into a single joint optimization objective could enhance implementation ease. Lastly, consider conducting experiments in additional environments like DMC and Metaworld to strengthen the paper's impact.