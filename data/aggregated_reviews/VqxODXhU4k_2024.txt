ID: VqxODXhU4k
Title: Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 8, 5, 8, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the nonparametric instrumental variable (NPIV) regression problem, introducing a new algorithm, SAGD-IV, which employs stochastic gradient descent in a function space to minimize populational risk. The gradient is computed by estimating the conditional density $\Phi$ and the conditional expectation operator $\mathcal{P}$. The approach is flexible, accommodating various supervised learning algorithms and supporting both continuous and binary outcomes. The authors also demonstrate the consistency of the NPIV estimator under regularity assumptions.

### Strengths and Weaknesses
Strengths:  
- The estimation of the gradient for SGD in the second stage for NPIV is novel.  
- The algorithm's applicability to non-quadratic loss functions is also a significant contribution.  
- The main result is agnostic about component estimators, and the paper is well-written, with thorough comparisons across existing literature.

Weaknesses:  
- The claim that existing NPIV methods cannot handle large, high-dimensional datasets is questionable, as the proposed method also requires computing the conditional density $\Phi$.  
- The novelty of the algorithm appears limited, as standard two-stage estimation methods could also apply gradient descent after obtaining $\mathcal{P}$ without needing to estimate $\Phi$.  
- Experimental results are unconvincing, showing only marginal improvements over standard methods in low-dimensional settings, with no high-dimensional experiments presented.  
- The consistency and sample-complexity of the proposed method remain unclear, and the proof of convergence is based on standard techniques.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the computational challenges associated with estimating $\Phi$ and how their method addresses high-dimensional datasets. Additionally, we suggest that the authors provide a more detailed discussion on the technical novelty and difficulty of proving the convergence of SAGD-IV. It would also be beneficial to elaborate on the implementation details of the functional gradient descent step in practical experiments. Finally, we encourage the authors to include more extensive experimental results, particularly in high-dimensional settings, to substantiate the claimed advantages of their method.