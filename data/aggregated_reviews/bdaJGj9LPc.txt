ID: bdaJGj9LPc
Title: Exploring Hypergraph Condensation via Variational Hyperedge Generation and Multi-Aspectual Amelioration
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HG-Cond, a hypergraph condensation framework designed to efficiently reduce hypergraph size while maintaining or enhancing model performance. The authors introduce key components such as a Neural Hyperedge Linker, Hypergraph Coreset Initialization, and a Multi-Aspectual Amelioration strategy. Extensive experiments validate the effectiveness of HG-Cond in condensing large-scale hypergraphs, addressing challenges related to high-order correlations and computational complexity.

### Strengths and Weaknesses
Strengths:
1. The manuscript introduces a novel framework specifically for hypergraph condensation, demonstrating strong empirical performance across multiple datasets.
2. The motivation is clear, and the paper is well-organized, contributing valuable insights to the community.

Weaknesses:
1. The comparative methods used are outdated, lacking recent methodologies from the last one or two years.
2. The performance after condensation on the CiteSeer dataset significantly differs from that of the whole dataset, undermining claims of comparable performance.
3. The multi-aspectual amelioration strategy appears disjointed and lacks coherence.
4. Key methodological sections lack mathematical formulations, relying on textual descriptions that hinder understanding.
5. The Gradient-Parameter Synergistic Matching introduces significant computational overhead, counteracting the framework's efficiency goals.

### Suggestions for Improvement
We recommend that the authors improve the comparison with recent methodologies to enhance the paper's relevance. Additionally, the authors should clarify the performance discrepancies observed on the CiteSeer dataset and provide a more cohesive explanation of the multi-aspectual amelioration strategy. We suggest including mathematical formulations in key methodological sections to improve clarity. Furthermore, the authors should address the computational overhead introduced by the Gradient-Parameter Synergistic Matching and consider discussing potential limitations or failure cases in more detail. Lastly, we recommend that the authors ensure all figures, such as Figure 1, are appropriately referenced in the main text.