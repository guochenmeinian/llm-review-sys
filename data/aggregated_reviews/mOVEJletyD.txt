ID: mOVEJletyD
Title: Slimmed Asymmetrical Contrastive Learning and Cross Distillation for Lightweight Model Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 6, 6, 6, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a self-supervised contrastive learning method aimed at enhancing lightweight network performance while reducing training costs. The authors propose a combination of slimmed asymmetrical contrastive learning (SACL) and cross-distillation (XD), enabling effective training without a large teacher model. The method is evaluated across various lightweight models and datasets, demonstrating its efficiency and superiority over existing approaches.

### Strengths and Weaknesses
Strengths:
1. The paper is well-motivated, addressing a significant challenge in training lightweight models with contrastive learning.
2. The novel concept of slimming the host encoder and creating asymmetrical encoding paths effectively reduces training costs and improves performance.
3. The writing is clear and easy to follow.
4. Comprehensive experiments validate the efficacy of the proposed method across diverse models and tasks.

Weaknesses:
1. The novelty of SACL+XD appears limited, as it seems to apply the slimmable neural network concept to existing methods.
2. The experiments lack comparisons with denser backbones like ResNet-50 and do not address the impact of weight initialization on performance.
3. The evaluation of computational reduction is based solely on training FLOPs; practical training time should also be included for a more comprehensive comparison.
4. The transfer learning study is limited, with only CIFAR and VOC datasets presented.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proposed methods by providing additional intuition on how cross-distillation mitigates distortion from asymmetrical encoding. It would be beneficial to explore trade-offs associated with different levels of asymmetry and sparsity. Additionally, we suggest including comparisons with denser backbones and alternative self-supervised learning methods to enhance the understanding of the proposed approach. Finally, incorporating practical training time metrics and expanding the transfer learning evaluation would strengthen the paper's contributions.