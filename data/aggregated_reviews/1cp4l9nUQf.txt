ID: 1cp4l9nUQf
Title: Uncovering Syllable Constituents in the Self-Attention-Based Speech Representations of Whisper
Conference: EMNLP/2024/Workshop/BlackBoxNLP
Year: 2024
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 4, 3, 3

Aggregated Review:
### Key Points
This paper presents a probing study on the Whisper ASR model, investigating how it encodes syllable structure and phonetic categories. The results indicate that the model captures features of syllable constituents (onset, nucleus, coda) primarily in its earlier layers, with temporal information being more indicative of syllable structure than phonetic categories. The probing approach employs attention-head pruning to analyze these interactions, revealing that while the model effectively encodes syllabic features, the expected relationships between phonetic categories and syllable structure are not clearly established.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated and clearly argued, contributing to the understanding of how transformer models encode linguistic structure.
- The experimental setup is logical, and the descriptions are easy to follow, enhancing the clarity of the findings.
- The work adds to the growing literature on the interpretability of large ASR models.

Weaknesses:
- The choice of TIMIT as the dataset raises concerns about its appropriateness for probing Whisper, as it may be too controlled, potentially questioning the relevance of the results.
- The results are somewhat expected, leaving the paper's novel contributions unclear, especially since the probing technique itself is not new.
- There is a lack of direct correlation between nucleus and vowel recall in layer 0, which is surprising given their similarities. The analysis is limited to broad phonetic categories, making it challenging to connect findings to theoretical insights about phonological structures.

### Suggestions for Improvement
We recommend that the authors improve the dataset choice, considering a more challenging corpus like LibriSpeech to enhance the probing relevance. Additionally, we suggest that the authors clarify the focus on English syllable structure, as it remains somewhat implicit. To strengthen the analysis, evaluating more fine-grained phonemic features or sentential-level intonation/accent could provide deeper insights into the model's internal representation of speech. Lastly, we advise increasing the size of the figures for better readability, as the current size may hinder comprehension.