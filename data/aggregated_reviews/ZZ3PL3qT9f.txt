ID: ZZ3PL3qT9f
Title: Causal Document-Grounded Dialogue Pre-training
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a causally-complete dataset construction strategy aimed at developing million-scale Document-Grounded Dialogue (DocGD) pre-training corpora. The authors propose a causally-perturbed pre-training method to enhance the capture of causality by introducing perturbations on variables and optimizing the overall causal effect. The effectiveness of this approach is demonstrated through experiments on three benchmark datasets, yielding varying degrees of improvement across different settings.

### Strengths and Weaknesses
Strengths:  
1. The motivation for creating causally-complete datasets and the novel causally-perturbed pre-training method are strong and innovative.  
2. The paper achieves superior performance on three benchmarks, even outperforming GPT-3.5 on two of them.  
3. It explores DocGD from a causal perspective, presenting causal relationships among variables for the first time.  

Weaknesses:  
1. The effects of external factors on causality are not measured, raising concerns about the comprehensiveness of the analysis.  
2. The paper lacks sufficient evidence to explain how the pre-training specifically benefits the Document-Grounded Dialogue task.  
3. The quality of data is constrained by the dialogue inpainter and paraphrase model, and the singular nature of data sources may limit generalizability.  

### Suggestions for Improvement
We recommend that the authors improve the explanation of how the pre-training works for the Document-Grounded Dialogue task and provide comparisons with existing pre-training methods. Additionally, addressing the concerns regarding data quality and generalizability is crucial. We also suggest exploring alternative evaluation metrics beyond BLEU, as the current metric may inadequately assess model proficiency. Finally, consider experimenting with popular decoder-only architecture models to broaden the scope of the analysis.