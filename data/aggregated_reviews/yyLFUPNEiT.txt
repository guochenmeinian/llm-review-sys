ID: yyLFUPNEiT
Title: What Distributions are Robust to Indiscriminate Poisoning Attacks for Linear Learners?
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper explores poisoning attacks on linear classifiers and updated experimental results on the effectiveness of feature extractors against such attacks. The authors investigate why poisoning attacks exhibit varying efficacy across different datasets, relating this variability to two factors: (1) (projected) constraint set size and separability. They theoretically characterize an optimal poisoning attack concerning Gaussian mixture data and provide empirical evidence supporting their claims. Additionally, the authors propose that the quality of the feature extractor is correlated with increased Sep/SD and Sep/Size metrics, with revised experiments indicating that deeper architectures and longer training epochs enhance the resistance of downstream classifiers to attacks. However, the correlation between these factors and error increase appears weaker compared to the original submission, particularly when the feature extractor does not utilize downstream training data.

### Strengths and Weaknesses
Strengths:
- The paper addresses significant issues regarding the robustness of datasets against poisoning attacks, presenting original and interesting questions.
- The theoretical analysis is robust, effectively linking empirical results to the theoretical framework.
- The empirical findings are comprehensive and well-presented, enhancing the understanding of dataset vulnerability.
- The new results provide empirical evidence supporting the main claim regarding feature extractor quality and its correlation with resistance to poisoning attacks.
- The authors have computed Pearson correlation coefficients, offering a quantitative measure of the relationship between identified factors and error increase.

Weaknesses:
- The contributions lack clear practical implications, and the analysis is limited to linear models, which may not generalize to more complex models.
- Some claims are overgeneralized or inadequately supported, particularly regarding the effectiveness of defenses and the novelty of findings related to linear classifiers.
- The revised experiments suggest a weaker correlation than previously reported, which may undermine the strength of the overall claims.
- The relationship between Sep/SD and Sep/Size is not consistently aligned in the new setup, indicating potential complexities in the feature extraction process.
- The presentation contains grammatical errors and could benefit from improved clarity and structure.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims, particularly regarding the effectiveness of linear models and the novelty of their findings. Specifically, revise statements that overclaim results, such as those regarding the commonality of linear models as victim models and the reasons for attack ineffectiveness. 

We suggest conducting experiments that vary separability to establish causation more clearly, as correlation alone does not suffice for the claims made. Additionally, clarify the experimental setup regarding the maximum poisoning rate and ensure that the definitions and terms used are intuitive for the reader. 

We encourage the authors to enhance the presentation in Section 5.1 by providing clearer connections between definitions and theorems. Furthermore, consider revising the title to better reflect the focus on dataset robustness and move less critical comparisons to the appendix to streamline the main content. Lastly, address the formatting issues in figures for better clarity and understanding. 

To strengthen the experimental design regarding feature extractors, we recommend including the Pearson correlation coefficients in both the rebuttal and the paper to substantiate claims of correlation. A comparison of the Pearson correlation with and without the changes made in the experiments would enhance the discussion. Additionally, consider exploring a few-shot learning setup that incorporates a small amount of clean downstream training data, as this may provide a more balanced approach to feature extractor training while addressing the base error alongside error increase from poisoning.