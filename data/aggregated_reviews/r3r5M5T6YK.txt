ID: r3r5M5T6YK
Title: Advancing Neuromorphic Computing Algorithms and Systems with NeuroBench
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 8
Original Confidences: 5, 4

Aggregated Review:
### Key Points
This paper presents a benchmark framework for neuromorphic computing, addressing the lack of standardized methods and benchmarks for spiking neural networks. The authors systematically identify challenges, such as differentiating algorithmic properties from hardware implementation characteristics. However, the work is still in progress, raising questions about its current contribution to the field and its utility in establishing a standard benchmarking methodology.

### Strengths and Weaknesses
Strengths:  
- The paper fills a significant gap in benchmarking for neuromorphic computing, highlighting the performance differences between ANN and SNN solutions.  
- It is well-structured and written in clear, idiomatic English, facilitating reader comprehension.  
- The involvement of over 100 researchers from more than 50 institutions adds credibility to the work.

Weaknesses:  
- The focus is primarily on inference evaluation, neglecting the energy and memory demands during training.  
- The "algorithm track" appears limited to PyTorch-based implementations, potentially excluding other neuromorphic algorithms.  
- The novelty of the proposed metrics is questionable, as many are standard and do not address the trade-offs between model performance and computational complexity.  
- The manuscript lacks clarity on certain terms and claims, and the rationale for differing benchmark datasets between tracks is not explained.

### Suggestions for Improvement
We recommend that the authors improve the scope of their work by either narrowing it to PyTorch and digital neuromorphic hardware or developing more universally applicable evaluation metrics. Additionally, the authors should discuss benchmarks for model training costs and how these differ between classical and neuromorphic implementations. It is essential to clarify how the framework accommodates analog or mixed-signal hardware and whether the "code harness" can be adapted for other platforms like SpiNNaker2 or Loihi. 

The authors should enhance the novelty of the algorithm track by constructing metrics that capture the trade-off between model performance and computational complexity. Furthermore, they should address the implications of quantization in ANNs for inference metrics and provide detailed guidelines for profiling neuromorphic hardware within the manuscript. 

Clarifying the differences in benchmark datasets for both tracks and supporting claims with citations will strengthen the paper. Lastly, we suggest the authors define "agile analytical models" or avoid vague terminology to enhance clarity.