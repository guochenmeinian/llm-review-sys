ID: 8moTQjfqAV
Title: Temporal-Difference Learning Using Distributed Error Signals
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 8, 7, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel reinforcement learning algorithm called Artificial Dopamine, inspired by the dopamine-based learning system in the brain, specifically targeting the ventral tegmental area (VTA) and the nucleus accumbens (NAc). The algorithm utilizes a biologically inspired design that eliminates the need for backpropagation by employing local error signals and a recurrent layer architecture. The authors evaluate the algorithm against established RL benchmarks, providing comprehensive results and ablation studies. Additionally, the paper extends the authors' algorithm to learn distributions of values, significantly improving sample efficiency in DMC environments. The authors conducted further ablation studies on DMC and MinAtar environments, analyzed the effects of varying layer sizes in a single-layer AD network, and expanded the discussion on biological assumptions by citing eight additional neuroscience works. They defined criteria of sufficiency to make their hypothesis falsifiable and clarified that their work is algorithm-focused rather than a direct biological model.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important research direction in biologically plausible reward-based learning, effectively linking computer science and neuroscience.
- The algorithm is well-motivated and explained, with clear details for conceptual understanding and implementation.
- High-quality figures and comprehensive evaluations enhance the paper's clarity and depth.
- The authors have made substantive improvements, including enhanced sample efficiency and additional ablation studies.
- The paper provides clear algorithmic insights relevant to biology, demonstrating the sufficiency of distributed error signals for reward-based learning.

Weaknesses:
- Certain details are unclear, particularly regarding the use of multiple tanh layers in the AD cells, leading to potential contradictions in the description.
- The number of runs for comparing RL algorithms is low, which could affect the robustness of the results; incorporating recent metrics like performance profiles and IQM/IQR would improve comparisons.
- The biological evidence supporting the modeling framework is weak, with insufficient linkage to empirical data regarding dopamine's role in local value updates.
- The text still conveys a biological focus that may mislead readers regarding the authors' intent of presenting a biologically inspired algorithm rather than a model of biological processes.
- Some reviewers expressed concerns that the changes primarily address algorithmic aspects, leaving the core message about biological plausibility unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the algorithm's architecture, particularly regarding the use of multiple tanh layers in AD cells, to unify notation and description. Additionally, increasing the number of runs for algorithm comparisons and incorporating metrics such as performance profiles and IQM/IQR would enhance the robustness of the evaluations. We also suggest strengthening the biological grounding of the model by providing more empirical evidence linking the proposed algorithm to known mechanisms in the brain and discussing alternative neuroscientific hypotheses. Furthermore, we recommend that the authors improve the clarity of the paper's intent by revising the abstract and key sections to emphasize the algorithmic focus over biological modeling. Providing alternative wording in critical places to sharpen the distinction between their algorithm and biological processes would also be beneficial. Finally, ensure that all figures, including the newly generated IQM/IQR plots, are integrated effectively into the main text for completeness.