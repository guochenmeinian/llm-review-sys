ID: GxL6PrmEUw
Title: Distributional Learning of Variational AutoEncoder: Application to Synthetic Data Generation
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: 6, 6, 6, -1
Original Confidences: 3, 2, 2, -1

Aggregated Review:
### Key Points
This paper presents a novel nonparametric distributional learning method for Variational Autoencoders (VAEs) called DistVAE. The authors provide a comprehensive theoretical analysis and empirical results demonstrating its superiority over existing methods like CTGAN, TVAE, and CTAB-GAN for synthetic data generation on real tabular datasets. The proposed decoder utilizes an infinite mixture of asymmetric Laplacian distributions, enhancing the model's capacity to capture underlying data distributions.

### Strengths and Weaknesses
Strengths:
1. The theoretical framework is original and significant.
2. Experimental results show DistVAE outperforms competitors.
3. The paper is well-structured and clearly written.

Weaknesses:
1. The experimental setting is limited to tabular datasets and a few synthesizers.
2. The claim regarding model capacity extension without sacrificing computational advantages lacks empirical comparison with various VAEs on classical image datasets.
3. The theoretical analysis contains too many assumptions, raising concerns about generality to real data.

### Suggestions for Improvement
We recommend that the authors improve the experimental setting by including comparisons with different VAEs on classical image datasets to validate claims about model capacity and computational efficiency. Additionally, a more detailed discussion of the limitations of the proposed method would be beneficial. Clarifying the rationale behind assigning a prior to $\alpha$ would enhance the theoretical analysis. Lastly, addressing the potential impact of using a 2D latent space on decoder capacity and exploring higher latent dimensions in experiments would provide further insights.