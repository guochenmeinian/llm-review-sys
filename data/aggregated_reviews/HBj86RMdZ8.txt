ID: HBj86RMdZ8
Title: The Importance of Online Data: Understanding Preference Fine-tuning via Coverage
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 6, 6, 7, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of optimization and learning methods for online reinforcement learning from human feedback (RLHF) and contrastive offline methods (DPO, IPO). The authors aim to clarify the differences between these methods, particularly regarding the sampling of new responses. They highlight that the reward parameterization is crucial for this distinction and introduce the concepts of global and local coverage to encapsulate these differences. The authors propose a novel Hybrid Preference Optimization (HyPO) algorithm that integrates offline data with online samples to manage KL divergence, supported by empirical results demonstrating its effectiveness.

### Strengths and Weaknesses
Strengths:
- The authors explore the theory of RLHF under a KL-regularized target, which aligns more closely with practical applications compared to prior works using non-regularized rewards.
- The introduction of the local coverage condition is a novel and insightful contribution to the analysis of KL-regularized targets.
- The paper effectively distinguishes between offline algorithms like DPO and online RL-based algorithms, corroborating recent findings that online methods significantly outperform offline ones.
- The discussion surrounding the differences between DPO and RLHF is well-articulated, particularly regarding the parameterization of rewards, enhancing the understanding of practical implications.

Weaknesses:
- Clarification is needed regarding terminology, as the online data in this paper is used solely for KL loss computation without querying human feedback, which differs from traditional online exploration methods.
- The empirical results do not sufficiently explore the online setting where new responses and human preference signals can be queried, despite evidence suggesting that online frameworks outperform offline ones.
- The computational cost of the HyPO algorithm is not adequately addressed, which is crucial for practical comparisons with other algorithms.
- The experimental validation is limited to specific models and datasets, raising questions about the generalizability of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the clarity of terminology regarding online exploration and its relation to human feedback. Additionally, it would be beneficial to include empirical results that explore the online setting with new responses and human preference signals. We suggest a thorough discussion of the computational costs associated with the HyPO algorithm to provide clearer guidance for practical applications. Furthermore, we encourage the authors to expand their experimental validation to include larger models and diverse datasets to assess the robustness of their findings. Finally, we recommend relocating Theorem E.1 to Section 5 for better coherence and clarity in the proof structure.