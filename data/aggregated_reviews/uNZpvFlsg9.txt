ID: uNZpvFlsg9
Title: PiCO: Peer Review in LLMs based on the Consistency Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 4, 3, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel unsupervised evaluation method for large language models (LLMs) that utilizes a peer-review mechanism, where models evaluate each other's anonymized answers. The authors propose a capability parameter for each LLM and solve a constrained optimization problem to maximize the consistency between capabilities and scores, ultimately ranking the models. The paper also introduces three new evaluation metrics: Permutation Entropy (PEN), Count Inversions (CIN), and Longest Increasing Subsequence (LIS), and discusses the implications of evaluating LLMs without human preference annotations.

### Strengths and Weaknesses
Strengths:
- The paper addresses a critical problem of LLM evaluation without human annotations, proposing an innovative method that is scalable and less biased.
- The introduction of the consistency optimization framework and new evaluation metrics is noteworthy and could inspire future research.
- The presentation is generally clear and well-structured.

Weaknesses:
- The reliance on the consistency assumption raises concerns about its applicability in practice.
- The evaluation metrics chosen are questioned for their relevance to ranking rather than time series comparison.
- The optimization algorithm is not adequately detailed, and potential issues with its formulation are highlighted.
- The selection criteria for the evaluated LLMs appear arbitrary, lacking justification for the exclusion of notable models like GPT-4 and Claude.
- Some equations and notations are unnecessarily complex, which could be simplified for clarity.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the iterative elimination of models, including the rationale for the 60% threshold and its optimality. Additionally, addressing potential scaling issues for larger model sets and providing a comparison of costs between fine-tuning a baseline model versus selecting the best from many candidates would enhance the paper. The authors should also include results for Precision@1 and RBP@3, and extend Fig 5 to include nine graphs with illustrative ranked lists for each dataset. Clarifying the implications of the ground truth ranking and the choice of evaluation metrics, as well as providing a detailed explanation of the consistency optimization algorithm, are essential. Finally, the authors should justify their selection of LLMs and consider comparing their method with a simple baseline using a powerful LLM for ranking preferences.