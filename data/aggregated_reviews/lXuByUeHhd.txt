ID: lXuByUeHhd
Title: DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 8, 7, 8, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 5, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DoReMi, a method for optimizing domain weights in pretraining datasets for language models (LMs). It utilizes a small proxy model trained via group distributionally robust optimization (Group DRO) to derive optimal domain weights, which are then applied to resample data for training larger models. The authors demonstrate that DoReMi significantly enhances LM performance, achieving a 6.5% increase in average few-shot downstream accuracy and matching baseline accuracy with 2.6x fewer training steps.

### Strengths and Weaknesses
Strengths:
- The originality of the method addresses the critical issue of optimal pretraining data mixture/weighting, offering a novel approach to learning domain weights.
- The method is well-designed, supported by extensive experimental results that validate its effectiveness.
- The clarity of presentation is commendable, making the paper easy to follow.
- The significance of the problem tackled is notable, as the method shows improvements in pretraining perplexity and downstream task performance.

Weaknesses:
- Insufficient motivation for learning domain weights; the authors do not convincingly justify why assigning a single scalar weight to each domain is ideal, especially given the variability of data quality within domains.
- Unclear generalization ability to larger model scales; the largest model tested is 8B, raising concerns about scalability.
- Limited evaluation tasks; the focus on pretraining perplexity and QA-related tasks does not comprehensively cover the range of tasks relevant to the domains.
- Lack of comparison with naive baselines that directly remove noisy domains from the pretraining corpus.

### Suggestions for Improvement
We recommend that the authors improve the motivation for learning domain weights by addressing the appropriateness of using single scalar weights for entire domains, considering instance-level noise. Additionally, we suggest including comparisons with naive baselines that remove noisy domains to strengthen the argument for DoReMi's effectiveness. To enhance generalization, we encourage the authors to test larger model scales, such as 65B, and to broaden the evaluation tasks to include coding and reasoning tasks relevant to the pretraining corpus. Lastly, we advise providing clearer guidelines for selecting the size of the proxy model to facilitate its application in practice.