ID: ZmeAoWQqe0
Title: Time Series as Images: Vision Transformer for Irregularly Sampled Time Series
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 3, 5, 4, 7, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 5, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for classifying irregularly sampled time series by transforming them into images and utilizing pre-trained Vision Transformers (ViT). The authors demonstrate the effectiveness of this approach through empirical studies, achieving state-of-the-art performance on several irregular time-series benchmarks and competitive results on regular datasets. They propose that translating time series data into images offers a more efficient representation than converting them into text, particularly for multivariate time series. The work raises significant questions about the efficacy of pre-trained Transformers in time series analysis and includes experimental results demonstrating the effectiveness of their method, including comparisons of performance metrics across different datasets and configurations.

### Strengths and Weaknesses
Strengths:  
- The paper provides a novel application of image-driven methods for representation learning in irregularly sampled time series, showcasing sufficient novelty and empirical effectiveness.  
- It is well-organized, clear, and presents extensive empirical evaluations, contributing valuable insights into the capabilities of ViT in time series classification.  
- The authors provide clear experimental results that support their claims regarding the advantages of using ViT models for time series data.  
- The integration of masked image modeling is well-articulated, highlighting its benefits in enhancing model performance.  
- The results are reproducible, with full code provided.  
- The response to reviewer feedback is thorough, addressing concerns and clarifying methodologies effectively.  

Weaknesses:  
- The experimental results raise concerns regarding robustness, as performance is sensitive to various plotting configurations and hyper-parameter choices.  
- The paper lacks a comprehensive analysis of critical factors influencing performance, such as the advantages of using ViT over other foundational models and the impact of design choices on results.  
- There is insufficient detail on hyper-parameter tuning, and the authors do not adequately address limitations or potential negative societal impacts of their work.  
- Some aspects, particularly related to the cross-domain transfer hypothesis, require further exploration and validation.  
- The discussion on the efficiency of their model compared to other non-vision baselines lacks clarity and depth.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of their hyper-parameter tuning procedures, including how validation and test sets are utilized. Additionally, please provide more extensive ablation studies to assess the robustness of ViTST against variations in line styles, colors, and grid layouts. It is crucial to discuss the computational costs of the proposed method compared to non-vision-based approaches and to clarify the theoretical factors contributing to successful cross-domain transfer. We also recommend that the authors improve the exploration of the cross-domain transfer hypothesis, as it remains an unresolved area. Incorporating the discussions and findings from reviewers R2, R3, and R4 into the manuscript would enhance its quality. Finally, addressing the efficiency comparison with other models more comprehensively would strengthen the overall argument, and we urge the authors to include a dedicated section discussing the limitations of their work, particularly regarding the method's performance across different datasets and the implications of using matplotlib for input construction.