ID: GX9nQZcq54
Title: Explanation for Trajectory Planning using Multi-modal Large Language Model for Autonomous Driving
Conference: thecvf
Year: 2024
Number of Reviews: 1
Original Ratings: 4
Original Confidences: 4

Aggregated Review:
**Key Points:**
The work presents a multi-modal approach that combines video and trajectory planning information to explain autonomous vehicle maneuvers, addressing limitations of previous systems reliant solely on visual data. A new dataset is created by simultaneously acquiring frontal images and trajectory planning information.

**Strengths and Weaknesses:**
Strengths include improved interpretation of driving decisions through the integration of frontal images and planned trajectories, as well as the collection of a new dataset. However, weaknesses are noted in the problem formulation, suggesting that a joint optimization of the language interpretation and trajectory planning modules would be more effective. Additionally, the reliance on expert human drivers for the training dataset raises concerns about the system's ability to explain incorrect decisions, and the motivation for creating a new, smaller dataset is questioned given the availability of existing large-scale datasets like nuScenes.

**Suggestions for Improvement:**
We suggest optimizing the language interpretation and trajectory planning modules jointly in a chain-of-thought manner rather than using an add-on module for interpretability. We also recommend considering the implications of training the VLM solely on expert trajectories and exploring the use of existing datasets for language annotations instead of collecting a new dataset.