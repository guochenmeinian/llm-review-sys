ID: NemifGnD2E
Title: GNeSF: Generalizable Neural Semantic Fields
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 7, 5, 4, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a generalizable 3D scene segmentation framework that integrates neural implicit representation with multi-view image features and semantic maps to reconstruct 3D geometric and semantic information. The authors propose a soft voting mechanism and a visibility module to synthesize semantic maps and perform 3D segmentation with only 2D supervision. The method operates within a generalizable NeSF setting, which does not require 2D semantic ground truth labels for novel scenes. The authors demonstrate comparable performance metrics with existing methods, such as 'Mask2Former + NeRF optimization' (70.03% vs. 70.27% mIoU), while achieving significantly improved inference speed (approximately 4 minutes compared to 10 hours for the alternative). The experimental results indicate improvements over existing techniques using solely 2D annotations.

### Strengths and Weaknesses
Strengths:
- The method is novel, addressing a crucial problem in 3D scene understanding by extending 2D segmentation maps into 3D space.
- The framework design is logical, effectively leveraging a cost volume for comprehensive scene information and utilizing a pre-trained 2D model for accurate semantic inference.
- The proposed soft voting mechanism enhances label assignment for new pixels, demonstrating significant improvements in segmentation performance.
- The authors effectively clarify key points regarding the method's operation and performance, showing competitive results against existing approaches while significantly improving inference speed.
- The inclusion of detailed comparisons and clarifications enhances the paper's rigor.

Weaknesses:
- The empirical validation lacks thorough testing of key aspects, such as the claimed advantages of the "generalisable" radiance field over test-time optimization regarding speed and data efficiency.
- The method's performance drops significantly when using pseudo labels compared to ground truth, indicating a potential limitation.
- The paper does not provide clear explanations for the network structure and training strategy, leading to reproducibility concerns.
- Comparisons with related methods are insufficient, particularly regarding the novelty of the contributions and the performance of the proposed method against existing baselines.
- The training expenses are higher than some existing methods, which may affect accessibility for users with limited resources.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation by explicitly comparing the performance of the generalizable radiance field with test-time optimization on new scenes, addressing speed and data economy advantages. Additionally, please clarify the training process, including whether an appearance loss is utilized, and provide more detailed explanations of the network architecture and training strategy to enhance reproducibility. We suggest improving clarity regarding the absence of 2D annotations at test time and the implications of using pseudo labels. Furthermore, we recommend including more detailed results on view consistency across multiple scenes and viewpoints to strengthen the submission. Finally, please highlight the merits of generalizable NeRF settings in relation to unseen scenes and reinforce the distinction between the proposed method and existing approaches, particularly in relation to recent works like Semantic-Ray and Panoptic Lifting.