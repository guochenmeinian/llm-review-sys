ID: BFbdO9GwTZ
Title: Generative Adversarial Training with Perturbed Token Detection for Model Robustness
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel generative adversarial training framework aimed at enhancing model robustness through the integration of gradient learning, adversarial sample generation, and perturbation labeling detection. The authors propose a method that bridges the gap between continuous perturbation and discrete text markers, generating discrete perturbation markers via gradient propagation. Extensive experiments across five datasets demonstrate the framework's effectiveness, surpassing recent results from ChatGPT.

### Strengths and Weaknesses
Strengths:  
- The paper proposes a strong and intuitive framework that effectively utilizes generative models for adversarial detection, significantly improving model robustness.  
- The method shows clear performance improvements over baselines, with accurate detection of perturbed tokens.  
- Empirical evidence from experiments supports the claims made regarding the framework's effectiveness.

Weaknesses:  
- The proposed method closely resembles the training method for the ELECTRA model, raising concerns about originality.  
- There is a lack of comparative analysis with relevant baseline methods and adversarial training techniques, which could strengthen the findings.  
- The evaluation is limited to static datasets, and the authors do not adequately address the performance on non-adversarial datasets or the weaknesses of continuous representations.

### Suggestions for Improvement
We recommend that the authors improve the comparative analysis by including baseline methods such as "Searching for an Effective Defender" and "Certified robustness to text adversarial attacks by randomized [mask]." Additionally, the authors should evaluate their framework against dynamic adversarial attack processes like textfooler. It is also essential to clarify the instructions given to ChatGPT models and to analyze the weaknesses of training with continuous representations. Finally, we suggest that the authors compare their results using the same BERT model instead of DeBERTa for a more equitable evaluation.