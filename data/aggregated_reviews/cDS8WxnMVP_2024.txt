ID: cDS8WxnMVP
Title: Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 7, 5, -1, -1
Original Confidences: 3, 3, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a tensor network (TN) perspective on convolutional networks, detailing forward passes, first-order, and second-order derivatives. The authors propose efficient implementations of algorithms such as KFAC and structured dropout, demonstrating faster computation and memory efficiency through experimental results. The paper also recasts convolutions as tensor networks, simplifying analysis and enhancing understanding of underlying tensor operations.

### Strengths and Weaknesses
Strengths:  
1. The paper is well-written with clear and consistent tensor network notations, supported by rigorous mathematical formulations and proofs.  
2. It provides a comprehensive study of TN representations and computations in CNNs, particularly in relation to KFAC and structured dropout, which are crucial for optimization.  
3. The inclusion of detailed diagrams and visualizations aids in understanding, while the experimental results validate the computational and memory efficiency of the proposed implementations.  

Weaknesses:  
1. The claimed potential impacts of the proposed representation remain unclear, as applications are limited to KFAC and structured dropout.  
2. The performance improvements demonstrated are not consistently convincing, especially when compared to standard optimized routines.  
3. The use of einsum for convolutions is not optimal, with mixed results indicating that TN implementations may not always outperform existing methods.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the potential benefits of their representation by including additional applications beyond KFAC and structured dropout. Additionally, addressing the performance trade-offs for specific architectures and providing a more comprehensive comparison with optimized convolution algorithms, such as Winograd or Fourier transform-based methods, would strengthen the paper. Finally, we suggest that the authors explore practical ways to estimate contraction order variability and clarify why certain architectures benefit from the TN approach while others do not.