ID: OP3sNTIE1O
Title: Data Augmentation with Diffusion for Open-Set Semi-Supervised Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to semi-supervised learning (SSL) that addresses the challenge of class distribution mismatch by utilizing diffusion models to convert out-of-distribution (OOD) unlabeled data into in-distribution samples. The authors propose a method that combines diffusion model training with a discriminator to filter irrelevant instances, significantly enhancing SSL performance in scenarios with large class distribution mismatches. Empirical results demonstrate improvements over state-of-the-art SSL methods, particularly in challenging contexts.

### Strengths and Weaknesses
Strengths:
1. The approach creatively combines ideas from generative modeling and SSL, effectively tackling the class distribution mismatch problem.
2. The results show significant performance enhancements over existing SSL methods, particularly in difficult scenarios, and the method can serve as a plug-in to existing approaches.
3. The paper includes good visualizations that clarify the generated samples and the effectiveness of the discriminator.

Weaknesses:
1. The computational costs associated with diffusion models are significant, and a more detailed analysis of the trade-offs between performance gains and these costs is needed.
2. The paper lacks comprehensive comparisons with other generative augmentation techniques in SSL, limiting the context of its contributions.
3. Experiments are confined to small-scale datasets, raising questions about the method's scalability to larger datasets.
4. The focus on successful cases neglects potential limitations or scenarios where the method may not perform well.
5. The explanation of the discriminator and its training process is somewhat confusing; further clarification on positive-unlabeled learning and alternative designs for the discriminator is warranted.
6. The introduction of several hyper-parameters necessitates a more thorough discussion of their sensitivity.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the trade-offs between performance gains and computational costs associated with diffusion models. Additionally, more extensive comparisons with advanced data augmentation techniques across various datasets would provide valuable context. Expanding experiments to include larger and more complex datasets would help assess the scalability of the method. A balanced discussion of limitations and scenarios where the method may struggle would enhance the paper's depth. Clarifying the training process of the discriminator and considering alternative designs would strengthen the presentation. Finally, a thorough analysis of the sensitivity to hyper-parameters should be included to bolster the findings.