ID: dVaWCDMBof
Title: DataComp: In search of the next generation of multimodal datasets
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 9, 9, 9, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DATACOMP, a benchmark and competition for multimodal dataset design, aimed at identifying a smaller yet more effective subset from COMMONPOOL, a dataset of 12.8 billion image-text pairs sourced from Common Crawl. The authors conduct extensive experiments on scaling trends and filtering strategies, resulting in DATACOMP-1B, a state-of-the-art multimodal dataset that enables training a CLIP ViT-L/14 model with reduced computational costs while achieving superior zero-shot performance compared to OpenAIâ€™s original CLIP model. The work emphasizes the significance of dataset quality in multimodal models, particularly in light of the increasing relevance of such datasets in the context of new foundation models. Additionally, the paper includes a comprehensive evaluation of the DataComp dataset, featuring new experiments on various tasks such as visual question answering (VQA) and image generation. The authors conducted new VQA evaluations using CLIP models, revealing a strong correlation between VQA performance and ImageNet accuracy. They also addressed the impact of face blurring on image generation quality and explored the dataset's potential for generative tasks by fine-tuning Stable Diffusion models. Furthermore, the authors implemented new filtering strategies based on object detection to enhance dataset quality.

### Strengths and Weaknesses
Strengths include the introduction of a public environment and substantial resources for multimodal dataset exploration, with over 300 baseline experiments conducted across 38 downstream tasks. The evaluation methods are robust, and the paper is well-structured and clear. The authors expanded the evaluation suite to include diverse tasks, enhancing the dataset's robustness. New experiments on VQA and image generation demonstrate the dataset's applicability across different domains, and the introduction of complex filtering methods shows a proactive approach to improving data quality. However, weaknesses are noted in the training target's suitability, which is primarily aligned with understanding tasks, potentially limiting its effectiveness for generative tasks. The filtering methods employed are considered somewhat simplistic and not particularly novel, lacking more complex filtering techniques that could enhance dataset quality. Additionally, the impact of face blurring on generative tasks remains a concern, as it may hinder the quality of generated images, and the preliminary results on image generation do not provide a complete picture, indicating the need for further exploration.

### Suggestions for Improvement
We recommend that the authors improve the dataset's applicability for generative tasks by expanding the training targets beyond understanding tasks. Incorporating more sophisticated filtering techniques, such as controlling for object categories, counts, and positioning within images, would enhance the dataset's robustness. We also recommend that the authors improve the discussion on the trade-offs between privacy and generative model performance, particularly regarding face blurring. Additionally, we suggest that the authors explore the use of captioning models to augment datasets, as this has shown promise in recent studies. Finally, we encourage the authors to conduct more extensive experiments on image generation to provide a clearer understanding of the dataset's capabilities in this area, as well as clarifying the licensing status of the datasets in the main text and addressing the BYOD (Bring Your Own Data) track's potential risks to strengthen the paper's documentation and ethical considerations.