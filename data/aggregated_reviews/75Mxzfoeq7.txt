ID: 75Mxzfoeq7
Title: No-Regret Learning in Dynamic Competition with Reference Effects Under Logit Demand
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on gradient descent dynamics in duopoly competitions with reference effects and logit demand. The authors propose an Online Projected Gradient Ascent (OPGA) algorithm, proving its convergence to a stationary Nash equilibrium (SNE) with a decreasing step size. This result is novel as the game is neither convex nor strongly monotone, extending previous work by Golrezaei et al. (2020) which focused on linear demands and uniform reference prices. The authors also provide a convergence rate for the algorithm. Additionally, the paper extends the original model by addressing scenarios where firms possess only approximate information about their sensitivity parameters $(b_i, c_i)$ and market share $d_i^t$. The authors propose methods for estimating these parameters from both uncensored and censored data, enhancing the practical applicability of the OPGA algorithm. The impact of noisy first-order oracles on convergence results is discussed, leading to Theorem 6.1, which establishes that price paths converge to a neighborhood of the unique SNE under certain noise conditions.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with sound and original results.
- The authors effectively connect their work to existing literature, clarifying why prior results do not apply.
- The proven theorems are novel, and the analysis is original.
- The paper effectively expands the model to include realistic data scenarios, increasing its relevance to real-world applications.
- The use of established algorithms like maximum likelihood estimation and generalized expectation-maximization provides a solid foundation for parameter estimation.
- Theorems and proofs are rigorously developed, demonstrating the convergence behavior of the proposed algorithm under noise.

Weaknesses:
- The analysis is highly specific to the considered duopoly games, which may limit interest in the broader machine learning community, despite the relevance of online learning and convergence to NE in NeurIPS.
- The authors frequently reference no-regret without defining the concept or providing bounds on regret, leaving unclear the implications for sublinear regret.
- The reliance on approximation errors may limit the robustness of the convergence results, particularly in high-noise environments.
- The distinction between the noisy first-order oracle and stochastic gradients could be elaborated further for clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the regret concept by defining it explicitly and providing bounds on regret. Additionally, addressing the realism of firms computing exact gradients and the necessary information for this computation would enhance the paper. We suggest discussing the generalizability of the Gumbel distribution assumption and its implications for convergence. Furthermore, we encourage the authors to include a plot demonstrating the learning rate $\eta^t = \Theta(1/t)$ in Theorem 5.2, as this would provide more informative insights. Lastly, we recommend that the authors improve the clarity of the distinction between noisy first-order oracles and stochastic gradients, possibly by providing more detailed examples or illustrations. Addressing the potential limitations of the model in high-noise scenarios could also strengthen the overall robustness of the findings.