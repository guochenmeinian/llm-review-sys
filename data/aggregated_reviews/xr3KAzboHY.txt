ID: xr3KAzboHY
Title: Calibrating “Cheap Signals” in Peer Review without a Prior
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 7, 5, 4, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 2, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for addressing bias and noise in peer review ratings through a one-shot scoring process that ranks papers based on their true quality without prior information. The authors propose a model that elicits reviewers' predictions of others' ratings, leveraging a prior distribution over states and assuming perfect Bayesian updating. The key innovation is the introduction of a noise-invariant surprisal vector, which allows for consistent comparison of papers across different noise levels. The method demonstrates theoretical guarantees and empirical validation, showing improved performance over traditional averaging methods in distinguishing paper quality.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant issue in peer review, aiming to de-bias and de-noise reviewer ratings.
- The concept of a noise-invariant surprisal vector is insightful and contributes to the originality of the work.
- The organization of the paper is generally clear, and the theoretical framework is well-established.

Weaknesses:
- The presentation suffers from clarity issues, with an unfocused introduction and several grammatical errors. The notation is heavy, making comprehension difficult.
- The assumption of calibrated agent predictions is inadequately justified, particularly in the context of potentially biased signals.
- The numerical experiments lack a comparison with the surprisingly popular (SP) mechanism, which could provide a relevant baseline.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by refining the introduction and reducing the complexity of notation. Moving the notation table to the main body and expanding on high-level ideas would enhance understanding. Additionally, justifying the assumption of calibrated predictions in the presence of biased signals is crucial. We suggest including recent references to update outdated citations and conducting numerical experiments that incorporate SP as a baseline for comparison. Finally, addressing the clarity of theoretical guarantees and assumptions in the context of the proposed method would strengthen the paper.