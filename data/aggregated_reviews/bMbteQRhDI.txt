ID: bMbteQRhDI
Title: FIARSE: Model-Heterogeneous Federated Learning via Importance-Aware Submodel Extraction
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FIARSE, a model-heterogeneous federated learning framework that addresses model heterogeneity by allowing clients with varying computational capabilities to extract submodels based on parameter importance. The authors propose an importance-aware dynamic submodel extraction method, supported by theoretical convergence analysis and empirical results demonstrating its superiority over existing methods like FedRolex and HeteroFL. The experiments utilize CIFAR10/100 and AGNews datasets.

### Strengths and Weaknesses
Strengths:
- The paper provides a convergence proof for the submodel extraction procedure, aligning with state-of-the-art results in asymptotic convergence.
- The experimental setup is appropriate, utilizing commonly used datasets and models in federated learning, and demonstrates consistent improvements over baselines.
- The theoretical analysis is solid, and the method is flexible, accommodating different model sizes and client distributions.

Weaknesses:
- FIARSE induces unstructured sparsity, which may hinder efficiency as not all hardware supports sparse operations, and performance may be lower compared to dense models.
- There is insufficient comparison with state-of-the-art methods that produce structured sparsity, such as FjORD and NeFL, raising concerns about the exclusion of these evaluations.
- The writing lacks cohesion, particularly in discussing related works, making it difficult to understand how the proposed approach differs from existing methods.
- The performance evaluation does not adequately address communication costs and computation overhead, nor does it explore the impact of varying client model heterogeneity distributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in the related works section, to better articulate the distinctions between FIARSE and existing methods. Additionally, we suggest conducting a more comprehensive comparison with state-of-the-art approaches, including FjORD and NeFL, to substantiate the claims of superiority. It would be beneficial to analyze the computational performance of FIARSE in relation to other methods, varying the submodel dimension, and to include discussions on communication costs and computation overhead. Finally, we encourage the authors to explore the sensitivity of FIARSE to the choice of threshold and its potential impact on model fairness across different client groups.