ID: gKLgY3m9zj
Title: An Information Theoretic Perspective on Conformal Prediction
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 8, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a series of upper bounds for the conditional entropy of labels given attributes, linking Conformal Prediction (CP) to information theory. The authors propose using these bounds as an alternative objective function for conformal training, demonstrating empirical validity. The work aims to relate the uncertainty measured by CP to the conditional entropy \( H(Y|X) \) and introduces a new objective for conformal training, enhancing efficiency by minimizing prediction set sizes.

### Strengths and Weaknesses
Strengths:
- The connection between CP and List Decoding is intriguing, although its application in the bounds is somewhat unclear.
- The use of CP's distribution-free and finite sample properties to derive empirical bounds is noteworthy and may extend beyond the CP community.
- The paper is well-written, with a clear explanation of prior literature and concepts, and the theoretical analysis is insightful.

Weaknesses:
- The role of the arbitrary distribution \( Q \) needs a more intuitive explanation, particularly its link to the underlying point-predictor.
- Practical implications of the findings require better clarification, and concrete examples, such as defining \( Q \) in Section 3.1, would enhance understanding.
- The technical contribution appears to stem from the inequality \( H(Y|X) < H(w_Y(Y, X)|w_X(Y, X)) \), and the proposed training method does not adequately address the challenge of smoothing the sorting operation in CP algorithms.
- Justification for using conditional entropy over \( \text{E}(|\mathcal{C}|) \) is insufficient, particularly regarding the efficiency of the upper bound compared to the original objective function.

### Suggestions for Improvement
We recommend that the authors improve the intuitive explanation of the role of the arbitrary distribution \( Q \) and provide an explicit construction in the Introduction to motivate the proposed approach. Clarifying the practical consequences of the findings and including concrete examples would enhance the paper's applicability. Additionally, the authors should justify the choice of conditional entropy over \( \text{E}(|\mathcal{C}|) \) more robustly, particularly in terms of efficiency. Addressing the challenges of optimizing CP algorithms, especially regarding the sorting operation, would strengthen the contribution. Finally, we suggest revisiting the discussion on list decoding to either integrate it more effectively into the main text or relocate it to a separate section for clarity.