ID: m5CAnUui0Z
Title: Label Delay in Online Continual Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new continual learning framework addressing label delays in data streams, where labels are revealed with a fixed delay after data collection. The authors introduce Importance Weighted Memory Sampling (IWMS), which effectively utilizes memory samples that resemble new unlabeled data to mitigate accuracy gaps caused by these delays, achieving performance comparable to scenarios without delays. The paper includes extensive experiments demonstrating the limitations of increased computational resources and advanced techniques like Self-Supervised Learning in overcoming the challenges posed by label delays. Additionally, the authors focus on the metric of Online Accuracy, arguing that it is context-dependent and should be clarified, as it may not universally represent the most critical performance measure. They acknowledge that while IWMS improves Online Accuracy, it may lead to marginal performance drops on previous tasks, particularly in specific datasets like FMoW, and recognize the need for a more comprehensive comparison with existing Semi-Supervised OCL methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a clear motivation for addressing the realistic problem of label delay.
- The proposed method is simple, easy to understand, and has the potential to inspire future research.
- Comprehensive empirical evaluation, including comparisons to various baselines and ablation studies, supports the effectiveness of the proposed approach.
- The authors have effectively addressed reviewer concerns and clarified their methodology.
- The inclusion of a live demo and code enhances the transparency and reproducibility of the research.

Weaknesses:
- The evaluation metric of Online Accuracy may overlook performance on previous tasks, raising concerns about the model's ability to maintain overall accuracy. The authors should clarify the rationale for using this metric and consider including Average Accuracy in the main draft.
- The choice of Online Accuracy as a primary metric lacks sufficient justification in the manuscript.
- The definition of Backward Transfer may not align with standard definitions in the literature.
- The paper lacks comparisons with existing Continual Learning methods in both supervised and semi-supervised contexts, which would enhance the discussion.
- The proposed setting is not clearly formulated, and the practicality of matching input data with delayed labels may be challenging in real-world scenarios.
- The memory buffer's definition and construction are unclear, and the authors should discuss its complexities and limitations more thoroughly.
- The manuscript does not adequately address related work on label delay, and additional relevant studies should be considered.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation metrics by including Average Accuracy alongside Online Accuracy to ensure that performance on previous tasks is not compromised. Additionally, we suggest that the authors improve the justification for using Online Accuracy as the primary metric, ensuring it is clearly articulated in the manuscript. We also recommend enhancing the discussion by comparing their method with existing Continual Learning approaches, particularly in supervised and semi-supervised contexts. Furthermore, the authors should clearly define the proposed setting and address the practical challenges of matching input data with delayed labels. A more detailed discussion on the memory buffer, including its construction and potential limitations, would be beneficial. Lastly, incorporating a comprehensive review of related work on label delay and including a limitation section that explicitly addresses the potential drawbacks of IWMS, particularly regarding Backward Transfer, would strengthen the manuscript. Including training time in the appendix would also provide valuable context for readers.