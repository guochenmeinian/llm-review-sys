ID: qVeDwgYsho
Title: CoPriv: Network/Protocol Co-Optimization for Communication-Efficient Private Inference
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 7, 6, 7, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CoPriv, a framework that optimizes the 2-party computation (2PC) inference protocol and deep neural network (DNN) architecture to reduce communication overhead. The authors propose a new 2PC protocol based on the Winograd transformation and develop DNN-aware optimizations, achieving communication reduction compared to state-of-the-art (SOTA) protocols like CrypTFlow2. The paper emphasizes that traditional methods focusing on ReLU-based metrics are less effective in minimizing communication costs.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to understand.
2. The observations in the motivation section are informative and well-supported.
3. The Winograd-based transformation with tile aggregation is well-motivated and illustrated.
4. The adaptive convolutional protocol is novel and effective.
5. The ablation study is conclusive and well-conducted, showing consistent communication reduction.

Weaknesses:
1. More ablation experiments are needed to demonstrate the individual contributions of ReLU pruning and re-parameterization.
2. The paper lacks clarity on why the DNN-aware adaptive convolution protocol reduces communication costs, particularly regarding the selection of protocol initializer.
3. The novelty of the proposed ReLU pruning method is unclear without an explanation of how L_{comm} affects training results.
4. Table 1 lacks merit, as it does not provide numerical speed-ups and may mislead readers with its current format.
5. The implications of end-to-end speedup are not well studied, and the effect on communication time and inference speedup is not adequately discussed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 1 by including specific numbers for each portion and the communication cost reduction achieved by each technique. Additionally, the authors should provide a detailed explanation of how the DNN-aware adaptive convolution protocol reduces communication costs and clarify the criteria for selecting the protocol initializer. To enhance the novelty of the ReLU pruning method, a comparison with DeepReduce/SNL/SENet regarding which ReLUs remain in the network and final accuracy would be beneficial. We also suggest revising Table 1 to include numerical speed-ups and reconsider the use of the acronym ASS for arithmetic secret sharing to avoid potential misunderstandings. Finally, we encourage the authors to analyze the end-to-end speedup using common setups to provide a more comprehensive assessment of their framework's performance.