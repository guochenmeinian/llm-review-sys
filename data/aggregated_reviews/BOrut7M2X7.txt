ID: BOrut7M2X7
Title: Divide-and-Conquer Posterior Sampling for Denoising Diffusion priors
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 7, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel posterior sampling scheme called divide-and-conquer posterior sampling (DCPS) that utilizes pre-trained diffusion models as priors to address Bayesian inverse problems. The authors refine the diffusion posterior sampler by employing a variational Gaussian approximation instead of a point mass approximation, aiming to improve the coherence of inference. The effectiveness of DCPS is demonstrated through various inverse problems, including image restoration and trajectory prediction, with promising results compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The paper addresses a relevant and interesting problem, providing a mathematically compelling approach.
- The experimental results are practical, showcasing improvements in both quantitative and qualitative aspects across various likelihoods.
- The manuscript is generally well-written, and the proposed algorithm is supported by solid theoretical evidence.

Weaknesses:
- The complexity of the algorithm may hinder understanding, particularly regarding the use of twisted potentials and their differences from previous approaches.
- There is insufficient justification for the design of intermediate potentials, and the paper lacks a theoretical analysis of approximation errors.
- The reliance on a single evaluation metric (LPIPS) for image restoration tasks is problematic, as it may lead to overfitting and does not encompass the full performance spectrum.

### Suggestions for Improvement
We recommend that the authors improve clarity by elaborating on the differences between their approach and the predictor-corrector method proposed by Rozet et al. Additionally, a more general-purpose posterior sampling pipeline that separates likelihood and data specification from posterior sampling would be beneficial. The authors should also provide a theoretical analysis of the intermediate potentials and their design, particularly for nonlinear forward models. Furthermore, we suggest incorporating multiple evaluation metrics, such as PSNR and SSIM, to provide a more comprehensive assessment of the algorithm's performance. Lastly, addressing the concerns regarding the algorithm's bias with only five Langevin steps would strengthen the paper's claims.