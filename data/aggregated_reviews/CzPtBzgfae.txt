ID: CzPtBzgfae
Title: Don't Compress Gradients in Random Reshuffling: Compress Gradient Differences
Conference: NeurIPS
Year: 2024
Number of Reviews: 25
Original Ratings: 5, 6, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of compressed Federated Learning algorithms within the context of Random Reshuffling (RR), proposing several variants, including Q-RR and DIANA-RR, which demonstrate improved convergence guarantees over traditional Q-SGD, particularly when the number of samples is small relative to the number of iterations. The authors also introduce the Q-NASTYA and DIANA-NASTYA methods in the Partial Participation setting, where a subset of clients is sampled for aggregation. They provide detailed bounds on expected error terms related to client sampling and compression variance, supported by theoretical results and experiments. The paper discusses the shuffling radius as a measure of performance dispersion and acknowledges the need for empirical comparisons with the EF21 method, which were initially omitted due to theoretical differences.

### Strengths and Weaknesses
Strengths:
- The proposed algorithms extend existing methods (DIANA, NASTYA) for the RR setting, supported by theoretical analysis for strongly convex, smooth functions.
- The authors effectively address concerns regarding the theoretical framework for partial participation, providing comprehensive results that clarify their approach.
- The inclusion of detailed mathematical proofs enhances the rigor of the analysis, particularly in establishing bounds on expected errors.
- The experiments complement theoretical results, highlighting the benefits of the DIANA variance-reduction technique in compression.
- The authors express a commitment to improving the paper based on reviewer feedback.

Weaknesses:
- The hypothesis regarding compression variance lacks a necessary condition; a lower bound demonstrating that Q-RR cannot improve over Q-SGD in worst-case scenarios is missing.
- The practical implications of the proposed methods compared to QSGD are not fully explored, leaving questions about when one approach may be preferable over the other.
- The initial omission of comparisons with the EF21 method raises concerns about the robustness of the experimental results.
- The reliance on FP64 for training may limit the practicality of their methods in typical academic settings, where lower precision formats are more common.
- The writing quality is inconsistent, with a need for clearer explanations of new quantities and notations, as well as a better structure for pseudocode.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the main text by explicitly discussing the results from the appendix regarding Q-RR's performance when training the entire network. Additionally, we suggest including more high-dimensional datasets or deep learning experiments to enhance the generalizability of the findings. To strengthen the theoretical framework, we advise the authors to provide a lower bound for Q-RR relative to Q-SGD. Furthermore, we recommend that the authors improve clarity by explicitly detailing the algorithm for partial participation, including whether the server waits for responses from the first \( C \) clients. A comparative analysis of the partial participation algorithm, including scenarios where QSGD might be more advantageous, would better inform readers of the practical applications of their methods. Lastly, we encourage the authors to refine the writing for better coherence and clarity, particularly in the introduction and the presentation of algorithms, and to discuss the implications of using FP64 versus lower precision formats more thoroughly to address potential concerns regarding computational feasibility.