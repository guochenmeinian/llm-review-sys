ID: Pw9vYSPJKk
Title: DiffusionRet: Diffusion-Enhanced Generative Retriever using Constrained Decoding
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel generative retrieval method called DiffusionRet, which utilizes a diffusion model to generate pseudo documents for query expansion. It subsequently employs long n-grams for retrieval, claiming improved performance over existing methods. Experimental results indicate that DiffusionRet achieves state-of-the-art results while maintaining a smaller parameter count compared to strong baselines.

### Strengths and Weaknesses
Strengths:
- The integration of a diffusion model for query expansion is innovative.
- Extensive experiments demonstrate the effectiveness of DiffusionRet across multiple datasets.

Weaknesses:
- The motivation for using a diffusion model lacks clarity, particularly in comparison to generative language models like T5.
- Insufficient discussion on the differences between DiffusionRet and existing N-gram-based generative retrieval methods, such as SEAL.
- The paper suffers from formatting issues and lacks comprehensive analysis supporting key claims, particularly regarding the benefits of long n-grams.

### Suggestions for Improvement
We recommend that the authors improve the motivation section by clearly articulating the advantages of using a diffusion model over generative language models of similar size. Additionally, the authors should provide a detailed comparison between DiffusionRet and SEAL, specifically addressing the implications of n-gram length. To enhance clarity, we suggest revising the formatting issues and ensuring that all claims are substantiated with appropriate analyses and ablation studies. Furthermore, we encourage the authors to clarify the experimental design, particularly regarding the evaluation of unseen documents in Experiment 5.3.