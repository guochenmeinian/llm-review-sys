ID: 6YIpvnkjUK
Title: The Sample-Communication Complexity Trade-off in Federated Q-Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 8, 8, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the sample and communication complexities of Federated Q-learning, emphasizing the trade-off between these two factors. The authors prove a lower bound on communication rounds necessary for linear speedup in sample complexity, specifically requiring at least $\Omega(\frac{1}{1-\gamma})$ rounds. They also introduce a novel algorithm, Fed-DVR-Q, which achieves optimal order sample and communication complexities while incorporating variance reduction techniques.

### Strengths and Weaknesses
Strengths:  
- The paper addresses an important and well-motivated problem in Federated Q-learning, providing a comprehensive understanding of the trade-offs involved.  
- The theoretical results, including lower and upper bounds, are solidly presented and contribute valuable insights to the community.  
- The introduction of the Fed-DVR-Q algorithm demonstrates optimal performance in both sample and communication complexities.

Weaknesses:  
- The theoretical focus of the work limits its practical applicability, as there is a lack of experimental evaluation of the proposed algorithm.  
- The lower and upper bounds are only applicable to synchronous Q-learning with IID samples, which may restrict the generalizability of the findings.  
- The paper does not sufficiently address the technical difficulties and novelties of the proposed approach, particularly in relation to existing literature on distributed reinforcement learning.

### Suggestions for Improvement
We recommend that the authors improve the paper by including an experimental evaluation of the Fed-DVR-Q algorithm to demonstrate its practical effectiveness. Additionally, we suggest elaborating on the differences between their approach and existing studies on distributed multi-armed bandits, as well as clarifying the discrepancies regarding the necessity of infrequent communication for achieving speedup in sample complexity. Furthermore, the authors should provide a more detailed discussion on the technical challenges faced in the federated setting and how their analysis differs from single-agent Q-learning. Lastly, we encourage the authors to address the typo in Eq. (7) and clarify the implications of their results for asynchronous sampling settings.