ID: ToMdTqVIb5
Title: Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for scientific reasoning in catalyst discovery by utilizing large language models (LLMs) through a Monte Carlo Tree search (MCR) approach. The authors propose a novel prompt design algorithm and a domain-specific reward function, demonstrating their method's effectiveness on a new dataset and a public dataset, significantly outperforming existing baselines. The study aims to address the challenge of suggesting top-k catalysts for chemical reactions while generating reasoning for each candidate.

### Strengths and Weaknesses
Strengths:  
- The proposed MCR method effectively narrows the search space for prompt design, showing strong empirical performance.  
- The reward function is well-suited for the specific chemical task, enhancing the model's adaptability.  
- The paper is well-written, providing relevant background information and detailed experimental support for the proposed approach.  

Weaknesses:  
- The motivation in the Introduction lacks clarity regarding the search space and the utility of query decomposition.  
- The description of the Monte Carlo Reasoner is difficult to understand, and visual aids could enhance comprehension.  
- Experiments were limited to text-davinci-003, hindering long-term reproducibility and exploration of open models.  
- Several sections of the paper are unclear, making it challenging to follow the methodology.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation in the Introduction by explicitly defining the search space and illustrating the benefits of query decomposition with concrete examples. Additionally, including a figure to clarify the Monte Carlo Reasoner would enhance understanding. To address reproducibility concerns, we suggest conducting experiments on open models and providing more extensive experimental results across different datasets. Furthermore, we encourage the authors to clarify ambiguous sections, such as the possible "mutations" of the prompt and the initialization of the tree in Algorithm 1, to facilitate reader comprehension.