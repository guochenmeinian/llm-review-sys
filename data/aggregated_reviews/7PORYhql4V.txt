ID: 7PORYhql4V
Title: Great Minds Think Alike: The Universal Convergence Trend of Input Salience
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 4, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the behavior of neural networks by analyzing the gradient fields of learned functions across varying network depths and widths. The authors define similarity between these gradient fields using cosine similarity and observe that increasing width leads to higher similarity, suggesting two main properties: (1) the average gradient norm increases with width, and (2) the direction of the average gradient remains stable while variance decreases. Additionally, the authors explore the distribution of input saliency maps, noting that models with different capacities converge towards a shared mean direction as capacity increases, supported by empirical evidence. Furthermore, the paper presents an investigation into the population mean of \( p(f|\theta_0) \) and \( p(f|\theta_1) \), emphasizing the need for a feasible model count to approximate the population mean. The authors control the number of models used, denoted as \( M_0 \) and \( M_1 \), demonstrating that increasing these values improves the approximation. The results indicate that models with distinct initializations yield similar outcomes to those with the same initialization but different batch orders, highlighting the greater impact of initializations on stochasticity. The authors clarify their hypothesis regarding ensembles and single models, asserting that both methods converge to the same population mean.

### Strengths and Weaknesses
Strengths:
- The authors formulate interesting hypotheses based on empirical observations regarding gradient field behavior in deep neural networks, particularly the stability of gradient direction with increasing network size.
- The empirical findings related to input saliency maps are well-supported and provide valuable insights into DNN applications.
- The paper provides a thorough analysis of the population mean approximation using various model counts.
- The results are well-supported by empirical data, demonstrating the relationship between model initialization and stochasticity.
- The authors effectively clarify their hypotheses regarding ensembles and single models.

Weaknesses:
- The claims lack sufficient reasoning and relevant experiments, particularly regarding the cosine similarity measure, which could result from either changes in angle or increases in average size. The authors should separately analyze the angle between normalized averages.
- The assumption of a spherically symmetric gradient distribution is questionable and lacks justification. The authors need to provide specific observations supporting this assumption.
- The writing lacks clarity and flow, particularly in explaining theoretical calculations and assumptions. The section on the saw distribution requires significant revisions for clarity.
- The presentation of theoretical claims is noted as difficult to follow, which may hinder comprehension.
- Several minor issues, such as undefined terms and unclear figures, detract from the overall presentation.

### Suggestions for Improvement
We recommend that the authors improve the justification for their claims by conducting experiments that separately analyze the angle between normalized averages and the size of the averages. Additionally, the authors should provide empirical observations to support the assumption of a spherically symmetric gradient distribution. The writing should be revised for clarity, particularly in the theoretical sections and the discussion of the saw distribution. Furthermore, we suggest that the authors improve the presentation of their theoretical claims to enhance clarity and facilitate understanding. The authors should ensure that all figures are clearly labeled and free of typographical errors to avoid confusion. Expanding the discussion on the implications of their findings regarding initializations and batch orders could also strengthen the manuscript.