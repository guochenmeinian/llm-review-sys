ID: G3IjhUERrD
Title: CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called CoF-CoT, a modification of Chain-of-Thought (CoT) that facilitates multi-step reasoning for natural language understanding (NLU) in dialogue systems. The core concept of CoF-CoT is to prompt large language models (LLMs) to execute multiple reasoning steps in a coarse-to-fine manner, including generating abstract meaning representations and logic forms. The authors evaluate CoF-CoT on two datasets, demonstrating its superiority over previous CoT variants and analyzing the impact of structured representations and step orders on reasoning performance.

### Strengths and Weaknesses
Strengths:  
1. The idea of using CoT for multi-step reasoning in LLMs is novel and addresses an interesting problem in the NLU domain.  
2. The paper is easy to follow and provides sufficient support for its claims.  

Weaknesses:  
1. The paper lacks comprehensive content on related work regarding NLU and multi-grained NLU in dialogue systems.  
2. There is insufficient experimental analysis, particularly regarding baselines and comparisons with other multi-step prompting methods.  
3. The method's performance in zero-shot settings is unclear, and the importance of each reasoning step is not adequately addressed.  

### Suggestions for Improvement
We recommend that the authors improve the literature review to include more related work on NLU and multi-grained NLU. Additionally, the authors should strengthen the experimental analysis by incorporating full parameter tuning and comparisons with non-CoT baselines, as well as multi-step prompting methods like least-to-most and plan-and-solve prompting. It would also be beneficial to clarify how the proposed method can be implemented in zero-shot and few-shot scenarios and to analyze the performance of CoF-CoT when intermediate ground truths are not available. Finally, we suggest that the authors provide a more comprehensive comparison with existing methods and elaborate on the advantages of using different LLMs.