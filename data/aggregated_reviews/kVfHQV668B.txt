ID: kVfHQV668B
Title: Towards Efficient Pre-Trained Language Model via Feature Correlation Distillation
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 4, 7, 5, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Feature Correlation Distillation (FCD), a novel method for compressing pre-trained language models (PLMs) through knowledge distillation. FCD introduces a two-part distillation loss that incorporates both token-level and sample-level relationships between teacher and student models, utilizing a Pearson correlation-based loss function to replace traditional KL divergence and MSE losses. The authors demonstrate that FCD achieves significant performance improvements on the GLUE benchmark compared to existing methods. Additionally, the paper provides a computational complexity analysis of the proposed relation maps, with complexities of $2BN^2D$ for token-level and $2B^2ND$ for sample-level relations, alongside a memory requirement of $BN^2 + B^2N$. The authors acknowledge the need for a deeper exploration of the overall efficiency of MiniLM and MiniLMv2 during training, which is influenced by hardware configurations and hyper-parameters.

### Strengths and Weaknesses
Strengths:
- The paper provides a clear motivation for addressing alignment mismatches between teacher and student models, proposing a method that effectively models token-level and sample-level relationships.
- The approach is simple yet effective, supported by theoretical proof linking PLC, KLD, and MSE.
- The presentation is clear and the experiments are well-organized, showcasing the benefits of FCD through comprehensive ablation studies.
- FCD outperforms task-agnostic distillation methods like TinyBERT and MiniLMv2, demonstrating its effectiveness.
- The computational complexity analysis of the proposed relation maps is well-articulated, clarifying the rationale behind sample-level relation modeling.

Weaknesses:
- The rationale behind the sample-level relation loss is questionable, as comparing tokens in the same position across different sentences may not accurately reflect their relationships.
- There are inconsistencies in the experimental results compared to previous works, raising concerns about the reproducibility of baseline results.
- The paper lacks a limitation section, and the authors should clarify the impact of batch size on sample-level performance.
- The analysis primarily focuses on the relation maps, leaving uncertainty about their overall impact on the entire training process.

### Suggestions for Improvement
We recommend that the authors improve the justification for the sample-level relation loss, perhaps by providing a more intuitive explanation of its utility. Additionally, clarifying the discrepancies in experimental results, particularly regarding the GLUE benchmark and the SST-2 results, would enhance the paper's credibility. We suggest including robustness checks, such as averaging across random seeds and incorporating error bars in figures. Furthermore, adding a limitations section would provide a more balanced view of the proposed method. Lastly, we recommend including empirical data on the overall training time and memory consumption savings in the revised version to provide a more comprehensive perspective on the method's efficiency.