ID: 08hStXdT1s
Title: Knowledge Diffusion for Distillation
Conference: NeurIPS
Year: 2023
Number of Reviews: 28
Original Ratings: 5, 6, 7, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DiffKD, a novel knowledge distillation method that utilizes a diffusion model to denoise student model features, thereby reducing the gap between teacher and student models. The authors propose a lightweight diffusion model and an adaptive noise matching module to enhance efficiency and accuracy. The method is evaluated across image classification, object detection, and semantic segmentation tasks, achieving state-of-the-art results. Additionally, the authors integrate mainstream GAN methods to transform student features in the context of knowledge distillation, employing a generator with the same architecture as their diffusion model and optimizing the generated refined student features using a discriminator and adversarial loss. A comparison of various adversarial losses revealed that these GAN methods consistently underperformed relative to the DiffKD approach, which achieved the highest accuracy of 73.62% on ImageNet.

### Strengths and Weaknesses
Strengths:
- The application of a diffusion model to knowledge distillation is innovative and addresses an important issue in bridging the gap between teacher and student features.
- The method is versatile, demonstrating effectiveness across various tasks and feature types.
- The paper is well-organized and presents quantitative experiments that validate the proposed approach.
- The thorough evaluation of GAN methods provides a clear comparison with the DiffKD approach, highlighting its superior performance.

Weaknesses:
- The technical contribution is not significant enough, with the core hypothesis that student features are noisy versions of teacher features lacking rigorous theoretical analysis.
- The design of the adaptive noise matching module is inadequately explained, and the determination of initial timesteps for reverse diffusion remains unclear.
- Experimental results show only marginal improvements, particularly on strong student models, and there is insufficient discussion on the performance of alternative generative models like GANs or FLOW.
- Some hyperparameters and their implications are not clearly defined, and important details regarding the training process are missing.
- The claim that diffusion models can approximate almost all distributions may undermine the assertion that student features are merely noisy versions of teacher features, suggesting that diffusion models serve primarily as a bridge rather than a denoising solution.
- The motivation and contributions of the paper could benefit from further refinement.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of the assumption that student features are noisy versions of teacher features. Additionally, the design principle of the adaptive noise matching module should be clarified, particularly regarding the analysis of the learned Î³ for different student models. The authors should also address how to determine the initial timesteps for the student model in reverse diffusion. Furthermore, we suggest including more visualized results, such as the affinity matrix, and discussing the potential of other generative models in comparison to the diffusion model. Lastly, the authors should ensure that all relevant parameters, such as Params and FLOPs, are fully reported in the tables and refine the clarity of the central claim regarding the relationship between student and teacher features to enhance the overall impact of the paper.