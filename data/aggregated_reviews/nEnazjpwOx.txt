ID: nEnazjpwOx
Title: Diffusion Models Meet Contextual Bandits with Large Action Spaces
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 6, 8, 5, -1, -1
Original Confidences: 2, 4, 5, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents diffusion Thompson sampling, utilizing a diffusion model to enhance exploration by leveraging rewards from similar actions. The authors derive efficient posterior approximations under a diffusion model prior and establish a regret bound for linear instances. They provide a closed-form solution for linear score functions and likelihoods, while approximating posteriors with a Gaussian distribution for nonlinear cases. The work includes experimental results across various combinations of linear and nonlinear rewards and diffusion models, revealing insightful observations.

### Strengths and Weaknesses
Strengths:  
- The proof of Theorem 4.1 employs innovative techniques, including recursive total variance decomposition and refined arguments quantifying posterior information gain.  
- The paper is well-written, with clear summaries of contributions and experimental results that demonstrate the benefits of the proposed approach.  
- The theoretical analysis and experiments highlight the advantages of learning the true hierarchical model compared to traditional methods like LinTS.

Weaknesses:  
- The impact of the number of layers \( L \) on the regret bound is discussed, but the paper lacks heuristics for selecting an appropriate \( L \) and justifications for these heuristics.  
- The proposed algorithm requires offline learning of the diffusion model, which is not adequately acknowledged, raising concerns about the cost of this learning process.  
- The Bayesian regret analysis is limited to the linear-Gaussian case, and the empirical evaluation relies on synthetic data, which may not fully capture the model's performance in real-world scenarios.

### Suggestions for Improvement
We recommend that the authors improve the discussion on selecting the number of layers \( L \) by providing heuristics and justifications for these choices. Additionally, the authors should address the cost and implications of offline learning for the diffusion model more thoroughly. It would be beneficial to expand the empirical evaluation to include scenarios with misspecified models and to compare the proposed heuristic for the non-linear diffusion model against other standard estimation techniques. Finally, a more comprehensive literature review is necessary to position the work within the context of existing approaches to large action spaces in contextual bandits.