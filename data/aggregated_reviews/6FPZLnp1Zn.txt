ID: 6FPZLnp1Zn
Title: Policy Optimization for Robust Average Reward MDPs
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a policy gradient algorithm for solving unichain average reward robust MDPs, demonstrating a linear convergence rate for increasing step sizes and a $O(1/k)$ convergence rate for fixed step sizes, where $k$ is the number of iterations. The authors extend existing results for policy gradient methods in discounted robust MDPs and average reward nominal MDPs to the unichain case. 

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and clearly defines its objectives.  
- It introduces a novel approach to robust average reward MDPs, showcasing multiple theoretical contributions, including linear convergence and global optimality.  
- The clarity and quality of the writing enhance the presentation of the proofs and results.  

Weaknesses:  
- Some claims appear too far removed from the exact results proved in theorems, necessitating clarifications.  
- The definition of the bias is incorrect in general cases, only holding for aperiodic Markov chains.  
- The presentation of theoretical results lacks sharpness, with assumptions not clearly stated.  
- The authors may have overlooked relevant prior work on robust average cost MDPs, which could affect the novelty of their contributions.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding the unichain assumption by mentioning it earlier in the paper, ideally in the abstract and introduction. Additionally, please clarify the polynomial time complexity claims, ensuring that the definitions and bounds on terms like $C_{PL}$ and $M$ are explicitly provided. We suggest addressing the uniqueness of definitions and results in Lemmas 3.2 and 4.2, and including proofs for Lemma 4.3 to solidify the theoretical foundation. Furthermore, we encourage the authors to refine the terminology used in describing their algorithm, potentially referring to it as a policy gradient rather than mirror descent, given the context of the Bregman divergence employed.