ID: 7LSEkvEGCM
Title: Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 7, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 2, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new paradigm for operator learning through the lens of frame theory, introducing the concept of Representation Equivalent Neural Operators (ReNO). The authors address a critical gap in the literature regarding the balance between the continuous nature of operators and the discrete nature of the data used for training, quantifying equivalence through aliasing error. They define "representation equivalence" as requiring zero aliasing error from the mode and investigate whether several popular architectures meet this definition. The framework analyzes existing operator learning architectures to determine their alignment with ReNO, emphasizing the importance of continuous-discrete equivalence (CDE) at each layer. The paper discusses the implications of the ReNO property on model performance and generalization, arguing that traditional numerical methods may not capture high-frequency content due to inherent limitations in the data derived from numerical solutions.

### Strengths and Weaknesses
Strengths:  
- The paper is well-motivated, effectively situated within the existing literature, and well-written, making it accessible.  
- It presents an interesting application of frame theory to address aliasing error in operator learning, with rigorous proofs supporting its claims.  
- The proposed ReNO framework is well-articulated and offers a pragmatic approach to discretization, which could enhance operator learning.  
- Experimental evidence supports the claims made, particularly regarding the performance of ReNO in super-resolution tasks, demonstrating a clear understanding of the limitations of existing methods.  

Weaknesses:  
- The overall presentation requires significant improvement, particularly in clarity and accessibility, with figures lacking captions and an overuse of inline equations complicating readability.  
- The definition of ReNO is criticized as being too strong, yielding only linear methods of approximation, and the title may obscure mathematical ideas for the sake of appeal.  
- The experimental section lacks detail and clarity, with insufficient elaboration on methodologies and results, and the numerical experiments are deemed insufficient to demonstrate the practical usability of ReNO.  
- Some responses to reviewer questions lack clarity, particularly regarding the relationship between their framework and existing literature, and the paper does not propose new architectures, which may restrict its contribution to the field.  

### Suggestions for Improvement
We recommend that the authors improve the overall presentation by adding captions to all figures and reducing the number of inline equations, potentially moving some details to an appendix. Clarifying notations upon their introduction would enhance readability. The authors should also improve clarity regarding the specific problem their work addresses, as it appears to differ from existing definitions of operator learning. Additionally, we suggest revising Definition 3.4 to allow for more practical applications and to explore the trade-off between aliasing and approximation errors. The experimental section should include more comprehensive details about the methodologies and results, as well as enhance the numerical experiments by including real datasets to better demonstrate the proposed framework's benefits. Furthermore, we encourage the authors to provide clearer explanations of frame theory concepts, particularly regarding the invertibility of the frame operator and the significance of the pseudo-inverse. Finally, discussing the pros and cons of specific choices, such as wavelet frames, and explicitly addressing how their results relate to the findings of Lanthaler et al. would enrich the analysis and provide valuable insights.