ID: 7uWzoGn4kv
Title: HENASY: Learning to Assemble Scene-Entities for Interpretable Egocentric Video-Language Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 6, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HENASY, a novel framework for learning egocentric video-language models that ground texts to visual scene entities. The authors propose a dual visual encoder approach to encode video features, allowing for individual matching of nouns and verb phrases in paired text. The framework is optimized using contrastive losses and a projection loss to enhance grounding quality. Experimental results indicate that HENASY performs well across various egocentric video tasks, including video/text retrieval, action recognition, and natural language queries.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow.
2. The addressed problem is significant, as grounded and interpretable egocentric video-language models have been underexplored, potentially sparking new research interests.
3. Extensive experiments validate the proposed method across a range of egocentric video-text tasks.

Weaknesses:
1. Performance improvements are marginal compared to HelpingHands, raising concerns about the efficiency of the complex components introduced.
2. Important studies on design choices are lacking, such as the impact of using GroupViT for processing video patch tokens and alternative initialization options for the global encoder.
3. The scalability of the proposed method with larger model sizes or more training data is not discussed.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the performance comparison with HelpingHands, including error analysis to substantiate the effectiveness of their method. Additionally, the authors should include results from using GroupViT directly and clarify the predicted mask in the projection loss calculation. Addressing the scalability and computational demands of HENASY is crucial, particularly for real-world applications. Finally, we suggest providing quantitative results for visual grounding tasks to strengthen the evaluation.