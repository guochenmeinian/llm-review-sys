ID: Q0KwoyZlSo
Title: On the Complexity of Learning Sparse Functions with Statistical and Gradient Queries
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 7, 6, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the complexity of learning juntas through restricted statistical query algorithms, specifically introducing Differentiable Learning Queries (DLQ). The authors characterize the query complexity of support learning for sparse functions over product distributions, establishing that the complexity can be expressed as a function of a "leap exponent" determined by the properties of the distributions involved. The paper also contrasts adaptive and non-adaptive queries and provides insights into the performance of gradient descent algorithms in learning these functions.

### Strengths and Weaknesses
Strengths:
- The paper extends prior work by Abbe et al. to a broader context, highlighting the need for different characterizations of query complexity based on the loss function.
- It effectively distinguishes between adaptive and non-adaptive queries, offering a clear separation in query complexity.
- The rigorous writing and careful presentation of results contribute to the paper's clarity.

Weaknesses:
- The notation in the proof of Theorem 5.1(a) is complex, and a high-level explanation of the construction would enhance understanding.
- The characterization of functions with the DLQ leap exponent lacks clarity, particularly in the hypercube setting, and does not account for rotational symmetry.
- Assumption 2.1 requires further elaboration, especially regarding its implications for continuous inputs.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the notation used in Theorem 5.1(a) by providing a high-level overview of the proof's construction. Additionally, the authors should clarify the general principles governing the DLQ leap exponent and consider discussing the implications of rotational symmetry in their analysis. Furthermore, we suggest expanding the explanation of Assumption 2.1 to include its translation to conditions on link functions in continuous cases. Lastly, we encourage the authors to elaborate on the architecture of the two-layer neural network used in their experimental validation.