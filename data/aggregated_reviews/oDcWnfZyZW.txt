ID: oDcWnfZyZW
Title: Unified Off-Policy Learning to Rank: a Reinforcement Learning Perspective
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 5, 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified approach for off-policy learning to rank (LTR) by formulating the problem as a click-model agnostic Markov Decision Process (MDP). The authors propose the Click Model-Agnostic Unified Off-policy Learning to Rank (CUOLR) method, which leverages offline reinforcement learning (RL) techniques to optimize ranking processes. The method incorporates state representation learning into Conservative Q-Learning (CQL) and demonstrates improved accuracy in ranking metrics compared to baseline estimators across various datasets. Additionally, the authors provide a response that clarifies previous questions and includes new experiments deemed helpful by the reviewers.

### Strengths and Weaknesses
Strengths:
- The manuscript is easy to follow, with a clear motivation for unifying click models and introducing the offline RL framework.
- The RL formulation of LTR is reasonable, providing valuable insights for the LTR community.
- Empirical findings show that CUOLR outperforms state-of-the-art off-policy LTR algorithms, demonstrating its effectiveness and robustness.
- The response effectively addresses the reviewers' questions, and the new experiments add value to the paper.

Weaknesses:
- The proposed method (CUOLR) lacks novelty as it does not represent a fundamentally new framework for offline RL; the state representation learning appears more as an engineering effort.
- Experimental results indicate that CUOLR (CQL) does not significantly outperform simpler RL baselines like SAC, suggesting room for improvement.
- The paper makes limiting assumptions about user behavior, potentially oversimplifying the complexity of positional biases.
- No explicit weaknesses were mentioned in the response review.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the CUOLR framework by exploring more innovative approaches to state representation learning beyond positional encoding and attention. Additionally, the authors should provide a clearer justification for the competitive performance of CUOLR (CQL) and CUOLR (SAC) in the context of offline RL. We suggest addressing the limitations regarding user behavior assumptions and discussing the potential impact of logging policy coverage on the learned rankings. Lastly, clarifying the presentation of the proposed method and ensuring all relevant technical details are included would enhance the manuscript's clarity. We also recommend that the authors continue to engage with reviewers to further enhance clarity and address any remaining concerns.