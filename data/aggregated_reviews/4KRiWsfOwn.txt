ID: 4KRiWsfOwn
Title: Merging Experts into One: Improving Computational Efficiency of Mixture of Experts
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called Merging Experts into One (MEO) aimed at enhancing the computational efficiency of mixture of experts (MoE) models for natural language processing tasks. The authors propose merging the parameters of selected experts before computation, which reduces the computational cost to that of a single expert. MEO can be applied at various levels (token, sequence, task) and significantly decreases FLOPS, exemplified by a reduction from 72G for MoE to 28.6G for MEO at the sequence level. The introduction of a token-level attention mechanism further boosts efficiency and performance, with MEO outperforming vanilla MoE on the GLUE benchmark. Extensive experiments validate MEO's effectiveness compared to MoE.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and effectively conveys the method and results.
- MEO is a simple yet impactful innovation that serves as a drop-in replacement for MoE across multiple levels.
- The proposed method demonstrates significant improvements in efficiency and accuracy on the GLUE benchmark.

Weaknesses:
- The experiments are primarily limited to NLU tasks, suggesting a need for broader testing across various tasks.
- The evaluation is confined to the BERT-base model, and testing on larger models could provide additional insights.
- The method's applicability appears restricted to experts functioning as affine transforms, limiting its generalizability.

### Suggestions for Improvement
We recommend that the authors improve the breadth of their experiments by testing MEO on a wider range of tasks beyond NLU and on larger model architectures. Additionally, addressing the communication overhead of MoE and clarifying the experimental setup, including GPU usage and expert deployment, would strengthen the paper. Finally, exploring the method's applicability to experts with nonlinearities could enhance its robustness and utility.