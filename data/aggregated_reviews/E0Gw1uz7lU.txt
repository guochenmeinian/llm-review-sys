ID: E0Gw1uz7lU
Title: Federated Conditional Stochastic Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 27
Original Ratings: 7, 6, 5, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 2, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the first provably efficient federated conditional stochastic optimization solutions using a biased gradient estimator, introducing three variants: FCSG, FCSG-M (with momentum), and Acc-FCSG-M (with variance reduction). The authors analyze the sample and communication complexities of all three algorithms, with Acc-FCSG-M achieving the lower bound of sample complexity for single-machine counterparts. Additionally, the paper introduces an algorithm for solving Federated AUPRC maximization, reformulated based on existing literature. The authors present a two-level problem where positive datasets are sampled to estimate score ranks, demonstrating that their approach can significantly enhance model performance in federated learning (FL) settings. Empirical performance is demonstrated on robust logistic regression and MAML tasks, as well as on the MNIST and CIFAR-10 datasets, with results indicating that their algorithms outperform existing baselines, such as CODA+ and FedAvg with cross-entropy loss.

### Strengths and Weaknesses
Strengths:  
- The research pioneers the study of nonconvex conditional stochastic optimization (CSO) within the federated learning framework, presenting a comprehensive analysis of sample and communication complexities.  
- The proposed accelerated algorithm matches the lower bound of sample complexity, and the writing is generally clear and well-structured.  
- The convergence analysis is extensive, and the paper is accessible to those unfamiliar with stochastic conditional optimization problems.  
- The inclusion of additional experiments on online AUPRC maximization enhances the motivation for the proposed method, demonstrating significant improvements in model performance across datasets.  
- The authors have addressed previous concerns and clarified technical challenges effectively.

Weaknesses:  
- The introduction lacks a strong motivation for federated CSO, failing to connect it with specific real-world problems that necessitate its use.  
- The evaluation assumes multiple tasks can be sampled at clients, which may not reflect practical federated learning settings; experiments on the ‘one task per client’ scenario are missing.  
- The algorithms assume all clients participate in each communication round, which is impractical in typical cross-device settings.  
- The paper contains grammatical errors and lacks comparisons with naive baselines on the MAML problem, limiting insights into the proposed method's contribution.  
- The convergence results do not provide final accuracy/loss metrics, making claims of outperforming baselines appear premature.  
- The notation in the proof may still require further refinement for clarity, and the assumptions related to variance reduction methods may be perceived as incremental contributions rather than novel advancements.

### Suggestions for Improvement
We recommend that the authors improve the introduction by providing stronger motivation for federated conditional stochastic optimization (CSO) and incorporating specific real-world examples that highlight its relevance. Additionally, consider evaluating the proposed algorithms in the ‘one task per client’ setting to strengthen the contribution. It would be beneficial to clarify the assumptions regarding client participation in communication rounds and address the grammatical issues throughout the paper. Including naive baselines for comparison on the MAML problem would enhance the understanding of the proposed methods' effectiveness. Finally, we suggest providing final accuracy/loss metrics alongside convergence results to substantiate claims of superior performance. Furthermore, we recommend improving the clarity of the notation in the proof and providing a more detailed discussion on the implications of the assumptions made regarding variance reduction methods to strengthen the perceived novelty of the contributions.