ID: 8SDsff42Lj
Title: Continual Learning with Global Prototypes: Beyond the Scope of Task Supervision
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 3, 4, 6, 5, 5, -1, -1
Original Confidences: 4, 4, 4, 3, 2, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on continual learning in NLP, focusing on catastrophic forgetting and proposing a method called NeiAttn, which utilizes global prototypes to establish relationships between task-specific data representations. The authors argue that misalignment between knowledge from observed tasks and future tasks contributes to catastrophic forgetting. Experimental results demonstrate that models effectively learning data representations related to global prototypes experience less forgetting, with NeiAttn outperforming baseline methods in both task-incremental and class-incremental learning settings.

### Strengths and Weaknesses
Strengths:
1. The motivation for the proposed method is strong, addressing a core issue in continual learning related to the misalignment of knowledge.
2. The consideration of pre-trained language models is significant, as this perspective is often overlooked in continual learning literature.
3. Experimental results validate the effectiveness of NeiAttn.

Weaknesses:
1. The clarity of the proposed method's writing is lacking, particularly regarding the core elements in Equation 9. A clearer overview, possibly through pseudo code or illustrations, would enhance understanding.
2. NeiAttn's marginal improvement over Prompt Tuning raises questions about the potential application of the training objectives from Equations 3 and 4 within the Prompt Tuning framework for better results.
3. The ablation results in Appendix F indicate that fewer neighbor attention layers yield better continual learning results, prompting questions about the overall benefit of using Neighbor Attention.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proposed method by providing a more structured overview, possibly through pseudo code or illustrations. Additionally, we suggest exploring the application of the training objectives from Equations 3 and 4 within the Prompt Tuning framework to potentially enhance performance. Lastly, we encourage the authors to clarify the benefits of Neighbor Attention in light of the ablation results and to address the implications of the selection of hyperparameters such as $\alpha_\tau$ and K, providing mathematical reasoning for their choices.