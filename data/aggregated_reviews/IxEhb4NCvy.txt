ID: IxEhb4NCvy
Title: SSDM: Scalable Speech Dysfluency Modeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 9, 3, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SSDM (Scalable Speech Dysfluency Modeling), which addresses the challenges of speech dysfluency modeling by proposing a cascade of modeling approaches that leverage representation learning, TTS, ASR, and self-supervised modeling. The authors introduce a synthetic dataset, Libri-Dys, and a connectionist subsequence aligner (CSA) to enhance dysfluency detection and alignment. Additionally, the authors propose a novel approach to automating the diagnosis of speech disorders using AI, which aims to reduce the high costs and accessibility issues associated with traditional treatments. The methodology involves processing dysfluent speech to produce detailed transcriptions that include dysfluencies, utilizing gestural scores derived from articulatory kinematics data and the CSA algorithm. The SSDM model demonstrates state-of-the-art performance across various datasets, including VCTK++, Libri-Dys, and nfvPPA, while the creation of the Libridys dataset provides a substantial resource for training and evaluation.

### Strengths and Weaknesses
Strengths:
- The paper tackles an important problem with significant potential impact, achieving strong results on relevant benchmarks.
- The innovative use of synthetic data addresses the scarcity of annotated datasets, benefiting from expert knowledge in phonetics.
- The combination of articulatory gestures, CSA, and a large-scale dataset contributes to a holistic solution for dysfluency modeling.
- The proposed AI solution has the potential to significantly reduce costs and time associated with speech disorder diagnosis.
- The use of gestural scores and CSA demonstrates innovative approaches to capturing speech dysfluencies.
- The paper is well-structured, with appendices providing detailed information without necessitating reference for comprehension.

Weaknesses:
- The model's training on 100% synthetic data raises concerns about overfitting to the specific TTS model used for Libri-Dys.
- The LLM component appears under-utilized, and its potential overfitting to the dataset's answer patterns warrants further investigation.
- There are concerns regarding the technical correctness of the evaluation, particularly in how timestamps for disfluencies are reported and utilized.
- The writing lacks clarity in some areas, making it difficult to understand the overall system and the rationale behind certain design choices, especially regarding the output format of the model and the representation of dysfluency annotations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in explaining the system's components and their interconnections. Specifically, provide a detailed overview of the training and inference processes, and clarify the differences between the solid and dashed lines in Figure 2. Additionally, ensure that the distinction between region-based outputs and single timestamps is clearly articulated to avoid confusion. We suggest including a comparison with traditional methods to evaluate the necessity of the LLM component and addressing the potential for leveraging SSDM for weak labeling of real-world data. Furthermore, clarify the evaluation metrics used, particularly regarding the human correction process and the computation of the MS score in relation to intersection-over-union (IoU) ratios. Lastly, ensure that all technical terms, such as dPER, are adequately defined within the main text.