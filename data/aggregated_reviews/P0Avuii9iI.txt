ID: P0Avuii9iI
Title: Differentially Private Image Classification by Learning Priors from Random Processes
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 6, 6, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for synthesizing pre-training data for private fine-tuning without requiring access to real data, leveraging properties of natural images. The authors propose a two-stage private learning framework, where the first stage tunes only the linear classifier and the second stage tunes all parameters. Additionally, the paper explores improving privacy-utility trade-offs in DP-SGD through a three-phase approach that learns priors from synthetic images, trains a head classifier, and then fine-tunes both the feature extractor and classifier. Empirical evaluations demonstrate significant improvements in accuracy across various datasets.

### Strengths and Weaknesses
Strengths:
1. The concept of generating 'free' pre-training data is intriguing, and the methods employed appear reasonable.
2. Experiments on vision datasets show the synthesized data's importance in private learning, especially with small privacy budgets.
3. The introduction of a two-stage approach for private learning is novel and yields improvements.
4. The paper is well-written, with clear organization and easy-to-follow results.

Weaknesses:
1. The potential for the two-stage approach to improve baseline methods remains unclear, particularly regarding its comparison with existing methods like HÃ¶lzl et al. [27].
2. The paper could benefit from a more detailed discussion on the motivation behind the two-stage approach, especially in relation to Kumar et al. [35].
3. The datasets used are relatively simple, and the method may not scale well with increasing class numbers, as indicated by the ImageNet experiments.
4. There is a lack of comparison with methodologies leveraging public non-synthetic data, which could highlight the gap between synthetic and real data.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the algorithm's explanation, particularly for Phase 1, and provide more details on the version of DP-SGD used, batch size, and momentum settings. Additionally, including comparisons with methods using public datasets for pre-training would strengthen the paper. We suggest providing further evidence for the importance of combining Phases 2 and 3, possibly by illustrating their performance in the same plot. Clarifying the privacy splitting strategy and demonstrating its effects for larger epsilon values would also enhance the paper's robustness. Lastly, consider addressing the potential application of this approach beyond image classification and including error bars in the plots for better interpretability.