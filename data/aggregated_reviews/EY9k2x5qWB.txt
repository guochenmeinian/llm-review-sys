ID: EY9k2x5qWB
Title: KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents KRLS, an RL-based method designed to enhance model-generated responses in Task-Oriented Dialogue (TOD) systems by reinforcing keyword generation. The authors introduce a token-wise reward function derived from a supervised fine-tuned decoder model, which scales keyword probabilities to ensure their prominence in generated responses. Additionally, the paper proposes a next-word sampling approach that utilizes a supervised language model to expedite the sampling process. The authors evaluate KRLS against established TOD metrics, including BLEU and success rates, and conduct human evaluations, demonstrating superior performance and faster generation compared to standard RL methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, with a clear motivation for focusing on keywords in responses.
- The proposed method is straightforward and practical, supported by solid experimental results and human evaluations.
- The use of a fine-grained reward function is a valuable contribution that aligns with the analysis of key information in dialogues.

Weaknesses:
- The necessity of the proposed method is questioned due to only marginal improvements over baselines and concerns about the fluency of generated responses.
- The explanation of the reward model is unclear, particularly regarding the calculation of rewards when generated and gold responses differ.
- The next-word sampling method lacks novelty, as it relies on inherent transformer capabilities.
- Confusion exists regarding the definitions and roles of the baselines, particularly in the context of "finetune + KRLS."

### Suggestions for Improvement
We recommend that the authors improve the clarity of the Reward Model section, specifically detailing how rewards are calculated when xgen does not equal xgold. Additionally, the authors should address the necessity of their method by providing a more thorough justification for its advantages over existing approaches, particularly concerning response fluency. Clarifying the distinctions between the various baselines and ensuring that comparisons in Table 2 account for the time required to fine-tune the model would also enhance the paper's rigor. Finally, we suggest that the authors elaborate on the implications of their findings in relation to the fluency of generated responses.