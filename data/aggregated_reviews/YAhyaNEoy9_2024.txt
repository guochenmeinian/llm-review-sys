ID: YAhyaNEoy9
Title: Auto-Enhance: Towards a Meta-Benchmark to Evaluate AI Agents' Ability to Improve Other Agents
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 4, 6
Original Confidences: 3, 4, 4

Aggregated Review:
### Key Points
This paper presents a novel "meta-benchmark" problem where a top-level agent aims to enhance the performance of reference agents across various benchmarks. The authors propose a framework for evaluating how LLM agents can improve other LLM agents on a range of tasks, both simple and complex, particularly in software engineering. The work is expected to inspire new methods for evaluating multi-agent collaboration.

### Strengths and Weaknesses
Strengths:  
- The concept of a meta-benchmark for assessing agents' self-improvement is timely and innovative.  
- The evaluation framework benefits from the inclusion of multiple domains, such as software engineering, cybersecurity, and ML experimentation.  

Weaknesses:  
- The reliance on manually defined milestones introduces subjectivity into the evaluation process.  
- The experiments reveal significant fragility and limitations in the agents' planning and execution, which diminishes the overall impact of the findings.  
- Variances and statistics of improved performance are not quantified in a step-by-step manner.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 1 by adding relevant content to indicate whether the results obtained by the benchmark test are significantly different. Additionally, we suggest addressing the issues noted in Line 132 regarding agents' hallucinations and clarifying the parameters relevant to LLM agents. For Lines 144 and 145, it is crucial to rectify the use of a non-existent column in the datasets and to implement "hyperparameter-ization." Furthermore, we encourage the authors to propose strategies to enhance the speed and efficiency of error identification and correction, as noted in Line 165. Lastly, we recommend expanding the library of included tasks and providing specific changes to achieve a more granular and robust expression of results, as discussed in Lines 181-183.