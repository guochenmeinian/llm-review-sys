ID: eFrdRuyHR9
Title: Transition Constrained Bayesian Optimization via Markov Decision Processes
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 6, 3, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extension of Bayesian optimization to incorporate transition constraints modeled through state dynamics, framing the problem as an optimal control challenge. The authors propose a novel acquisition function that serves as a cost metric in planning scenarios, solved via the Frank-Wolfe algorithm. The effectiveness of the approach is evaluated through various benchmark tasks, demonstrating its applicability in optimizing black-box functions with transition constraints. Additionally, the authors propose a method for optimizing policies in reinforcement learning by focusing on the utility of trajectories, arguing that taking the expectation of utility over trajectories is intractable. They advocate for a formulation where the expectation is embedded within the utility function and clarify the concept of "expected trajectory" and its relation to empirical distributions.

### Strengths and Weaknesses
Strengths:
- The paper introduces a well-structured framework for optimizing complex black-box functions with transition constraints, showcasing practical applications.
- The solution strategy utilizing the Frank-Wolfe algorithm is innovative and contributes significantly to the field.
- The authors provide a clear rationale for their approach to optimizing policies in reinforcement learning, emphasizing the tractability of their method compared to traditional expectations.
- The paper builds on established literature, grounding its methodology in previous research, which enhances its credibility.

Weaknesses:
- The presentation of the generative model is unclear, particularly regarding the derivation and introduction of actions, leading to confusion in notation.
- The background section lacks clarity in distinguishing the paper's contributions from existing literature.
- Important related concepts, such as Bayesian Reinforcement Learning, are not adequately discussed.
- The analysis of the chosen acquisition function is insufficient, and the empirical results do not convincingly outperform existing methods.
- The conceptualization of the "expected trajectory" is inadequately addressed, leading to potential confusion.
- There are concerns regarding the clarity of the derivation of the objective and its implications for the utility function, particularly in relation to empirical distributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the generative model derivation and formally introduce actions to enhance understanding. The background section should clearly delineate the paper's contributions from prior work, and the authors should include a discussion of Bayesian Reinforcement Learning. Additionally, a more thorough analysis of the acquisition function's selection and its performance compared to alternatives would strengthen the paper. We also suggest that the authors improve the clarity of the "expected trajectory" concept by explicitly defining it and its relevance to the utility function. Furthermore, we encourage the authors to simplify the derivation of the objective to avoid unnecessary complications and ensure that the relationship between $\mathcal{U}(d_\pi)$ and the expression derived from Eq. (4) is clearly articulated, particularly in light of the noise term in $V$. Lastly, addressing the minor typos noted in the reviews will enhance the overall quality of the paper.