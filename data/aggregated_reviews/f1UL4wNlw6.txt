ID: f1UL4wNlw6
Title: The Art of Saying No: Contextual Noncompliance in Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 4, 9, 7, 7, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive approach to understanding non-compliance in generative language models (LLMs) by proposing a taxonomy that categorizes various forms of non-compliance beyond standard safety concerns. The authors develop a dataset, CoCoNot, which includes prompts for both appropriate and inappropriate non-compliance responses. They benchmark several generative models against this dataset and evaluate the effectiveness of supervised finetuning and direct preference optimization in enhancing contextual non-compliance.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, logically organized, and clearly motivates the need for nuanced non-compliance in LLMs.
- The CoCoNot dataset is well-documented and provides a holistic measurement of model performance across various non-compliance categories.
- The evaluation of multiple LLMs highlights the current state of model compliance and the need for further research in this area.

Weaknesses:
- The taxonomy appears largely incremental, with some categories overlapping with existing literature on safety concerns.
- Certain sections, particularly Section 5, yield standard conclusions that may not significantly advance the field.
- Some definitions within the taxonomy, such as "False presuppositions" and "False information," are underdefined, leading to ambiguity.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the taxonomy by identifying new interactions that necessitate different types of contextual non-compliance. Additionally, we urge the authors to clarify the optimality of the system prompt used in their experiments, as the current reference to Figure 4 is non-functional. We suggest visualizing Table 2 as a figure for better clarity, and including a histogram of compliance rate deltas to enhance the presentation of results. Furthermore, we encourage the authors to acknowledge the non-exhaustiveness of the taxonomy and consider the implications of using a GPT model in constructing synthetic data. Lastly, we recommend separating results for models with and without instruction tuning in the benchmark section for clearer analysis.