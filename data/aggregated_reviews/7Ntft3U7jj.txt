ID: 7Ntft3U7jj
Title: Uncovering the Redundancy in Graph Self-supervised Learning Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 6, 7, 5, -1
Original Confidences: 4, 4, 4, 4, -1

Aggregated Review:
### Key Points
This paper presents new insights into the redundancy of graph self-supervised learning models, revealing that parameters and learned representations are highly redundant. The authors propose a novel pre-training and fine-tuning paradigm, SLIDE, which achieves superior performance with fewer parameters. Comprehensive experiments demonstrate that even random parameter reductions do not significantly affect performance, highlighting the redundancy issue.

### Strengths and Weaknesses
Strengths:
- The paper deepens understanding of graph self-supervised learning models, aligning with current research and uncovering important discoveries regarding redundancy at neuron and layer levels.
- It is well-organized and clearly written, making it enjoyable to read.
- The experiments are well-designed and executed, showcasing the robustness of the proposed model.

Weaknesses:
- There is insufficient explanation of empirical results, particularly regarding why SLIDE outperforms fine-tuning despite reducing parameters.
- The rationale for using Random Fourier Features (RFF) is not adequately addressed, and the significance of Equation 2 is unclear.
- The analysis of learned weights \( W \) is lacking, which is crucial for understanding model dynamics.
- The study focuses solely on node classification, neglecting other tasks like link prediction and graph classification, which could strengthen the findings.
- The motivation for "de-correlating the learned embeddings" in SLIDE is unclear, and the comparison with "full fine-tune" lacks fairness without including "linear probing."

### Suggestions for Improvement
We recommend that the authors improve the explanation of empirical results, particularly clarifying the performance of SLIDE compared to fine-tuning. Additionally, providing a detailed rationale for the use of Random Fourier Features and discussing the significance of Equation 2 would enhance clarity. An analysis of the learned weights \( W \) should be included to offer insights into model interpretability. Expanding the study to include other tasks beyond node classification would solidify the findings. Finally, we suggest a more equitable comparison in the experiments by including "linear probing" to evaluate the performance of SLIDE against other methods.