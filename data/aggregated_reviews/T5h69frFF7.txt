ID: T5h69frFF7
Title: UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 7, 6, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an innovative approach to unsupervised neural speech separation, named UNSSOR, which transforms an ill-posed problem into a well-posed one by leveraging conditions where the number of microphones exceeds the number of speakers. The authors propose a method that utilizes a complex-valued deep neural network (TF-GridNet) to estimate individual sources at a reference microphone, addressing the under-determined problem by incorporating physical constraints. Key contributions include establishing a linear-filter constraint between each speaker's reverberant images at microphone pairs, devising loss functions inspired by blind deconvolution, and addressing frequency permutation issues through an intra-source magnitude scattering loss term. The algorithm computes relative room impulse responses (RIRs) in closed form using a least squares optimization problem, relying solely on observable mixtures. The authors claim that UNSSOR can be trained for under-determined separation, such as monaural unsupervised speech separation, based on over-determined training mixtures. The method is tested in a monoaural unsupervised setting and shows promising results on the SMS-WSJ dataset, approaching the performance of supervised models.

### Strengths and Weaknesses
Strengths:  
- The authors propose a novel method for unsupervised speech separation, presenting a new perspective compared to existing methods.  
- The approach of reducing equations using physical constraints is well-structured and theoretically sound, leading to a robust unsupervised multi-mixture speech source separation algorithm.  
- Innovative loss functions, including the intra-source magnitude scattering regularizer ($L_{ISMS}$), guide the model towards desired sound objects and demonstrate strong empirical results across various source separation tasks.  
- The method achieves competitive performance metrics (SDR, SI-SDR, PESQ, eSTOI) on SMS-WSJ compared to other unsupervised multi-mic algorithms and a supervised baseline.  

Weaknesses:  
- The evaluation is limited to the SMS-WSJ dataset, which consists of synthetic mixtures, raising concerns about generalization to real-world scenarios.  
- UNSSOR shows a significant performance gap (~4 dB) compared to the supervised Monaural PIT baseline, raising questions about the advantages of unsupervised methods.  
- The MC loss function is referenced but not clearly defined in the paper; clarification on its innovation is needed.  
- The authors do not compare UNSSOR with other unsupervised baselines like MixIT, which would strengthen claims of superiority.  
- The rationale for choosing unsupervised methods over supervised ones remains unclear, necessitating experimental evidence of benefits as dataset sizes increase.  
- The effectiveness of UNSSOR may be conflated with the performance of the TF-GridNet architecture; comparisons with other separation structures are necessary.  
- The absence of comparisons with varying STFT settings raises questions about the comprehensiveness and fairness of the results.  

### Suggestions for Improvement
We recommend that the authors improve the dataset evaluation by including real-world mixtures to better assess the generalization of UNSSOR. Clarifying the MC loss function's innovation and its relation to reference [1] is essential. To enhance the robustness of their findings, the authors should compare UNSSOR with various separation structures beyond TF-GridNet and include MixIT in the comparisons. Additionally, standardizing STFT settings would strengthen the validity of their results. We also suggest improving the clarity of terminology by using "metric" instead of "measure" on line 65 and replacing "Hermittan" with "Hermitian" on line 120. The authors should clarify the definition of mixture consistency on line 146, referencing existing literature, and provide a brief explanation of the rationale behind matching FCP-estimated images with the mixture in Eq. (6) on line 161. Clarification is needed regarding the $\xi$ hyperparameter on line 167 and the rationale for non-causal filtering on line 179. The interpretation of $Z(c)$ as a virtual microphone estimate should be elaborated on line 193. Finally, the authors should clarify the matching process in the mixture consistency loss when using only one input in the monoaural setting on line 232. Addressing these points will enhance the paper's clarity and rigor.