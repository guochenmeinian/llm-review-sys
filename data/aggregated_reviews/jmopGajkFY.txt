ID: jmopGajkFY
Title: MEGA: Multilingual Evaluation of Generative AI
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive evaluation of Generative LLMs across various multilingual tasks and languages, specifically addressing three research questions: (1) how LLMs perform on multilingual benchmarks compared to fine-tuned SOTA models, (2) the performance trends of these models across languages, and (3) effective prompting strategies for non-English languages. The authors utilize 16 tasks and 70 languages, revealing that fine-tuned models generally outperform LLMs, particularly in low-resource settings, although prompting techniques like translate-test prompting can mitigate this gap.

### Strengths and Weaknesses
Strengths:
- The paper tackles a crucial issue of evaluating LLMs and presents thorough analyses across a diverse set of languages and models.
- It offers valuable insights into the performance of LLMs, particularly regarding data contamination and the effects of different prompting strategies.
- The experimental design is robust, with careful tuning of prompts and a wide range of tasks.

Weaknesses:
- The paper lacks a comparative analysis with other language-specific pre-trained models, which could enhance the findings.
- Certain sections, especially the experimental writing, are convoluted and lack clarity, potentially hindering reproducibility.
- The connection between the research questions and the conclusions drawn is not clearly articulated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental section by addressing convoluted writing and ensuring all necessary details for reproducibility are included, such as the number of runs and parameters used for fine-tuning. Additionally, we suggest that the authors investigate the fourth research question regarding the internal use of English, as it could significantly impact their findings. Furthermore, enhancing the statistical analyses to systematically explore discrepancies in results and considering the influence of writing systems would strengthen the conclusions. Lastly, we advise correcting the size of Figure 1 for better readability and ensuring consistent bar-order in histograms for easier comparison.