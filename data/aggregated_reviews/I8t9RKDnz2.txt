ID: I8t9RKDnz2
Title: State Regularized Policy Optimization on Data with Dynamics Shift
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 6, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a context-aware method called SRPO (State Regularized Policy Optimization) aimed at enhancing the training of reinforcement learning (RL) algorithms in environments with dynamics shifts. The authors exploit the property of similar stationary state distributions across environments with analogous structures but differing dynamics, using this to regularize policies and improve data efficiency. The paper includes a theoretical guarantee on the performance of policies regularized by optimal state distributions from other dynamics, supported by experimental results demonstrating SRPO's effectiveness in both online and offline settings.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and easy to follow, with rigorous experimental validations and insightful theoretical analyses.
- SRPO effectively addresses the challenge of training RL algorithms on data with diverse dynamics shifts, a common issue in real-world applications.
- The authors leverage the observation that similar environments yield similar optimal stationary state distributions, facilitating efficient data reuse and improved performance.
- The theoretical contribution includes a lower-bound performance guarantee for policies regularized by optimal state distributions.

Weaknesses:
- The theoretical foundation regarding the similarity of optimal stationary state distributions across different dynamics requires further evidence to establish its generalizability.
- Confusion exists around the sample-based surrogate mechanism for estimating state density ratios, particularly regarding the limitations of $D_{\text{fake}}$ in capturing the stationary state distribution $d_\pi$.
- The paper lacks comprehensive analyses comparing SRPO with other regularization methods, particularly in explaining its advantages over DARC-style algorithms.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the assertion that environments with similar structures inherently share similar optimal stationary state distributions, as this claim may not universally hold. Additionally, further clarification is needed regarding the limitations of $D_{\text{fake}}$ in accurately representing $d_\pi$. We suggest providing more extensive analyses comparing SRPO with DARC-style regularization, detailing the specific scenarios where SRPO demonstrates superior performance. Furthermore, including a list of implementation details related to discriminator training would enhance the clarity of the training process. Lastly, addressing potential failure cases and the limitations of the assumptions made in the paper would strengthen the overall contribution.