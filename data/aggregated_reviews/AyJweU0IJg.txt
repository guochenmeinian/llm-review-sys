ID: AyJweU0IJg
Title: Mode Collapse in Variational Deep Gaussian Processes
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 6
Original Confidences: 4, 3

Aggregated Review:
### Key Points
This paper presents an investigation into mode collapse in Variational Deep Gaussian Processes (DGPs) and proposes a new initialization strategy for variational parameters to mitigate this issue. It identifies limitations in current approaches, particularly regarding identity mean functions, and provides empirical evidence through experiments on toy datasets and UCI benchmarks. The authors also offer insights into the whitening effect during optimization, contributing original theoretical explanations to the field.

### Strengths and Weaknesses
Strengths:
- The paper addresses a relevant problem in DGPs and proposes a credible solution, supported by experimental validation.
- It is well-structured and clearly articulates its objectives, making it accessible to readers.
- The insights into the identity mean function and whitening effects are valuable for practitioners.

Weaknesses:
- The paper may require a solid understanding of Gaussian Processes and variational inference, limiting its accessibility.
- The dependence on the choice of initial kernel parameters could pose a risk if not managed properly.
- It lacks a robust theoretical framework, relying on heuristic explanations, and the experiments are primarily limited to toy datasets, necessitating further testing on larger, real-world datasets.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework to provide a more rigorous analysis of the proposed approach. Additionally, expanding the experimental setup to include more real-world datasets and scenarios would enhance the robustness and applicability of the findings.