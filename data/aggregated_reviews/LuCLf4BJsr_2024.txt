ID: LuCLf4BJsr
Title: Chain of Agents: Large Language Models Collaborating on Long-Context Tasks
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework called "Chain-of-Agents" (CoA) aimed at enhancing the long-context handling capabilities of large language models (LLMs). CoA employs a multi-agent system where worker agents sequentially process different segments of input text, while a manager agent synthesizes these contributions into a coherent output. The authors conducted experiments across various long-context tasks, demonstrating significant performance improvements over existing methods, including Retrieval-Augmented Generation (RAG) and full-context models. Additionally, the authors illustrate how CoA allows for adjacent workers to communicate, effectively addressing long dependency issues without the need for question decomposition. They assert that their unique contribution lies in applying chain communication to long input tasks, distinguishing their work from existing models like RecurrentGPT, which focuses on long output.

### Strengths and Weaknesses
Strengths:
- The Chain-of-Agents method is an interesting and experimentally validated approach.
- CoA is versatile, requiring no task-specific training, and shows significant performance improvements across multiple long-context tasks.
- The writing and presentation are clear, making the paper easy to follow.
- The effectiveness of CoA is demonstrated through comparative results, showing a significant performance improvement over LongAgent.
- The authors are open to integrating their approach with existing models, indicating a collaborative spirit in advancing the field.

Weaknesses:
- The novelty of the submission is questionable, as breaking long texts into chunks and processing them sequentially is a well-established practice. The authors do not sufficiently discuss similarities with existing works like RecurrentGPT and LongAgent.
- Some components of the proposed method are noted as common practices in multi-agent system design, which may diminish the perceived novelty.
- The experimental baselines are relatively weak, limiting the ability to assess the proposed method's advantages over stronger existing baselines.
- The paper lacks a comprehensive exploration of effective collaboration methods among agents in multi-chunk processing, which could enhance its novelty.
- The lack of open-source code for LongAgent limits comprehensive benchmarking and comparison.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the novelty of their work by providing a more thorough comparison with existing methods, particularly LongAgent and RecurrentGPT. Additionally, the authors should consider testing their framework against stronger baselines to better demonstrate its effectiveness. Improving the clarity of how CoA differentiates itself from existing methods, particularly regarding the decomposition of input questions, would also be beneficial. Exploring the most effective methods of collaboration in multi-chunk processing could add significant technical depth to the paper. Furthermore, clarifying the chunk processing methodology and addressing potential information loss during sequential processing would strengthen the paper's contributions. Lastly, we suggest including a more detailed discussion on the communication mechanisms between agents and their implications for performance, as well as providing open-source code to facilitate comprehensive benchmarking.