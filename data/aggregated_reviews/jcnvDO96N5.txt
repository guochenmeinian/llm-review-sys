ID: jcnvDO96N5
Title: MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 5, 7, 7, 5, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new optimization algorithm for deep neural networks, named MKOR, which builds upon the Kronecker-factored curvature (K-FAC) algorithm. The authors propose a rank-one approximation for the covariance matrices of activations and gradients, enhancing the efficiency of Hessian calculations and enabling more frequent updates. Experimental results indicate significant speed improvements over existing first and second-order optimization algorithms, particularly in the context of large language models.

### Strengths and Weaknesses
Strengths:
1. The efficient second-order optimization algorithm is crucial for training deep neural networks.
2. Experimental results demonstrate notable speed-ups compared to baseline algorithms.
3. The paper is generally well-written, with a clear main contribution.
4. The method effectively addresses the applicability of second-order optimizers for large language models, supported by sound experiments.

Weaknesses:
1. Insufficient justification for the rank-1 approximation's impact on accuracy and convergence.
2. Lack of theoretical support for why rank-1 suffices for Fisher information approximation.
3. Missing important baseline comparisons, such as with Eva, which shares similarities with MKOR.
4. Unclear methodology regarding the switching mechanism between MKOR and first-order optimizers, as well as the choice of hyperparameters like $\zeta$.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the rank-1 approximation, addressing its accuracy and convergence implications. Additionally, the authors should clarify the differences between MKOR and Eva, emphasizing MKOR's advantages. An analysis of how to choose and dynamically adjust $\zeta$ for optimal performance should be included. Furthermore, the methodology for switching between MKOR and first-order optimizers needs clearer articulation, particularly regarding loss measurement and update frequencies. Finally, exploring the potential of MKOR in scenarios with high update frequencies, as seen in KAISA, could enhance the paper's contributions.