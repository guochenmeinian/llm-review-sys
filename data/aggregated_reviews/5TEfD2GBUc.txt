ID: 5TEfD2GBUc
Title: FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FANToM, a benchmark designed to evaluate Theory of Mind (ToM) in natural conversations, focusing on asymmetric information scenarios. The authors aim to assess models' understanding of characters' beliefs and information possession, demonstrating that existing language models (LLMs) significantly underperform compared to human capabilities. The benchmark consists of 256 conversations and approximately 6,000 questions, with initial evaluations revealing insights into model performance and the identification of illusory ToM.

### Strengths and Weaknesses
Strengths:
- The benchmark is a novel and timely contribution to the evaluation of LLMs regarding ToM in conversation settings.
- It is well-designed, grounded in key ToM principles, and provides empirical results that highlight the shortcomings of current models.
- The authors' intention to release the benchmark for general use is a significant advantage for the research community.

Weaknesses:
- The lack of immediate availability of the benchmark raises concerns about its practical utility.
- The construction of the dataset relies on GPT-based models, which may introduce biases that are not adequately addressed.
- The human validation process lacks transparency regarding guidelines, annotator agreement, and evaluation criteria, raising questions about the benchmark's quality.
- The focus on information-asymmetric conversations is not sufficiently justified, and the distinction between InfoAccess and Answerability is unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the benchmark's release timeline and licensing details. It is essential to provide either the benchmark or the code for its construction and evaluation to facilitate reproducibility. Additionally, the authors should elaborate on the human validation process, including the number of annotators and their criteria, to enhance the benchmark's credibility. Addressing potential anthropomorphism earlier in the paper would help contextualize the benchmark's usage. Furthermore, we suggest clarifying the differences between InfoAccess and Answerability, as well as justifying the choice of task settings for measuring information asymmetry. Lastly, including a comprehensive results table and discussing the implications of lexical overlap would strengthen the analysis.