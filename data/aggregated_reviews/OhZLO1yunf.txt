ID: OhZLO1yunf
Title: NEWTON: Are Large Language Models Capable of Physical Reasoning?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new benchmark called Newton, aimed at systematically studying physical reasoning in large language models (LLMs). The dataset comprises 700 objects across 8 physical attributes and includes 160K pre-generated questions. The creation process involves annotation, filtering, template generation, and evaluation of existing LLMs. The questions focus on foundational attribute understanding, explicit application, and implicit scenario-based analysis. The authors conclude that LLMs struggle with physical concepts and decision-making in implicit scenarios, though fine-tuning can enhance performance.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with a clear motivation and structured experiments.
- The systematic approach to studying physical reasoning is commendable, and the dataset is significantly larger than existing benchmarks.
- Ablation studies strengthen the arguments, and fine-tuning models on the dataset is a notable effort.

Weaknesses:
- The paper lacks discussion on the relationship between physical reasoning and "Embodied Question Answering (EQA)" tasks, which could provide valuable context.
- Questions can be ambiguous without specific instances of objects, as different materials may lead to varied physical properties.
- The metrics used for evaluation are difficult to understand, raising concerns about the clarity of results.
- The benchmark may not account for richer object descriptions that could influence reasoning.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the relationship between physical reasoning and EQA tasks to clarify the contributions of their work. Additionally, conducting experiments to explore whether prompting can enhance LLM performance would be beneficial. The authors should also consider addressing the ambiguity in questions by providing specific object instances and clarifying the evaluation metrics for better comprehension. Lastly, extending the benchmark to include reasoning based on richer object descriptions could enhance its applicability.