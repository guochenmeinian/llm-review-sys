ID: gI11vXg1W4
Title: PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Pluggable Reward-driven Contextual Adapter (PRCA) designed to enhance retrieval question answering by treating Large Language Models (LLMs) as black boxes. The main contributions include: 1) enabling the use of LLMs without fine-tuning, 2) providing insights into LLM usage through reinforcement learning, and 3) demonstrating comprehensive performance improvements across three QA datasets. The authors propose a two-stage training process where the first stage extracts context using a BART-large model, and the second stage optimizes PRCA parameters based on ROUGE-L scores.

### Strengths and Weaknesses
Strengths:
- Novel method leveraging LLMs in a retrieval framework without parameter access.
- Sensible pipeline design with effective context extraction and reward training.
- Empirical results show consistent performance improvements across various retriever and generator combinations.

Weaknesses:
- Missing details on the approach and experiments hinder reproducibility.
- Limited evaluation metrics, relying solely on GPT-4 may introduce bias.
- Marginal improvements in results suggest the need for significance testing.
- Lack of human evaluation to support claims and missing experiments on more complex QA datasets.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing more detailed descriptions of the critic network, including its initialization and training. Clarifying the two-stage training process and specifying hyperparameters such as learning rates would enhance reproducibility. Additionally, conducting a significance study to validate the improvements brought by PRCA is essential. Expanding the evaluation to include human assessments and additional complex datasets would strengthen the claims. Finally, including latency tests of the pipeline with and without the context generative model would address concerns regarding efficiency.