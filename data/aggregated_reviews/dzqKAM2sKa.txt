ID: dzqKAM2sKa
Title: Hypernetwork-based Meta-Learning for Low-Rank Physics-Informed Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 7, 8, 6, -1, -1, -1
Original Confidences: 2, 3, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel low-rank PINN architecture designed to solve parameterized partial differential equations (PDEs) using a hypernetwork framework that computes solutions in a rank-adaptive manner. The authors propose a two-stage learning framework that trains a low-rank PINN (LR-PINN) across varying PDE parameters and utilizes a hypernetwork to generate initial conditions for new parameter configurations. The method is extensively evaluated on 1D and 2D PDE tasks, demonstrating significant improvements in computation and performance, particularly in overcoming known failure modes of traditional PINNs.

### Strengths and Weaknesses
Strengths:
- The architectural improvements regarding low-rank approximation and the use of a hypernetwork to predict singular values are novel and interesting.
- The method section is well-written, organized, and easy to follow, with extensive experiments supporting the proposed architectural changes.
- The results show impressive accuracy improvements, particularly in addressing failure modes with fewer parameters.

Weaknesses:
- The crucial role of the hypernetwork in overcoming failure cases is unclear; a more prominent motivation for its robustness is needed.
- An ablation study on architectural changes would enhance understanding of the proposed improvements.
- The comparison with the HyperPINN method and the inclusion of meta-learning strategies in computational cost analysis require further clarification.

### Suggestions for Improvement
We recommend that the authors improve the motivation for why the proposed approach is more robust against failure modes, possibly connecting it to the remarks in appendix D.2 and appendix B. Additionally, conducting an ablation study on the architectural changes would provide clarity on the improvements. It may also be beneficial to compare the proposed method against the HyperPINN in Table 2 as another method without pre-training. Furthermore, a fair comparison of computational costs should include meta-learning strategies, especially since some parametrizations of $\beta$ are in the training set for Phase 1. Lastly, clarifying the role of singular vectors in performance improvement and how the datasets are decomposed would enhance the paper's comprehensibility.