ID: kqBUgrkm1c
Title: Easy Learning from Label Proportions
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to learning from label proportions (LLP) through the introduction of EasyLLP, which addresses the performance degradation of the EPRM method when the hypothesis class lacks expressiveness. EasyLLP calculates the corrected instance-level loss before averaging, simplifying the implementation by treating the problem as a regression task. The authors provide theoretical analyses and demonstrate the effectiveness of EasyLLP against baseline models across multiple datasets.

### Strengths and Weaknesses
Strengths:  
- The paper tackles an important problem in the LLP community and introduces a novel loss correction method that could benefit related issues.  
- It effectively identifies and analyzes the limitations of EPRM, presenting findings that are both interesting and novel.  
- Proposition 4.2 and its proof are significant contributions, supporting the soundness and ease of implementation of EasyLLP.  
- The clarity of the approach and the thorough evaluation against baselines on various datasets enhance the paper's value.  

Weaknesses:  
- The comparison with literature [16], which is closely related, is lacking and should be discussed in the experiments.  
- The rationale behind how EasyLLP resolves the limitations of EPRM is not sufficiently clear and requires further elaboration.  
- The empirical evaluation is limited to broad datasets, and the authors should consider more complex real-world datasets to demonstrate the robustness of their approach.  
- The paper could benefit from more intuitive discussions following theorems and propositions to enhance accessibility and understanding.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the comparison with literature [16] and clarify how EasyLLP addresses the limitations of EPRM. Additionally, sharing code or providing pseudocode for EasyLLP would enhance reproducibility. We encourage the authors to evaluate their method on more complex datasets that reflect real-world challenges, such as class imbalance and intricate feature spaces. Finally, we suggest adding more intuitive explanations and discussions following each theorem and proposition to aid reader comprehension.