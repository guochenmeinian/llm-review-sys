ID: AprsVxrwXT
Title: MVGamba: Unify 3D Content Generation as State Space Sequence Modeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MVGamba, a lightweight Gaussian reconstruction model that utilizes a multi-view Gaussian reconstructor based on the RNN-like State Space Model (SSM). MVGamba efficiently generates long sequences of Gaussians with linear complexity, integrating off-the-shelf multi-view diffusion models for unified 3D content generation from single or sparse images and text prompts. Experimental results indicate that MVGamba outperforms state-of-the-art baselines while maintaining a significantly smaller model size (approximately 0.1x).

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to understand.
2. The inference speed is relatively fast compared to optimization-based methods.
3. The introduction of the Mamba architecture is novel within the 3D generative models field.
4. The computational cost of training is lower than that of LGM and LRM.
5. The method demonstrates robustness to multi-view inconsistency.

Weaknesses:
1. The performance in Fig. 6(b) suggests that as token length increases, performance also improves, contradicting the authors' aim to reduce computation costs.
2. No failure cases are presented to illustrate the limitations of MVGamba.
3. Fig. 2(b) may inaccurately represent Mamba's complexity as constant rather than linear.
4. The video results in supplementary materials show only marginal improvements over LGM.
5. The reconstruction quality is lower than that of concurrent GRM and GS-LRM works.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Fig. 2(b) to accurately reflect Mamba's linear complexity. Additionally, we suggest including failure cases to demonstrate the model's limitations. The authors should also provide supporting tables or numbers to substantiate the claim of "0.1Ã— of the model size." Furthermore, we encourage the authors to explore the performance of a non-pixel-aligned transformer model to address multi-view inconsistency issues and consider training MVGamba with a larger number of views to assess potential quality improvements.