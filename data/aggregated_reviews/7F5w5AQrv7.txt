ID: 7F5w5AQrv7
Title: Task-Aware Self-Supervised Framework for Dialogue Discourse Parsing
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a task-aware self-supervised framework for dialogue discourse parsing, addressing limitations of existing parsing approaches constrained by predefined relation types. The authors propose a graph-based model, DialogDP, designed to alleviate error propagation and learning bias, and introduce a self-supervised mechanism utilizing both bottom-up and top-down parsing strategies. Empirical studies demonstrate the framework's effectiveness and flexibility across dialogue discourse parsing datasets and downstream tasks.

### Strengths and Weaknesses
Strengths:
- The introduction of a task-aware paradigm enhances the versatility of the dialogue discourse parser for downstream tasks.
- The design of the graph-based model, DialogDP, effectively addresses error propagation and learning bias.
- Empirical studies validate the effectiveness and flexibility of the proposed framework.

Weaknesses:
- The paper lacks a clear motivation for addressing identified problems, such as learning bias and incompatibility of predefined relations.
- Experimental settings are unclear, particularly regarding BERT token limitations and dataset processing.
- The comparison with BERT-large raises concerns about fairness against models using the base version.
- An in-depth analysis of each component is required to substantiate claims about performance improvements.
- Reproducibility is hindered by missing methodological details and hyperparameters.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind addressing specific problems in dialogue discourse parsing. Additionally, the authors should provide clearer experimental settings, particularly regarding the handling of BERT token limitations and dataset processing. To ensure fair comparisons, we suggest using consistent model sizes in evaluations. Furthermore, an in-depth analysis of each component's impact on performance should be included to strengthen the claims made. Lastly, we encourage the authors to clarify methodological details and hyperparameters to enhance reproducibility.