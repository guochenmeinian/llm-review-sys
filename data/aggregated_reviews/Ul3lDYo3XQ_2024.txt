ID: Ul3lDYo3XQ
Title: AGILE: A Novel Reinforcement Learning Framework of LLM Agents
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 6, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AGILE, a novel framework for LLM agents designed for complex conversational tasks, incorporating memory, tools, and expert interactions, all optimized through reinforcement learning (RL). The authors introduce ProductQA, a dataset for evaluating the framework, and demonstrate that AGILE agents can outperform GPT-4, even when based on smaller models. The architecture includes an LLM backbone controlled by an executive module, which facilitates memory retrieval and tool usage. The framework is fine-tuned using Proximal Policy Optimization (PPO) after initial training via imitation learning. Additionally, the paper emphasizes the importance of seeking expert advice in long-horizon tasks, showing that RL training enhances the agent's ability to request advice when necessary, leading to improved performance, particularly in scenarios where the cost of advice is low. The agent's implementation on the HotPotQA dataset yields competitive results against specialized baselines.

### Strengths and Weaknesses
Strengths:
- The paper introduces a comprehensive framework, AGILE, that effectively integrates various components such as memory, tools, and expert consultations, optimized through RL.
- The development of the ProductQA dataset provides a valuable resource for conversational QA in online shopping contexts.
- The methodology is well-detailed, enhancing reproducibility, and the experiments validate the framework's capabilities, showing significant performance improvements over GPT-4.
- The introduction of a real-world relevant task that incorporates expert advice is a notable contribution.
- Effective demonstration of RL training's role in improving agent performance and decision-making, with competitive results on HotPotQA.

Weaknesses:
- The novelty of the agent architecture is questionable, as it closely resembles existing frameworks like WebGPT, with limited differentiation in the approach to human expert interaction.
- The experiments focus on a single agent, lacking exploration of multiple agents or planning applications.
- The marginal improvement from RL raises questions about its effectiveness, with no comparison to standard PPO provided.
- Concerns regarding the effectiveness of RL training and the practicality of large-scale, inexpensive advice are present.
- The paper does not sufficiently address scalability concerns or the generalizability of the proposed method across diverse tasks.
- The novelty of the method's components may not be clearly articulated in the current formulation.

### Suggestions for Improvement
We recommend that the authors improve the framing of the paper to clarify its contributions, potentially focusing on RL in RAG agent settings or the training of metacognitive abilities. It would be beneficial to provide a clearer distinction between AGILE and existing frameworks like Voyager, and to include comparisons with standard PPO to substantiate claims of performance improvements. Additionally, we suggest exploring the scalability of the memory component and addressing how expert advice can be optimized for real-world applications. Clarifying the evaluation metrics and providing more comprehensive details on the training process, including standard deviation information for RL experiments, would enhance the paper's rigor. Furthermore, addressing the concerns regarding the effectiveness of RL training and the feasibility of advice costs would strengthen the paper's arguments. Finally, further evaluations on additional datasets would enhance the robustness of the findings.