ID: M3BIsgGQNb
Title: Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Emu3D, a novel approach in text-conditioned 3D generation that produces high-quality 3D meshes and materials using Physically-Based Rendering (PBR). The authors propose a two-stage process: generating images from standard viewpoints and reconstructing the 3D shape and appearance. Emu3D enhances mesh generation by predicting a signed-distance field (SDF) for improved quality and refining textures for greater detail. It outperforms existing methods in both image-to-3D and text-to-3D tasks, achieving superior visual quality and text alignment.

### Strengths and Weaknesses
Strengths:  
- The quality of generated geometry, texture, and PBR materials provides an efficient and high-quality generation workflow.  
- The paper is well-presented, with comprehensive experiments validating the results.  
- The proposed pipeline introduces effective components and is supported by intuitive design choices justified through ablation studies.

Weaknesses:  
- The generated PBR material is limited to albedo, metallicity, and roughness, lacking other components like index of reflection and scattering, which restricts its representational capabilities.  
- The technical novelty is weak, particularly regarding the use of SDF and the texture refiner, which are extensions of existing methods.  
- The PBR resolution of 512x512 with a 2x2 grid for 4-view is relatively low compared to concurrent works.

### Suggestions for Improvement
We recommend that the authors improve the generated PBR material by incorporating additional components to enhance surface reflectance modeling. Additionally, we suggest providing a detailed discussion comparing Emu3D with Unique3D regarding texture resolution and performance. To better evaluate the texture refiner, we recommend including qualitative examples that show texture maps before and after refinement, particularly around seams. Furthermore, addressing the variance in albedo prediction due to the stochastic nature of the diffusion model and providing a failure case analysis would strengthen the paper. Lastly, we encourage the authors to consider open-sourcing the test sets and generated 3D assets to facilitate future comparisons.