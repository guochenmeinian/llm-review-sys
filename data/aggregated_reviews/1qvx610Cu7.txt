ID: 1qvx610Cu7
Title: Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 6, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents EvalPlus, an evaluation framework designed to assess the code generation capabilities of large language models (LLMs) by leveraging both LLM-based and mutation-based strategies. The authors introduce an extended dataset, HumanEval+, which enhances the evaluation benchmark for LLMs like ChatGPT and GPT-4. Experimental results indicate that the extended dataset can identify more issues in generated code, revealing a 13.6-15.3% reduction in performance across 19 LLMs compared to the original HumanEval dataset. Additionally, the authors propose a refined set of test cases that maintain code coverage while reducing the number of tests.

### Strengths and Weaknesses
Strengths:
- The paper proposes a novel test case generation method that effectively combines LLM-based and mutation-based strategies.
- It evaluates 19 LLMs on the extended dataset, demonstrating the limitations of the original benchmark.
- The introduction of a test-suite reduction method allows for quicker evaluations without sacrificing coverage.

Weaknesses:
- Some technical aspects, such as the quality of seed inputs generated by ChatGPT, remain unclear, impacting the soundness of the approach.
- The paper lacks comparisons with other test case generation methods and related works, limiting the context for its contributions.
- There are vague claims regarding the ability of the proposed benchmark to reflect the true performance of LLMs and capture all possible scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the quality and correctness of the seed inputs generated by ChatGPT, including details on the number of inputs generated and the prompts used. Additionally, we suggest providing a more comprehensive discussion on how randomness in type-aware input mutation affects evaluation quality. The authors should also clarify the definitions and correctness of assertions added as contracts in the evaluation process. Furthermore, we encourage the authors to include comparisons with other test case generation methods, particularly those like AlphaCode and CodeT, to better justify the novelty of EvalPlus. Lastly, exploring the performance of EvalPlus in more complex, real-world coding scenarios would enhance the robustness of their findings.