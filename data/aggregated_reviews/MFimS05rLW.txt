ID: MFimS05rLW
Title: Investigating the Effect of Pre-finetuning BERT Models on NLI Involving Presuppositions
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the method of pre-finetuning, where models are fine-tuned on additional tasks before targeting a specific task, focusing on presupposition in natural language inference (NLI). The authors pre-finetune on three tasks: discourse relation classification, coherence modeling, and sarcasm detection, and assess the impact on the ImPres dataset, which also involves presupposition. The contributions include advancing research on presupposition, demonstrating the benefits of pre-finetuning for related tasks, and providing a solid linguistic background on presupposition.

### Strengths and Weaknesses
Strengths:
- The paper effectively explores a challenging linguistic phenomenon, presupposition, and its relevance to model performance.
- It demonstrates that pre-finetuning on related tasks improves performance on the target task, providing a foundation for future research.
- The model output analysis offers valuable insights into model behavior, enhancing interpretability.

Weaknesses:
- The approach to testing the impact of label noise through data corruption is questionable, as it may not convincingly isolate the effects of data quantity from label quality.
- The paper lacks clarity on whether results are averages from multiple runs, raising concerns about robustness.
- The narrow focus on presupposition may limit the paper's relevance to a broader audience.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their findings by conducting over- and under-sampling tests instead of randomizing labels to assess the impact of data quantity without introducing noise. Additionally, clarifying whether the results in Tables 2, 3, and 4 are averages from multiple runs would enhance transparency. We also suggest including a discussion on the applicability of their findings to other NLI tasks, such as MNLI, and addressing the existing literature on pre-finetuning to contextualize their contributions better. Finally, a summary of results and their significance to the NLI field would make the paper more accessible to non-experts.