ID: rLpLjCBW4J
Title: Preconditioning Matters: Fast Global Convergence of Non-convex Matrix Factorization via Scaled Gradient Descent
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 4, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on low-rank matrix factorization (LRMF) using a variant of gradient descent known as ScaledGD, along with its alternating version, AltScaledGD. The authors prove that these algorithms converge at a faster rate than traditional gradient descent, with no dependence on the matrix condition number. Notably, AltScaledGD can converge without requiring small initialization or small step sizes. The work extends previous analyses by allowing for small or moderate random initialization, thus broadening the applicability of these algorithms.

### Strengths and Weaknesses
Strengths:  
- The theoretical results are robust, demonstrating that ScaledGD and AltScaledGD converge faster than standard gradient descent for LRMF.  
- The paper provides a clean and elegant analysis, clarifying the algorithms' performance without reliance on condition number.  
- The introduction of random initialization as a viable option enhances the practical applicability of the algorithms.

Weaknesses:  
- The motivation for studying ScaledGD and AltScaledGD is unclear, particularly regarding their implementation in more complex optimization problems like neural networks.  
- The organization of the technical sections is confusing, particularly the presentation of results for rank-1 versus rank-d cases, which complicates understanding the generalization of results.  
- The presentation is rushed in places, lacking explicit explanations that could improve clarity.

### Suggestions for Improvement
We recommend that the authors improve the motivation section by clearly articulating the relevance of ScaledGD and AltScaledGD in broader optimization contexts, particularly in neural networks. Additionally, we suggest that the authors reorganize the technical sections to clearly delineate the results for rank-1 and rank-d cases, ensuring that the connections between them are explicitly stated. Furthermore, we encourage the authors to enhance the clarity of their presentation by avoiding vague phrases like "it can be deduced" and instead providing detailed explanations of their derivations. Lastly, we recommend including experimental evidence to demonstrate the algorithms' performance on approximately low-rank matrices and their applicability to real-world datasets.