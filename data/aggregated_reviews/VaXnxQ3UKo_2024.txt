ID: VaXnxQ3UKo
Title: AlphaMath Almost Zero: Process Supervision without Process
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 4, 4, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AlphaMath, an innovative framework designed to enhance the mathematical reasoning capabilities of large language models (LLMs) by utilizing Monte Carlo Tree Search (MCTS) for autonomous generation of process supervision and step-level evaluation signals. The authors integrate a value model with the LLM, proposing a step-level beam search strategy to improve inference efficiency. Additionally, the paper introduces a training method categorized as almost unsupervised/weakly-supervised learning, highlighting the small gap between this approach and supervised methods, which underscores its potential. The authors utilize the open-sourced `vllm` inference framework, noting that inference time increases significantly when the number of generations is set to 20 due to CPU execution. Experimental results indicate that AlphaMath achieves performance comparable to or superior to existing state-of-the-art methods across various datasets without relying on human annotations or GPT-4.

### Strengths and Weaknesses
Strengths:  
1. The proposed method effectively eliminates the need for human or GPT-4 annotations, leading to significant efficiencies in both cost and time.  
2. The application of MCTS enhances LLM performance in reasoning tasks by facilitating optimal path searches, and training with MCTS-derived trajectories is commendable.  
3. The integration of the value model with the step-level best-of-n approach is well-articulated and demonstrates robustness in results.  
4. The introduction of the step-level beam search strategy is a promising approach to reduce computational costs.  
5. The authors effectively address reviewer concerns and clarify their methodology.

Weaknesses:  
1. The paper's results are all program-aided, which is not clearly stated, potentially skewing comparisons with non-program-aided LLMs.  
2. The benchmark is limited to GPT models; including other LLMs like Gemini and Claude would provide a more robust evaluation.  
3. The presentation lacks clarity in several areas, and some claims in the introduction may be overstated.  
4. The paper lacks detailed experimental settings, particularly regarding the inference framework and its implications.  
5. There is a need for clearer presentation of results related to weighted voting and best-of-n comparisons.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their results by explicitly stating the program-aided nature of their findings to ensure fair comparisons. Including a broader range of LLMs in the benchmark would strengthen the evaluation. Furthermore, we suggest that the authors provide an illustrative image of the Step-Level Beam Search (SBS) to aid in explaining the algorithm. Additionally, addressing the over-claims in the introduction and clarifying the modifications made compared to the original AlphaGo and AlphaGo Zero papers would enhance the paper's credibility. We also recommend that the authors improve the clarity of the experimental settings by providing more details on the inference framework and its configurations, as well as explicitly presenting the results of weighted voting and best-of-n comparisons to enhance the understanding of their methodology and findings.