ID: KakzVASqul
Title: Prediction and Control in Continual Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 7, 6, 4, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new continual reinforcement learning method, PT-TD, which utilizes two value networks: one for transient memories and another for permanent memories. The authors demonstrate that the PT-TD algorithm achieves improved performance and reduced forgetting in both semi-continual and continual reinforcement learning settings. The paper also introduces algorithms for prediction and control, employing permanent and transient memories to adapt to non-stationary environments. Theoretical results for a tabular learner and empirical evidence show that the proposed method outperforms traditional TD-based algorithms across various environments.

### Strengths and Weaknesses
Strengths:  
- The PT-TD algorithm is well-proven and supported by rigorous mathematical proofs and detailed experiments.  
- The paper is well-written, making complex concepts accessible.  
- The extensive empirical investigation demonstrates the method's superiority over existing approaches in diverse environments and with various function approximators.  
- The separation of semi-continual and continual reinforcement learning is appreciated.

Weaknesses:  
- The algorithm, while sound, is not particularly groundbreaking, and the experimental environments are simplistic.  
- Some equations, such as Theorem 8 and Equation 5, are confusing and lack clarity.  
- The paper does not provide sufficient details on the implementation of the PT-DQN algorithm, hindering reproducibility.  
- The limitations of the proposed methods, particularly regarding their applicability to policy-gradient based RL, are not adequately discussed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the formulation in Theorem 8 by explicitly indicating which network is being referenced (P or T). Additionally, we suggest providing a detailed explanation of the intuition behind the updates in Equations 4 and 5. It would be beneficial to include a section in the appendix with pseudocode for the PT-DQN algorithm to enhance reproducibility. Furthermore, addressing the questions regarding multi-task RL, confidence intervals, and the use of learning rates in the experiments will strengthen the paper. Lastly, we encourage the authors to explore the extension of their methods to policy-gradient based RL and discuss potential future work in this area.