ID: 3A5VgiH5Pw
Title: Towards Multi-dimensional Explanation Alignment for Medical Classification
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 8, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Med-MICN, a novel end-to-end framework for explainable medical image classification that integrates medical image classification, neural symbolic reasoning, and concept semantics. The authors propose an automated process for obtaining concept labels and demonstrate that Med-MICN achieves superior accuracy and interpretability across various datasets. The framework consists of four modules: feature extraction, auto-annotation, concept embedding, and a neural symbolic layer, incorporating fuzzy logic rules for enhanced interpretability. The experimental results indicate that Med-MICN outperforms baseline networks and traditional concept bottleneck models.

### Strengths and Weaknesses
Strengths:
- The integration of medical image classification with neural symbolic reasoning and concept semantics enables comprehensive interpretability.
- Med-MICN shows superior accuracy and interpretability across diverse datasets.
- The extensive experiments validate the model's performance, supported by impressive visualizations.

Weaknesses:
- The captions for figures, particularly figure 3, lack sufficient detail.
- The rationale for analyzing interpretability through concept embedding, neural-symbolic reasoning, and saliency maps is unclear, and alternative dimensions are not considered.
- The presentation of mathematical symbols and figures is inconsistent, affecting readability.
- There is insufficient evaluation of the interpretability of generated heatmaps and their alignment with clinical labels.
- The description of the Neural-Symbolic Layer is not clear, particularly regarding the functions of "Concept Polarity" and "Concept Relevance."

### Suggestions for Improvement
We recommend that the authors improve the detail in figure captions, especially for figure 3, to enhance clarity. Additionally, the authors should provide a clearer rationale for their chosen dimensions of interpretability and consider including other relevant aspects. We suggest that the authors enhance the presentation of mathematical symbols and figures for better readability and clarity. Furthermore, the authors should evaluate the interpretability of the generated heatmaps and compare them with baseline methods. Lastly, we encourage the authors to clarify the functions of "Concept Polarity" and "Concept Relevance" in the Neural-Symbolic Layer description.