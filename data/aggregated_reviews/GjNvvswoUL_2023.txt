ID: GjNvvswoUL
Title: DICES Dataset: Diversity in Conversational AI Evaluation for Safety
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 9, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the DICES Dataset, which consists of two annotated datasets of human-bot conversations: DICES-990 with 990 conversations and DICES-350 with 350 conversations. The authors aim to evaluate safety perceptions in generative AI through a demographic lens, addressing biases in previous annotated datasets, particularly against African American English speakers, by diversifying annotators and incorporating classifiers as suggested by Halevy et al. They expand the definition of safety to include harms, biases, and misinformation, and provide metrics for dataset evaluation. The DICES dataset aims to facilitate an intersectional examination of safety evaluations in conversational AI systems. The authors acknowledge limitations, such as the lack of consideration for individual personality traits and the oversimplified demographic breakdown, which may omit nuances within groups. They recognize the risk of cultural bias due to limited representation of raters from diverse backgrounds and argue that the dataset's public availability is crucial for improving model safety. The dataset boasts an unprecedented size in terms of human annotations, enabling significant statistical analysis of demographic diversity.

### Strengths and Weaknesses
Strengths:  
The DICES dataset provides a substantial resource for studying the impact of rater demographics on safety perceptions, with 2.5 million safety annotations across diverse demographics. The authors effectively address concerns regarding annotation biases and demonstrate thoughtful consideration of demographic representation among annotators. They plan to continuously update the dataset, enhancing its relevance and granularity over time. The extensive number of annotations per conversation enhances the reliability of the dataset, allowing for in-depth statistical and qualitative studies, which address a critical need in generative AI safety research.

Weaknesses:  
The dataset's size remains relatively small in terms of unique items, which may limit its applicability and utility for training purposes. The demographic breakdown is oversimplified, potentially overlooking nuances within groups and limiting the analysis of intersectional effects and individual differences. Additionally, there is a potential bias towards native English speakers, which may affect comprehensibility and inclusivity. The paper also lacks concrete experimental validation to demonstrate the effectiveness of the proposed annotation method.

### Suggestions for Improvement
We recommend that the authors improve the demographic breakdown to include more nuanced categories that reflect the complexity of individual differences. We suggest exploring how in-groups define harm, as this could provide deeper insights into safety perceptions across different demographics. Additionally, we encourage the authors to address the implications of non-native English speakers in the dataset to enhance comprehensibility. It would be beneficial to clarify the metrics and results of the dataset evaluation more explicitly within the paper. We also suggest addressing the lack of experimental validation by providing results that demonstrate the impact of the proposed classification dataset on language model performance. Lastly, we encourage the authors to cite relevant benchmarks, such as BOLD and ToxiGen, to contextualize the risks associated with AI advancements and ensure compliance with formatting guidelines throughout the paper.