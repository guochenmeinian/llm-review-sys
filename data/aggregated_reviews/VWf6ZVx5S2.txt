ID: VWf6ZVx5S2
Title: Transforming Vision Transformer: Towards Efficient Multi-Task Asynchronous Learner
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method that integrates Mixture-of-Experts (MoE) and Low-Rank Adaptation (LoRA) into pre-trained Vision Transformers (ViT) to create a multi-task model. The authors propose clustering channels in the feedforward network (FFN) of transformer blocks based on similarity to form experts, with each expert fine-tuned using LoRA. Additionally, a multi-task optimization method is introduced to mitigate knowledge forgetting during task optimization. The final model is designed to be a unified structure for efficient inference.

### Strengths and Weaknesses
Strengths:
- The paper is easy to follow, with clearly stated components.
- The clustering of channels into experts is an intuitive and novel approach that leverages pre-trained model parameters effectively.
- The multi-task optimization (MTO) method is useful and has broad applicability in multi-task model training.
- The unified model design theoretically supports fast inference.

Weaknesses:
- The necessity of the router in the model's final form is unclear; the authors should clarify its role and implications if omitted from the start.
- There is a lack of extensive ablation studies on clustering; the authors should investigate the optimal number of clusters and the performance impact of not clustering.
- The applicability of the proposed Quality Retaining (QR) loss to other multi-task learning (MTL) models needs exploration.
- Comparisons in Table 1 should independently assess MTO methods against QR and MTL architectures against EMTAL without QR.
- The experiments focus on classification tasks, while practical MTL scenarios often involve pixel-to-pixel predictions; results on datasets like NYUv2 and Takonomoy would be beneficial.
- Inference time comparisons are necessary, given that fast inference is a claimed contribution.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the router's necessity and its implications if not included from the beginning. Additionally, the authors should conduct more ablation studies to determine the optimal clustering strategy and its effects on performance. Exploring the QR loss's applicability to other MTL models would enhance the paper's contribution. For a fair comparison, the authors should ensure that MTO methods are evaluated independently of QR and that MTL architectures are compared without QR. Including results from pixel-to-pixel prediction tasks and providing inference time comparisons will strengthen the paper.