ID: eEK99egXeB
Title: OpenDataVal: a Unified Benchmark for Data Valuation
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 9, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified benchmarking framework, OpenDataVal, for implementing and comparing various data valuation metrics. It includes an open-source code package and a leaderboard for users to evaluate individual data points and assess different metrics across tasks such as noisy data detection and point manipulation. The authors aggregate implementations of nine techniques and provide initial benchmark comparisons, highlighting that no single algorithm outperforms others across all evaluation metrics.

### Strengths and Weaknesses
**Strengths:**
- The paper is well-written and structured, making it accessible to readers.
- It offers a comprehensive tool for implementing and comparing well-known model-based data valuation metrics, serving as a valuable resource for researchers.
- The inclusion of diverse datasets and evaluation metrics enhances its relevance to the broader ML community.

**Weaknesses:**
- The framework currently focuses solely on model-based metrics, omitting model-free approaches that could provide a more unified perspective.
- There is limited discussion on the robustness of the framework against malicious behaviors, such as data duplication by data owners.
- The evaluation relies on logistic regression as the sole model, which may not adequately represent the performance variability across different models.

### Suggestions for Improvement
We recommend that the authors improve the framework by incorporating model-free data valuation metrics and discussing potential integrations with model-dependent metrics. Additionally, addressing robustness against malicious attacks and extending the framework to accommodate distributed or federated settings would enhance its applicability. The authors should also consider providing a more detailed analysis of hyperparameter sensitivity and include a wider variety of models in their evaluations to ensure comprehensive benchmarking.