ID: jUpVFjRdUV
Title: Scalable 3D Captioning with Pretrained Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for generating descriptive text for 3D objects using large language models (LLMs), specifically introducing Cap3D, an automatic framework that produces captions through a multi-step process involving rendering images, generating captions with BLIP-2, filtering with CLIP, and summarizing with GPT-4. The authors contribute a dataset of 660k 3D-text pairs and argue that their captions often provide more detail than human-generated ones, consolidating information from multiple viewpoints. They demonstrate that Cap3D outperforms human annotations in quality, cost, and speed, particularly in text-to-3D tasks, and emphasize the importance of geometric captioning. The evaluations indicate that Cap3D generates captions with fine-grained geometric detail, surpassing human-authored annotations, while also addressing concerns regarding the robustness of their generated captions, showing high semantic similarity across multiple outputs.

### Strengths and Weaknesses
**Strengths:**
1. The approach effectively bridges the gap between 3D objects and text modalities, addressing a significant problem in the field.
2. The dataset facilitates meaningful improvements in text-to-3D model performance, showcasing the method's effectiveness.
3. The integration of multiple models (BLIP-2, CLIP, GPT-4) enhances caption detail and robustness.
4. The use of a voting system effectively filters out inaccurate captions from extreme viewpoints.
5. The authors provide quantitative and qualitative analyses to support their claims about caption quality.

**Weaknesses:**
1. The method currently relies solely on the Objaverse and ABO datasets, limiting its applicability to other 3D datasets.
2. The generated captions often lack complexity, primarily providing category information without considering spatial relationships in 3D.
3. The reliance on rendered images may obscure some 3D details, such as geometry and occlusion.
4. The pipeline is not end-to-end, which may introduce inefficiencies and complicate the process.
5. The comparison with human-authored captions may not comprehensively evaluate all properties of the 3D objects.

### Suggestions for Improvement
We recommend that the authors explore the application of their method on additional 3D datasets to build a more extensive text-3D paired dataset. Additionally, the authors should consider enhancing the complexity of generated captions by incorporating spatial relationships and other 3D-specific information. To address potential captioning errors, we suggest implementing explicit methods for filtering incorrect information during the summarization process with GPT-4. Furthermore, we recommend improving the end-to-end nature of their pipeline to streamline the process and enhance efficiency. Lastly, the authors might evaluate the robustness of the generated captions by comparing the outputs of different models in the Cap3D pipeline and conducting a qualitative assessment of various aspects such as categories, colors, textures, shapes, components, and functions to provide a more comprehensive evaluation of the model's performance.