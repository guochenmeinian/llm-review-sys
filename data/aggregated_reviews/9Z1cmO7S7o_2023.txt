ID: 9Z1cmO7S7o
Title: Generating QM1B with PySCF$_{\text{IPU}}$
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 6, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents QM1B, a large dataset containing over 1 billion samples for molecular potential energy prediction, generated using PySCF on Intelligence Processing Units (IPUs). QM1B is the first dataset of its size in the molecular machine learning community, and training on it has shown performance gains with the SchNet model. The authors also explore the use of float32 for data generation in density functional theory (DFT), acknowledging the associated higher numerical errors. They propose that while float32 is sufficient for generating data, particularly for pre-training models, the precision may be inadequate for certain tasks. The authors conducted experiments demonstrating a significant degradation in performance when simulating float64 in software, with a noted increase in running time by approximately 6x. They emphasize the importance of lower precision in DFT and suggest that their open-sourced PySCF-IPU could facilitate further research in this area.

### Strengths and Weaknesses
Strengths:  
- QM1B is a significantly large dataset, beneficial for the molecular machine learning community.  
- The data generation framework is hardware-optimized for IPUs, enhancing dataset construction speed.  
- The baseline model SchNet shows improved predictions with the larger dataset.  
- The authors provide a clear rationale for using float32 and present experimental results supporting its efficacy for specific applications.  
- They acknowledge the limitations of float32 and propose avenues for future research, including detailed experiments and visualization of numerical precision.  

Weaknesses:  
- The dataset's lower precision (float32) may limit its applicability in rigorous pharmaceutical research.  
- Concerns remain regarding the decreased precision of the dataset, which may impact the reliability of results in certain contexts.  
- The paper lacks detailed characterization of the QM1B dataset and its practical application scenarios.  
- There is insufficient discussion on the time cost of using float64 compared to float32, leaving some questions unanswered.  

### Suggestions for Improvement
We recommend that the authors improve the paper by:  
1. Adding explanations about why predicting molecular potential energies is a suitable task for training foundation models, rather than relying on self-supervised learning tasks common in NLP.  
2. Conducting experiments with larger transformer-based models (e.g., Graphormer) on the QM1B dataset to compare performance across different model sizes.  
3. Providing a more thorough discussion on the time cost implications of using float64 instead of float32 in the data generation process.  
4. Further characterizing the QM1B dataset and exploring its application scenarios to clarify its practical utility in research.  
5. Addressing the compensatory relationship between precision and scale to enhance the dataset's applicability in rigorous research contexts.  
6. Discussing the concerns regarding the decreased precision of the dataset more comprehensively to strengthen the paper.