ID: BHXsb69bSx
Title: ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 8, 7, 8, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ToolkenGPT, a novel method that enables language models to utilize external tools efficiently by training additional output embedding tokens (toolkens) for each tool. The approach allows for invoking tools during inference without the need for full model fine-tuning, thereby accommodating more tools and unfamiliar tools compared to in-context prompting methods like ReAct. The authors demonstrate the effectiveness of ToolkenGPT across various domains, including arithmetic, knowledge-based QA, and embodied plan generation.

### Strengths and Weaknesses
Strengths:
- The method is innovative and efficiently scales to a large number of tools with limited examples.
- Empirical results validate ToolkenGPT's advantages over existing methods, particularly in accommodating more tools.
- The paper is well-structured, clearly written, and presents a comprehensive evaluation.

Weaknesses:
- The evaluation lacks direct comparisons with fine-tuning methods like Toolformer, leaving questions about relative performance and accuracy.
- There are confounding variables in the evaluation, such as differing amounts of training data, which complicate the interpretation of results.
- The paper contains several typos and writing errors that detract from its clarity.
- The claimed "plug-and-play" capability is not adequately evaluated, raising questions about its practical implementation.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including direct comparisons with fine-tuning methods like Toolformer to clarify performance differences. Additionally, we suggest addressing confounding variables in the evaluation, such as ensuring consistent training data across comparisons. The authors should also proofread the paper to correct typos and enhance clarity. Furthermore, we encourage the inclusion of ablation studies to isolate the effects of the toolken embedding and the sub-routine of tool calls. Lastly, we recommend providing a clearer explanation of the "plug-and-play" capability and addressing the potential implications of using "massive tools" in the context of the paper.