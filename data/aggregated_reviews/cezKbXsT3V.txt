ID: cezKbXsT3V
Title: On Separate Normalization in Self-supervised Transformers
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 7, 6, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SepNorm, a normalization technique for Transformer models that separates the normalization of the [CLS] token from the other tokens in a sequence, contrasting with the traditional ShareNorm method. The authors argue that this separation helps mitigate dimensional collapse, as evidenced by improved uniformity scores and downstream performance across various datasets. The paper includes a case study and ablation studies that support the effectiveness of SepNorm.

### Strengths and Weaknesses
Strengths:
- The paper proposes a straightforward yet effective solution to address dimensional collapse, enhancing performance.
- It is relevant to a broad audience interested in self-supervised Transformer architectures.
- The experiments are well-executed, with a controlled setup that substantiates the method's merits.
- The paper is generally self-contained, accessible to a general ML-savvy audience.
- The application of separate normalization parameters provides sufficient novelty despite the concept not being entirely new.

Weaknesses:
- The structure of the paper could be improved, particularly in sections 3.1 and 3.2, where method descriptions and experimental results are conflated.
- There is a lack of uncertainty/confidence/error bars in the experimental results and significance testing.
- The contribution may not seem relevant to a larger audience without follow-up studies.
- The paper does not empirically compare SepNorm against other normalization methods, such as Powernorm.

### Suggestions for Improvement
We recommend that the authors improve the paper's structure by clearly separating method descriptions from experimental results, particularly in sections 3.1 and 3.2. Additionally, including uncertainty measures and significance testing in the experimental results would enhance the robustness of the findings. We suggest that the authors discuss the limitations of their method, particularly regarding its applicability to other normalization techniques and potential avenues for future work. Furthermore, we encourage the authors to conduct a broader range of experiments across various tasks to demonstrate the generalizability of SepNorm.