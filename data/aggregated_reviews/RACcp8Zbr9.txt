ID: RACcp8Zbr9
Title: Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 7, 6, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 3, 1, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the use of advice in Markov Decision Processes (MDPs), focusing on the tradeoff between consistency and robustness when integrating potentially erroneous machine-learned advice. The authors propose the Projection Pursuit Policy (PROP), which utilizes either action suggestions (black-box) or both actions and Q-values (grey-box) from an advisor. The algorithm projects the advisor's suggested action onto a ball centered around a robust policy's action, with the radius of the projection varying based on the advisor type. The analysis demonstrates that the grey-box setting can achieve superior consistency and robustness compared to the black-box setting. Additionally, the authors provide a thorough rebuttal to reviewer concerns, clarifying raised issues and committing to addressing feedback, including plans to incorporate empirical assessments in future work to validate the practicality of their approach.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clear, with substantial effort in explaining the setting, definitions, and approach, enhancing readability.
- The theoretical analysis is robust, providing sound support for the proposed algorithm and its performance guarantees.
- The results extend beyond previous works, offering promising insights for future research.
- The authors provide clear and satisfactory responses to reviewer questions, enhancing the clarity of the paper.
- There is a strong commitment to improving the work by incorporating empirical results, which is appreciated by reviewers.

Weaknesses:
- The paper lacks empirical validation and practical demonstrations of the proposed algorithm's applicability, which could strengthen its impact.
- Some assumptions, particularly regarding the grey-box advice and the access to a robust policy, may limit the practical relevance of the findings.
- The paper does not sufficiently compare the PROP algorithm with existing methods, particularly in discrete action spaces.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation of their algorithm by including practical examples in the main body, demonstrating its applicability and effectiveness. Additionally, we suggest addressing the limitations of the assumptions made regarding the grey-box advice, particularly the $(\infty, \epsilon)$-consistent policies, and clarifying how a robust policy can be obtained in time-varying environments. A thorough comparison with existing learning-augmented algorithms would also enhance the paper's contribution and clarity. Finally, we encourage the authors to provide more concrete examples of the problems their approach aims to solve, which would contextualize their work better.