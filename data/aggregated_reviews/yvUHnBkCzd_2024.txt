ID: yvUHnBkCzd
Title: Personalized Federated Learning with Mixture of Models for Adaptive Prediction and Model Fine-Tuning
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 4, 5, 6, 5, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a personalized federated learning algorithm, Fed-POE, aimed at real-time predictions in non-stationary environments. It allows clients to fine-tune models online by combining locally fine-tuned models with multiple federated models, ensuring efficient adaptation to evolving data streams. Theoretical analysis and experiments on real datasets demonstrate its effectiveness in achieving sublinear regret bounds and improved online prediction accuracy.

### Strengths and Weaknesses
Strengths:
- The proposed algorithm effectively addresses real-time prediction challenges by enabling online model fine-tuning, ensuring continuous adaptation to evolving data streams.
- It enhances personalization by combining locally fine-tuned models with federated models, leveraging the strengths of both approaches.
- The paper provides solid theoretical analysis and experimental validation, demonstrating the practical effectiveness of the proposed algorithm.

Weaknesses:
- Contributions should be clearly listed for better clarity.
- Baselines in Table 1 are outdated, lacking comparisons with more recent methods published in 2023.
- Fed-POE shows limited improvements on specific datasets like Air and FMNIST.
- The combination process may introduce significant computational overhead, especially for clients with limited resources.
- Managing multiple personalized models as client numbers increase poses scalability challenges.
- The novelty of the method is questioned, as it appears to be a combination of existing approaches with limited conceptual originality.
- Experimental results indicate that accuracy improvements are not significant, and the trade-off between accuracy and computational overhead needs analysis.
- More experiments are needed to assess the impact of old data replay size and batch size on real-time predictions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of contributions by listing them explicitly. Additionally, the authors should compare their method against more recent algorithms published in 2023 to strengthen the baseline comparisons. To address the limited improvements observed on certain datasets, we suggest conducting further experiments to analyze the effectiveness of Fed-POE. The authors should also evaluate the computational overhead introduced by the model combination process and assess whether this trade-off is reasonable. Finally, we encourage the authors to design experiments to analyze the effect of batch size on experimental results and to clarify the advantages of their method in the theoretical bounds presented.