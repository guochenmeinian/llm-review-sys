ID: Y1ZsLONDI2
Title: Soft ascent-descent as a stable and flexible alternative to flooding
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 7, 4, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called SoftAD (soft ascent-descent) aimed at enhancing the flooding method in machine learning. The authors propose two main modifications: first, transitioning from an average loss threshold to a point-wise loss threshold, and second, replacing the absolute function with a continuous, bounded, monotonic function for measuring differences. The paper includes theoretical foundations, empirical studies across multiple datasets, and comparisons of convergence properties, claiming that SoftAD achieves comparable classification accuracy while exhibiting a smaller generalization gap and lower model norms.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and easy to follow, with clear organization and insightful discussions linking flooding and sharpness.
- The soft truncation concept is a valuable enhancement to the flooding method, supported by intuitive experimental results.
- Theoretical guarantees for stationarity are provided, and the empirical results demonstrate the method's effectiveness across various datasets.

Weaknesses:
- The motivation for the method may be unclear, particularly regarding the relevance of metrics like model complexity or average surrogate loss at test time.
- The proposed method lacks comprehensive theoretical support, particularly in generalization analyses.
- The complexity of implementing SoftAD may hinder its adoption, as it requires managing both ascent and descent phases.
- There is insufficient discussion on the relationship between SoftAD and the previously proposed iFlood method, which also employs a pointwise approach.
- The paper does not adequately address the sensitivity of performance to hyperparameters beyond the threshold parameter θ.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind SoftAD, particularly in relation to other metrics like model complexity. Additionally, we suggest providing a more thorough theoretical analysis to support the claims of generalization. It would be beneficial to conduct an ablation study to determine the contributions of the pointwise component versus the soft truncation component. Furthermore, we encourage the authors to include discussions on the relationship between SoftAD and iFlood, emphasizing the differences and similarities. Lastly, we recommend a detailed analysis of the sensitivity of performance to hyperparameters other than θ, as well as practical guidance on optimizing θ for better results.