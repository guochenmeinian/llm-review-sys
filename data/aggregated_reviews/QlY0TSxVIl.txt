ID: QlY0TSxVIl
Title: Revisiting Automated Topic Model Evaluation with Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to automatically evaluating topic models using large language models (LLMs) and attempts to determine the optimal number of topics through K-nearest neighbors (KKNs). The authors claim that LLMs can accurately assess topic coherence and assist in identifying reasonable topic counts. The study builds on previous evaluation metrics like NPMI, highlighting its limitations in correlating with human evaluations.

### Strengths and Weaknesses
Strengths:  
- The paper provides extensive evaluation and a solid experimental setup, contributing valuable insights to the community.  
- It is well-grounded in existing literature and represents a significant advancement in the use of LLMs for these tasks.  
- The findings are generally sound, with sufficient support for the claims made.  

Weaknesses:  
- The paper is challenging to read in certain sections, with suggestions for improved clarity.  
- There is a need for greater acknowledgment of prior work in nonparametric Bayesian models related to topic estimation.  
- Some aspects, such as the retrieval of the most probable documents in the text labeling algorithm, require further clarification.

### Suggestions for Improvement
We recommend that the authors improve the readability of the paper by removing mathematical notations and definitions from the introduction. Additionally, specifying the correlation type measured in Figure 1 would enhance clarity. It would also be beneficial to expand the discussion on previous nonparametric Bayesian models for topic estimation. Finally, we suggest providing a brief explanation of how to retrieve the 10 most probable documents in the text labeling algorithm to aid understanding.