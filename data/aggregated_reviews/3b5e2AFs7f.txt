ID: 3b5e2AFs7f
Title: On Formal Feature Attribution and Its Approximation
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 4, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to feature attribution called Formal Feature Attribution (FFA), which defines feature importance as the proportion of explanations where a feature occurs. The authors address the computational challenges of exact FFA calculation by proposing an efficient approximation technique based on the duality of abductive explanations. The empirical evaluation of FFA is conducted across various datasets, comparing its performance with existing methods like SHAP and LIME. The authors acknowledge the limitations of these existing methods, noting that their attribution scores do not align with FFA, which they argue should prompt further examination of what these explainers compute. They clarify that their method respects the Dummy Axiom by not assigning non-zero attributions to unused features and discuss the potential for future work to explore larger datasets like CIFAR-10 and the challenges posed by the size of ImageNet for formal reasoning.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and coherent, making it accessible to readers.
- It contributes to the field of Explainable AI by filling a gap in formal foundations for feature attribution.
- The authors demonstrate a comprehensive approach, balancing theoretical foundations and empirical validation.
- The authors provide a clear and formal attribution measure (FFA) that is easy to understand.
- They acknowledge the limitations of existing methods and propose a thoughtful comparison with their approach.
- The paper addresses reviewer concerns and commits to clarifications in the final version.

Weaknesses:
- The paper lacks proofs for its propositions, particularly Proposition 2, which is not self-explanatory and requires clarification.
- The discussion of the weighted variant (Definition 2) is insufficient, as it is not explored theoretically or practically in the main text.
- There are no formal guarantees for the approximation algorithm, raising concerns about the reliability of the results.
- The experimental comparisons with LIME and SHAP are questionable, as they measure different aspects and may not provide a fair evaluation of FFA.
- The authors imply that FFA is the superior method without sufficiently acknowledging other viable approaches.
- There is a lack of exploration regarding how well users understand FFA compared to other methods.
- The paper does not sufficiently detail how FFA identifies significant interactions, which could enhance its credibility.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Proposition 2 by providing a detailed proof and addressing the ambiguity regarding the variable \( \omega \). Additionally, the authors should explore the implications of the weighted variant (Definition 2) more thoroughly within the main text. It is crucial to establish formal guarantees for the approximation algorithm to enhance the interpretability of the results. We also suggest that the authors clarify the rationale behind comparing FFA with LIME and SHAP, as they do not measure the same constructs. Furthermore, we recommend that the authors treat FFA as one of several sensible choices for feature importance measures rather than the only option. Providing a more detailed explanation of how FFA identifies significant interactions could strengthen the paper's claims. Finally, we encourage the authors to include a clearer distinction between SHAP and KernelSHAP to avoid reader confusion.