ID: HwhRehMr4a
Title: Future-Dependent Value-Based Off-Policy Evaluation in POMDPs
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 3, 7, 7, 7, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to off-policy evaluation (OPE) in partially observable Markov decision processes (POMDPs) by introducing a future-dependent value function (FDVF). The authors derive a Bellman-like equation and a minimax algorithm for FDVFs, supported by finite sample theoretical guarantees. The paper also provides empirical evidence through experiments, particularly in a CartPole environment, to validate the proposed method's effectiveness.

### Strengths and Weaknesses
Strengths:
- The introduction of the future-dependent value function to tackle OPE in POMDPs is a significant contribution, potentially addressing the curse of horizon in learning.
- The paper is well-structured, with clear presentation and thorough discussions connecting to related works in OPE and system dynamics.
- The theoretical framework surrounding minimax learning and OPE is robust, complemented by empirical evaluations.

Weaknesses:
- The concept of incorporating future information into value functions is not entirely novel, as similar ideas have been explored in prior works.
- The statistical theory presented is somewhat standard, contingent on specific assumptions being met.
- The experimental section is limited, focusing primarily on a simple CartPole environment, which may not adequately demonstrate the method's effectiveness across diverse scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing more intuitive explanations of the future-dependent value functions and their motivations. Additionally, expanding the experimental evaluation to include more complex environments and additional baselines would strengthen the findings. It would also be beneficial to clarify the completeness assumption in the examples provided and to explore the trade-off between history and future proxies quantitatively. Furthermore, addressing the potential limitations of the algorithm in non-stationary settings could enhance the paper's applicability.