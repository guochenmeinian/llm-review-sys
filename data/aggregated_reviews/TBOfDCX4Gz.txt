ID: TBOfDCX4Gz
Title: GEQ: Gaussian Kernel Inspired Equilibrium Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 6, 4, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to deep equilibrium models (DEQs) by replacing linear kernels with Gaussian kernels, which enhances feature extraction capabilities. The authors propose a method termed patch splitting to improve performance, demonstrating the effectiveness of their model on datasets such as CIFAR10, CIFAR100, and saliency maps. Additionally, the paper critically examines the use of Multi-Layer Perceptrons (MLPs) in equilibrium models, arguing for the preference of kernels due to their simpler formulation and greater theoretical insights. The authors clarify that merely adding non-linear activations to the equilibrium function does not significantly enhance performance, as evidenced by comparisons with MDEQ. They propose that their Generalized Equilibrium Model (GEQ) outperforms other models due to its architecture's similarity to attention modules, which offer better expressive power than traditional linear kernels. The paper also discusses the theoretical implications of the model being infinitely deep and wide, exploring its potential connections to neural tangent kernels (NTK) and Gaussian processes.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a clear contribution by demonstrating that the proposed model is effectively infinitely deep and wide.
- It includes convergence bounds, stability analysis, and satisfactory experimental results, which support the proposed method's effectiveness.
- The exploration of matching the performance of equilibrium models with traditional deep learning models is a meaningful direction.
- The authors provide a clear rationale for using kernels over MLPs, emphasizing the complications MLPs introduce in equilibrium formulations.
- The paper effectively highlights the limitations of adding non-linear activations, supported by empirical comparisons with MDEQ.
- The introduction of the GEQ and its relationship to attention modules presents a novel perspective on model performance.

Weaknesses:
- There are significant issues with the mathematical results, leading to concerns about their soundness.
- The definition and role of the function \( f \) are unclear, particularly regarding its relation to ReLU activation and its implications in the model.
- The discussion on the generalization bounds is somewhat trivial, merely stating that they are "tighter" without deeper analysis.
- The experiments do not convincingly demonstrate that the proposed model outperforms standard neural networks, particularly on larger datasets like ImageNet.
- The paper lacks a comprehensive exploration of the implications of using kernels versus MLPs beyond theoretical insights.
- The discussion on the performance of MDEQ could benefit from more quantitative data to substantiate claims.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of \( f \) and its relationship to ReLU activation, addressing the ambiguities raised in the reviews. Additionally, we suggest enhancing the discussion on the model's connection to NTK and its implications for future research on Gaussian processes. To strengthen the experimental validation, we recommend including larger datasets such as ImageNet and exploring more robust training strategies to substantiate claims of superiority over standard neural networks. Furthermore, we recommend that the authors improve the exploration of the implications of using kernels versus MLPs by providing additional empirical data. Finally, including quantitative comparisons of MDEQ's performance would strengthen the argument regarding the limitations of non-linear activations in equilibrium models.