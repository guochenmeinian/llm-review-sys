ID: 4yXnnCK3r9
Title: On Proper Learnability between Average- and Worst-case Robustness
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 7, 6, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel exploration of probabilistically robust PAC learning, demonstrating that the finiteness of the VC dimension is insufficient for proper learning in this context. The authors propose Lipschitz losses that bridge average and worst-case scenarios, allowing for proper learning under certain conditions. They also extend the discussion to adversarially robust PAC learning and tolerant PAC learning, contributing to the existing literature on these topics.

### Strengths and Weaknesses
Strengths:  
- **New problem setting:** The introduction of probabilistically robust PAC learning is a significant contribution that may attract further interest in the field.  
- **Non-proper learnability:** The finding that finite VC dimension does not guarantee proper learning is intriguing and challenges previous empirical assumptions.  
- **Technical rigor:** The paper is technically sound, with well-structured proofs and a variety of tools that may benefit the broader community.  
- **Clarity and motivation:** The paper is generally well-written and presents interesting results that are relevant to the learning theory community.

Weaknesses:  
- **Unevenness of presentation:** The clarity diminishes in later sections, making it harder to parse results. Important results are less detailed, and the paper concludes abruptly without discussing implications.  
- **Writing and notation issues:** There are inconsistencies in notation and definitions, such as the use of different labels for VC dimension and Rademacher complexity, and missing definitions for key terms like "proper learning rule."  
- **Lack of high-level intuitions:** The paper could benefit from clearer explanations of the high-level ideas behind certain proofs and results.  
- **Missing references:** Several relevant works in robust learning theory are not cited, which could enhance the paper's context and depth.

### Suggestions for Improvement
We recommend that the authors improve the presentation by expanding on the results in later sections, breaking up long equations for better readability, and adding a discussion section to elaborate on the implications of their findings. Additionally, we suggest that the authors clarify the definitions of key terms, ensure consistent notation throughout the paper, and include equation numbers for easier reference. Addressing the high-level intuitions behind proofs, particularly for Lemma 3.2 and Theorem 4.3, would also strengthen the paper. Finally, we encourage the authors to incorporate missing references to provide a more comprehensive background on robust learning.