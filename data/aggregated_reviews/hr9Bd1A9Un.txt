ID: hr9Bd1A9Un
Title: Benchmark of Machine Learning Force Fields for Semiconductor Simulations: Datasets, Metrics, and Comparative Analysis
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: -1, 6, 6, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: -1, 5, 1, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two new benchmark datasets for training machine learning force fields (MLFF) on semiconductor materials, specifically hafnium oxide (HfO) and silicon nitride (SiN). The datasets include out-of-distribution (OOD) test sets and additional simulation/static metrics for evaluating MLFFs, benchmarking ten models, including descriptor-based and graph neural network (GNN)-based methods. The authors acknowledge the limited diversity of their dataset but emphasize its strategic importance in fostering interest in MLFF within the research community. They plan to introduce additional datasets, including Hf-Zr-O and Hf-Si-O systems, while balancing diversity with model stability. The authors clarify their data splitting methodology and enhance documentation for clarity, while also discussing the importance of AI/ML in semiconductor research and the limitations of current models, proposing future improvements.

### Strengths and Weaknesses
**Strengths:**
- The datasets are constructed using density functional theory (DFT), ensuring high-quality data.
- Introduction of OOD test data based on amorphous structures enhances the evaluation of MLFF generalizability.
- The paper provides simulation and static metrics that are useful for assessing MLFF performance in realistic settings.
- A comprehensive benchmarking of diverse ML approaches offers valuable insights into model performance.
- The manuscript addresses critical issues in semiconductor material research and provides a focused dataset.
- The authors have made substantial revisions based on reviewer feedback, enhancing clarity and documentation.
- They propose future directions for dataset expansion and model evaluation.

**Weaknesses:**
- The dataset lacks diversity, including only two materials, and could benefit from the inclusion of more complex materials.
- The authors did not adequately discuss the limitations of their dataset and evaluation approaches.
- Clarity is occasionally hindered by the reliance on supplementary information, making it harder to read.
- The initial documentation was unclear regarding data splits and usage instructions.
- The scaling studies did not initially follow a logarithmic approach, which could provide more meaningful insights.

### Suggestions for Improvement
We recommend that the authors improve the diversity of the dataset by including additional complex materials beyond SiN and HfO. Clarifying how the train, validation, and test data are split would enhance transparency. We suggest conducting log scaling experiments to better illustrate the performance of their models with varying dataset sizes. Including a data scaling plot to illustrate performance changes relative to training data size would also be beneficial. Furthermore, we recommend that the authors add a paragraph discussing the limitations and potential societal impacts of their work. To enhance accessibility for non-domain readers, we encourage the authors to provide more detailed explanations of the problem description, data formats, evaluation metrics, and comparisons with existing benchmarks, ensuring comprehensive documentation is maintained throughout the dataset to facilitate user understanding and accessibility.