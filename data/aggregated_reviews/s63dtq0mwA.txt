ID: s63dtq0mwA
Title: Understanding Information Storage and Transfer in Multi-Modal Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 5, 6, 7, 7, -1
Original Confidences: 3, 3, 4, 4, -1

Aggregated Review:
### Key Points
This paper presents an approach to investigate the layer in which multi-modal large language models (MLLMs) retrieve factual information, revealing that MLLMs primarily access early MLP and self-attention layers, contrasting with the middle-layer retrieval seen in traditional LLMs. The authors propose a method involving the replacement of input text tokens to trace causal relationships and identify how visual tokens influence information transfer. They introduce the VQA-Constraints dataset and a model-editing algorithm, MULTEDIT, which can correct factual errors in MLLMs.

### Strengths and Weaknesses
Strengths:
- The paper provides a method to locate layers in MLLMs responsible for factual information retrieval, which is timely given the current interest in MLLM research.
- It introduces a new dataset that supports the empirical study of MLLMs.
- The insights regarding the retrieval of information from early layers and the role of visual tokens are novel and could inform future MLLM designs.
- The presentation is clear, and the findings are well-supported by empirical data.

Weaknesses:
- While the observation that MLLMs retrieve information from early layers is intriguing, the authors do not explain why this differs from LLMs, which is crucial for understanding the implications of their findings.
- The paper lacks quantitative statistics on the specific layers responsible for information retrieval, despite constructing a new dataset that could facilitate such analysis.
- The overall approach is not particularly novel, as it adapts existing methods without substantial innovation beyond the token replacement technique.
- Section 5, which discusses editing MLLM knowledge, diverges from the core contributions and does not enhance the paper's value.

### Suggestions for Improvement
We recommend that the authors improve the explanation of why MLLMs retrieve information from early layers compared to LLMs, as this insight is vital for contextualizing their findings. Additionally, we suggest providing numerical statistics on layer contributions to enhance the empirical rigor of their claims. Clarifying the novelty of their approach in relation to prior work would strengthen the paper, and we advise revisiting Section 5 to ensure it aligns more closely with the main contributions of the study.