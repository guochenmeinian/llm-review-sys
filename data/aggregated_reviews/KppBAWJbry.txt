ID: KppBAWJbry
Title: Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 7, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel attack termed "privacy backdoors," which introduces vulnerabilities into foundation models, making them susceptible to membership inference attacks (MIAs). The core idea of the attack is to "poison" target data points in pretrained models, allowing adversaries to infer which data points were used for fine-tuning when victims upload their models to open-source platforms. The approach is evaluated on vision models (CLIP) and large language models (LLMs) (GPT-Neo), demonstrating robustness and effectiveness across various fine-tuning and inference methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, organized, and accessible to readers without expertise in privacy attacks.
- The proposed approach is novel and technically sound, supported by extensive experiments across different models and methods.
- The study addresses a significant and timely threat related to the use of pre-trained models.
- It introduces a new privacy attack setting and method, contributing to the important research topic of data privacy for foundational models.

Weaknesses:
- The assumption that the attacker possesses part of the training data is unrealistic, implying greater knowledge than typical in membership inference scenarios.
- The threat model is questionable; it assumes victims would finetune a poisoned model ($F_{adv}$) instead of the original pretrained model ($F_{pre}$), which is unlikely given the popularity and reliability of $F_{pre}$.
- The justification for victims choosing $F_{adv}$ over $F_{pre}$ lacks persuasiveness, especially when both models perform similarly.
- The methodology does not align with conventional definitions of a backdoor, as it lacks a trigger and predefined output.
- The intention of section 2.2 is unclear, particularly regarding its similarities to federated learning.
- The evaluation of LLMs is insufficient, focusing only on GPT-Neo-2.7B, while larger models like LLaMA and Mistral are more commonly studied.
- Performance evaluation of LLMs based solely on log perplexity loss is insufficient; benchmarks akin to accuracy in vision models would enhance clarity.
- The authors claim that maximizing loss for LLMs is ineffective but provide no explanation or experimental results to support this assertion.
- The reported lower validation loss after finetuning does not adequately reflect model performance; more concrete evaluations are needed.

### Suggestions for Improvement
We recommend that the authors improve the realism of their assumptions regarding the attacker's knowledge and clarify the definition of "backdoor" in the context of their work. Additionally, we suggest that the authors enhance the clarity of the threat model by explicitly addressing why victims would prefer to finetune $F_{adv}$ over $F_{pre}$. It would be beneficial to elaborate on the similarities to federated learning in section 2.2 and provide a more comprehensive evaluation of LLMs, including larger models, along with concrete performance metrics rather than relying solely on validation loss. We encourage the authors to include experimental results demonstrating the ineffectiveness of maximizing loss for LLMs and to discuss the impact of different data distributions on the proposed approach. Finally, we advise the authors to address the stealthiness of the attack by evaluating zero-shot performance on unrelated datasets and to clarify the model used in their experiments.