ID: W23ZTdsabj
Title: Are Vision Transformers More Data Hungry Than Newborn Visual Systems?
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 4, 7, 4, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the learning efficiency of Vision Transformers (ViTs) by comparing their invariant object recognition performance to that of newborn chicks exposed to a similar number of images. The authors find that ViTs can learn view-invariant representations akin to chicks, suggesting they are not more data-hungry than biological systems. The study employs a video game engine to simulate impoverished visual environments for both ViTs and chicks, demonstrating that ViTs can develop animal-like object recognition capabilities through attention-based learning mechanisms.

### Strengths and Weaknesses
Strengths:  
The paper presents a novel approach by training chicks with a regimented visual schedule, ensuring a fair comparison with ViTs. The writing is clear, and the logical structure is well-articulated. The authors tested multiple architectures to confirm the generality of their results, and the embedding space visualization provides insights into the performance of multiple attention-head models.

Weaknesses:  
A significant weakness is the unsubstantiated claim that ViTs are more "data-hungry" than biological brains, which contradicts previous findings. The authors fail to cite relevant literature supporting this claim and do not justify the continued investigation of this point. Additionally, the use of supervised linear classification during testing is questionable, as it does not reflect the unsupervised learning conditions of real animals. The simulated input's fidelity to real chicks' visual statistics is also unclear, raising concerns about the experimental design.

### Suggestions for Improvement
We recommend that the authors better address earlier works regarding the data hunger of ViTs compared to real animals. Additionally, we suggest replacing the supervised linear classification in the test phase with a simpler classifier that requires little or no training, such as a correlation classifier. It is also crucial to clarify whether the simulated input during training accurately reflects the visual statistics encountered by real chicks, including the movement patterns and frequency of these cycles.