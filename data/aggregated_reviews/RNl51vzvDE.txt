ID: RNl51vzvDE
Title: GNNGuard: A Fingerprinting Framework for Verifying Ownerships of Graph Neural Networks
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GNNGuard, a fingerprinting framework for GNNs that identifies pirated models through a joint training of a Univerifier and a graph fingerprint. The authors demonstrate the framework's effectiveness across various graph-based tasks, achieving high accuracy without degrading the GNN's utility. The paper includes extensive empirical evaluations and theoretical proofs.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and easy to follow.
- GNNGuard achieves desirable performance on different graph-based tasks.
- The framework effectively protects GNNs from model stealing.
- The empirical results are convincing, utilizing a wide range of real-world datasets.

Weaknesses:
- The motivation for GNNGuard is incomplete, lacking a clear definition of what constitutes a pirated GNN.
- The novelty of GNNGuard is limited, with insufficient discussion on the unique challenges of model fingerprinting for GNNs.
- The assumption of Gaussian output distribution in Theorem 3.2 is questionable and requires empirical discussion.
- The experiments assume real-world pirated models are fine-tuned or KD-based, which may not reflect actual scenarios.
- Background knowledge on model watermarking is over-presented and not directly relevant to GNNGuard.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of pirated GNNs and discuss its implications for real-world applications. Additionally, the authors should clarify the unique challenges of model fingerprinting on GNNs and how GNNGuard addresses these challenges. It would be beneficial to empirically discuss the error associated with the Gaussian assumption in Theorem 3.2 and provide evidence for its validity. We suggest adding experiments to assess the transferability of GNNGuard with various training methods for pirated models. Lastly, consider revising the background section to focus more on GNN-specific issues rather than general watermarking techniques.