ID: pQvAL40Cdj
Title: Detecting Any Human-Object Interaction Relationship: Universal HOI Detector with Spatial Prompt Learning on Foundation Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 7, 4, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for human-object interaction (HOI) detection called UniHOI, which utilizes vision-language foundation models and large language models (LLMs) for flexible recognition of complex interactions in images. The framework comprises a visual HOI detector, a HO prompt-guided decoder for high-level relation representations, and a knowledge retrieval module for generating descriptive texts for interaction categories. The authors demonstrate the effectiveness of UniHOI on benchmarks HICO-DET and V-COCO, achieving notable performance in both supervised and zero-shot settings.

### Strengths and Weaknesses
Strengths:
- The performance of UniHOI is impressive, particularly in zero-shot settings.
- The integration of LLMs and foundational models is a forward-thinking approach in computer vision tasks.
- The authors commit to releasing the code for reproducibility.

Weaknesses:
1. There is a lack of ablative studies for each component, particularly for the HO prompt decoder under the closed-set setup.
2. The use of BLIP2 with ViT-L raises concerns, as existing methods like GEN-VLKT typically utilize CLIP with ViT-B, which may skew performance comparisons.
3. Inference speed is a concern, as UniHOI is shown to be three times slower than GEN-VLKT due to the need for individual feature computation for each image.
4. The novelty of the HO prompt-based decoder is limited, as it primarily utilizes spatial location for foundational model output features.
5. Knowledge retrieval is only applied in open-world scenarios; its potential in closed-world setups is unclear.

### Suggestions for Improvement
We recommend that the authors improve the paper by conducting an ablative study of each component, particularly the HO prompt decoder, under a closed-set setup. Additionally, we suggest providing performance comparisons using CLIP ViT-B to ensure fair evaluation against existing methods. Addressing the inference speed issue is crucial; the authors should explore optimizations to reduce computation time. Furthermore, we encourage the authors to clarify the novelty of the HO prompt-based decoder and consider discussing the applicability of knowledge retrieval in closed-world scenarios. Finally, a more thorough discussion of limitations and failure cases would enhance the manuscript's depth.