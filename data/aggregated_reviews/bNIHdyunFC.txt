ID: bNIHdyunFC
Title: Learning Layer-wise Equivariances Automatically using Gradients
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 7, 7, 6, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for learning an interpolation between non-equivariant and equivariant models through a combination of convolutional and non-convolutional linear layers. The authors propose optimizing the contribution of these layers using Bayesian model selection, which involves evaluating a marginal likelihood term approximated via the Laplace method and a Kronecker factorization. Experiments demonstrate that the interpolated models outperform non-equivariant models and perform competitively against strictly equivariant models. The paper also discusses the ability to select between multiple symmetry groups.

### Strengths and Weaknesses
Strengths:  
- The paper effectively combines ideas from prior work and demonstrates superior performance compared to existing baselines.  
- It employs a Bayesian approach that allows for automatic hyperparameter learning, improving upon manual tuning methods.  
- The empirical evidence supports the claim that the model selection process can adaptively choose between equivariant and non-equivariant layers based on task symmetries.

Weaknesses:  
- The method does not learn equivariant models from scratch but selects from pre-specified models, a limitation shared with prior work.  
- The practical relevance of learning such selections is questionable, as appropriate levels of equivariance are often known a priori.  
- The main paper focuses primarily on translation equivariance, with more general results relegated to supplementary material.  
- Clarity in the explanation of the method could be improved, particularly regarding hyperparameters introduced in section 3.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the method's explanation, particularly by addressing the hyperparameters in the introduction and restructuring sections for better flow. Additionally, we suggest including a more comprehensive evaluation of the method's scalability to multiple symmetry groups and exploring its applicability to various data modalities beyond images. It would also be beneficial to provide a comparison with regular equivariant architectures to better contextualize the performance gains of the proposed method. Lastly, addressing the limitations more explicitly in the paper would enhance its overall rigor.