ID: mRwgczYZFJ
Title: A Critical Review of Causal Reasoning Benchmarks for Large Language Models
Conference: AAAI
Year: 2023
Number of Reviews: 4
Original Ratings: 2, 2, 2, -1
Original Confidences: 3, 2, 3, 3

Aggregated Review:
### Key Points
This paper examines the desiderata for benchmark datasets and tasks aimed at assessing the ability of large language models (LLMs) to perform causal inference. The authors propose four criteria to evaluate existing benchmarks, categorizing them according to these criteria. The paper serves as a valuable resource for the community in selecting or designing appropriate benchmarks for LLMs in causal inference tasks. It provides a comprehensive overview of relevant works and discusses the strengths and deficiencies of various benchmarks in detail.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to understand, making it accessible to a broad audience.
- It offers a significant impact on future research at the intersection of causality and LLMs by presenting a unifying set of quality criteria for NLP causal reasoning benchmarks.
- The authors provide a thorough review of recent papers on causal reasoning capabilities in LLMs, citing relevant works and evaluating the implementation of proposed criteria.

Weaknesses:
- There is a citation error on page 2, and a reference is missing towards the end of the same page.
- The criteria proposed require more refinement to be operationalizable, as it is unclear how to build a benchmark that is both non-retrievable and consistent with real-world mechanics.
- The paper could benefit from including experimental results to underscore the significance of the presented claims.
- It lacks a discussion on the ambiguous interpretations of causal relations in natural language, particularly regarding the proposed scenario of causal chains.

### Suggestions for Improvement
We recommend that the authors improve the operationalization of the proposed criteria to clarify how to build benchmarks that align with real-world mechanics. Additionally, the authors should consider discussing the potential need for different criteria for distinct causal tasks, as well as providing a specific task or query that fulfills all proposed criteria. Furthermore, addressing the ambiguity in natural language interpretations of causal relations would enhance the paper's depth. Lastly, correcting the citation error and ensuring consistent font sizes in figures would improve the overall presentation.