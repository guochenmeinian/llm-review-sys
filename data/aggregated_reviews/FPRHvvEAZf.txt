ID: FPRHvvEAZf
Title: Safe Decision Transformer with Learning-based Constraints
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 7, 6
Original Confidences: 3, 3, 3

Aggregated Review:
### Key Points
This paper presents the Constrained Q-learning Decision Transformer (CQDT), an innovative method aimed at enhancing policy synthesis in safe offline reinforcement learning (RL). By integrating Q-learning with decision transformer techniques, CQDT employs a novel trajectory relabeling scheme that utilizes learned value functions to recalibrate future reward and cost predictions in trajectory data. This significantly improves the synthesis of near-optimal policies while adhering to safety constraints. The authors demonstrate CQDT's effectiveness through extensive empirical evaluations across various safety-critical benchmarks, although concerns arise regarding its relevance to the workshop's main topic.

### Strengths and Weaknesses
Strengths:
- CQDT effectively synthesizes optimal trajectories by stitching suboptimal segments.
- The model shows robust performance across multiple safety-critical RL benchmarks while maintaining safety constraints.
- The trajectory relabeling mechanism enhances data efficiency and policy quality derived from offline datasets.
- Comprehensive empirical evaluations, including analyses of both sparse and dense reward environments, validate CQDT's superiority over existing methods.

Weaknesses:
- The integration of Q-learning with decision transformers introduces complexity, potentially complicating tuning and maintenance in practical applications. The training time should be reported in the appendix.
- The performance of the relabeling strategy in sparse reward environments lacks sufficient evidence, raising concerns about CQDT's ability to balance cost and reward in real-life scenarios.
- There is a risk of overfitting to relabeled trajectories, particularly with low diversity in the offline dataset.
- The paper lacks theoretical justification for the proposed method's effectiveness and does not sufficiently discuss its limitations.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the effectiveness of CQDT, as this would strengthen the paper. Additionally, clarifying the technical challenges in applying Constrained Penalized Q-learning to CDT would enhance the method section. It would also be beneficial to address the higher costs incurred by CQDT compared to CDT and explore the implications of sparse rewards being given only at the end of an episode. Lastly, providing clearer notation and ensuring that all necessary details are included in the main text, rather than relying on the appendix, would improve readability and comprehension.