ID: BHxsP5fSHv
Title: OKRidge: Scalable Optimal k-Sparse Ridge Regression
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 6, 7, 8, 8, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 3, 3, 4, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel branch-and-bound technique called OKRidge for solving sparse Ridge regression problems, particularly in the context of learning differential equations. The authors propose three main contributions: (1) a saddle point formulation that enables the use of a branch-and-bound algorithm tailored to the problem's sparsity, (2) an efficient algorithm that constructs tight lower bounds from this formulation, and (3) a beam search method that utilizes previous searches to expedite current node exploration. The paper includes extensive experiments demonstrating the method's efficiency compared to existing solvers.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clear, with a comprehensive literature review.
- Extensive experiments validate the proposed method, including sensitivity analyses on hyperparameters.
- The empirical results are promising, showcasing the method's effectiveness against various benchmarks.

Weaknesses:
- Some technical details may be unclear to non-experts, particularly regarding the integer constraint $z_j \in \{0, 1\}$.
- The paper lacks a detailed complexity analysis of the entire algorithm.
- There are concerns about the scalability of OKRidge with respect to the number of features, particularly inverting large matrices.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing a justification for the integer constraint $z_j \in \{0, 1\}$ in the introduction. Additionally, the authors should include a complexity analysis of the entire algorithm to better inform readers of its efficiency. To enhance the experimental section, consider using the Benchopt framework for more impactful figures, such as summarizing Figure 1 into a one-row figure. Lastly, we suggest addressing the scalability issues related to the number of features, particularly in the context of deriving lower bounds.