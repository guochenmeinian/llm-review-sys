ID: LNfWowAErI
Title: Dyadic Learning in Recurrent and Feedforward Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 9, 9
Original Confidences: 2, 3

Aggregated Review:
### Key Points
This paper presents a novel parametrisation of networks of two-state neurons, encompassing weight-space configurations from symmetric to feedforward and skew-symmetric. The authors develop a generalised saddle-point based learning framework that includes Equilibrium Propagation (EP) in symmetric Hopfield models, Dual Propagation in feedforward models, and a new two-phase learning algorithm for skew-symmetric Hopfield models (SSHM). The training for SSHM, while two-phased like EP, differs significantly in its phases. The inherent negative feedback in SSHM offers greater robustness to large nudging signals during training and potentially enhances resilience against adversarial perturbations.

### Strengths and Weaknesses
Strengths:
- The paper presents a well-developed theoretical framework that encompasses various local learning approaches.
- SSHMs are highlighted as understudied, with the paper emphasizing their potential advantages, particularly regarding negative feedback's beneficial properties in temporal-difference approximations to backpropagation.
- The writing is clear, with motivation and experimental results effectively articulated.

Weaknesses:
- The theory is dense and would benefit from a more detailed explanation of terms and motivations, which would enhance comprehension in a longer format.
- Additional commentary on the differences from EP would strengthen the paper, particularly regarding the implications of minmax optimisation steps versus two minimisation steps and their relevance to neuroscience and biological learning.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical framework by providing a more in-depth explanation of key terms and motivations. Additionally, we suggest including more comments on the differences to EP, specifically addressing the implications of minmax optimisation steps compared to minimisation steps and exploring their potential relevance to neuroscience and biological learning.