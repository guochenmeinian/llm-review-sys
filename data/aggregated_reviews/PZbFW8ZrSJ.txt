ID: PZbFW8ZrSJ
Title: TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TabularBench, the first comprehensive benchmark for evaluating the adversarial robustness of tabular deep learning models in real-world applications. The authors propose a structured evaluation of over 200 models across various domains, addressing the absence of standardized benchmarks for tabular adversarial attacks. The benchmark includes a dataset zoo, model zoo, and an initial analysis of common tabular classification methods, utilizing advanced attack mechanisms and robustification techniques inspired by computer vision research. Additionally, the authors introduce the CAA attack in the Appendix for self-containment and emphasize that their code is open for incorporating new attacks. They also present six new adversarial-training based data-augmentation techniques and a live public leaderboard for standardized benchmarking, aiming to accelerate research in tabular ML robustness to match the maturity seen in computer vision.

### Strengths and Weaknesses
Strengths:  
- TabularBench is a pioneering contribution, establishing a new standard for adversarial robustness evaluation in tabular data.  
- The evaluation process is rigorous, and the methodologies are clearly presented.  
- The work promotes the development of secure AI technologies with practical implications for real-world applications.  
- The authors provide a Python library for running and evaluating models/defenses on the benchmark, enhancing accessibility.  
- The benchmark encourages community engagement by allowing submissions of new attacks and robust accuracy metrics.  
- The introduction of a dataset zoo and new data-augmentation techniques provides valuable resources for researchers.  
- The live public leaderboard and API facilitate standardized evaluation of models and attacks.

Weaknesses:  
- The benchmark relies solely on the CCA attack, which is not well-introduced and is based on an un-reviewed paper, raising concerns about its validity.  
- The reliance on CAA as the main attack may limit the benchmark's applicability.  
- There is a lack of scalability discussion for larger datasets and complex models.  
- Some reviewers express concerns about the dual submission of the method and benchmark, suggesting it may affect the acceptance of the paper.  
- The documentation on the GitHub repository could be improved, particularly regarding installation instructions and adding results to the leaderboard.  
- The interpretability of adversarial attacks and defenses in tabular models is not sufficiently addressed.

### Suggestions for Improvement
We recommend that the authors improve the introduction of the CCA attack to ensure the paper is self-contained. Additionally, the authors should include a detailed description of the CAA attack, its limitations regarding adaptive attacks, and clarify how the benchmark can be utilized beyond the CAA attack. Exploring strategies for scaling the benchmark to larger datasets and more complex models, possibly discussing techniques like data parallelism or algorithmic optimizations, would be beneficial. A detailed analysis of the computational resources required to run the benchmark would enhance transparency and reproducibility. 

To improve the transferability of defense mechanisms, conducting experiments on diverse tabular datasets is advisable. Expanding the evaluation to include a wider range of datasets would strengthen the generalizability of the findings. Furthermore, we suggest adding the possibility to include results from adaptive attacks in the leaderboard, as they are crucial for effectively evaluating model robustness. Finally, providing clearer documentation on the GitHub repository, including installation instructions and how to add new results, would significantly enhance user experience. Addressing the concerns regarding the dual submission could also strengthen the paper's acceptance prospects.