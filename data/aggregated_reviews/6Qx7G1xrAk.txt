ID: 6Qx7G1xrAk
Title: ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text Translation
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a composite speech-language model (ComSL) for speech-to-text (S2TT) translation, leveraging existing pre-trained models such as Whisper and mBART. The authors propose several modality alignment methods that do not require additional tools, achieving state-of-the-art (SOTA) performance on the CoVoST2 dataset. The model is evaluated on multiple tasks, including speech-to-text translation, and demonstrates data efficiency through cross-modality loss functions.

### Strengths and Weaknesses
Strengths:
- Good performance reported on CoVoST speech-to-text translation tasks.
- Insightful design of large-scale spoken language models and effective use of concatenated embeddings for modality alignment.
- Well-written with a clear explanation of motivations and methods, including a detailed analysis and ablation study.

Weaknesses:
- The contribution is limited as the training process may be unstable, with hyperparameters for multiple losses not adequately explained.
- The improvement over Whisper is minimal when trained without pseudo speech-to-text data, raising questions about the true impact of the proposed approach.
- The evaluation primarily focuses on speech-to-text tasks, neglecting to benchmark the model on automatic speech recognition (ASR) tasks, which is unexpected given the multi-task training approach.

### Suggestions for Improvement
We recommend that the authors improve the differentiation of their method from existing models like SpeechT5 and SLAM by clearly articulating the unique aspects of their approach. Additionally, the authors should provide a thorough explanation of how hyperparameters for loss weights are determined and their impact on training stability. It would be beneficial to include ASR evaluation results to substantiate the model's performance across tasks. Finally, we suggest adding statistical significance testing to the results to enhance the interpretation of performance improvements.