ID: qK4iS49KDm
Title: Neural network learns low-dimensional polynomials with SGD near the information-theoretic limit
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on learning single index models under Gaussian inputs, specifically focusing on the function class defined by $f_*(x)=\sigma_*(\langle \theta_*,x \rangle)$. The authors demonstrate that by reusing batches in an SGD-based training of a two-layer neural network, one can achieve a vanishing $L_2$ error with $O(d \text{polylog}(d))$ samples, approaching the information-theoretic limit. The analysis reveals that reusing samples allows the algorithm to escape the CSQ lower bound, leading to a sample complexity of $\tilde{O}(d)$, irrespective of the information exponent.

### Strengths and Weaknesses
Strengths:  
1. The paper addresses a significant question regarding the complexity of gradient descent algorithms on neural networks, advancing the understanding of SGD with reused batches beyond previous works.
2. It considers strong recovery, which is technically challenging, and provides a clear end-to-end analysis with established learning guarantees.
3. The paper is well-written, offering new perspectives on designing SQ algorithms and presenting interesting intuitions regarding the reduction of the information exponent through monomial transformations.

Weaknesses:  
1. The training procedure is slightly non-standard, and while understandable, the necessity of momentum in the first-layer training is unclear.
2. The empirical validation is minimal, with only one experiment, and the authors do not address standard SGD practices such as simultaneous training of layers or using standard activation functions.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the necessity of momentum in the first layer of training. If it is essential, they should elaborate on its importance in the current analysis. Additionally, we suggest providing more empirical validation to support their claims, including experiments that incorporate standard SGD practices. The authors should also clarify the dependence of constants in their big-O notation on the dimension $d$ and explicitly present this in their final bounds. Furthermore, a discussion on the relationship between their work and prior studies, particularly regarding the use of polynomials versus neural networks, would enhance the manuscript's depth.