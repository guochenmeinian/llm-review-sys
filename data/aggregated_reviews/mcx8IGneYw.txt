ID: mcx8IGneYw
Title: Neural Lighting Simulation for Urban Scenes
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 4, 7, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LightSim, a lighting-aware camera simulation system designed to enhance robot perception by generating relightable digital twins from real-world sensor data. The system facilitates applications such as actor insertion, modification, removal, and re-rendering. It employs a multi-stage process to train a dynamic Neural Radiance Fields (NeRF) model, decomposing scenes into static and dynamic components while learning illumination as a high dynamic range (HDR) sky dome. The authors propose a method for lighting simulation that leverages real data through a novel data pair training scheme, combining synthetic paired images with real data to train a neural deferred rendering network. The paper discusses challenges related to strong directional light and emphasizes the importance of integrating various data sources for improved lighting estimation.

### Strengths and Weaknesses
Strengths:
1. The authors propose a comprehensive system for outdoor illumination estimation in urban driving scenes, leveraging physics-based rendering for controllable dynamic scene simulation.
2. The integration of multiple data sources, particularly supervision from sun angles and GPS data, enhances the illumination estimation process.
3. The paired learning scheme for the neural renderer yields high-quality relighting and editing results that surpass those of purely synthetic data.
4. The authors provide a clear discussion on temporal consistency and the potential for optimization to recover lighting information.
5. The inclusion of additional visualizations and results enhances the paper's clarity and depth.

Weaknesses:
1. The paper appears to integrate existing works without significant innovation, making its unique contributions difficult to discern.
2. The novelty of the approach is perceived as limited, lacking significant new ideas that could substantially enhance the system.
3. The motivation for using neural deferred rendering requires clarification, especially regarding the necessity of training an additional network for rendering.
4. There is a need for clearer explanations regarding the acquisition of 3D bounding box annotations and the implications for reproducibility.
5. The paper does not adequately address failure cases related to strong directional light and the dependency of later stages on estimated lighting.
6. Inconsistencies in terminology and descriptions, such as the material map and the definition of digital twins, need to be addressed for clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation for neural deferred rendering and explicitly state the unique contributions of their work compared to existing literature. Additionally, we suggest providing detailed explanations of the material parameters used in the reconstruction process and ensuring consistent terminology throughout the paper. It would also be beneficial to include examples of the albedo reconstruction results and address the limitations of the relighting results, particularly regarding reflections and temporal consistency. We encourage the authors to improve the discussion on temporal consistency by explicitly mentioning its emergence from consistently timed inputs in the revised manuscript. Furthermore, the authors should include examples demonstrating the potential of the proposed inpainting network to recover the sun's location and consider a hybrid approach of inference combined with short test-time optimization. A more detailed discussion of failure cases with strong directional light is necessary, educating readers about the limitations. We also suggest that the authors clarify how 3D bounding box annotations are obtained and consider integrating off-the-shelf models for automatic predictions. Lastly, we encourage the authors to provide additional results and analysis regarding the dependency of later stages on estimated lighting, as well as to explore the potential for material editing within their framework.