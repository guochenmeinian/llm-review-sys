ID: GIlsH0T4b2
Title: Two-Stage Learning to Defer with Multiple Experts
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 5, 7, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 1, 2, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies generalization bounds for learning to defer in a two-stage algorithmic framework, where a classifier is fitted using a surrogate loss followed by a rejector that determines which expert to defer to. The authors propose a new family of surrogate loss functions and provide theoretical analysis, including bounds on the difference between target and surrogate losses. The paper addresses the gap in the literature for two-stage algorithms, particularly in multi-expert settings, and presents empirical results demonstrating improved performance with more experts.

### Strengths and Weaknesses
Strengths:
- The theoretical results are relatively well-presented and fill a significant gap in the existing literature on learning to defer.
- The problem setup is realistic and relevant, particularly for applications involving pretrained models.
- The proposed surrogate loss functions are novel and well-motivated, with strong theoretical guarantees.

Weaknesses:
- The empirical results lack comparisons to other surrogate methods, which limits the assessment of the proposed approach's effectiveness.
- Some proofs, particularly for Theorems 5 and 7, are unclear and may contradict counterexamples, raising questions about their soundness.
- The experimental section does not provide sufficient insights, as it primarily shows that performance improves with more experts without comparing to baseline classifiers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proofs for Theorems 5 and 7, addressing potential contradictions with counterexamples. Additionally, the authors should include empirical comparisons with existing surrogate methods to strengthen their claims. It would be beneficial to discuss how the proposed surrogate reduces to other methods in the literature, particularly in the single-expert case. Furthermore, we suggest enhancing the experimental section to include performance metrics of the originally trained classifier after the second-stage adoption, providing insights into whether this training degrades the classifier's performance.