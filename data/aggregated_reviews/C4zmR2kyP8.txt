ID: C4zmR2kyP8
Title: Stabilizing Zero-Shot Prediction: A Novel Antidote to Forgetting in Continual Vision-Language Tasks
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel continual learning (CL) method called ZAF (Zero-shot Antidote to Forgetting) aimed at improving the retention of previously learned skills in vision-language (VL) models without relying on historical data replay. The authors propose zero-shot stability as a key indicator of anti-forgetting capabilities, integrating a stability regularization term and an efficient EMA-LoRA neural architecture. The method demonstrates significant performance improvements across various benchmarks, supported by both empirical studies and theoretical analysis. Furthermore, the authors address concerns regarding the differentiation from self-supervised learning (SSL) by providing substantial experimental support for their claims.

### Strengths and Weaknesses
Strengths:
1. The introduction of zero-shot stability regularization and the EMA-LoRA architecture offers a novel approach to addressing the forgetting problem in CL.
2. The combination of empirical and theoretical rigor enhances the validity of the proposed method.
3. The extensive experiments conducted on multiple VL benchmarks showcase ZAF's effectiveness and computational efficiency compared to existing methods.
4. The authors have clarified previous concerns regarding the comparison to related works, enhancing the paper's overall quality.
5. The experimental evidence presented supports the effectiveness of the proposed method in leveraging unlabeled data for continual learning.

Weaknesses:
1. The evaluation of ZAF is limited to a small number of subtasks; broader experimentation with more diverse datasets is necessary for comprehensive validation.
2. The paper lacks detailed numerical comparisons in Section 3.2, and the qualitative insights provided are insufficiently actionable.
3. The reliance on additional unaligned data during training raises concerns about the method's novelty and its classification as semi-supervised learning.
4. There remains some skepticism about the clear separation of the proposed method from SSL, which has not been fully resolved.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation by conducting experiments across a larger variety of subtasks and datasets to provide a more comprehensive assessment of ZAF's effectiveness. Additionally, we suggest including detailed numerical comparisons in Section 3.2 to enhance the actionability of the insights presented. Clarifying the motivation behind the loss term in Eq. 4 and discussing the implications of using additional unaligned data in the context of semi-supervised learning would also strengthen the paper. Finally, we recommend improving the clarity of the distinction between their approach and SSL to further strengthen their argument, and providing more detailed explanations or examples could help address lingering doubts regarding this separation.