ID: UqvEHAnCJC
Title: End-to-End Ontology Learning with Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 7, 4, 4, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel methodology for Ontology Learning using Large Language Models (LLMs) and ontology paths, termed OLLM. The authors introduce two ontology datasets from Wikipedia and ArXiv, training an LLM on titles, summaries, and ontology paths to facilitate sub-graph generation. The methodology is enhanced through post-processing techniques for summing and pruning subgraphs to construct a complete ontology. New metrics are proposed for comparing the generated ontology with ground truth and baseline techniques. The paper demonstrates that OLLM outperforms many baseline methods, particularly in producing semantically accurate ontologies.

### Strengths and Weaknesses
Strengths:
- The work introduces a novel technique leveraging LLMs and ontology paths for ontology learning.
- Two distinct ontology datasets are constructed for experimental validation.
- New metrics for ontology evaluation based on semantic similarity are proposed.
- OLLM shows superior performance compared to most baseline methods.
- The paper is well-written and addresses a significant real-world problem.

Weaknesses:
- The performance of OLLM is not consistently superior across all metrics, particularly the motif distance metric.
- There is a significant drop in performance with smaller datasets like ArXiv.
- The generalizability of OLLM to different domains without an initial ontology is unclear.
- The assumption that each document contains at least one relevant concept may not hold in real-world applications.
- The paper lacks a detailed explanation of the training process and does not include a comparative analysis with other ontology learning methods.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the performance discrepancies observed with the motif distance metric. Additionally, the authors should address the significant performance drop when using smaller datasets and clarify the generalizability of OLLM to various domains without prior ontologies. It would be beneficial to explore the implications of assuming that each document contains relevant concepts and to ensure the consistency of the generated ontology. We suggest including entity resolution mechanisms in the post-processing procedures to merge semantically equivalent concepts. Finally, a comparative analysis with other ontology learning methods would strengthen the paper's claims regarding OLLM's effectiveness.