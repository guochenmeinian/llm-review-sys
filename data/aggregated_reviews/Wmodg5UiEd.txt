ID: Wmodg5UiEd
Title: Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on linear contextual dueling bandits with adversarial feedback, proposing an algorithm named robust contextual dueling bandits (RCDB) that utilizes uncertainty-weighted maximum likelihood estimation (MLE). The authors establish a regret bound of the order \( \widetilde{O}(d\sqrt{T} + CT) \), which aligns with known lower bounds in both adversarial and non-adversarial settings. The paper includes a thorough experimental evaluation to support its theoretical claims.

### Strengths and Weaknesses
Strengths:  
- The topic of adversarial feedback in dueling bandits is well-motivated and significant.  
- The proposed algorithm RCDB incorporates uncertainty-dependent weighting into the MLE, showcasing innovation.  
- The theoretical performance of RCDB is rigorously analyzed, and the experimental results validate its effectiveness.  
- The related work section is comprehensive and accurate.  

Weaknesses:  
- The originality of the technical contributions is questionable, as the main results seem to rely on established techniques from prior research.  
- Assumption 3.1's reliance on a linear reward may limit the framework's applicability; the authors should clarify potential extensions.  
- The analysis in Section 5.2 regarding the unknown case appears trivial.  
- There is a lack of explicit discussion on the computational challenges of determining actions \( (a_t, b_t) \).  
- The paper's presentation is dense, with some mathematical notations introduced abruptly, making it difficult to follow.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by addressing the abrupt introduction of mathematical notations, such as the Gram matrix in line 209. Additionally, please elaborate on the technical challenges encountered in the proof of Theorem 5.3 and clarify how the approximations in Section 4 affect the regret analysis. It would also be beneficial to include variance in the experimental results presented in Figure 1. Lastly, consider discussing the implications of Assumption 3.1 more thoroughly, particularly regarding the potential for extending the framework beyond linear rewards.