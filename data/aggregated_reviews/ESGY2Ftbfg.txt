ID: ESGY2Ftbfg
Title: Pre-training Language Models for Comparative Reasoning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to comparative reasoning by proposing a scalable data collection method that mines entities from structured resources, enhancing language models (LMs) through a new pre-training paradigm. The authors demonstrate improvements in comparative reasoning tasks, specifically in question answering, question generation, and summarization, using T5 and BART models. Additionally, they introduce a benchmark for evaluating comparative reasoning tasks, extracted from various datasets.

### Strengths and Weaknesses
Strengths:
- The paper provides substantial resources for comparative reasoning research, establishing a foundation for future studies.
- The proposed pre-training paradigms yield significant performance improvements in comparative reasoning tasks, particularly in lower-resource settings.
- The thorough analysis of pre-training objectives supports the authors' contributions, showcasing the benefits of their approach.

Weaknesses:
- The contextualization of results with current large language models, such as ChatGPT, is unclear, particularly regarding zero-shot and in-context learning settings.
- The framing of comparative reasoning as a unique phenomenon lacks sufficient evidence, potentially blurring the lines with multi-hop question answering.
- The pre-training dataset is relatively small, raising concerns about catastrophic forgetting and the generalizability of the models to other tasks.

### Suggestions for Improvement
We recommend that the authors improve the contextualization of their findings by providing more details on the evaluation of ChatGPT and its relevance to their results. Clarifying the objectives in sections 3.3.1 and 3.3.2 would enhance understanding of whether the model learns to generate questions and answers in an unsupervised manner. Additionally, including comparisons to existing synthetic question generation methods and addressing the potential for catastrophic forgetting would strengthen the paper. Finally, discussing the quality and open-sourcing of the Diffen dataset, along with providing examples, would enhance transparency and utility for future research.