ID: X6Eapo5paw
Title: Latent SDEs on Homogeneous Spaces
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for variational inference on latent functions using stochastic differential equations (SDEs) on homogeneous spaces, specifically focusing on the n-sphere. The authors propose a generative model where a path in latent space is generated according to a prior, with observations taken from this path at discrete points. The method leverages the properties of SDEs on compact latent spaces, allowing for the definition of uninformative priors and connections between initial and stationary distributions. The authors apply their approach to various regression, classification, interpolation, and extrapolation tasks, demonstrating its utility.

### Strengths and Weaknesses
Strengths:
- The proposed method is mathematically elegant, utilizing SDEs on compact latent spaces to facilitate uninformative priors and meaningful correspondences between distributions.
- The performance across various tasks is compelling, showcasing the method's effectiveness and the ability to access posteriors over entire latent trajectories.

Weaknesses:
- The presentation of the material is poor, with a disconnect between the setup in equations (1) and (2) and their applications, particularly regarding the objective functions for regression and classification tasks.
- The introduction lacks clarity, particularly in distinguishing path-valued observations from finite discrete observations.
- The condition on $h$ in line 103 is confusing, and the presentation of the ELBO around equation (2) does not adequately address the KL divergence issues.
- The inference network's necessity is unclear, and the notation around equation (7) is confusing.
- The extrapolation example with rotated MNIST raises questions about the reliance on Chebyshev polynomials and the choice of end time points during optimization.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, particularly in the introduction and the connection between equations (1) and (2). It would be beneficial to explicitly define the objective functions for the regression and classification tasks. Additionally, we suggest clarifying the role of the condition on $h$ and the necessity of the inference network. A more thorough discussion of the ELBO and its implications for KL divergence should be included. Furthermore, we encourage the authors to provide insights into the extrapolation example, particularly regarding the choice of end time points and the implications for general applications.