ID: dOJ6CqWDf1
Title: Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called "weak-to-strong search" for aligning large language models (LLMs) without fine-tuning. The authors propose leveraging the log-likelihood difference between tuned and untuned small language models to guide the decoding of a frozen large model. The approach is applicable to both white-box and black-box models and is validated through experiments in controlled-sentiment generation, summarization, and instruction following.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, with a clear motivation and promising empirical results.
- The weak-to-strong search method is novel and computationally efficient, demonstrating effectiveness across various tasks.
- The theoretical grounding is solid, with detailed mathematical formulations.

Weaknesses:
- The comparison between the proposed method and existing techniques, particularly those relying on small language models for alignment, lacks depth, making the novelty less clear for non-experts.
- There is insufficient discussion on the performance dependency of the tuned weaker model and the overhead introduced during inference.
- The paper does not adequately address how the method compares to other inference-time techniques, such as in-context learning and prompting methods like CoT.

### Suggestions for Improvement
We recommend that the authors improve the comparison with existing decoding-based alignment baselines and clarify the novelty of their approach. An ablation study using the log-likelihood difference for token/chunk-wise PPO should be included to enhance completeness. Additionally, the authors should discuss the implications of using a tuned weaker model and the overhead during inference more thoroughly. Exploring how weak-to-strong search could enhance reasoning abilities on datasets like GSM8K would also be beneficial.