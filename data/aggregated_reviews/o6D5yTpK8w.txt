ID: o6D5yTpK8w
Title: Exploring Graph Pre-training for Aspect-based Sentiment Analysis
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two pre-training paradigms aimed at enhancing the generation model for Aspect-Based Sentiment Analysis (ABSA): Element-level Graph Pre-training and Task-level Graph Pre-training. The Element-level Graph Pre-training focuses on core sentiment elements using a sentiment element-level masking strategy, while the Task-level Graph Pre-training decomposes the quadruple extraction task into multiple subtasks to build an opinion tree from scratch. Experimental results validate the effectiveness of these methods.

### Strengths and Weaknesses
Strengths:
- The proposed pre-training methods are reasonable and well-detailed, contributing to the understanding of ABSA.
- The manuscript is well-organized, and the implementation is reproducible, with sufficient support for claims.

Weaknesses:
- Experiments are limited to the ACOS dataset, reducing the generalizability of the findings.
- The study is perceived as incremental, lacking substantial novelty compared to existing works, and does not convincingly explain the performance improvements observed.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by including additional datasets to enhance the robustness of their findings. Furthermore, we suggest providing more detailed explanations of the performance improvements and discussing case studies or error analyses to substantiate the effectiveness of each proposed component. Additionally, clarifying the implementation details, particularly regarding the fine-tuning settings and prompts used for the LLaMa-7B model, would strengthen the paper. Lastly, addressing the identified grammatical errors and typos will enhance the overall presentation.