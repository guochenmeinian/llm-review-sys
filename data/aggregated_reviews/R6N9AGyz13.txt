ID: R6N9AGyz13
Title: Parallelizing Model-based Reinforcement Learning Over the Sequence Length
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 5, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new model-based reinforcement learning method, PaMoRL, which parallelizes world model training and eligibility trace estimation using efficient parallel scan operations. The authors claim that PaMoRL achieves competitive performance and significant training efficiency improvements on the Atari 100K and DM Control benchmarks.

### Strengths and Weaknesses
Strengths:
- The writing is generally clear and easy to follow.
- The novel design of accelerating eligibility trace computation with parallel scans can benefit model-free RL methods.
- The experiments are comprehensive and solid, demonstrating significant speed-ups in model generation and eligibility trace estimation.

Weaknesses:
- The core idea revolves around parallel scans, which have been previously utilized in Mamba, yet the authors provide limited discussion on this connection.
- The benefits of the proposed modified linear attention compared to SSM with scan lack quantitative evidence.
- The results should include metrics recommended in [i] for more reliable comparisons.
- There is insufficient analysis of how PaMoRL compares to recent works like [ii].
- The GPU overhead from scanners needs clarification regarding its scalability with different configurations.
- Limited ablations make it difficult to assess the importance of various algorithmic changes.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the connections to Mamba and provide a more thorough analysis of the benefits of the modified linear attention. Additionally, please report results using metrics from [i] to enhance comparison reliability. It would be beneficial to include a comparison with the recent work [ii] and clarify how GPU overhead scales with different configurations. Finally, we suggest conducting more ablation studies to better understand the impact of the various algorithmic changes on both data and hardware efficiency.