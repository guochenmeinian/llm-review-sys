ID: 1kgK0r8PGg
Title: Exponentially Convergent Algorithms for Supervised Matrix Factorization
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 7, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel supervised dictionary model with two variations—feature-based and filter-based—aimed at classification tasks involving both high- and low-dimensional features. The authors propose a low-rank projected gradient descent (LPGD) algorithm, which guarantees exponential convergence and effectively combines supervision with reconstruction to learn low-rank matrices. The paper includes theoretical guarantees and empirical validation of the algorithm's performance against standard classification methods, particularly in cancer-related gene sequence analysis. While the authors acknowledge that their work does not reinvent supervised dictionary learning (SDL), they provide a strong theoretical analysis that demonstrates the relevance of their approach.

### Strengths and Weaknesses
Strengths:
1. The paper is clearly written and easy to follow.
2. The proposed algorithm offers theoretical convergence guarantees and demonstrates improved performance over prior algorithms and standard classification methods.
3. The authors provide a clear theoretical framework and guarantees for their LPGD algorithm.
4. The integration of supervision with reconstruction enhances the method's applicability.
5. The authors are responsive to reviewer feedback and willing to make necessary revisions.

Weaknesses:
1. Benchmarking against deep neural networks is lacking, which could provide insights into performance gaps and trade-offs between interpretability and classification accuracy.
2. The paper's claims regarding global convergence and the definition of supervised dictionary learning are not rigorously established, leading to potential confusion about the method's novelty and applicability.
3. Some reviewers express that the application of the method to a breast cancer dataset does not present surprising results, as similar approaches have been previously documented.
4. There are concerns regarding the orthogonality of the basis vectors produced by the algorithm and the clarity of the joint optimization formulation.

### Suggestions for Improvement
We recommend that the authors improve the literature review to include sparse (overcomplete) dictionary learning, as this is a well-known aspect of the field. Additionally, we suggest clarifying the definition of supervised dictionary learning and its key properties, particularly regarding sparsity. It would also be beneficial to include a comparison with deep learning approaches and address how the proposed method differs from PCA, especially in terms of performance and theoretical implications. Furthermore, we recommend improving the clarity of the joint optimization formulation to better demonstrate how it yields orthogonal supervised basis vectors. We suggest revising the terminology from "supervised dictionary learning" to "supervised low-rank matrix factorization" to align with existing literature. Lastly, it would be advantageous for the authors to incorporate discussions on the unrolling of their algorithm into a deep network and its potential for GPU implementation, as well as to consider publishing the source code to enhance reproducibility.