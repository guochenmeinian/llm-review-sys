ID: YbsvNFD21C
Title: Robust Sparse Regression with Non-Isotropic Designs
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 5, 5, 4, 7, -1, -1, -1, -1
Original Confidences: 2, 2, 2, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents efficient estimators for robust and sparse linear regression in the presence of both oblivious and adaptive adversaries. The authors propose two main results: achieving $O(\sqrt{\epsilon})$ error with a sample complexity of $O(k^2/\epsilon)$, and achieving $o(\epsilon)$ error with a sample complexity of $O(k^4/\epsilon^3)$, with improvements for correlated Gaussian designs. The authors also provide statistical query lower bounds and analyze the performance of their algorithms under specific assumptions regarding noise and the distribution of the design matrix.

### Strengths and Weaknesses
Strengths:
- The authors improve the state-of-the-art results for $O(\sqrt{\epsilon})$ error, achieving sample complexity independent of the $l-2$ norm of the true weight vector, $\beta_{*}$.
- They introduce robust algorithms that outperform existing methods under desired conditions on the moments of the distribution.
- The theoretical analyses are rigorous, with clear proofs and assumptions stated.

Weaknesses:
- The algorithms may require significant computational resources, potentially limiting practical applications.
- There is a lack of empirical evidence comparing the proposed methods with state-of-the-art approaches, even in simple scenarios.
- The presentation of the sample complexity bounds is unclear, and the transition between results for $O(\sqrt{\epsilon})$ and $o(\sqrt{\epsilon})$ errors appears abrupt.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the sample complexity bounds by fixing the goal to achieving $\epsilon$ error and rewriting the bounds accordingly. Additionally, we suggest that the authors provide a more detailed discussion of related work at the level of techniques, as the current presentation in Section 2 is dense and informal. A conclusion section and pointers to future work should be included to enhance the manuscript's structure. Furthermore, we encourage the authors to include empirical comparisons with existing methods to substantiate their claims and to address the computational resource concerns associated with their algorithms.