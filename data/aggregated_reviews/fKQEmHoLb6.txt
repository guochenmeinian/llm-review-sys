ID: fKQEmHoLb6
Title: Learning Energy-Based Prior Model with Diffusion-Amortized MCMC
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 3, 4, 8, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel training and sampling procedure for learning energy-based generative models, specifically introducing the DAMC (Diffusion-Amortized MCMC) sampler and a new learning algorithm for Latent-space Energy-Based Models (LEBMs). The method is evaluated against traditional (few-step) MCMC approaches and diffusion models on various computer vision tasks, demonstrating theoretical and empirical effectiveness.

### Strengths and Weaknesses
Strengths:
- The paper introduces an original algorithm with a clear methodological framework and extensive benchmarking results.
- The theoretical analysis is sound, and the experimental evaluations convincingly support the proposed method's effectiveness.

Weaknesses:
- The necessity for additional energy-based modeling methodology is not clearly articulated for a general audience, particularly regarding its advantages over diffusion models.
- The claim of "theoretical evidence that the learned amortization of MCMC is a valid long-run MCMC sampler" lacks explicit formulation, such as a proposition or theorem.
- The basic concept of using an auxiliary model to amortize learning in energy-based models has been previously explored, necessitating clearer connections to existing literature and methodologies.
- The performance of the proposed method on CIFAR-10 is significantly lower compared to prior works, and the long-run MCMC analysis is based on fewer iterations than typically employed in the field.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation for energy-based modeling, particularly in explaining its advantages over diffusion models. Providing additional context on the limitations of previous approaches would enhance understanding. 

We suggest making the theoretical claim regarding the learned amortization of MCMC more explicit, potentially as a proposition or theorem. 

Additionally, we encourage the authors to include a more comprehensive comparison with existing methods in the literature, particularly those that utilize auxiliary models for amortization. 

Finally, we recommend that the authors explore and report results using deeper network architectures to validate the effectiveness of their method further, as this could provide valuable insights into its scalability and performance.