ID: 639RkUOmW8
Title: No-Regret Learning with Unbounded Losses: The Case of Logarithmic Pooling
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of the logarithmic pooling method for prediction using expert advice, focusing on minimizing log loss. The authors propose a generalization of logarithmic pooling where expert weights are not fixed but learned, aiming for low regret in predictions. The algorithm employs online mirror descent with a Tsallis entropy regularizer, achieving an upper bound on regret of approximately \( m^{3/2}n\sqrt{T}\log T \) in the regime where \( T \gg m, n \).

### Strengths and Weaknesses
Strengths:
- The problem setting is natural and well-motivated, with clear explanations of necessary assumptions supported by illustrative examples.
- The algorithm is efficient and well-studied, contributing a non-trivial online adversarial prediction regret bound for logarithmic pooling.
- The paper is well-written, with rigorous and sound theorems, and provides detailed proofs.

Weaknesses:
- The scope is limited to logarithmic pooling and log loss, with unclear tightness of the regret bound.
- Insufficient motivation is provided for the adversarial setting requiring weights to be predicted before observing expert forecasts.
- The practical implications of the calibration condition are not adequately discussed, particularly when experts are decoupled from the adversarial outcomes.
- The paper lacks experimental results and practical applications.

### Suggestions for Improvement
We recommend that the authors improve the motivation for the adversarial setting, particularly why predicting weights before observing expert forecasts is necessary. Additionally, we suggest providing a clearer justification for the calibration condition in the context of expert learners. Including a thorough review of related literature on online portfolio selection and a comparison with linear pooling would enhance the paper's clarity. Furthermore, we encourage the authors to present a detailed example illustrating the significance of logarithmic pooling with log loss and to explicitly discuss the challenges posed by the normalization term in relation to linear optimization.