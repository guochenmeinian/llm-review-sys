ID: HtNQXg979A
Title: Models See Hallucinations: Evaluating the Factuality in Video Captioning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates factual error issues in video captioning by conducting human analysis and annotation to identify these problems. The authors propose a weakly-supervised factuality metric, FactVC, which demonstrates superiority over other baselines through extensive experiments. The paper also introduces two human-annotated factuality datasets and highlights that 56% of current model-generated sentences contain factual errors.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated and addresses a significant issue in video captioning, providing valuable contributions to the field.
- The writing is clear and the organization effectively follows the contributions.
- Extensive experiments validate the effectiveness of FactVC as a metric for detecting factual errors.

Weaknesses:
- Some relevant references are missing, which could enhance the context of the work.
- The scale of the human-annotated datasets is relatively small, potentially affecting the reliability of the findings.
- The definition of factual errors is vague, raising questions about the classification of errors beyond factual inaccuracies.

### Suggestions for Improvement
We recommend that the authors improve the scale of the human-annotated datasets to enhance the robustness of their findings. Additionally, consider testing more recently designed models, such as VLCap, to provide a contemporary comparison. It would also be beneficial to clarify the definition of factual errors and discuss the distinctions between FactVC and similar approaches, particularly in relation to the use of CLIP versus VideoCLIP. Furthermore, we suggest providing examples of model-generated captions alongside human annotations to strengthen the analysis in Table 8. Lastly, consider employing pair-wise ranking for evaluations, as it may yield more reliable results.