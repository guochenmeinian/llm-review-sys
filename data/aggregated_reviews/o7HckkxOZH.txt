ID: o7HckkxOZH
Title: Regression with Cost-based Rejection
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into regression with cost-based rejection, a relatively unexplored area in machine learning. The authors propose a formulation of expected risk and derive the Bayes optimal solution for this problem. They introduce a surrogate loss function that treats rejection as binary classification and provide conditions for consistency. The effectiveness of their approach is supported by experimental results.

### Strengths and Weaknesses
Strengths:
- The problem of regression with cost-based rejection is novel and significant.
- The formulation of expected risk and the derivation of the Bayes optimal solution are meaningful contributions that may inspire future research.
- The proposed training approach is reasonable, with theoretical analyses backing its validity.
- Experimental results substantiate the importance of considering cost-based rejection in regression.

Weaknesses:
- The concept of "classification calibrated binary classification loss" is not introduced in Theorem 4, and relevant references are missing.
- The first two cited references are repeated, indicating a need for thorough reference checking.
- Typos, such as a missing right parenthesis in Eq. (2), detract from the paper's professionalism.
- The paper could benefit from a general Bayes optimal solution applicable to arbitrary regression losses rather than being limited to mean squared error.
- The empirical evaluation lacks comparisons with existing methods, making it difficult to assess the proposed method's efficiency.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Theorem 4 by introducing the concept of "classification calibrated binary classification loss" and including relevant references. Additionally, please ensure that all references are unique and correctly cited. Addressing the typos, particularly the missing parenthesis in Eq. (2), would enhance the paper's quality. We also suggest exploring a general Bayes optimal solution for various regression losses. Furthermore, we encourage the authors to include comparisons with baseline methods in their experimental evaluation to better demonstrate the proposed method's effectiveness.