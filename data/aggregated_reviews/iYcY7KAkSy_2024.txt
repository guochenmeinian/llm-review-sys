ID: iYcY7KAkSy
Title: Spiking Token Mixer:  An event-driven friendly Former structure for spiking neural networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Spiking Token Mixer (STMixer) architecture, designed to address the challenges of implementing spiking self-attention operators on asynchronous neuromorphic chips. The STMixer utilizes operations compatible with asynchronous environments, such as convolutional and fully connected layers. The authors claim that STMixer achieves performance comparable to or better than existing spikformer-like models in synchronous scenarios.

### Strengths and Weaknesses
Strengths:  
1. The paper is well-organized and clearly written, making it accessible to readers.  
2. It addresses a significant problem in the spiking neural network (SNN) field, proposing an innovative solution.  
3. The authors provide various experiments and ablations to demonstrate the effectiveness of the proposed method.  

Weaknesses:  
1. The technical contribution is limited, lacking in-depth analysis and overlooking some existing work.  
2. Key points, particularly regarding operators, are not clearly explained.  
3. The work does not incorporate the latest results in the field, such as meta-spikeformer, and its performance is not state-of-the-art (SOTA).  
4. The proposed method's impact on energy consumption is unclear, and the ablation studies could include a broader range of datasets and models.  
5. Some references are incorrect or inaccessible, and the mathematical descriptions lack rigor, omitting crucial details.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of key points, especially regarding the proposed operators. A more thorough analysis of the STMixer's performance compared to existing models, including Spike-driven Transformer v1/v2, should be included. We suggest providing a detailed explanation of the method and addressing the energy consumption of the SML block. Additionally, the authors should ensure that all references are accurate and accessible, and rigorously describe the mathematical formulations in Section 4.3. Finally, we encourage the authors to explore the implications of the residual connections in the context of asynchronous scenarios.