ID: HB6KaCFiMN
Title: Animate3D: Animating Any 3D Model with Multi-view Video Diffusion
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 5, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Animate3D, a framework for animating static 3D models through a multi-view video diffusion model (MV-VDM) and 4D Gaussian Splatting optimization. It introduces a large-scale dataset of 38K animated 3D objects (MV-Video) for spatiotemporal supervision. The architecture employs a two-stage pipeline that first reconstructs coarse motions from generated multi-view videos and then refines them using 4D Score Distillation Sampling (4D-SDS). The authors claim that Animate3D outperforms existing methods in generating high-quality 4D objects.

### Strengths and Weaknesses
Strengths:
1. The introduction of a large-scale dataset for multi-view video generation addresses the challenge of limited 4D datasets.
2. Qualitative results indicate the ability to generate high-quality 4D objects with improved texture and consistency.
3. The paper is well-written, clearly articulating the motivation for using a multi-view diffusion model.

Weaknesses:
1. The method appears to be a straightforward combination of existing techniques without significant innovation.
2. Motion diversity is limited, with only six animations per ID, which may not adequately capture the range of motions for a single 3D object.
3. Motion controllability is constrained, resulting in simple, repetitive motions that lack complexity.
4. Experimental results are insufficient, lacking quantitative comparisons and adequate qualitative examples.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the method section, as the notation is overloaded and inconsistently explained. Additionally, the authors should provide a more comprehensive discussion of the differences between their approach and existing text-to-4D methods. It would be beneficial to include empirical comparisons with more 4D generation methods, such as 4DGen and TC4D, to validate the effectiveness of Animate3D. The authors should also conduct ablation studies to demonstrate the contributions of specific architectural components, such as the MV2V-Adapter and the spatiotemporal attention block. Furthermore, addressing the limitations regarding motion diversity and controllability would enhance the paper's impact. Lastly, we suggest improving the overall presentation and clarity of figures and text throughout the manuscript.