ID: 0XeNkkENuI
Title: The Road Less Scheduled
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 10, 8, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel optimization procedure for deep learning that eliminates the need for predefined learning rate schedules, claiming to achieve performance comparable to schedule-based approaches while offering good any-time performance. The method can be integrated with standard optimizers and is theoretically grounded, demonstrating desirable properties. Evaluations across various tasks indicate that its performance is comparable to or slightly better than standard baselines, particularly excelling in any-time performance. The authors acknowledge the significant differences in scale between their benchmarks and modern large-scale training runs, noting that their evaluations are time-consuming due to extensive hyperparameter tuning. They introduce a general online-to-batch analysis framework, enhancing the understanding of their method's convergence properties and emphasizing the importance of understanding the theoretical underpinnings of their method in relation to the online convex optimization framework.

### Strengths and Weaknesses
Strengths:
- The problem addressed is highly relevant, with the potential to significantly impact deep learning practices by simplifying experimental procedures.
- The theoretical contributions are solid, and the empirical evaluations are extensive, covering a wide range of tasks with strong baselines.
- The method exhibits significant novelty, as it is the first to provide this type of any-time performance.
- The paper is well-structured and easy to follow, with clear figures and multiple seed validations.
- The authors are open to feedback and willing to conduct additional experiments to address concerns regarding hyperparameter dependence on training duration.

Weaknesses:
- The evaluation of deep learning tasks could be enhanced, particularly regarding hyperparameter tuning and the transferability of hyperparameter values, which are not thoroughly discussed.
- The paper lacks comprehensive comparisons with Polyak averaging and EMA variants, which are essential for validating the claims made.
- Some claims regarding the method's performance and efficiency may be overstated without sufficient empirical backing.
- The paper does not sufficiently emphasize the well-known connections between learning rate schedules and weight averaging, which may lead to over-claiming.
- The extent to which the method is genuinely schedule-free remains unclear, as optimal hyperparameters may still depend on training duration.
- There is a lack of concrete evidence linking the theoretical claims to practical performance, raising questions about the falsifiability of their predictions.

### Suggestions for Improvement
We recommend that the authors improve the discussion on hyperparameter sensitivity, particularly how different hyperparameters affect performance across various settings. It would be beneficial to provide a full range of hyperparameter tuning for both the proposed method and the baselines. Additionally, we suggest including comparisons with Polyak averaging and EMA to strengthen the empirical validation of the proposed method. We encourage the authors to clarify the evaluation of gradients at specific sequences and address the potential implications of using $y_t$ instead of $z_t$ in the update rule to enhance the paper's clarity. Furthermore, we recommend that the authors improve the clarity of their limitations regarding hyperparameter tuning and its implications for the schedule-free claim, acknowledging that if hyperparameters must be tuned for different training lengths, the method may not be truly schedule-free. Contextualizing their results more effectively by referencing relevant literature, such as Sandler et al. (2023), would also be beneficial. Finally, we encourage the authors to derive and test falsifiable predictions from their theoretical framework to strengthen the empirical foundation of their claims.