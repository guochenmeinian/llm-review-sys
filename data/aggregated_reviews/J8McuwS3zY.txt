ID: J8McuwS3zY
Title: Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 4, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Memory Efficient Fine-Tuning (MEFT) technique, which reduces training activation memory by up to 84% compared to full fine-tuning, while achieving similar accuracy to traditional Parameter-Efficient Fine-Tuning (PEFT) methods. The authors propose combining reversible models with existing PEFT techniques to create MEFT algorithms, validated across four architecture backbones on the GLUE dataset. Key insights include the importance of weight initialization to prevent effects on the underlying language model before training. 

### Strengths and Weaknesses
Strengths:
* The proposed MEFT algorithm addresses a significant issue in existing PEFT techniques, making LLM PEFT training more accessible by reducing activation memory.
* The paper provides valuable insights, supported by empirical results and detailed illustrations.
* MEFT variants outperform existing PEFT techniques on the GLUE dataset across multiple architecture backbones.
* Reproduction code is included with the submission.

Weaknesses:
* The training time of MEFT is inadequately explored, only noted as "double the training time," leaving the tradeoff between memory and computation efficiency unclear.
* Key comparisons with popular memory-efficient training techniques, such as tensor rematerialization (gradient checkpointing) and ZeRO, are missing, despite their relevance and compatibility with PEFT approaches.
* Figures 1, 2, 3, 5, and 6 have small font sizes that hinder readability, and some figures lack clear explanations, complicating the reading experience.

### Suggestions for Improvement
We recommend that the authors improve the exploration of MEFT's training time to clarify the computation-memory tradeoff. Additionally, we suggest including comparisons with existing memory-efficient techniques like tensor rematerialization and ZeRO, as these are relevant to the proposed method. Furthermore, enhancing the readability of figures by increasing font sizes and providing clearer explanations for all visual elements would significantly improve the presentation of the paper.