ID: vNTIxJUbQ1
Title: Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving
Conference: thecvf
Year: 2024
Number of Reviews: 1
Original Ratings: 7
Original Confidences: 4

Aggregated Review:
**Key Points**  
This paper presents improvements aimed at reducing memory consumption and enhancing training/inference speed of Driving Vision-Language Models (VLMs) through three main strategies: 1) eliminating the ViT encoder while retaining the patch embedding layer, 2) integrating multi-camera embeddings into a single representation via a gated pooling attention method, and 3) employing smaller language models with optional quantization and LORA techniques. 

**Strengths and Weaknesses**  
We find the paper to be highly relevant to our workshop and appreciate the empirical results that substantiate its claims. However, the review does not mention any specific weaknesses or limitations of the proposed methods.

**Suggestions for Improvement**  
We suggest that the authors consider providing additional insights into the potential trade-offs associated with the proposed methods, particularly regarding performance metrics and the impact of the smaller language models.