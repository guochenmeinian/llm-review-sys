ID: hizSx8pf0U
Title: DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection
Conference: NeurIPS
Year: 2023
Number of Reviews: 26
Original Ratings: 6, 6, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DeepfakeBench, a comprehensive benchmark for deepfake detection that includes 15 detection methods and 9 datasets. The authors provide a unified codebase that standardizes data management, algorithm implementation, and evaluation metrics, facilitating fair comparisons and deeper insights into model performance. The paper conducts thorough evaluations, analyzing the effects of various factors such as data augmentation and backbone architectures on detection performance, particularly focusing on face forgery techniques, including GAN-generated and diffusion-generated data. The authors investigate the competitive performance of Naive detectors compared to more complex methods, attributing this to experimental consistency and the effectiveness of enhancements like data augmentation. They also discuss the implications of training data similarity on overfitting, emphasizing that using a large number of similar frames can lead to overfitting as the model may memorize irrelevant features instead of focusing on forgery-related characteristics.

### Strengths and Weaknesses
**Strengths:**
1. The codebase is comprehensive, implementing 15 algorithms and supporting 9 datasets, providing a modular and user-friendly platform for consistent evaluations.
2. A systematic and detailed evaluation is performed, with intuitive visualizations and insights into the performance of various architectures and techniques.
3. The authors have extended their analysis to include GAN and diffusion model-based deepfakes, demonstrating promising detection capabilities.
4. The paper is well-written and acknowledges reviewer feedback, enhancing readability and clarity.

**Weaknesses:**
1. The paper lacks a comprehensive comparison with other existing benchmarks, missing an explicit review of related codebases, which may limit its novelty.
2. Limited qualitative analyses are provided, with insufficient exploration of why certain methods perform better on specific datasets, particularly regarding the relationship between frame similarity and overfitting.
3. Some sections lack clarity, particularly regarding cross-domain evaluation and the implications of hyperparameter settings, which may confuse readers.
4. The discussion on societal impacts and ethical considerations is minimal, which is a significant oversight in the context of deepfake technologies.

### Suggestions for Improvement
1. We recommend that the authors improve the comparison with existing benchmarks by including a table that lists algorithm/dataset coverage of related codebases to highlight their contributions.
2. The authors should consider adding video-level evaluations, even in the absence of dedicated video-level detectors, to enhance the comprehensiveness of their framework.
3. We suggest providing a clearer explanation of hyperparameters used in experiments, ideally in a dedicated table, to improve clarity.
4. The authors should enhance the presentation of figures and tables, ensuring they are large enough for clarity.
5. We recommend that the authors delve deeper into the observed phenomena in their experiments, particularly providing explanations for why certain methods perform as they do, especially regarding the performance of naive detectors and the advantages of specific backbone architectures.
6. We also suggest incorporating a more detailed discussion on the potential negative societal impacts of the collected datasets and implementing controlled access mechanisms to mitigate misuse.
7. Finally, we recommend improving the clarity of the explanation regarding overfitting, particularly addressing the relationship between frame similarity and model generalization, to alleviate confusion among readers.