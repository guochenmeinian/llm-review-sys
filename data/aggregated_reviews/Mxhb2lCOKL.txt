ID: Mxhb2lCOKL
Title: Mitigating Test-Time Bias for Fair Image Retrieval
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for fair text-based image retrieval, specifically addressing bias in target retrieval sets through a proposed Post-hoc Bias Mitigation (PBM) approach. The authors evaluate their method, which involves sorting candidate images from different demographic groups and selecting an equal number from each group, using various techniques for estimating sensitive attributes on four standard datasets: Occupation 1 and 2, MS-COCO, and Flickr30k. The authors argue that existing debiasing methods fail to ensure equal representation during evaluation, leading to persistent bias. Additionally, the paper compares the performance of their PBM method against existing in-processing methods, revealing that MI-clip and Adversarial Learning perform similarly to PBM when the dataset is balanced, while Debias Prompt underperforms. The authors acknowledge limitations regarding the trade-off between group and individual fairness, the lack of synthetic samples for underrepresented groups, and the binary assumption of gender neutrality.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, addressing an important and socially relevant problem.
- The proposed solution is intuitive and shows practical effectiveness in experiments.
- The analysis of bias sources is insightful, and the method is easy to implement without requiring model retraining.
- The authors provide a clear experimental comparison demonstrating the effectiveness of their PBM method against existing techniques.
- Acknowledgment of limitations shows a reflective approach to their research.
- The detailed response to reviewer queries enhances the clarity and confidence in their findings.

Weaknesses:
- The technical contribution is limited and incremental, as the idea of post-processing is not novel; similar methods have been proposed in prior works.
- The reliance on the performance of the sensitive group classifier raises concerns about potential bias in the classification process.
- The paper lacks detailed discussions on the fairness/utility trade-off and the implications of using inferred demographic data.
- The trade-off between group and individual fairness remains inadequately addressed.
- The absence of techniques to handle insufficient representations of certain demographic groups is a notable gap.
- The assumption of binary gender neutrality is limited and requires further exploration.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of using a sensitive group classifier, including its potential biases and how it may affect recall performance. Additionally, we suggest providing more ablation studies on the performance of the gender/race classifier and its impact on bias. Clarifying the fairness metric definitions and discussing the relationship between bias and recall in relation to the distribution of sensitive attributes in the test set would enhance the paper. Furthermore, we encourage the authors to include a more critical discussion of the chosen fairness scenario and its implications for different applications. We also recommend improving the discussion on the trade-offs between group and individual fairness to provide a more balanced perspective. Incorporating techniques such as synthetic samples to address issues of insufficient demographic representation would be beneficial. Lastly, we encourage the authors to explore the concept of gender neutrality as a continuous variable rather than a binary one, which could enhance the robustness of their findings. Improving the clarity of figures, particularly Figure 4, would also enhance presentation quality.