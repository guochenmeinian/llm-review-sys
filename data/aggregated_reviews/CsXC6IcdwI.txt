ID: CsXC6IcdwI
Title: EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 8, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents EHRSHOT, a benchmark dataset and pretrained model designed for few-shot learning in healthcare, focusing on structured electronic health record (EHR) data rather than clinical text. The dataset includes structured data from 6,712 patients' full medical timelines, encompassing 39.2 million clinical events across 893,773 encounters. The authors clarify that the exclusion of clinical text is due to the complexities of ensuring HIPAA compliance for patient privacy. They also release the weights of a 141M parameter foundation model pretrained on the structured data of 2.57M patients' EHRs. The paper defines 15 clinical prediction tasks and addresses issues of class imbalance in the dataset, which has implications for model evaluation metrics like AUROC.

### Strengths and Weaknesses
Strengths:
- EHRSHOT provides a comprehensive longitudinal dataset that extends beyond ICU data, filling a significant gap in the ML for healthcare domain.
- The focus on structured EHR data fills a gap in existing benchmarks, which predominantly feature text-based models.
- The authors are the first to publish the full weights of a clinical foundation model, promoting reproducibility and collaboration.
- The authors provide a clear explanation of their few-shot sampling methodology and its implementation.
- The paper is well-written, making it easy to follow and understand, demonstrating a commitment to patient privacy while planning for responsible data release.

Weaknesses:
- Access to the dataset and model is currently restricted, which limits verification of the claims made in the paper.
- The choice of GBT as a baseline for few-shot learning may not be entirely appropriate, as its performance in large sample size settings is uncertain.
- The dataset may not fully represent the diversity and complexity of clinical cases in real-world healthcare settings.
- The handling of longitudinal data and the specifics of model performance comparisons could be elaborated further.
- Some results, particularly regarding the performance of the CLMBR model compared to GBM, may require additional clarity and justification.
- The terminology surrounding the CLMBR model could be confusing and may benefit from clearer definitions.

### Suggestions for Improvement
We recommend that the authors improve access to the dataset and model to allow for verification of their claims. Additionally, providing more discussion on the appropriateness of GBT as a baseline for few-shot learning would enhance the paper. We suggest improving the clarity of the problem formulation regarding longitudinality in predictions and providing more detailed explanations of the performance discrepancies between GBM and CLMBR, particularly for long time horizon prediction tasks. Clarifying the rationale behind the removal of clinical text, detailing the sampling process for few-shot learning, and addressing the dataset's representativeness would also add value. Finally, we encourage the authors to consider evaluating their model on other datasets to further validate its performance and applicability.