ID: vYmvgxpgwH
Title: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 5, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of compute-optimal inference for large language models (LLMs), focusing on strategies that balance inference-time computation with performance. The authors propose a new inference strategy, REward BAlanced SEarch (REBASE), which combines Monte Carlo Tree Search with reduced inference costs, demonstrating that smaller models can outperform larger ones under constrained computational budgets. The study evaluates various inference strategies, revealing that advanced algorithms can enhance accuracy while using fewer resources, as exemplified by the Llemma-7B model achieving comparable accuracy to the Llemma-34B model on the MATH500 dataset with half the FLOPs.

### Strengths and Weaknesses
Strengths:
- The paper addresses a compelling topic relevant to the NeurIPS audience.
- It includes a comprehensive experimental investigation supporting its claims.
- The proposed REBASE strategy shows promising results, outperforming existing methods.

Weaknesses:
- The theoretical depth is lacking, with limited exploration beyond the two theorems presented, which may be inadequate for a flagship venue like NeurIPS.
- The findings regarding training equally accurate models with fewer resources are not particularly surprising.
- There are numerous typos, indicating a need for additional proofreading.
- The empirical conclusions are heavily tied to specific benchmarks, raising questions about their broader applicability.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework of the paper to enhance its depth and relevance for a flagship venue. Additionally, expanding the experimental evaluation to include a wider range of model sizes and diverse training datasets would strengthen the claims about compute-optimal inference. It is also advisable to address the inference cost of the reward model in the analysis and to clarify the significance of the proposed reward reranking in relation to the structure of math problems. Lastly, we suggest thorough proofreading to eliminate typos and improve overall presentation quality.