ID: 45uZxlMLol
Title: Annotation Sensitivity: Training Data Collection Methods Affect Model Performance
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates how the order of annotation questions affects hate speech (HS) and offensive language (OL) detection. The authors compare five conditions of annotation, revealing that (a) proportions of OS and HL differ, (b) discrepancies in HS labels arise based on conditions, (c) performance of BERT/LSTM classifiers varies with annotation strategy, (d) cross-condition evaluations yield different results, and (e) additional evaluations on prediction scores and training set sizes are necessary. The study emphasizes the impact of annotation instrument design on model performance and advocates for increased transparency and documentation in annotation practices.

### Strengths and Weaknesses
Strengths:
- The paper addresses the underexplored topic of annotation sensitivity, contributing valuable insights to the quality and reliability of machine learning datasets.
- It presents a well-designed experiment with five annotation conditions, allowing for a comprehensive analysis of task structure effects.
- The findings have broader implications for various NLP tasks, encouraging further exploration of annotation sensitivity and best practices in instrument design.

Weaknesses:
- The paper lacks a clear and detailed explanation of the five experimental conditions, which hinders understanding of their implications.
- It does not sufficiently analyze the reasons behind observed differences in annotations, such as order effects or fatigue effects.
- There is no demonstration of the superiority of any tested condition, making it difficult to draw actionable conclusions.

### Suggestions for Improvement
We recommend that the authors improve the clarity and detail regarding the five experimental conditions to enhance reader understanding. Additionally, we suggest conducting a more thorough analysis of the results to explore underlying factors influencing annotation differences. The authors should also consider refining their experimental design to provide clearer evidence supporting one approach over others. Finally, including metrics such as Krippendorff's alpha for disagreement would strengthen the evaluation of annotation quality.