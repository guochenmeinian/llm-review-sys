ID: B5LpWAaBVA
Title: Online Nonstochastic Model-Free Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extension to model-free reinforcement learning (RL) that incorporates disturbance-based policies, which adjust a base policy using pseudo-disturbances to account for unmodeled deviations in dynamics. The authors propose three methods for estimating these pseudo-disturbances, including the gradient of the TD error, differences between auxiliary value functions, and discrepancies between observed and simulated states. The algorithm, MF-GPC, demonstrates sublinear regret bounds under certain assumptions and shows effectiveness in noisy OpenAI Gym environments.

### Strengths and Weaknesses
Strengths:
- The introduction of disturbance-based policies represents a novel approach in model-free RL, extending existing disturbance techniques to this setting.
- The mathematical guarantees regarding the pseudo-disturbances and the regret bound for MF-GPC enhance the paper's contributions.
- The clarity of presentation and the well-motivated problem setting are commendable.
- Empirical results indicate significant improvements in performance across various noisy control tasks.

Weaknesses:
- The practical applicability of the proposed framework is questionable, particularly for PD2 and PD3, which require specific conditions or access to simulators.
- PD1's performance is unreliable, as evidenced by empirical results, raising concerns about its general applicability.
- The analysis is limited to linear dynamical systems, which may restrict the framework's applicability to other system types.
- The paper lacks a comprehensive discussion of limitations, particularly regarding the effectiveness of pseudo-disturbances in diverse environments.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including comparisons with additional state-of-the-art methods, such as max-entropy RL algorithms like Soft Actor-Critic, to strengthen the claims of robustness. Additionally, we suggest providing more detailed explanations regarding the estimation of gradients for PD1 and clarifying the rationale behind the choice of units in PD2. The authors should also consider discussing the limitations of their approach more thoroughly, particularly in relation to the generalizability of the pseudo-disturbances across different environments and the implications of requiring a simulator for PD3. Finally, enhancing the readability of evaluation plots, particularly by increasing font sizes, would improve presentation quality.