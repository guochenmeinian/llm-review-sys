ID: ZZfqK9Rzed
Title: FedDSE: Distribution-aware Sub-model Extraction for Federated Learning over Resource-constrained Devices
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FedDSE, a method for extracting sub-models in federated learning that addresses statistical heterogeneity among clients on resource-constrained devices. The authors propose a technique where clients adaptively extract neurons based on activation values from their local datasets, aiming to reduce conflicts and enhance training efficiency. The paper includes theoretical analyses and experimental results demonstrating the superior performance of FedDSE compared to state-of-the-art (SOTA) methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with clear explanations of motivation, methodology, and results.
- Strong theoretical analysis supports the reliability of experimental findings.
- The approach effectively addresses the common issue of statistical heterogeneity in federated learning.

Weaknesses:
- Missing comparisons with several important baselines, such as FedProx and FedL2P, which complicates the assessment of the method's usefulness.
- The method incurs additional computational costs for inference and activation calculations, which may be prohibitive for resource-constrained devices.
- Lack of analysis regarding the resource consumption of FedDSE compared to methods specifically designed for resource-constrained federated learning, such as FedRolex.
- Limited experimental validation on small datasets raises concerns about generalization to larger, more complex datasets.

### Suggestions for Improvement
We recommend that the authors improve the paper by including comparisons with key baselines like FedProx, FedL2P, and FedRolex to better illustrate the method's advantages. Additionally, the authors should address the computational overhead associated with inference and activation calculations, clarifying its impact on efficiency for local training. We also suggest expanding the experimental validation to include larger datasets and more complex models to assess the generalization of FedDSE. Finally, a more thorough discussion of related works that do not require client-side inference for sub-model extraction would enhance the paper's depth.