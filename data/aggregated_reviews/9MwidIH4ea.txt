ID: 9MwidIH4ea
Title: Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 4, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel dual-guided spatial-channel-temporal attention mechanism (DG-SCT) aimed at addressing audio-visual problems by leveraging pre-trained audio and visual encoders. The authors demonstrate improvements across various audio-visual tasks, including event localization, parsing, segmentation, and question answering, while also introducing a benchmark for audio-visual few-shot and zero-shot tasks on AVE and LLP datasets. Additionally, the authors discuss the importance of both modality-specific and modality-mutual information in audio-visual tasks, clarifying that while modality-specific information is crucial, modality-mutual information facilitates the alignment and integration of modalities, enhancing understanding in complex tasks. They also address concerns regarding the VGG-Sound dataset, noting that it is not explicitly stated as a subset of AudioSet and that the 40k dataset should not be directly compared to the 200k dataset due to differences in class coverage.

### Strengths and Weaknesses
Strengths:
- The proposed DG-SCT achieves state-of-the-art results on several audio-visual tasks.
- The mechanism effectively learns channel-wise, spatial, and temporal information, enhancing the integration of pre-trained encoders.
- The paper is well-written and presents extensive experiments that validate the proposed method.
- The authors effectively emphasize the significance of integrating modality-specific and modality-mutual information for improved performance in challenging audio-visual tasks.
- The clarification regarding the VGG-Sound dataset and its relationship to AudioSet is appreciated, demonstrating responsiveness to reviewer concerns.

Weaknesses:
- The notation in Section 3.3 is overly complicated, making it difficult to follow; utilizing Figure 2 (4) for illustration and walking through each step would enhance clarity.
- Results in Tables 2 and 3 highlight only better performance without discussing scenarios where the proposed system underperformed, lacking potential explanations or hypotheses.
- Some abbreviations, such as LLP and CMBS, are introduced without definitions, which could confuse readers.
- The design of DG-SCT lacks clarity, particularly regarding the implementation of cross-modal attention mechanisms and the effectiveness of temporal modeling.
- The number of trainable parameters and efficiency comparisons (e.g., FLOPs) are not reported, which is critical for understanding the computational costs involved.
- There is a lack of clarity in the initial examples provided, which may have led to misunderstandings regarding the role of modality-specific information.
- The authors did not conduct the experiments that some reviewers expected, which could have strengthened their claims.

### Suggestions for Improvement
We recommend that the authors improve the clarity of notations in Section 3.3 by incorporating Figure 2 (4) and explaining each step in detail. Additionally, consider adding discussions for cases where the proposed system performed worse, as seen in Tables 2 and 3, to provide insights into these outcomes. It would also be beneficial to define abbreviations like LLP and CMBS upon their first mention. Furthermore, we suggest clarifying the design of DG-SCT, especially regarding the implementation of cross-modal attention and the effectiveness of temporal modeling. Including the number of trainable parameters and efficiency comparisons would enhance the paper's comprehensiveness. We also recommend improving the clarity of examples related to modality-specific and modality-mutual information to avoid confusion. Lastly, we suggest that the authors conduct the experiments comparing their model with others that do not focus on modality-mutual information, as this would provide valuable insights into the performance differences.