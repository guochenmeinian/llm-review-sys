ID: 1dpmeH6IHa
Title: I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 8, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents I2EBench, a comprehensive benchmark for evaluating Instruction-based Image Editing (IIE) models. It includes a dataset of over 2,000 images and 4,000 instructions across 16 evaluation dimensions, designed to assess image editing quality automatically while aligning with human perception through user studies. The benchmark aims to provide insights for improving IIE models and will be open-sourced for community use.

### Strengths and Weaknesses
Strengths:
1. The benchmark offers a holistic assessment of IIE model capabilities, addressing a wide array of evaluation dimensions.
2. It emphasizes alignment with human perception, enhancing its relevance.
3. The large dataset provides a robust foundation for thorough testing.
4. The inclusion of user studies adds depth to the evaluation process.
5. The authors commit to open-sourcing I2EBench, facilitating knowledge sharing.

Weaknesses:
1. The Google Drive link for benchmark images is inaccessible.
2. The authors do not compare their work with a related study that employs a similar evaluation pipeline.
3. Key evaluation dimensions, such as action change and shape/size change in high-level editing, are not discussed.
4. The evaluation details for multiple models lack sufficient specificity, including hyperparameters and GPU types.
5. The rationale for using GPT-4V for evaluation needs further clarification, and alternative models should be considered.
6. The insights in Section 5 are not constructive, as they highlight the challenges of low-level editing without offering solutions.

### Suggestions for Improvement
We recommend that the authors improve accessibility by ensuring the Google Drive link is functional. Additionally, the authors should include a comparison with the related work to clarify the novelty of their approach. It is essential to discuss the evaluation dimensions of action change and shape/size change in high-level editing. We also suggest providing more detailed evaluation specifics, including hyperparameters and GPU types, to enhance reproducibility. Clarifying the rationale behind using GPT-4V and exploring alternative models would strengthen the evaluation. Lastly, we encourage the authors to address the limitations highlighted in Section 5 by providing constructive insights into the challenges faced in low-level editing tasks.