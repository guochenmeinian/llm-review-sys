ID: yWSxjlFsmX
Title: Is Mamba Compatible with Trajectory Optimization in Offline Reinforcement Learning?
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the potential of leveraging Mamba for trajectory learning, specifically through the lens of Decision Mamba. The authors analyze its performance across trajectory learning scenarios (gym/mujoco) and derive conclusions from rigorous experiments, which are valuable for future research related to Mamba. The study also examines Mamba's capabilities in trajectory optimization within offline reinforcement learning, demonstrating that Mamba DT achieves state-of-the-art performance with fewer parameters. However, the paper's contributions to the field are perceived as limited due to the prior discussion of its findings within the community.

### Strengths and Weaknesses
Strengths:
1. The paper presents novel insights into Mamba's potential for trajectory learning, with findings that suggest Mamba may outperform Transformer models under specific conditions.
2. The writing quality and visualizations are commendable, and the input concatenation experiments provide practical insights for other sequence-based decision-making models.
3. Extensive evaluations highlight DeMa's effectiveness and efficiency compared to existing methods, addressing parameter size and scalability issues.

Weaknesses:
1. Many discoveries are not new to the community, leading to a perception of weak technical contributions. The empirical nature of the findings lacks depth, particularly regarding the advantages of Transformer-like models for short sequences and the importance of hidden attention.
2. The experimental focus is limited to standard Atari and mujoco tasks, raising questions about Mamba's performance in environments requiring long-term planning or tasks with delayed rewards.
3. Some symbols are undefined prior to use, and there is insufficient discussion on the relationship between the proposed methods and reinforcement learning.

### Suggestions for Improvement
We recommend that the authors improve the depth of their analysis by providing more in-depth explanations of their findings, particularly regarding the advantages of Transformer-like models and the hidden attention mechanism. Additionally, we suggest expanding the experimental scope to include tasks that require long-horizon planning skills. The authors should also ensure that all symbols are defined before use and enhance the discussion on the relationship to reinforcement learning in sections 3.2 and 3.3. Finally, we advise refining the limitations section in the conclusion to accurately reflect genuine limitations.