ID: 8GSCaoFot9
Title: Conservative State Value Estimation for Offline Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 3, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Conservative State Value Estimation (CSVE) for offline reinforcement learning, which penalizes the V-function on out-of-distribution (OOD) states, aiming for conservative value estimation under specific state distributions. The authors develop a practical actor-critic algorithm based on CSVE and evaluate its performance on classic continual control tasks, claiming superior results compared to prior methods like Conservative Q-Learning (CQL) and COMBO. Additionally, the paper includes a comparative analysis of the CSVE algorithm against other methods in offline reinforcement learning (RL), proposing that data quality significantly influences the performance of all algorithms, with CSVE showing more advantages on datasets from random to medium types compared to expert datasets. The policy is derived from balancing in-sample learning and exploration through conservative value estimation, as detailed in Eq. 9.

### Strengths and Weaknesses
Strengths:  
- The paper provides both theoretical and empirical results, contributing to the understanding of conservative state value estimation.  
- The proposed method is a novel approach that addresses the overestimation of values in offline RL by focusing on state values rather than state-action values.  
- The authors provide a clear framework for understanding the performance of CSVE across different dataset types, highlighting its advantages in specific scenarios.  
- The derivation of the policy through Eq. 9 effectively balances learning and exploration, showcasing a thoughtful approach to value estimation.

Weaknesses:  
- The significance of the proposed method is unclear, particularly in comparison to penalizing the Q-function for OOD actions. The core issue of overestimation and extrapolation error in offline RL remains unaddressed.  
- The theoretical assumptions, such as $\text{supp}~ d \subseteq \text{supp}~ d_u$, are strong and difficult to satisfy, limiting the applicability of the results.  
- The empirical evaluation lacks sufficient depth to convincingly support the effectiveness of CSVE's core component.  
- There is a lack of clarity regarding the differentiation between out-of-distribution (OOD) and out-of-support states, leading to unresolved concerns about the implications of these definitions on the algorithm's performance.  
- The assurance for out-of-support states in theorems is questioned, suggesting a potential gap in the theoretical framework.  
- The paper suffers from organizational issues and lacks clarity in certain sections, particularly regarding the experimental results and the optimization of key equations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental results by bolding the top scores and including aggregate scores to facilitate interpretation. Additionally, the authors should provide more intuition on why state-based methods may outperform state-action methods, and explicitly compare V-function bounds computed by CSVE and CQL. It would be beneficial to clarify how the "Model-based Exploration on Near States" is performed and optimize Equation 9 with a detailed explanation of the trade-offs involved. An ablation study on varying parameters like $\lambda$ and $\tau$ should be included in the main body to enhance the analysis. Furthermore, we recommend that the authors improve the clarity of definitions regarding out-of-distribution and out-of-support states to address the concerns raised. Lastly, we suggest that the authors provide a more robust theoretical assurance for out-of-support states within their framework, as this could strengthen the overall validity of their claims regarding the CSVE algorithm. Finally, we encourage the authors to discuss the limitations of their work more thoroughly, particularly regarding the performance of CSVE with varying data quality sources.