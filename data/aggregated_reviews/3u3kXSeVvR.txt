ID: 3u3kXSeVvR
Title: Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into multilingual neural machine translation (mNMT) quality through the lens of knowledge transfer. The authors propose a new measure called Representational Transfer Potential (RTP), which assesses similarities between languages regarding both positive and negative transfer. A key finding is that RTP correlates with improved translation quality, particularly for low and mid-resource languages, while results for high-resource languages show slight declines. The authors also introduce an auxiliary similarity loss aimed at promoting invariant representations across languages, supported by experiments on TED Talks and Flores-101 datasets.

### Strengths and Weaknesses
Strengths:
- The paper is clear and well-written, with strong evaluation metrics.
- The proposed method demonstrates significant improvements in translation quality for low and mid-resource languages.
- The novel representation metric RTP effectively elucidates multilingual knowledge transfer.

Weaknesses:
- The evaluation is limited to English as the target language, raising concerns about generalizability.
- RTP's dependence on bilingual model performance questions its robustness as a metric.
- The motivation for the xsim loss is inadequately articulated, and the independence of variables in Figure 2 is not fully justified.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including target languages other than English to enhance generalizability. Additionally, we suggest providing results for the Many-to-One model to clarify its performance relative to the Many-to-Many model. The authors should also address the interdependence of RTP and BLEU scores in their analysis and offer more rigorous justifications for the use of xsim loss. Finally, we encourage the authors to conduct ablation studies on xsim loss and multi-parallel batching to verify their contributions more concretely.