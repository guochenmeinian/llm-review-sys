ID: UNOeQGHNaN
Title: Robust Representation Learning via Asymmetric Negative Contrasting and Reverse Attention
Conference: NeurIPS
Year: 2023
Number of Reviews: 25
Original Ratings: 3, 3, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel adversarial training (AT) framework, ANCRA, that emphasizes robust feature learning through two characteristics: exclusion and alignment. The authors propose asymmetric negative contrast (ANC) to enforce exclusion by minimizing the similarity of features between different classes, and reverse attention (RA) to enhance alignment by weighting features based on the linear classifier's parameters. Empirical evaluations across three benchmark datasets, including WideResNet models, demonstrate significant improvements in robust accuracy, particularly against adaptive attacks. However, concerns have been raised regarding the limited contribution of the ANC component and discrepancies in reported results compared to established benchmarks.

### Strengths and Weaknesses
Strengths:
1. The proposed framework is well-motivated and introduces novel techniques for robust representation learning.
2. The empirical results show impressive performance improvements compared to state-of-the-art methods on multiple datasets, particularly against adaptive attacks.
3. The motivation behind ANC and RA is considered novel and potentially insightful for future research in adversarial training.
4. The paper is generally well-written, making the concepts accessible to readers.

Weaknesses:
1. The experimental validation is limited, with results primarily reported for ResNet18 and PreActResNet18; broader comparisons with models like WideResNet are necessary.
2. The contribution of the ANC component appears limited, with some reviewers noting that it does not surpass state-of-the-art results.
3. There are significant discrepancies between the reported results and those from the AutoAttack Leaderboard, raising concerns about the validity of the findings.
4. The writing style could be improved for clarity, as some sections are difficult to follow and contain vague terminology.
5. The authors do not provide error bars for significance validation, and the results under various attacks raise questions about their reliability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in sections that are currently vague or convoluted. Additionally, providing results on a wider range of models, such as WideResNet, and including comparisons with state-of-the-art performance from RobustBench would strengthen the validation of the proposed method. It is also essential to include error bars in the results to support the significance of findings. Furthermore, addressing the discrepancies in accuracy under different attack scenarios and clarifying the role of the auxiliary probability \( p \) in the reverse attention mechanism would enhance the paper's rigor. Lastly, a discussion of the limitations of the approach should be included to provide a balanced view.