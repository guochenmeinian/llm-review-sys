ID: BqTv1Mtuhu
Title: Designing Robust Transformers using Robust Kernel Density Estimation
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 7, 7, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel adaptation of a nonstandard self-attention module, akin to those used in transformers, by reinterpreting the self-attention mechanism as a non-parametric kernel density estimator (KDE). The authors propose utilizing robust versions of KDEs to enhance the performance of transformers in various experimental settings. While the method does not significantly improve standard performance on clean datasets, it shows advantages in robustness against adversarial samples and dataset shifts, particularly in image, time series, and NLP datasets. The authors acknowledge the challenges encountered in creating a comprehensive solution and indicate that their approach is supported by empirical assessments. They also recognize the potential for further enhancements, particularly in relation to other fundamental elements of transformers that contribute to robustness.

### Strengths and Weaknesses
Strengths:
- The core contribution is a new deep learning methodology that yields substantial performance improvements across multiple data types and robustness scenarios, indicating a fundamental enhancement to transformer methodology.
- The manuscript provides a focused approach to improving transformer robustness through self-attention mechanisms.
- The paper is well-written, providing a clear overview of robust KDEs and their application to transformers.
- Empirical assessments support the proposed method, lending credibility to the findings.
- The novelty of the approach lies in its departure from conventional deep learning methods, offering insights into the non-robustness of transformer architectures.

Weaknesses:
- The primary weakness is that the proposed method appears to be a trivial modification, merely substituting a component of the transformer with an existing robust version.
- The paper lacks clarity on whether the method is a post-hoc modification or requires in-processing training, and it does not adequately address the computational inefficiencies of robust KDEs compared to standard methods.
- The authors acknowledge limitations in their approach, suggesting that other elements of transformers also require attention for robustness improvements.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the training process and how the robust attention mechanism impacts gradient computation. Additionally, addressing the computational complexities associated with robust KDEs in comparison to standard methods would strengthen the paper. It would also be beneficial to include a more thorough analysis of the limitations and potential trade-offs of the proposed methods, particularly regarding their applicability to large datasets. Furthermore, we suggest incorporating references to prior literature that discusses the links between transformers and kernel methods to enhance the literature review. Finally, we recommend that the authors improve the discussion on how their method can be integrated with various transformer designs and explore additional avenues for enhancing robustness beyond the self-attention mechanism.