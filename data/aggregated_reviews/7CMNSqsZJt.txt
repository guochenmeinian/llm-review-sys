ID: 7CMNSqsZJt
Title: ContextCite: Attributing Model Generation to Context
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 7, 5, 8, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents the task of context attribution, aiming to identify which parts of the input context influence the output of a language model. The authors propose ContextCite, a method that utilizes a sparse linear model to assess how the removal of different context segments affects the model's output probability. The paper includes two evaluation metrics based on generated response probabilities and demonstrates ContextCite's effectiveness through experiments on small LLMs, alongside potential applications such as verifying generated statements and improving response quality.

### Strengths and Weaknesses
Strengths:
- The introduction of context attribution is well-motivated and could initiate a new line of research in interpretability.
- ContextCite is built on a simple yet effective idea, showing promising performance in evaluations.
- The paper is well-structured, clearly written, and includes comprehensive experiments with appropriate metrics and datasets.

Weaknesses:
- The task definition is limited, focusing solely on whole responses rather than token-level attributions, which could enhance understanding of issues like hallucination.
- The relationship between probability changes and context contributions is not fully deterministic, raising questions about the model's applicability to tasks with complex dependencies.
- The evaluation is conducted on closely related tasks and small models, which may limit generalizability to other NLP tasks and larger models.

### Suggestions for Improvement
We recommend that the authors improve the task definition by discussing potential extensions to token-level attributions, particularly for summarization tasks. Additionally, we suggest providing a more thorough exploration of the relationship between omitted context and response coherence, as well as addressing the generalizability of ContextCite beyond the evaluated tasks and models. Lastly, we encourage the authors to include more theoretical justification for the effectiveness of the linear surrogate model and to consider testing more sophisticated surrogate models in their experiments.