ID: vBx0yNQmik
Title: Federated Virtual Learning on Heterogeneous Data with Local-global Distillation
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 6, 4, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called FedLGD that utilizes distilled virtual data on both clients and the server to train federated learning (FL) models. The authors propose iterative distribution matching to distill local virtual data for model training, addressing synchronization and class imbalance issues, thus enhancing FL efficiency and scalability. They also introduce federated gradient matching to distill global data on the server, incorporating a regularization term in the local loss function to align local and global features. The evaluation of FedLGD on benchmark and real datasets demonstrates its superiority over existing heterogeneous FL algorithms.

### Strengths and Weaknesses
Strengths:
1. The authors effectively use visualization to clarify the limitations of local data distillation in federated virtual learning, strengthening the motivation for their method.
2. The proposed method maintains local data privacy by utilizing averaged local gradients to distill global virtual data.
3. Experimental results validate both performance improvements and privacy protection.

Weaknesses:
1. The initialization for data distillation necessitates clients to compute data statistics and the server to aggregate them, raising privacy concerns. The authors should consider using random initialization or other strategies and justify their choice.
2. Unlike VHL, which employs an untrained StyleGAN, FedLGD requires iterative updates of global virtual data. The authors need to justify whether FedLGD would still outperform VHL if StyleGAN were updated similarly.
3. The structure of the proposed method lacks clarity, making it challenging to follow. The authors should improve the connection between the overall pipeline and its components.
4. The presentation quality is inadequate, with numerous typos and grammatical errors. Notations are inconsistent, such as $i$ representing both data and client indices, and the subscript $t$ disappearing in various contexts.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the method's structure to enhance reader comprehension. Additionally, the authors should unify the notation used throughout the paper to avoid confusion. We suggest addressing the privacy concerns related to data statistics aggregation by exploring alternative initialization strategies. Furthermore, the authors should provide a more detailed analysis of the experimental results, particularly regarding the performance of FedLGD compared to other methods like VHL and the implications of dataset size on model performance. Lastly, we encourage the authors to include a theoretical analysis to support the claimed performance improvements.