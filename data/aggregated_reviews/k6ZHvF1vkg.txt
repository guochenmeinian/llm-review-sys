ID: k6ZHvF1vkg
Title: Beyond Optimism: Exploration With Partially Observable Rewards
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration strategy for Monitored MDPs, where the agent cannot directly observe the environment's rewards due to modifications by a monitor. The authors propose an algorithm that alternates between goal-conditioned exploration and exploitation policies, demonstrating that their approach outperforms traditional exploration methods in environments with unobservable rewards. The paper introduces Monitored MDPs and provides empirical results showing significant performance improvements over baseline methods.

### Strengths and Weaknesses
Strengths:
- The algorithm is simple and intuitive, making it accessible.
- The paper is well-written, with clear explanations and useful illustrations.
- The experimental results show that the proposed method significantly outperforms baselines in various environments.

Weaknesses:
- The distinction between "unobserved reward" and a reward of 0 should be clarified, emphasizing that the agent knows it hasn't observed the reward.
- The definition of the agent's state in Monitored MDPs lacks clarity; it should specify whether the agent has access to both the environment and monitor states.
- The claim regarding the growth rate of $\log(t)$ compared to $N_t(s, a)$ is misleading and requires correction.
- Corollary 1's justification is unclear, particularly regarding the convergence of $\hat{Q}$ to $Q^*$ and the role of the monitor.
- The assertion that the policy maximizes the speed of reaching the targeted state is inaccurate; a different reward structure could yield faster results.
- The cost incurred by the agent needs clarification regarding when it is applied.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the "unobserved reward" concept by explicitly stating that it indicates the agent's awareness of not having observed it. Additionally, the authors should clarify the agent's state access in Monitored MDPs, specifying whether it includes both the environment and monitor states. The authors must correct the misleading statement about the growth rates in line 174 and provide a clearer justification for Corollary 1, particularly how $\hat{Q}$ converges to $Q^*$. We suggest revising the claim about the policy's efficiency in reaching the targeted state and providing a more detailed explanation of the cost incurred by the agent. Finally, we encourage the authors to compare their method against other intrinsic exploration techniques to contextualize their contributions better.