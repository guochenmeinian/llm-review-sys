ID: Fxi7pRmnYJ
Title: Adapting Segment Anything Models to Medical Imaging via Fine-Tuning without Domain Pretraining
Conference: AAAI
Year: 2024
Number of Reviews: 4
Original Ratings: 7, 6, 9, 8
Original Confidences: 4, 4, 4, 2

Aggregated Review:
### Key Points
This paper presents a variant of low-rank adaptation method for SAM and MedSAM models applied to medical imaging tasks. The authors propose a custom lightweight ConvNet head after the SAM encoder, demonstrating that fine-tuning SAM is more effective than fine-tuning MedSAM, asserting that medical pretraining is unnecessary for successful adaptation. The findings indicate that generalist models like SAM can excel in medical applications without dedicated pre-training.

### Strengths and Weaknesses
Strengths:
* The adaptation method shows significant improvements in SAM's performance without extensive pretraining or fine-tuning.
* The writing is clear and comprehensive, making the methodology and results easily understandable.
* The paper challenges the notion that foundation models require domain-specific pretraining, successfully demonstrating the effectiveness of SAM.
* Various decoder networks are experimented with, and preprocessing steps, training hyperparameters, and memory requirements are well-documented.

Weaknesses:
* The novelty of the method is limited, as LoRA is already a popular parameter-efficient fine-tuning approach.
* There are no quantitative comparisons with specialist models trained from scratch, such as UNet.
* The study is based on a single dataset, lacking additional medical image segmentation datasets for validation.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing quantitative comparisons with specialist models like UNet to substantiate their claims. Additionally, including results from at least two more medical image segmentation datasets, such as BraTS or VerSE, would enhance the robustness of their findings. Clarifying the input and output dimensions for the images and detailing how the UNet decoder's priors are utilized in the proposed ConvNet would also strengthen the methodology. Addressing these weaknesses will significantly improve the quality of the work.