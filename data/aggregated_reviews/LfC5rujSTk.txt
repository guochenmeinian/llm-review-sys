ID: LfC5rujSTk
Title: Can LLMs Implicitly Learn Numeric Parameter Constraints in Data Science APIs?
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 6, 3, 6, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical study investigating the implicit learning capabilities of large language models (LLMs) regarding numeric parameter constraints in data science library APIs, specifically focusing on PyTorch and NumPy. The authors introduce a benchmark called DSEVAL, which comprises 19,600 problems across 12 APIs and evaluates various LLMs in three settings: full program generation, all parameter prediction, and individual parameter prediction. The findings indicate that while LLMs can memorize common patterns, their performance significantly declines with increased complexity of constraints, particularly when faced with uncommon input tensor ranks or shapes. The authors reveal that open-source LLMs exhibit significantly lower performance compared to state-of-the-art models like GPT-4, especially on complex constraints, which has not been previously observed in standard coding benchmarks. Additionally, the authors analyze attention weights, noting that LLMs often focus on irrelevant tokens, leading to incorrect predictions.

### Strengths and Weaknesses
Strengths:
- The paper addresses an interesting and relevant problem regarding LLMs' comprehension of numeric constraints in data science APIs.
- It provides a comprehensive evaluation across 28 APIs and employs a rigorous methodology, including the use of SMT solvers for validation.
- The introduction of the DSEVAL benchmark is a notable contribution, highlighting the performance gap between open-source models and state-of-the-art models.
- The construction of DSEVAL is significant, facilitating future research in the domain of data science code generation.
- The attention analysis provides valuable insights into LLM behavior and common error patterns.

Weaknesses:
- The significance of the problem may be overstated, as the issue of numeric parameter constraints appears easily resolvable, raising concerns about the potential for further research.
- The paper lacks clarity in some experimental details, such as the selection criteria for APIs and the construction of the benchmark.
- There is minimal exploration of advanced prompting techniques or potential solutions for improving LLM performance in this domain.
- Some reviewers find the findings predictable and argue that the problem investigated is less significant.
- The paper lacks experimental support for the proposed reasons behind LLMs' performance issues, particularly regarding uncommon inputs and attention mechanisms.
- The evaluation may be perceived as limited in scope compared to other benchmarks, and the authors did not initially provide concrete evidence for the ease of extending the dataset.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental details, particularly regarding the selection and representation of the APIs in the DSEVAL benchmark. Additionally, we suggest incorporating error bars or conducting statistical significance tests to strengthen the validity of the findings. The authors should also explore advanced prompting techniques, such as the ReAct strategy, to assess their impact on LLM performance. Furthermore, including a human baseline for comparison would provide valuable context for interpreting LLM performance. We recommend that the authors provide experimental evidence to support the reasons behind LLMs' struggles with uncommon input tensors and attention to irrelevant parameters. Additionally, exploring potential solutions during the pre-training and fine-tuning stages, rather than solely during inference, could enhance the robustness of their conclusions. Lastly, we suggest that the authors include attention map visualizations and comprehensive error pattern categorization in their next revision to strengthen the analysis of LLM behavior and claims regarding the extensibility of the dataset.