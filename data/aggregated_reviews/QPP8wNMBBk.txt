ID: QPP8wNMBBk
Title: Unsupervised Lexical Simplification with Context Augmentation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an unsupervised approach to lexical simplification by augmenting the target context with additional sentences containing the target word. The authors propose a method that combines sampling and clustering of sentences, generating replacements for the target word using a pretrained language model (LM) and extracting the most frequent candidates. The results show that their method outperforms previous work, particularly when combined with GPT-3.5, achieving state-of-the-art performance. The approach is motivated by the need to handle rare words effectively and to provide sufficient context for simplification.

### Strengths and Weaknesses
Strengths:
- Well-motivated approach addressing lexical simplification for rare words.
- Achieves state-of-the-art results, especially when combined with GPT-3.5.
- Provides a clean and effective augmentation-based extension of previous methods.

Weaknesses:
- The contributions are somewhat narrow and primarily focused on engineering a well-performing system.
- The method is seen as incremental, building on existing work without introducing significant novel concepts.
- The choice of fasttext embeddings may not be optimal given the intended mitigation of over-representation issues.

### Suggestions for Improvement
We recommend that the authors improve the clarity of parameter settings and provide more detailed specifications to enhance reproducibility. Additionally, exploring the use of other pre-trained embeddings, such as GloVe or word2vec, in place of fasttext could strengthen the approach. We also suggest addressing the questions regarding the effect of the number of clusters on the approach and the typical overlap between the two sets of candidates for lexical simplifiers/substitutes.