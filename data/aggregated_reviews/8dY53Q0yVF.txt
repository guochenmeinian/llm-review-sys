ID: 8dY53Q0yVF
Title: An Adversarial Example for Direct Logit Attribution: Memory Management in GELU-4L
Conference: EMNLP/2024/Workshop/BlackBoxNLP
Year: 2024
Number of Reviews: 2
Original Ratings: -1, -1
Original Confidences: 4, 2

Aggregated Review:
### Key Points
This paper presents an analysis of memory management in transformer models, specifically focusing on a 4-layer model based on the GPT-2 architecture. The authors identify a "writing component" in the first layer and "erasing components" in the third layer, which produce output vectors that counteract those of the writing component. The findings challenge assumptions made in the direct logit attribution (DLA) method, suggesting that it may lead to misleading interpretations when not considering the erasure effect.

### Strengths and Weaknesses
Strengths:  
- The paper addresses interesting research problems regarding the interactions between components' outputs, warranting further investigation.  
- It uncovers significant limitations of the direct logit attribution method, previously used for model interpretability.

Weaknesses:  
- The study's findings may lack generalizability due to its focus on a single 4-layer transformer model.  
- The choice of the specific L0H2 head and the experimental design lacks adequate justification, including the selection of prompts, some of which are ungrammatical.  
- The meaningfulness of the findings is not sufficiently highlighted, as even the highest projection ratios are relatively low, indicating that "erasing heads" do not completely erase the information from L0H2.  
- The writing lacks coherence, with definitions of introduced concepts being too condensed and difficult to understand.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by extending the scope to include other attention heads and components. Additionally, the authors should provide a clearer justification for their experimental design choices, particularly regarding the selection of the tiny transformer model and the specific head. It would be beneficial to analyze the limitations of direct logit attribution in more detail within the experimental section. Finally, we suggest enhancing the coherence of the writing and expanding the definitions of key concepts to improve comprehension.