ID: zaXuMqOAF4
Title: Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 25
Original Ratings: 5, 8, 4, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of positional encoding methods, particularly focusing on the limitations of No Position Encoding (NoPE) and standard Position Encoding (PE) in extrapolating beyond training lengths. The authors propose a new method, weave position encoding, and introduce Mesa-Extrapolation, which recalibrates position IDs to enhance extrapolation performance without incurring additional costs. The effectiveness of Mesa-Extrapolation is supported by empirical experiments across various benchmarks. Additionally, the paper explores extrapolation success based on the hidden state value "o" in relation to a threshold "H," proposing that extrapolation succeeds for sequences longer than H and fails for shorter ones. The authors discuss the selection of specific dimensions for embedding vectors and the assumptions underlying their chunk-based triangular attention mechanism, which posits that tokens in the middle of the context are related to surrounding tokens and those at the beginning of the sequence.

### Strengths and Weaknesses
Strengths:
- The presentation of weave PE and Mesa-Extrapolation is clear, with detailed explanations aiding comprehension.
- Comprehensive experiments validate the effectiveness of Mesa-Extrapolation.
- The authors effectively addressed previous concerns and clarified complex assumptions, leading to an improved understanding of their model.
- The paper's findings on extrapolation and attention mechanisms are supported by experimental evidence and align with established principles in the field.
- The paper includes an analysis of latency and memory usage, contributing to its practical relevance.

Weaknesses:
- Major concerns arise regarding the similarity between Star PE and Self-Extend LLM, suggesting a need for clarification on their contributions.
- Specific discrepancies between the implementation results and theoretical equations, particularly in Figure 1 and Section 4.1, raise questions about accuracy.
- The empirical performance of the proposed method varies across tasks, indicating potential limitations in its applicability.
- There remains confusion regarding the interpretation of Assumption 1 and the selection of dimensions 1 and 6, which may appear data-specific and challenging to reproduce.
- The validation of the assumption regarding the relevance of middle tokens in the attention mechanism is not thoroughly justified.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relationship between Star PE and Self-Extend LLM, ensuring that the contributions of each are distinctly articulated. Additionally, the authors should address the discrepancies noted in the implementation results and theoretical equations, particularly in Figure 1. A comparison of performance between Mesa-Extrapolation and Self-Extend LLM would provide valuable insights. Furthermore, enhancing the empirical evaluations, especially in contexts like Ruler and RepoQA, would strengthen the paper's claims regarding the method's effectiveness. We also suggest improving the clarity of Assumption 1 by explicitly defining "o" as the hidden state value rather than the sequence length. Additionally, providing a more robust justification for the choice of dimensions 1 and 6 would alleviate concerns about their reproducibility across different setups. Finally, we encourage the authors to validate their assumption regarding the importance of middle tokens in the context of the attention mechanism with further experimental evidence.