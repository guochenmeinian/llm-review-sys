ID: R4xpvDTWkV
Title: SGFormer: Simplifying and Empowering Transformers for Large-Graph Representations
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called Simplified Graph Transformers (SGFormer) that utilizes a linear attention mechanism for graph data, claiming efficiency by eliminating the softmax function in dot product attention. The authors argue that the removal of softmax addresses the quadratic complexity it introduces, which limits performance on graphs with a high number of nodes. They propose using a single attention layer instead of multiple layers, asserting that this suffices for signal denoising. Theoretical insights into the expressivity of their proposed one-layer global attention mechanism compared to multi-layer approaches are provided. Experimental results indicate that SGFormer achieves competitive performance on node property prediction tasks and demonstrates scalability to large graphs.

### Strengths and Weaknesses
Strengths:
- SGFormer simplifies the design of Transformers for large graphs, enhancing efficiency and scalability.
- The theoretical justification for the methodology is provided, aiding understanding and potential improvements.
- The authors effectively address the scalability issues associated with softmax attention in large graphs.
- Empirical results across various datasets appear promising, and the writing is clear and accessible.
- The theoretical insights regarding the expressivity of one-layer attention are valuable.

Weaknesses:
- The novelty of the linear attention mechanism is limited due to the elimination of nonlinearity, necessitating further analysis of the impact of removing softmax.
- There is insufficient analysis on the implications of removing softmax, particularly regarding its limitations and potential negative impacts on performance.
- Gains on small datasets are marginal, and optimal baseline results for large datasets seem to be omitted, raising concerns about the completeness of comparisons.
- The claim of not using positional encodings (PEs) is questionable, as the GNN representations may act as implicit PEs.
- Performance significantly deteriorates without GNNs, suggesting that the combination of GNN and linear attention is crucial, which may undermine the claim of the method's effectiveness based solely on linear attention.
- The paper lacks a direct comparison with existing literature that critiques softmax, which could enhance the discussion on its efficacy.

### Suggestions for Improvement
We recommend that the authors improve the novelty analysis by conducting a detailed study on the implications of eliminating softmax in isolation. Specifically, please address the following questions: 1) From an expressivity standpoint, why does it make sense to eliminate softmax, given the better results on the experimented datasets? 2) What are the limitations and repercussions of this change, particularly in cases where it may negatively impact performance? Additionally, please address the omission of optimal baseline results for large datasets and consider incorporating comparisons with efficient transformers like GraphGPS. Clarifying the role of GNNs in relation to PEs is essential, as is demonstrating the method's performance independent of GNNs. Lastly, we suggest exploring the expressiveness of the single-layer attention mechanism and its ability to learn complex functions, potentially through pretraining on large datasets followed by fine-tuning.