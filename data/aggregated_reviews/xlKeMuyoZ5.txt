ID: xlKeMuyoZ5
Title: Large Language Models' Expert-level Global History Knowledge Benchmark (HiST-LLM)
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 9, 6, 6, 7, -1
Original Confidences: 4, 3, 3, 2, -1

Aggregated Review:
### Key Points
This paper presents a structured representation of human historical knowledge, comprising 36,000 data points across major historical societies and periods, curated by the Seshat Board. The authors evaluate several commercially available language models (LLMs) using a 4-way multiple-choice evaluation scheme, revealing that while leading LLMs outperform random guessing, they do not reach expert-level accuracy, with performance varying by region and time period. The study highlights the importance of addressing ethical concerns related to bias in English-language sources and the implications of the Seshat dataset's quantitative approach.

### Strengths and Weaknesses
Strengths:
- The dataset is extensive and meticulously curated, providing a valuable resource for evaluating LLMs' historical knowledge.
- The evaluation methodology is rigorous, with detailed analysis yielding insightful results regarding model performance across different historical contexts.
- The work fills a significant gap in research on the historical accuracy of LLMs, promoting interdisciplinary applications.

Weaknesses:
- The evaluation is limited to a small number of LLMs, potentially overlooking insights from other models.
- The reliance on English-language sources may introduce bias and limit the dataset's comprehensiveness.
- The 4-way multiple-choice evaluation may not adequately capture the nuances of LLMs' understanding and reasoning capabilities.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including additional LLMs, such as Anthropic's Claude 3, Google's Gemini, and Mistral's models, to provide a broader comparison. Furthermore, exploring the performance of LLMs using chain-of-thought prompting techniques could yield statistically significant insights. We also suggest conducting a review of existing training corpora to assess the presence of historical text and potential data pollution. Lastly, addressing the unclear licensing of the dataset and expanding the discussion on ethical concerns, particularly regarding bias and copyright, is essential for enhancing the paper's impact.