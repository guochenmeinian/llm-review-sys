ID: j5BuTrEj35
Title: Scaling Data-Constrained Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 8, 8, 7, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the optimal tradeoff between the size of large language models (LLMs) and the amount of training data, particularly focusing on the effects of data repetition across multiple epochs. The authors extend the Chinchilla scaling law by incorporating a multiplicative decay factor for repeated data and model size, demonstrating that training on repeated data up to four times yields similar performance to training on unique data. They conduct extensive experiments across various model sizes and training protocols, revealing that data repetition is often more beneficial than merely increasing model parameters when unique data is constrained.

### Strengths and Weaknesses
Strengths:
- The research addresses a crucial empirical question in LLM training.
- It proposes a potentially more accurate model for data and parameter scaling.
- The extensive experimental setup covers a wide range of models and scenarios, yielding interesting insights.
- The paper is well-written and presents non-trivial observations.

Weaknesses:
- There is ambiguity regarding the use of validation versus test loss, which could lead to overfitting concerns.
- The significance of small loss differences in results is questionable.
- The proposed scaling law lacks robust validation metrics for comparison with existing models.
- The choice of the specific parametric forms in the scaling equations is not clearly justified, raising concerns about their applicability across different data types and model sizes.

### Suggestions for Improvement
We recommend that the authors clarify the distinction between validation and test loss in their reporting to avoid potential overfitting issues. Additionally, we suggest providing statistical significance for the observed loss differences to strengthen the findings. To enhance the validation of the proposed scaling law, including comparative fitting metrics such as rÂ² would be beneficial. We also encourage the authors to concisely explain the rationale behind the chosen parametric forms in equations 5 and 6, and to consider the varying effects of data complexity on the scaling behavior. Lastly, addressing the implications of their findings for larger models and datasets would provide a more comprehensive understanding of the scaling laws in practice.