ID: Xy7DoWSNZX
Title: CAP:  Correlation-Aware Pruning for Highly-Accurate Sparse Vision Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 3, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Correlation Aware Pruner (CAP), an innovative framework for unstructured pruning of vision transformer models, which effectively considers weight correlations. The authors propose a second-order method utilizing the empirical Fisher matrix to derive saliency scores for weight pruning, demonstrating improved model size-accuracy tradeoffs and achieving high sparsity levels (75-80%) with minimal accuracy loss (<1%). The paper also explores various training techniques, including learning rate scheduling and data augmentation, to enhance the pruning process.

### Strengths and Weaknesses
Strengths:
- The proposed method achieves state-of-the-art results in unstructured pruning, allowing ViT models to reach unprecedented sparsity levels without significant accuracy degradation.
- CAP is well-motivated, addressing the correlation of removed weights and emphasizing the importance of learning rate schedules.
- The paper is well-organized, with sufficient and sound experimental validation across multiple models, including ConvNeXt and DeiT.

Weaknesses:
- Unstructured pruning presents challenges on GPU devices, as the authors only evaluate latency on a sparsity-aware CPU inference engine, limiting practical applicability.
- Some results for SViTE are missing in Table 2, and the ordering of sections could be improved by placing CAP at the bottom.
- The source of improvement in end metrics remains unclear, as comparisons with similar methods like WoodFisher are not adequately addressed, and ablation studies do not sufficiently evaluate the proposed method's components.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of CAP on GPU devices to better demonstrate its practical applicability. Additionally, please ensure that all relevant results, particularly for SViTE, are included in the tables and consider reordering sections for clarity. We also suggest conducting more comprehensive ablation studies to clarify the contributions of different components of the method, including comparisons against WoodFisher for gradual pruning scenarios. Lastly, addressing the limitations regarding the computational cost of the Fisher matrix in large models and exploring potential relaxation strategies would strengthen the paper.