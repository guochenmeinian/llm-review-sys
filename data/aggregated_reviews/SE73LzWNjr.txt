ID: SE73LzWNjr
Title: Nearly Optimal VC-Dimension and Pseudo-Dimension Bounds for Deep Neural Network Derivatives
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to estimate the Vapnik-Chervonenkis (VC) dimension and pseudo-dimension of deep neural network (DNN) derivatives with ReLU activation functions. The authors provide theoretical analysis and proofs, addressing gaps in learning error estimations for physics-informed machine learning applications, including partial differential equations and network compression. Additionally, the paper demonstrates the use of DNNs for approximating functions in Sobolev spaces, achieving a nearly-optimal approximation rate. Overall, the study offers a framework for analyzing and optimizing DNNs while incorporating mathematical concepts like VC-dimension and pseudo-dimension.

### Strengths and Weaknesses
Strengths:  
* The topic is highly relevant to deep learning, offering an innovative approach to estimating VC-dimension and pseudo-dimension of DNN derivatives.  
* The paper is well-written, presenting mathematical language and definitions clearly, with detailed and logically structured proofs.  
* It introduces two significant theorems addressing the approximation rate problem of DNNs in Sobolev spaces and the generalization error in loss functions involving DNN derivatives.  
* The proposed method has potential applications in various areas of physics-informed machine learning.

Weaknesses:  
* Some aspects of the paper could be clearer; for instance, the introduction could better highlight the main contributions and provide a more comprehensive overview of the state-of-the-art and existing research limitations.  
* The references section could be more thorough, covering additional related studies for a better literature overview.  
* The paper assumes familiarity with Sobolev training, which may hinder readability. Technical proofs could be deferred to the appendix, and notations should be introduced before being stated.  
* Certain sentences are repeated, affecting the overall clarity.

### Suggestions for Improvement
We recommend that the authors improve the introduction to more effectively demonstrate the main contributions and provide a detailed overview of existing research limitations. Additionally, the authors should enhance the references section to include more related studies. To improve readability, we suggest deferring some technical proofs to the appendix and introducing notations prior to their use. Furthermore, the authors should address the repetition of sentences throughout the paper to enhance clarity.