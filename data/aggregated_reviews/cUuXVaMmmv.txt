ID: cUuXVaMmmv
Title: Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 7, 7, 6, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to sequential decision-making within a hierarchical framework, focusing on achievement distillation combined with the Proximal Policy Optimization (PPO) algorithm. The authors propose two self-supervised tasks—Intra-trajectory achievement prediction and cross-trajectory achievement matching—to enhance representation learning. The method demonstrates significant performance improvements in the challenging Crafter environment, outperforming strong baselines like Dreamer-v3.

### Strengths and Weaknesses
Strengths:
1. The introduction of a self-supervised loss within hierarchical decision-making is innovative and easily integrable with PPO.
2. The empirical results show a compelling performance boost over existing model-based RL algorithms in the Crafter environment.

Weaknesses:
1. The assumption that the agent lacks knowledge of unlocked achievements raises concerns about its realism and impact on performance, particularly in comparison to baseline methods that do assume such knowledge.
2. The evaluation is limited to the Crafter environment, raising questions about the generalizability of the proposed method to other settings.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing a real-world example where the assumption of the agent's ignorance of achievements is necessary, which would clarify its practical applicability. Additionally, we suggest conducting experiments in various environments beyond Crafter to demonstrate the generality of achievement distillation. It would also be beneficial to compare the proposed contrastive losses with an "oracle" objective to better understand the impact of the knowledge assumption on performance. Finally, clarifying whether the findings are specific to policy gradient methods or applicable to value-based approaches would enhance the evaluation of the proposed method's potential.