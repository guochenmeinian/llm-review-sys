ID: chCrhE2kl4
Title: TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method, TK-KNN, aimed at improving the generation of pseudo-labels in intent detection, particularly when labeled data is scarce. The authors propose a scoring function that combines predicted probabilities and cosine similarity to select top-k instances for supervised training, thereby addressing biases associated with fixed thresholds in pseudo-labeling. The evaluation demonstrates the method's efficacy across multiple intent classification datasets, especially in low-resource settings.

### Strengths and Weaknesses
Strengths:  
- The proposed method effectively tackles the challenges of intent detection with a high number of classes, showing improved performance over existing methods.  
- The evaluation is comprehensive, providing confidence intervals and comparisons with various baselines.  
- The approach is relatively straightforward, allowing for potential application to other supervised learning problems.

Weaknesses:  
- The ablation study is incomplete, lacking thorough investigation of all hyperparameters.  
- Concerns exist regarding the novelty of the method, as top-k selection and kNN techniques are already established in the literature.  
- The hypothesis that instances close in latent space share the same label may not hold true universally, necessitating further evidence.

### Suggestions for Improvement
We recommend that the authors improve the ablation study to include all hyperparameters mentioned, particularly addressing the role of Î³ in their analysis. Additionally, we suggest providing a more comprehensive discussion of related work to clarify the novelty of their contributions. The authors should also justify the choice of using the closest predicted instance for scoring and consider experimenting with varying values of k across training cycles to enhance robustness. Finally, we encourage the authors to provide clearer explanations of their findings, particularly regarding the stability of GAN-BERT with increasing labeled data.