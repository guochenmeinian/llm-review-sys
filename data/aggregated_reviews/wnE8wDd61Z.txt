ID: wnE8wDd61Z
Title: Knowledge Graph Compression Enhances Diverse Commonsense Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a differentiable graph compression algorithm aimed at extracting salient knowledge from a commonsense knowledge graph (CSKG) to generate diverse commonsense explanations. The authors evaluate their method on the ComVE and abductive NLG tasks, demonstrating that their approach outperforms baseline methods in terms of the quality and diversity of generated explanations. The primary contribution is the CSKG compression algorithm, which shows potential for broader applications beyond commonsense generation.

### Strengths and Weaknesses
Strengths:
- The novel approach of compressing commonsense knowledge graphs is well-motivated and has implications for other tasks like commonsense QA and multi-hop reasoning.
- Comprehensive experiments and analyses validate the effectiveness of the proposed method, showing improvements over existing models.
- The paper is well-written, with clear methodology and illustrative case studies.

Weaknesses:
- The introduction lacks clarity, particularly in transitioning from CSKG to concept-level CSKG, and does not convincingly justify the choice of ConceptNet over other CSKGs.
- The method's reliance on language models raises questions about the necessity of concept selection if the language model already contains the required knowledge.
- The evaluation primarily uses automated metrics, lacking human evaluation to assess the quality and logicality of generated outputs.

### Suggestions for Improvement
We recommend that the authors improve the introduction to provide a more compelling motivation for their study, emphasizing the importance of concept-level understanding in commonsense reasoning. Additionally, including a comparison to the original COMET as another baseline would enhance the evaluation context. We also suggest clarifying how the training objectives guide the selection of relevant concepts and how these signals are back-propagated. Finally, improving the readability of Figure 3 by enlarging the font or repositioning the legend would enhance clarity.