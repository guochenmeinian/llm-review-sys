ID: X5MH7iut9K
Title: Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly
Conference: NeurIPS
Year: 2023
Number of Reviews: 29
Original Ratings: 7, 6, 4, 5, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 5, 5, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark called TA-Bench for transfer-based attacks, implementing over 30 methods and evaluating them on 10 popular victim models on ImageNet. The authors aim to provide a systematic, fair, and practical means for researchers to compare different transfer-based attack methods. The evaluation results yield novel insights, particularly regarding the effectiveness of using Vision Transformer (ViT) architectures and the MoreBayesian method for enhancing transferability. Additionally, the authors emphasize the importance of testing defense methods and targeted attacks, proposing to enrich the dataset from the NIPS2017 adversarial competition to enhance the evaluation of adversarial robustness. They acknowledge the need for clarity regarding the selection methodology of the 30+ techniques and the limitations of hyperparameter tuning, which are critical for assessing the validity of their claims.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow, showcasing solid engineering work with comprehensive experimental results.
2. TA-Bench offers practical value by integrating various methods and providing extensive evaluations, contributing significantly to the field of adversarial machine learning.
3. The authors' work is recognized as contributing significantly to the development of the field and express a willingness to incorporate additional techniques and results to strengthen their benchmark.

Weaknesses:
1. The benchmark is limited to ImageNet, which raises concerns about the credibility of results; extending evaluations to other datasets like MNIST or JFT300 would enhance robustness.
2. The methodology for selecting the 30+ techniques is unclear, and the paper does not adequately consider adversarial defense methods, leading to potentially misleading robustness assessments.
3. The lack of a shared codebase limits the ability to evaluate the paper's claims and makes it difficult to assess the benchmark's accessibility and usability.
4. The evaluation lacks a systematic approach to hyperparameter tuning and does not discuss the standards and metrics for evaluation explicitly, leading to potential overfitting issues.
5. The taxonomy used in the paper does not convincingly differentiate between techniques with varying objectives.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by extending evaluations to additional datasets, including smaller and larger ones, to enhance robustness. It is crucial to clarify the methodology used for selecting the 30+ techniques and to incorporate adversarial defense methods to provide a more accurate assessment of robustness. Additionally, we suggest providing the codebase to facilitate reproducibility and usability. The authors should establish a standardized protocol for hyperparameter tuning and ensure that the sets of target models used for hyperparameter tuning are distinct from those used to report final results to avoid data leakage. We encourage the authors to provide specific details on which techniques had hyperparameters selected through cross-validation and to consider reusing the taxonomy from [59] to strengthen their classification of techniques. Lastly, we advise exploring advanced optimizers with adaptive step sizes for I-FGSM, as a fixed step size may not be optimal.