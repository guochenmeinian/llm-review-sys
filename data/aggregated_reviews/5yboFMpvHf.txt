ID: 5yboFMpvHf
Title: Federated Model Heterogeneous Matryoshka Representation Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 6, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FedMRL, a method utilizing distillation to address model heterogeneity in Federated Learning (FL). FedMRL learns a small proxy homogeneous global model in a federated manner, distilling knowledge to heterogeneous client models. The authors employ a Matryoshka Representation Learning (MRL) approach to enhance representation knowledge interaction, generating multi-dimensional and multi-granular representations. Theoretical analysis and experiments validate the effectiveness of FedMRL.

### Strengths and Weaknesses
Strengths:
1. The fusion of representations from global and local models into a single representation vector, followed by their detachment in a Matryoshka manner, is intriguing and inspiring.
2. The writing is clear and easy to follow.
3. Transmitting a global model with a lower feature dimension is promising, potentially reducing communication overhead.

Weaknesses:
1. FedMRL shares a proxy global model, which incurs high communication costs, conflicting with claims of "low communication costs." There is a lack of numerical results comparing communication and computation costs with similar methods (FedKD and FML).
2. The use of only two datasets for image tasks is insufficient for FL.
3. The client models in experiments lack sufficient heterogeneity, as they consist of CNN networks with identical structures, limiting the demonstration of FedMRL's effectiveness in MHeteroFL.
4. There is only one baseline for the model split category, and FedGH also falls within this category.
5. Details on computing FLOPs are missing.
6. The proof of Theorem 2 contains a flaw in deriving Eq. (31) from Eq. (30), and the existence of solutions for $\eta$ in Eq. (32) may be compromised.
7. The privacy analysis lacks sufficient substantiation and would benefit from theoretical analysis or experimental results.

### Suggestions for Improvement
We recommend that the authors improve the justification of the novelty of their approach by discussing related works such as FedGH and ProxyFL. Additionally, the authors should provide qualitative results and quantitative analysis to address the extra computational costs at the client side. To enhance the evaluation, we suggest including more diverse CNN architectures and exploring the performance of FedMRL with deeper networks or other neural architectures like ResNet or Transformers. Furthermore, the authors should include a comparison of the theoretical convergence rate with traditional distillation-based methods and provide details on computing FLOPs. Lastly, we recommend strengthening the privacy analysis with theoretical or experimental support to substantiate claims regarding the independence of the global and local models.