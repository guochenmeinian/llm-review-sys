ID: v5Aaxk4sSy
Title: Improving Adversarial Robustness via Information Bottleneck Distillation
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Information Bottleneck Distillation (IBD) to enhance adversarial robustness through the information bottleneck principle. The authors propose two distillation strategies: soft label distillation and adaptive feature distillation, which leverage the predictions of robust models to maximize mutual information. Experimental results indicate that these strategies effectively improve adversarial robustness against various attacks.

### Strengths and Weaknesses
Strengths:  
1. The paper is technically sound, with an elegant motivation and formulation of the proposed method.  
2. The novel formulation of the information bottleneck objective in the context of adversarial robustness distillation is well-motivated and clearly distinguished from existing works.  
3. The IBD method significantly enhances the robustness of Deep Neural Networks (DNNs) against attacks, demonstrating effectiveness across benchmark datasets.  

Weaknesses:  
1. Some derivation details, particularly in the last step of Eq. (11), are overly simplified, making them difficult to understand.  
2. The proposed methods heavily depend on a robust teacher model; thus, the impact of varying teacher models on the proposed methods requires further experimental evaluation.  
3. The paper contains two hyperparameters, $\alpha$ and $\beta$, which need analysis regarding their impact on the proposed method.  
4. The experimental benchmarks used are somewhat outdated; newer adversarial training methods should be considered to enhance the universality of the findings.  
5. Minor typographical errors and inconsistencies in presentation need correction.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the derivation details, particularly regarding Eq. (11). Additionally, the authors should conduct experiments to evaluate the effects of varying teacher models on the proposed methods. It would be beneficial to explicitly list the main contributions of the work to aid readers unfamiliar with the relevant methods. We also suggest providing a more detailed explanation of the two optimization processes related to the IBD strategies. Finally, addressing the trade-off between robustness and clean accuracy in the experimental results would strengthen the paper.