ID: ls4Pfsl2jZ
Title: Multi-step Jailbreaking Privacy Attacks on ChatGPT
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates privacy risks associated with large language models (LLMs) like ChatGPT and the New Bing. The authors propose a novel multi-step jailbreaking prompt that successfully extracts personally identifiable information (PII), revealing vulnerabilities in the models' privacy defenses. Experimental results demonstrate the effectiveness of this approach against both ChatGPT and New Bing, highlighting potential privacy risks from application-integrated LLMs.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a timely and well-motivated topic concerning privacy leakage in LLMs.  
- It presents a novel multi-step jailbreaking prompt that effectively evades existing privacy defenses.  
- The writing is clear and the methodology is well-structured.  

Weaknesses:  
- The effectiveness of the proposed attack has only been demonstrated on limited datasets and models, necessitating further evaluation for broader applicability.  
- The study lacks a systematic exploration of various types of personal data leakage and does not provide sufficient justification for the chosen datasets.  
- The paper does not introduce significant technical innovation or new methodologies, limiting its contribution to the field.  

### Suggestions for Improvement
We recommend that the authors improve the breadth of their experiments by evaluating the multi-step attack on a more diverse set of datasets and larger LLMs. Additionally, the authors should consider exploring more types of personal data leakage and provide a rationale for their dataset selection. To enhance the technical contribution, we suggest proposing potential defenses against the identified privacy risks or developing a resource that could aid further research in this area. Finally, we advise the authors to ensure that sensitive information in case studies is permanently anonymized rather than merely obscured.