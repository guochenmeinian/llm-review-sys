ID: TwdX1W3M6S
Title: Online Iterative Reinforcement Learning from Human Feedback with General Preference Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 7, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper explores Reinforcement Learning from Human Feedback (RLHF) using a general preference model, formulating a learning objective to identify a policy preferred by a KL-regularized preference oracle. The authors propose a sample-efficient training algorithm for both online and offline learning scenarios and validate their framework through empirical studies. Additionally, the paper introduces a two-player game approach to RLHF, combining pessimism in offline learning with exploration/exploitation in online settings.

### Strengths and Weaknesses
Strengths:
1. The paper addresses RLHF under a general preference oracle, capturing non-transitive preferences.
2. The authors present an efficient algorithm applicable in both offline and online contexts.
3. The exposition is clear, and the authors provide practical implementations alongside theoretical guarantees.
4. Empirical results demonstrate the proposed method's effectiveness compared to traditional reward-based models.

Weaknesses:
1. The empirical validation lacks depth; comparisons with a broader range of state-of-the-art RLHF methods are needed.
2. The reproducibility of the proposed method is unclear, necessitating a more detailed description of the framework or algorithm.
3. The experimental verification does not sufficiently support the paper's claims, particularly regarding the novelty and soundness of the empirical study.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their experimental details, particularly how the reward models are trained and the exact policy optimization methods used in both online and offline settings. Additionally, including examples of the prompts used for training the preference model and responses generated by different methods would enhance understanding. To strengthen the empirical validation, we suggest comparing the proposed algorithm against other online RLHF methods and conducting ablation studies to assess the contributions of various elements in the empirical designs. Lastly, addressing the theoretical versus practical discrepancies in the algorithm and exploring alternative choices for the practical enhancer would further solidify the paper's contributions.