ID: RBI4oAbdpm
Title: Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 5, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a neural network model known as the Light Encoder Heavy Decoder (LEHD) and its training method for addressing combinatorial optimization problems. The authors propose a reinforcement learning self-improvement method (RL-SI) that enhances the limitations of supervised learning (SL) and traditional reinforcement learning (RL) in training for LEHD. The proposed RPC method updates partial solutions to improve inference outcomes. Experimental results indicate promising performance on large-scale Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), with the RL-SI method demonstrating significantly improved efficiency and generalization performance compared to classical constructive models like POMO and Sym-NCO, particularly on the CVRP100 dataset. However, the reliance on supervised learning and the need for optimal solutions during training pose limitations.

### Strengths and Weaknesses
Strengths:  
**S1.** The architecture of the Heavy Decoder Model effectively yields optimal outputs across various node sizes.  
**S2.** The RL-SI method effectively overcomes the shortcomings of both SL and RL training approaches.  
**S3.** The experimental results demonstrate promising performance on large-scale TSP and CVRP instances, with RL-SI showing improved efficiency and performance metrics.  
**S4.** The paper is well-structured and easy to read.  

Weaknesses:  
**W1.** The novelty of the training method and RPC is limited.  
**W2.** The method's applicability is constrained when optimal solutions are unavailable, which is common in practical CO problems.  
**W3.** The optimization degree of the algorithm appears to be the upper limit for the LEHD model.  
**W4.** The number of instances used in node experiments (128) is insufficient, as previous studies typically utilized 1,000 instances for similar experiments.  
**W5.** The paper may require further elaboration on the implementation details and the specific advantages of RL-SI over existing methods.  

### Suggestions for Improvement
We recommend that the authors improve the novelty of the training method and RPC by exploring existing divide-and-conquer strategies. Additionally, we suggest clarifying the training process for small-scale problem instances and addressing the confusion regarding training time. It would be beneficial to include a comparison with LKH in Table 1 and to provide more details on the selection of partial solutions for RPC updates. Furthermore, we recommend improving the clarity of the implementation details for the RL-SI method to enhance understanding of its advantages. Lastly, consider providing more comprehensive comparisons with existing models to further substantiate the claims of improved efficiency and performance, and correct minor typographical errors to enhance the overall presentation of the paper.