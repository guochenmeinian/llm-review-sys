ID: MHOhNKeJk8
Title: Graph Meets LLM for Review Personalization based on User Votes
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MAGLLM, an innovative approach for review personalization that utilizes user voting behavior and integrates heterogeneous graphs with large language models (LLMs). The authors demonstrate that voting signals outperform traditional authorship data, achieving better user coverage. The evaluation spans multiple e-commerce domains and includes comprehensive comparisons against various baseline methods.

### Strengths and Weaknesses
Strengths:
- The paper identifies significant limitations in existing personalization methods that rely solely on authorship.
- It effectively demonstrates the advantages of using voting signals, supported by empirical evidence.
- The methodology is sound, with thorough evaluations and clear explanations of the heterogeneous graph construction and meta-path definitions.

Weaknesses:
- The reliance on a single dataset (Ciao) limits the generalizability of the findings, despite some analysis using the Edmunds dataset.
- There is insufficient discussion regarding the computational requirements and scalability of the MAGLLM approach.
- The evaluation setup is flawed due to the use of a 4-core dataset, which omits critical user/product interactions.
- The truncation of reviews to 150 tokens may lead to loss of important information, and this limitation is not adequately analyzed.
- The hyperparameter selection process is not described, raising concerns about potential overfitting.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by evaluating MAGLLM on a broader range of datasets beyond Ciao. Additionally, a detailed analysis of computational costs and scalability should be included to address practical deployment concerns. The authors should also provide a more comprehensive comparison with other LLM-based recommendation systems and justify their choice of LLaMA-7B, exploring alternative architectures. Furthermore, the evaluation setup should be revised to avoid the artificial constraints of the 4-core dataset, and the impact of truncating reviews should be analyzed. Lastly, a clearer discussion of hyperparameter selection and model robustness is necessary to enhance the paper's technical rigor.