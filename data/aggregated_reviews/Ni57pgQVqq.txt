ID: Ni57pgQVqq
Title: APoLLo : Unified Adapter and Prompt Learning for Vision Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents APoLLo, a framework that integrates adapter and prompt tuning for visual-language pretrained models, aimed at enhancing generalization in few-shot scenarios. The authors propose a multi-modal augmentation strategy utilizing large language models (LLMs) and text-conditioned diffusion models, alongside a contrastive-consistency loss to improve performance on various downstream tasks. The effectiveness of APoLLo is demonstrated through evaluations on ten datasets, achieving state-of-the-art results.

### Strengths and Weaknesses
Strengths:
- The proposed method uniquely unifies adapter and prompt tuning, facilitating few-shot learning without sacrificing zero-shot generalization.
- The introduction of multi-modal augmentation and cross-attention adapter layers significantly contributes to the field, supported by thorough ablation studies.
- Empirical results show strong performance improvements across multiple benchmarks.

Weaknesses:
- The submission leans towards an empirical study, with limited novelty beyond combining existing methods, as both loss functions and augmentation techniques have been previously explored.
- Clarity regarding the quality and quantity of augmentations generated by the LLM and diffusion models is lacking, raising concerns about the reproducibility of results.
- Visualization of cross-modal alignment is not convincing, as it only demonstrates single salient objects without robust validation of effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the impact of augmentation quality and quantity by providing detailed descriptions of the LLM and diffusion models used. Additionally, addressing the effectiveness of augmentations on various model sizes could enhance understanding of the method's requirements. We also suggest including more comprehensive ablation studies on adapter architecture choices to bolster the novelty of the approach. Lastly, enhancing the visualization techniques to better demonstrate cross-modal alignment would strengthen the paper's claims.