ID: PnbCA4ylIc
Title: Goal Driven Discovery of Distributional Differences via Language Descriptions
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 6, 3, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new task called D5 (goal Driven Discovery of Differences between text Distributions via language Descriptions), aimed at identifying and describing meaningful differences between two text corpora based on user-specified research goals. The authors propose a language model (LM)-based approach to generate outputs that facilitate the exploration of these differences. They created two datasets, SYND5 and OPEND5, to evaluate the D5 task based on validity and relevance. The evaluation includes both objective validity criteria and subjective assessments of relevance, demonstrating the potential of LMs in aiding human exploration of hypotheses regarding differences in corpora.

### Strengths and Weaknesses
Strengths:
- The creation of the OPEND5 and SYND5 datasets demonstrates originality and addresses an interesting NLP task.
- The paper is well-written, clearly defining the proposed task and its objectives, emphasizing the real-world significance of enabling exploration of goal-relevant differences.
- The qualitative analysis is thorough and well-executed, contributing valuable insights to the community.
- The proposed evaluation criteria provide a framework for similar tasks, and the OpenD5 dataset is a valuable contribution if properly released and documented.

Weaknesses:
- The justification for the D5 task is unclear, lacking concrete examples or real-world applications that highlight its significance.
- The quality of the datasets, particularly OPEND5, is questionable due to potential biases from manual collection and the absence of ground truth.
- The validity of the proposed evaluation criteria is undermined by the reliance on authors for prompt creation and subjective assessments, raising concerns about the robustness of the system to variations in prompt phrasing.
- The interpretation of discoveries is challenging, with no clear strategies provided to address this issue, and the claim of generating novel insights remains unsubstantiated due to the lack of domain expert involvement.
- The reliance on Mechanical Turkers for validity judgments may introduce biases due to their lack of expertise, and limitations regarding bias reinforcement and the absence of a clear pathway for mitigation are significant.
- The paper lacks a conclusion section and does not adequately discuss the uneven distribution of examples across domains, and the acknowledgment of limitations does not adequately address real-world harms.

### Suggestions for Improvement
We recommend that the authors improve the articulation of the D5 task's significance by providing concrete real-world applications or scenarios where it could add value. Additionally, the authors should clarify the dataset quality and ensure that ground truth is established for OPEND5. It would be beneficial to include strategies for interpreting discoveries and to address the potential biases introduced by relying on Turkers' judgments. We suggest enhancing the robustness of the system by conducting studies on how variations in prompt phrasing affect generated hypotheses and including non-author participants in prompt creation and evaluation to enhance reliability. The authors should provide a clearer analysis of the validity of discoveries generated, particularly in relation to the OpenD5 dataset, and incorporate a discussion on the limitations of bias reinforcement while establishing pathways for mitigation. Finally, addressing the implications of using the system in non-English contexts and the potential inequities in access to resources would strengthen the paper. Adding a conclusion section and discussing the uneven distribution of data examples across domains would also be beneficial.