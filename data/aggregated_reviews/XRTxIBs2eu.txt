ID: XRTxIBs2eu
Title: Block-State Transformers
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 6, 6, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel long-range transformer architecture called the Block-State Transformer (BST), which integrates State Space Models (SSMs) with Block-Recurrent Transformers to enhance efficiency in language modeling. The authors claim that their model outperforms established baselines like Transformer-XL and Block-Recurrent Transformers across various datasets, including PG19 and arXiv Math. The evaluation demonstrates reasonable speedup and comparable performance to traditional transformers, with a focus on both local and global contextualization.

### Strengths and Weaknesses
Strengths:
1. The combination of SSMs and transformers is a compelling approach that leverages the strengths of both methods while mitigating their weaknesses.
2. The evaluation is thorough and well-executed, providing clear results on language modeling tasks.
3. The writing is clear, effectively conveying complex ideas.

Weaknesses:
1. The scale of experiments is limited, with only up to 380M parameters, which may not capture the model's scalability potential.
2. There is a lack of diverse evaluations beyond language modeling, particularly regarding long-range capabilities and comparisons with strong baselines like CoLT5.
3. The paper does not adequately address existing work on SSMs that achieve similar performance, nor does it compare against recent architectures that combine SSMs and attention.
4. The method's assumptions about hardware and efficiency comparisons may limit its practical impact.

### Suggestions for Improvement
We recommend that the authors improve the scalability of their experiments by conducting evaluations with larger model sizes to better understand the model's performance. Additionally, we suggest including more insightful experiments beyond language modeling, such as those on Long Range Arena and other long-context benchmarks. A comprehensive comparison with strong baselines, particularly those that have been recently published, is essential for contextualizing the contributions of the BST. Furthermore, a detailed discussion of the limitations, particularly regarding the computational aspects of S4 models and their implications for performance, would strengthen the paper.