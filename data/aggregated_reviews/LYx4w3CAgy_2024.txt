ID: LYx4w3CAgy
Title: LLM-Check: Investigating Detection of Hallucinations in Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 5, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LLM-Check, a method for detecting hallucinations in large language models (LLMs) through the analysis of internal hidden states, attention maps, and output prediction probabilities from a single response. The authors propose special scores based on these internal states to identify hallucinations in both white-box and black-box settings. The paper introduces two distinct analytical approaches: Eigen-analysis of internal representations and output token uncertainty quantification. The Eigen-analysis reveals consistent patterns in hidden states and model attention during hallucinations compared to grounded responses, while uncertainty quantification assesses the likelihood of predicted tokens. The authors observe that Eigen-based analysis generally outperforms output token uncertainty in detecting hallucinations, although the latter shows effectiveness in synthetic scenarios. The methods demonstrate significant improvements in detection performance while being computationally efficient, validated through experiments on benchmark datasets like FAVA and RAGTruth. The authors also explore combining detection methods but find no significant improvement over the best individual method.

### Strengths and Weaknesses
Strengths:
1. The proposed methods are highly compute-efficient, requiring only a fraction of the runtime compared to other baseline approaches.
2. LLM-Check can detect hallucinations within a single response, which is crucial for applications where generating multiple responses is impractical.
3. The paper covers a wide range of scenarios for hallucination detection, including settings with and without external references and varying levels of access to the model.
4. The paper effectively utilizes diverse scoring methods to enhance hallucination detection without incurring computational overhead.
5. The Eigen-analysis demonstrates a clear advantage in detection performance across various datasets.

Weaknesses:
1. The paper lacks theoretical justification for the use of certain scores, such as the EigenScore method, and does not explain why these scores should work in this context.
2. The theoretical motivation for the EigenScore in the context of token embeddings is insufficiently explained, particularly regarding alignment in non-hallucinatory content.
3. Important comparisons with standard classifiers on hidden representations are missing, and the paper does not adequately assess domain robustness.
4. There is a lack of robust evidence demonstrating score consistency across different text domains.
5. The method is grounded on heuristics without sufficient insight, particularly regarding the assumptions made in black-box settings and the lack of cohesion between different detection approaches.

### Suggestions for Improvement
We recommend that the authors improve the theoretical background by providing explanations for the effectiveness of the EigenScore method in this setup, rather than its original context of self-consistency. Additionally, we suggest including comparisons with standard classifiers on hidden representations to enhance the robustness of the findings. It would also be beneficial to assess the domain robustness of the proposed methods and include results that demonstrate the robustness of scores across different distributions of texts, particularly between Wikipedia articles and Question Answering datasets from FAVA. Furthermore, we encourage the authors to clarify terminologies such as "white-box settings" and "black-box settings" for better understanding. Lastly, consider adding a figure illustrating the overall pipeline to enhance clarity.