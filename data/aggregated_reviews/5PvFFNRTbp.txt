ID: 5PvFFNRTbp
Title: A Frustratingly Easy Post-Training Quantization Scheme for LLMs
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a post-training quantization (PTQ) scheme, referred to as Z-FOLD, aimed at enhancing memory efficiency in Transformer architectures, specifically for large language models (LLMs) like OPT and BLOOM. The authors propose a method that incorporates additional scaling factors into the quantization process, utilizing alternating least square factorization to minimize the distance from original weights. The evaluation is conducted on various model sizes, demonstrating improved quantization performance over existing methods.

### Strengths and Weaknesses
Strengths:
- The proposed Z-FOLD scheme is straightforward and easy to implement.
- The results appear solid and reproducible, showcasing state-of-the-art performance across multiple tasks.
- The methodology introduces a novel approach to memory efficiency through weight folding.

Weaknesses:
- The evaluation lacks comparisons with established baselines, making it difficult to ascertain the contribution of this work.
- Limited novelty is noted, as many baseline results are derived from previous works without sufficient new benchmarks.
- Concerns regarding the impact of the Z-FOLD operation on memory efficiency and performance are raised.

### Suggestions for Improvement
We recommend that the authors improve their evaluation by including comparisons with accepted baselines such as LLM.int8() and SmoothQuant to clarify the value added by their approach. Additionally, it would be beneficial to provide quantified results on inference time, GPU memory footprint, and the GPU hours required for computing the scaling factors across various model sizes. Addressing the questions regarding the accuracy discrepancies in smaller models and the specifics of the hardware and software used for evaluations would enhance the paper's clarity and robustness. Finally, we suggest extending the evaluation to include recent benchmarks like TruthfulQA and MMLU to demonstrate the method's effectiveness comprehensively.