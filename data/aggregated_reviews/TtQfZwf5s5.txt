ID: TtQfZwf5s5
Title: MT2: Towards a Multi-Task Machine Translation Model with Translation-Specific In-Context Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multi-task translation model, MT2, which employs an in-context learning (ICL) paradigm to integrate various translation tasks, including translation memory, terminology-constrained translation, and document-level translation. The model is pre-trained on a large parallel corpus and fine-tuned on a dataset created through retrieval and alignment, enhancing its contextual understanding. Extensive experiments demonstrate that MT2 significantly outperforms both the base model and ChatGPT.

### Strengths and Weaknesses
Strengths:
- The integration of multiple translation tasks into a unified model is beneficial for real-world applications.
- Empirical results show substantial improvements in translation quality, supported by extensive experiments.
- The paper is well-structured, clearly written, and provides a reproducible method for data construction.

Weaknesses:
- The model is bilingual, raising questions about its multilingual capabilities and generalization to other languages.
- The importance of multi-task training is not convincingly demonstrated, and comparisons with models focusing on single tasks are lacking.
- Traditional evaluation metrics like BLEU and ROUGE may not adequately assess translation quality, necessitating a combination of manual evaluations and advanced metrics.

### Suggestions for Improvement
We recommend that the authors improve the clarity and substantiation of the benefits of "knowledge transfer of different tasks" in the abstract. Additionally, please provide explicit details on the alignment method's application in context retrieval to enhance reader understanding. It would also be beneficial to include quantitative results for the second context-enhancement strategy and to utilize a broader range of evaluation metrics beyond BLEU. Finally, addressing minor grammatical and stylistic issues will enhance the overall presentation of the paper.