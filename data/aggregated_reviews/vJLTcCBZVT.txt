ID: vJLTcCBZVT
Title: Improving Subgroup Robustness via Data Selection
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 2, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a data-centric model debiasing technique called Data Debiasing with Datamodels (D3M) aimed at improving worst-group accuracy by identifying and removing harmful training examples. The authors propose an extension, Auto-D3M, which operates effectively without explicit group annotations. The method is validated through experiments on various datasets, demonstrating its superiority over traditional balancing techniques.

### Strengths and Weaknesses
Strengths:
1. The delineation of full-information, partial-information, and no-information regimes is clear, with a comprehensive discussion of D3M and Auto-D3M's advantages, particularly in the no-information regime where the Auto-D3M algorithm is expected to serve as a strong baseline.
2. D3M and Auto-D3M outperform common data balancing techniques by removing fewer data points while achieving better worst-group accuracy.
3. Sections 5.2 and 6 provide a thorough understanding of group alignment scores and the TRAK matrix, aiding practitioners in discovering spurious correlations without group annotations.
4. The explanations of D3M, Auto-D3M, and TRAK are clear, with well-articulated mathematics.

Weaknesses:
1. Table 1's comparison for the no-information regime is limited to ERM, a weak baseline; additional comparisons with methods like MaskTune, uLA, and DivDis would enhance the analysis.
2. Section 6 lacks a reference and comparison to a relevant paper that also employs a data-centric method for bias mitigation.
3. The absence of error bars in Tables 1, 2, 3, and Figure 6 diminishes the scientific rigor; conducting experiments over multiple random seeds and providing standard deviations or confidence intervals is recommended.
4. The writing contains typos and grammatical errors, and some references are outdated.

### Suggestions for Improvement
We recommend that the authors improve the comparison in Table 1 by including additional relevant methods such as MaskTune, uLA, and DivDis. Additionally, a reference to the relevant paper in Section 6 should be included. To enhance scientific rigor, we suggest running experiments over multiple random seeds to provide error bars in Tables 1, 2, 3, and Figure 6. Furthermore, we advise the authors to proofread the manuscript to correct typos and update outdated references. Lastly, we encourage the authors to explore the impact of varying model configurations on the method's effectiveness and to clarify the hyperparameter k's distinction between D3M and TRAK.