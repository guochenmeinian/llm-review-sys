ID: kMnoh7CXrq
Title: DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DenseFormer, a Transformer architecture variant that utilizes a weighted average of outputs from previous layers as inputs for subsequent layers. The weights are learned parameters, allowing for both positive and negative values. The authors demonstrate perplexity improvements in language modeling on OpenWebText2 and analyze the learned weights, revealing interesting patterns. The architecture aims to enhance information flow and reduce computational overhead by selectively applying the weighting module.

### Strengths and Weaknesses
Strengths:
- The method is straightforward and easy to implement, yielding significant perplexity improvements with minimal overhead.
- Extensive experiments validate the architecture's effectiveness across various metrics, including inference throughput and training speed.
- The code is well-organized and accessible, facilitating reproducibility.

Weaknesses:
- The evaluation lacks robustness, particularly regarding zero-shot performance on downstream tasks and the architecture's generalizability to standard model shapes.
- The non-standard model shape raises concerns about the validity of results, as the narrow and deep configuration may not reflect real-world applications.
- The paper could benefit from a more thorough discussion of hyperparameter tuning and its impact on model performance.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including zero-shot performance assessments on downstream tasks such as BLiMP, CBT, and PIQA. Additionally, conducting experiments with more standard model shapes, such as a 768/12 configuration, would clarify the architecture's effectiveness across different setups. We also suggest providing a detailed analysis of hyperparameter tuning processes, particularly regarding learning rates, to enhance the understanding of their impact on model stability and performance. Lastly, addressing the architecture's robustness across various tasks beyond language modeling would strengthen the paper's contributions.