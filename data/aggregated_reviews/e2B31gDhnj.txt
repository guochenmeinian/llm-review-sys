ID: e2B31gDhnj
Title: Domain Private Transformers for Multi-Domain Dialog Systems
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the concept of domain privacy for transformers, a novel property of differential privacy in language models. It proposes a redaction schedule fine-tuning method to achieve domain privacy while maintaining performance. The authors introduce policy functions for token-level domain classification and demonstrate the effectiveness of their approach through experiments on membership inference attacks, showing resilience comparable to existing differentially private language models.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- It defines an important property of private transformers and presents extensive experiments that support its main claims.
- The notion of domain privacy is novel and relevant, particularly for entities managing data from multiple sources.
- The experimental setup using LiRA as a test bed for privacy leakage is commendable.

Weaknesses:
- The definition and method may be challenging to generalize for larger language models or those with extensive pre-trained knowledge.
- The title may be misleading, suggesting a broader scope than what is justified by the work.
- There is a lack of detailed discussion comparing this work with existing literature on privacy in language models, weakening its contribution.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the title to better reflect the scope of the work. Additionally, the authors should enhance the discussion of how their methods relate to existing research on language model privacy. Furthermore, we suggest providing more detailed evaluations of leakage information when domains share common features, as well as addressing the applicability of their methods to larger language models with significant pre-trained knowledge.