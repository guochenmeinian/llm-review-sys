ID: ZmSg4f16uo
Title: Optimistic Meta-Gradients
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 7, 6, 7, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the connection between optimization and meta-learning, specifically focusing on gradient-based meta-learning (GBML) in the single task setting. It establishes equivalences between gradient descent (GD) with momentum and GBML, as well as GD with Nesterov acceleration and the Bootstrapped Meta-Gradient (BMG) algorithm. Theoretical analyses indicate that while GBML accelerates convergence by a constant factor, it does not improve the $O(1/T)$ rate, whereas BMG achieves an improved rate of $O(1/T^2)$. Experiments validate these theoretical findings through applications in quadratic minimization and ImageNet classification.

### Strengths and Weaknesses
Strengths:
1. The mathematical arguments are clearly presented, with proof techniques well outlined.
2. The paper is sound, with experiments conducted on both simple and complex problems, including ImageNet.
3. The connection between BMG and GD with Nesterov acceleration is a novel contribution.
4. The framework provides a useful tool for analyzing meta-learning algorithms and comparing them with convex optimization approaches.

Weaknesses:
1. The focus on a single task setting limits the paper's impact, as meta-learning often aims to leverage information across multiple tasks.
2. The assumption of a convex objective function is quite strong.
3. The analysis is restricted to convex cases, lacking empirical investigation into non-convex scenarios.
4. There is no code provided for reproducibility, and limitations of the work are not adequately discussed.

### Suggestions for Improvement
We recommend that the authors improve the paper by considering the theory in a multi-task setting to enhance its relevance and applicability. Additionally, including more experimental results on large-scale problems beyond ImageNet would strengthen the paper's impact. We suggest addressing the limitations of the work more thoroughly and providing code for reproducibility. Finally, clarifying the experimental setup, including hyperparameter tuning and learning rate scheduling, would enhance the transparency of the results.