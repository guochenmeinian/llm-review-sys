ID: XmN7ZNbUAe
Title: Distributed Inference and Fine-tuning of Large Language Models Over The Internet
Conference: NeurIPS
Year: 2023
Number of Reviews: 5
Original Ratings: 5, 7, 6, 6, -1
Original Confidences: 4, 4, 5, 3, -1

Aggregated Review:
### Key Points
This paper presents cost-efficient inference and fine-tuning methods for large language models (LLMs) on geodistributed devices within consumer-grade networks. The authors propose an algorithm that addresses two main challenges: ensuring reliable computing despite abrupt device disconnections and effectively partitioning LLMs across devices with varying hardware capabilities. Their simulations and real-world experiments demonstrate that the proposed method outperforms existing approaches for running inference on consumer-grade hardware.

### Strengths and Weaknesses
Strengths:
- The motivation for utilizing idle compute resources for LLM inference and fine-tuning is both realistic and socially beneficial.
- The paper presents a clear and practical idea, applying multiple optimizations such as quantization of weights and activations, which, while not new, are effectively integrated.
- The experiments, particularly the real-world setup, are reasonable and well-executed.

Weaknesses:
1. The fine-tuning process under the proposed setting lacks clarity, particularly regarding recovery from failures during training.
2. The communication costs associated with tensor parallelism are inadequately addressed, as the assumption that each server/client can hold a pipeline stage may not reflect realistic scenarios.
3. The relevance of the performance of Cache-less in relation to the failure rate in Table 1 is unclear.
4. There is insufficient discussion on server utilization across different GPU servers in a heterogeneous computing environment.
5. The paper does not explore hardware requirements for various LLMs, such as TPUs or high-end CPUs.
6. The innovative aspects of the paper are not sufficiently elaborated, particularly in comparison to existing solutions like DeepSpeed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the fine-tuning process and provide a detailed description of how to recover from failures during training. Additionally, addressing the communication costs of tensor parallelism in more realistic scenarios would enhance the paper's robustness. It would also be beneficial to include a discussion on server utilization across different GPU servers and explore hardware requirements for various LLMs. Furthermore, we suggest that the authors clarify the innovative aspects of their work in comparison to existing frameworks like DeepSpeed and provide a more comprehensive analysis of the experimental results related to fine-tuning. Lastly, citing Algorithms 2/3 more specifically in the text would improve clarity.