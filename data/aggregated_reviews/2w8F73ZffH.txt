ID: 2w8F73ZffH
Title: Is Contrastive Learning Necessary? A Study of Data Augmentation vs Contrastive Learning in Sequential Recommendation
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive empirical study on the role of data augmentation versus contrastive learning (CL) in sequential recommendation systems (SRS). The authors investigate whether CL is necessary for improving SRS performance, concluding that data augmentation alone can achieve comparable or superior results. The study compares eight different augmentation strategies and three popular CL baselines, highlighting the potential of data augmentation to address data sparsity issues in SRS.

### Strengths and Weaknesses
Strengths:
1. The authors systematically study various data augmentation techniques and their contributions to SRS, demonstrating that certain strategies can significantly enhance performance.
2. The paper is well-organized, and the experiments are carefully designed to address meaningful research questions.
3. The authors aim to inspire further fundamental studies in contrastive learning rather than focusing solely on complex methods.

Weaknesses:
1. The results for CL baselines differ significantly from those reported in original papers, raising questions about their validity.
2. The experimental section lacks complexity analysis, and the authors do not clarify how different augmentation strategies interact with CL methods.
3. The paper does not clearly state its contributions to the web community, which is essential for the conference context.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their experimental results, particularly by justifying discrepancies in the performance of baseline CL methods. Additionally, analyzing performance across different user groups instead of item groups could provide more meaningful insights. The authors should also include detailed parameter tuning and settings for the compared baselines in the evaluation section to enhance reproducibility. Furthermore, conducting a more comprehensive comparison of computational and memory costs between the proposed method and baselines would be beneficial, including metrics such as training and inference times. Finally, the authors should clarify the contributions of their work to the web community and consider experimenting with denser datasets for a more robust analysis.