ID: Pm0UzCehgB
Title: SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 6, 8, 6, -1, -1
Original Confidences: 4, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a question-to-query dataset for multi-model electronic health record (EHR) databases, encompassing relational, document, and graph databases, with a total of 40,000 question-query pairs. The authors evaluate closed and open-sourced large language models (LLMs) on this dataset, revealing that most LLMs achieve low accuracy, indicating substantial room for improvement. The dataset is notable for its inclusion of multiple languages and data augmentation methods, which are particularly beneficial in the healthcare domain.

### Strengths and Weaknesses
Strengths:
- The dataset is large-scale and designed to inspire further research in healthcare applications, demonstrating significant social impact.
- It employs diverse database models and query languages, providing a valuable resource for the database and AI community.
- The in-depth analysis of LLM performance across various question types offers valuable insights for future research.

Weaknesses:
- The total number of synthetic patient samples is not clearly stated, which is crucial for understanding the complexity of patient search tasks.
- The queries lack multi-hop complexity and real-world applicability, appearing overly simplistic compared to existing datasets.
- There is insufficient evidence justifying the necessity of multiple data models for EHRs, as most existing research focuses on relational databases.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing an overview or teaser in the introduction to help readers with diverse backgrounds understand the paper's significance. Additionally, consider introducing more background knowledge about electronic healthcare to clarify the distinction between "healthcare" and "clinic." 

We suggest that the authors discuss how privacy is ensured with the augmented data, particularly since it is used for LLM evaluation. Identifying which language poses the greatest challenge for LLMs and providing observations or hypotheses regarding this would also enhance the paper. 

Furthermore, we encourage the authors to clarify the data splitting strategy used in their experimental setup and to provide reference points, such as simple baselines or fine-tuned models, to gauge the benchmark's difficulty. Lastly, addressing the complexity of the semantic space in comparison to existing datasets like MIMICSQL and EHRSQL would strengthen the paper's contributions.