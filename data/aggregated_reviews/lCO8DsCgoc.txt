ID: lCO8DsCgoc
Title: ZipZap: Efficient Training of Language Models for Ethereum Fraud Detection
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "ZipZap," a novel framework designed to enhance the efficiency of training language models (LMs) for detecting fraud in Ethereum networks. The authors propose a frequency-aware compression technique to optimize the embedding layer, reducing model parameters and training time. The evaluation demonstrates that ZipZap achieves a superior efficiency-effectiveness trade-off compared to existing methods, although it primarily focuses on BERT-based models and Ethereum data.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated and proposes an effective training framework for LMs on Ethereum data.
- It is well-structured, easy to follow, and shows notable improvements in efficiency over existing studies.

Weaknesses:
- The generalizability of the proposed method to other domains or tasks is unclear, as it lacks discussion on extending the compression technique beyond Ethereum fraud detection.
- The motivation for optimizing training time over improving model efficacy is not convincingly articulated, and the evaluation is limited to a singular type of attack.

### Suggestions for Improvement
We recommend that the authors improve the motivation for their efficiency optimization, clarifying the benefits of faster pre-training beyond assisting researchers. Additionally, please provide more details on the partition-wise matrix V_x and the rationale for setting K=8 in frequency-aware compression. Consider discussing the potential for decomposing addresses into sub-word components and conducting an ablation study on transaction dropping rates. Finally, we suggest addressing the generalizability of ZipZap's methods to other types of attacks and applications outside of Ethereum fraud detection.