ID: TW99HrZCJU
Title: Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates offline reinforcement learning (RL) in the context of imbalanced datasets, proposing a method called density-ratio weighting (DW) that optimizes a parameterized density-ratio model for each transition. The authors demonstrate that DW effectively anchors a policy close to high-reward trajectories, outperforming state-of-the-art methods across 72 imbalanced datasets with varying characteristics. The method integrates well with existing algorithms and shows promise in enhancing performance on challenging datasets.

### Strengths and Weaknesses
Strengths:
1. The writing quality is clear and accessible.
2. The approach is well-motivated and builds on existing techniques.
3. The paper provides sufficient technical details for reproducibility and extensive benchmarking across numerous datasets.

Weaknesses:
1. The novelty is limited, as the method does not sufficiently differentiate itself from existing reweighting or filtering techniques in offline RL. The authors fail to adequately connect DW with similar prior work and justify its advantages over advantage weighting (AW).
2. There is a lack of detail regarding the dataset setup and insufficient comparisons with other methods, particularly in noisy dataset scenarios.
3. The theoretical analysis is weak, and the paper does not adequately address the role of the discount factor in offline RL or the necessity of penalties like Bellman flow and KL-divergence.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by providing a clearer distinction between DW and existing methods, particularly in relation to AW. Additionally, the authors should include more detailed descriptions of the dataset setup and expand comparisons with other relevant methods, especially in the context of noisy datasets. We suggest conducting a theoretical analysis of the discount factor's role in offline RL and providing further experiments to demonstrate the necessity of penalties. Finally, addressing the concerns regarding the stability of importance weights and their evolution over time would enhance the robustness of the findings.