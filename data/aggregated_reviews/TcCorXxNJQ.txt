ID: TcCorXxNJQ
Title: FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel aggregation algorithm, FLoRA, for low-rank federated fine-tuning, addressing mathematical inaccuracies in prior methods by stacking LoRA matrices instead of averaging them. The authors provide a convergence analysis and experiments demonstrating FLoRA's efficiency and privacy preservation in both homogeneous and heterogeneous environments.

### Strengths and Weaknesses
Strengths:  
1. The paper identifies and addresses a significant issue in LoRA federated learning from a unique perspective, with straightforward and convincing mathematical analysis that can inform related works.  
2. It is well-written and easy to follow.  
3. The experiments are comprehensive and validate the proposed method's effectiveness.  

Weaknesses:  
1. The paper overlooks some related works, such as [1], which also discusses LoRA in federated fine-tuning and should be included.  
2. Figure 4's results indicate some single clients outperforming the server, requiring further explanation. Additionally, the subtitle 'The Impact of Heterogeneous...' does not align with the title 'Standalone...' of Figure 4, necessitating refinement.  
3. The choice of using Llama over state-of-the-art models like Llama-2 or Llama-3 needs justification.  
4. Writing inconsistencies exist, with repeated sentences in lines 64 and 70.  
5. The discussion on accelerating convergence lacks indexing in the main text and experimental validation.  
6. More comparison is needed regarding FLoRA's advantages over the 'aggregation of local update' in Figure 2.  
7. The foundation models used for experiments are limited to the Llama series; exploring other models could enhance validation.

### Suggestions for Improvement
We recommend that the authors improve the literature review by incorporating related works, particularly [1]. Additionally, please clarify the results in Figure 4 and ensure consistency in figure titles and subtitles. Justifying the choice of Llama over other models is essential. We also suggest addressing the writing issues by eliminating repeated sentences and ensuring all discussions are properly indexed and validated. Further elaboration on FLoRA's advantages compared to other aggregation methods is necessary, and we encourage the exploration of diverse foundation models for validation. Lastly, including an analysis of computational and communication costs in the revised paper would strengthen the manuscript.