ID: TZtw5YgxTE
Title: MIM4DD: Mutual Information Maximization for Dataset Distillation
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 6, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method named MIM4DD that maximizes the mutual information between synthetic and real images during dataset distillation. The authors derive a lower bound of mutual information and formulate it as a learning objective, demonstrating significant performance improvements when integrated with existing dataset distillation methods. The paper emphasizes the importance of information theory in this context and provides a rigorous mathematical foundation for the proposed approach. Additionally, the authors acknowledge reviewer feedback and express plans to include supplemented results in the final version, indicating their commitment to enhancing the work.

### Strengths and Weaknesses
Strengths:
1. The introduction of mutual information into dataset distillation is a natural and under-explored idea, appealing to researchers in the field.
2. The paper is well-written and technically sound, with necessary analyses on the learned synthetic data.
3. The proposed loss function leads to remarkable performance improvements when combined with state-of-the-art distillation methods.
4. The authors demonstrate a willingness to improve their work based on reviewer feedback, showing appreciation for the reviewer's expertise.

Weaknesses:
1. The experimental results are incomplete, lacking independent training results with the MIM4DD loss and an ablation study on larger beta values.
2. The theoretical justification for why the method works is insufficient, particularly regarding the relationship between mutual information and classification information learned from synthetic images.
3. The method has not been tested on more popular dataset distillation frameworks, and the derivation of the mutual information loss may have foundational issues.
4. The initial rating of the paper was low (3), indicating that there may still be significant areas needing improvement.

### Suggestions for Improvement
We recommend that the authors improve the completeness of their experimental results by including independent training outcomes with the MIM4DD loss and conducting a more thorough ablation study on larger beta values. Additionally, the authors should clarify the theoretical basis for their method's effectiveness and explore its application within more widely used dataset distillation frameworks. We also suggest that the authors address the identified writing errors and ensure consistency in definitions throughout the paper to enhance clarity. Finally, improving the clarity and depth of their findings in the final version will help ensure that the supplemented results effectively address the reviewers' concerns.