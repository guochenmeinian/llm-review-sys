ID: 0uXtFk5KNJ
Title: BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 8, 5, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents BAdam, a novel optimization method designed for memory-efficient full parameter finetuning of large language models (LLMs) and a new approach to pretraining using the BAdam optimizer in a continual pretraining context. BAdam employs a block coordinate descent (BCD) framework with Adam as the inner solver, allowing for the partitioning of model parameters into blocks and updating them sequentially. The authors provide theoretical convergence analysis and experimental results that demonstrate BAdam's effectiveness in terms of memory efficiency and performance compared to existing methods like LoRA and LOMO. They also plan to open-source their implementation code, facilitating reproducibility and supporting distributed training.

### Strengths and Weaknesses
Strengths:
1. The paper addresses the significant challenge of enabling full parameter finetuning of LLMs under memory constraints, offering an original solution by integrating BCD with Adam.
2. The theoretical convergence analysis, although limited to the deterministic case, is sound and adds credibility to the proposed method.
3. Comprehensive experiments convincingly showcase BAdam's advantages in memory efficiency, running time, convergence behavior, and downstream performance across various models and datasets.
4. The planned open-source implementation will facilitate reproducibility and ease of use, with support for distributed training enhancing the capability to finetune large-scale models efficiently.
5. The commitment to memory efficiency is notable, with specific examples provided for model training.

Weaknesses:
1. The theoretical analysis is restricted to the deterministic case; extending convergence results to the stochastic setting would enhance the paper's robustness.
2. The focus on finetuning limits exploration of BAdam's applicability during the pretraining phase, which could be discussed conceptually to broaden the work's impact.
3. The evaluation of larger models (7B and 8B) lacks sufficient quantitative results, relying heavily on MT-bench, which may not be objective.
4. The implementation uses layer-wise updates instead of block-wise updates, which should be clarified or explored further.
5. The paper's novelty is questioned due to similarities with existing work, particularly regarding the motivation for using BCD in LLM finetuning.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis by extending convergence results to the stochastic setting. Additionally, we suggest including a conceptual discussion on the feasibility and potential benefits of applying BAdam during the pretraining phase. To enhance the evaluation, we advise providing more quantitative results for the 7B and 8B models, possibly incorporating benchmarks like GSM8K and MMLU. Clarifying the implementation details regarding block-wise versus layer-wise updates is essential. Lastly, addressing the similarities with existing work will help differentiate BAdam's contributions more clearly. Furthermore, we recommend improving the clarity of the continual pretraining experiment results to further validate the efficiency of BAdam and providing more detailed documentation on the integration process and distributed training setup to enhance user experience and accessibility.