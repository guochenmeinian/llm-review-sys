ID: 6ldTxwhgtP
Title: Towards A Richer 2D Understanding of Hands at Scale
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a large-scale hand-object interaction dataset named Hands23, comprising 257K images, 401K hands, 288K objects, and 19K second objects, sourced from various datasets and novel videos. It features fine-grained annotations, including detailed hand-grasp labels and object segmentation, enhancing the study of hand-object interactions in both ego and exo views. The authors demonstrate that a baseline model (Mask-RCNN) performs well on Hands23 and generalizes effectively to other datasets, providing a valuable resource for the community in computer vision and robotics. Additionally, the paper introduces a detection framework that explicitly models interactions between hands and objects through an association component, allowing for a nuanced understanding of object relationships. The authors argue that their backbone, a 101-layer deep ResNeXt-101, is not merely a feature extractor but a critical component that processes data extensively before reaching the detection heads. They emphasize that the backbone's training, which includes 400K iterations and exposure to over 6 million training images, enables it to recognize complex object interactions effectively. The authors assert that the primary technical contribution lies in problem formulation, dataset creation, and analysis rather than in algorithmic advancements.

### Strengths and Weaknesses
Strengths:
- The dataset is extensive and covers a wide range of hand-object interactions in both first and third-person views.
- Rich annotations include hand bounding boxes, segmentation, contact types, and grasp classifications, offering a comprehensive resource.
- Ethical considerations, such as privacy and demographic analysis, are well addressed.
- The baseline model effectively predicts interaction information and shows good generalization to the Ego4D dataset.
- The framework's explicit modeling of interactions enhances the understanding of object relationships.
- The backbone's depth and extensive training contribute significantly to its performance in detection tasks.
- The focus on problem formulation and dataset creation as key contributions adds value to the research.
- The paper is comprehensive with excellent supplementary materials.

Weaknesses:
- The motivation for introducing the "second object" is insufficiently discussed, limiting understanding of its applications.
- The zero-shot generalization to the Ego4D dataset, which only includes first-person views, lacks challenge; demonstrating generalization to third-person data would be beneficial.
- The paper could benefit from even richer annotations.
- The decision to annotate only non-crowd instances with a minimum area could be reconsidered to include crowded areas.
- The advantages of the current annotation type compared to previous works could be further elaborated.
- The reliance on the backbone's complexity may limit generalizability to simpler models.
- The distinction between explicit and implicit modeling could be further clarified for readers.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the motivation and applications of the "second object" in the dataset. Additionally, we suggest demonstrating zero-shot generalization to third-person data to enhance the robustness of the findings. The authors should consider enriching the annotations further and explore the feasibility of annotating crowded areas. Furthermore, we encourage a more detailed comparison of the current annotation types with those from prior works to clarify their advantages. Lastly, we recommend improving the clarity regarding the distinction between explicit and implicit modeling to enhance reader understanding, and providing more detailed comparisons with simpler models to illustrate the advantages of their approach.