ID: x9eFgahVBI
Title: From Unstructured Data to In-Context Learning: Exploring What Tasks Can Be Learned and When
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents three theoretical analyses related to in-context learning (ICL). Section 2 demonstrates the use of CBOW for (country)-(capital) ICL tasks. Section 3 highlights the significance of positional embeddings, multiple layers in autoregressive language models, and blocked noise structures for ICL. Section 4 discusses the potential failure of ICL due to systematic mismatches between training and testing sequences. The paper investigates ICL emergence in both CBOW and Transformer models, identifying co-occurrence as essential for ICL in CBOW and emphasizing the necessity of positional information for Transformers.

### Strengths and Weaknesses
Strengths:
- The paper is clear and easy to follow, with strong theoretical conclusions supported by empirical simulations.
- It enhances understanding of how training data structure influences ICL capabilities, providing a mix of theoretical proofs and empirical validations.

Weaknesses:
- The scope appears mismatched with the title and abstract, which suggest a broader investigation of ICL in unstructured data, while the paper focuses on simplified synthetic scenarios.
- The experimental design lacks detail, such as the number of training sentences and the methodology for generating training and evaluation data, raising concerns about the applicability of findings to real-world settings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding ICL by providing a more careful definition of the term, especially in relation to generation tasks. Additionally, we suggest that the authors revise the main storyline to reduce overselling in the title and abstract. It would also be beneficial to include more experimental details in the main paper and to clarify the disparity between the synthetic settings studied and real-world ICL scenarios. Finally, we encourage the authors to address the concerns regarding the relevance of training examples to test instances in their experimental design.