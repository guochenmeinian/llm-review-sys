ID: z13UqeDT39
Title: Disentangled Knowledge Tracing for Alleviating Cognitive Bias
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Disentangled Knowledge Tracing (DisKT), a novel approach to address cognitive bias in knowledge tracing (KT) models used in Intelligent Tutoring Systems (ITS) by employing causal analysis. The authors identify the confounding effect of students' historical accuracy rate distribution as a primary contributor to cognitive bias, which leads to cognitive underload for high-performing students and overload for low-performing ones. DisKT effectively separates the modeling of familiar and unfamiliar knowledge and incorporates a contradiction attention mechanism to mitigate biases from guessing and mistakes. Extensive experiments on 11 baseline and 3 synthetic datasets demonstrate that DisKT significantly outperforms 14 baseline models in assessment accuracy while reducing cognitive bias. However, the paper has issues related to logical coherence, clarity, and the sufficiency of experiments.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and free of grammatical errors, facilitating comprehension.
- It provides a thorough analysis of cognitive bias in KT models and presents a unique causal perspective.
- The DisKT model effectively addresses cognitive bias and enhances interpretability through innovative mechanisms.
- The experimental results are comprehensive, covering multiple datasets and comparisons with various recent models.

Weaknesses:
- Key concepts, such as "underperformers" and "overperformers," are vaguely defined, and the causes of cognitive bias require further clarification.
- The validation primarily relies on a single dataset, ednet, which limits the generalizability of results; additional datasets or complementary metrics like $E_{KL}$ are recommended.
- The rationale behind the new metrics for guessing and mistaking rates lacks clarity, raising questions about their calculation.
- The integration of the IRT variant's contribution to interpretability is not sufficiently detailed.
- The paper struggles with logical coherence and fails to establish intrinsic connections among cognitive bias, contradictory psychology, and interpretability.
- The methods section contains too many separate symbols, making it difficult to understand.
- The correspondence between the methods and the motivations presented in the introduction needs clarification.
- The reliance on historical performance may overlook students' current learning progress, limiting the model's adaptability.
- The connection of this work to the theme of the Web is unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of key definitions, particularly for "underperformers" and "overperformers," and provide a detailed explanation of the causes of cognitive bias. Additionally, we suggest conducting experiments on a broader range of datasets or incorporating the $E_{KL}$ metric to better demonstrate DisKT's effectiveness. The authors should clarify the rationale behind the guessing and mistaking rates, ensuring that calculations accurately reflect student mastery. We also advise elaborating on how the IRT variant enhances interpretability, potentially through visual analysis or concrete examples. Furthermore, we recommend improving the clarity of the methods section by reducing the number of separate symbols used and clarifying the relationship between the methods and the motivations outlined in the introduction. It would be beneficial to address how the model accounts for current learning tasks and short-term memory to enhance adaptability. Lastly, the authors should provide real-world examples to illustrate the target problem and the application of the proposed method, and discuss its relevance to the Web more explicitly.