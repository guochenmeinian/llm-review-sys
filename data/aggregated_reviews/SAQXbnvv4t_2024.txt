ID: SAQXbnvv4t
Title: AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AlchemistCoder, a series of code language models fine-tuned on multi-source data to enhance code generation and generalization capabilities. The authors propose using "AlchemistPrompts" to harmonize conflicts in multi-source code corpora and incorporate code comprehension tasks into the training process. Their method shows improved performance on various benchmarks compared to baseline models of similar size, demonstrating the efficacy of multi-source data integration and instruction evolution techniques.

### Strengths and Weaknesses
Strengths:
1. The paper identifies a significant challenge in fine-tuning code language models using single-source data and effectively addresses it with multi-source data.
2. Empirical results indicate substantial improvements over baseline models, with detailed ablation studies validating the proposed methods.
3. The authors provide a wide range of evaluations, showcasing strong results across both code generation tasks and standard benchmarks.

Weaknesses:
1. The clarity of the writing is lacking, particularly in motivating the introduction of the method and explaining dataset improvements over prior work.
2. The selection criteria for samples improved using AlchemistPrompts are unclear, raising questions about the randomness versus targeted selection of the 5% of data modified.
3. The core concept of AlchemistPrompts lacks substantial novelty, appearing similar to existing instruction evolution techniques, and the reliance on high-quality data from GPT-4 may skew performance comparisons.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in motivating the introduction of their method and providing concrete examples of low code quality and diversity observed in prior work. Additionally, the authors should clarify the selection criteria for the 5% of data modified by AlchemistPrompts, specifying whether it is random or targeted. Furthermore, we suggest that the authors conduct additional experiments to explore the impact of multi-source data on model performance across different programming languages, as current evaluations are limited to Python. Lastly, a more in-depth discussion on the contributions of data diversity and harmonization to performance improvements would strengthen the paper's claims.