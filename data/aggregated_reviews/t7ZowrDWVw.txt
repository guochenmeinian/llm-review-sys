ID: t7ZowrDWVw
Title: Achieving Cross Modal Generalization with Multimodal Unified Representation
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 5, 5, 6, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel task called Cross Modal Generalization (CMG), aiming to learn a unified discrete representation from paired multimodal data during pre-training, enabling zero-shot generalization in downstream tasks with only one modality labeled. The authors propose the Dual Cross-modal Information Disentangling (DCID) module and Multi-Modal Exponential Moving Average (MM-EMA) to facilitate bidirectional supervision and align semantically equivalent information in a shared discrete latent space. The effectiveness of the proposed methods is validated through extensive experiments across various tasks.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a meaningful technical direction with a reasonable motivation.
- The proposed methods, DCID and MM-EMA, are intuitive and technically sound, demonstrating solid experimental results.
- Thorough ablation studies highlight the contributions of each component, and the paper addresses major challenges in learning unified multimodal representations.

Weaknesses:
- The organization of the content is lacking, particularly in the introduction, which could benefit from more promising proposals.
- The visualization in Figure 4 is inadequate, presenting very few instances; a more comprehensive comparison is recommended.
- The evaluation of the model's ability for zero-shot learning on seen modalities is missing, and the authors should consider adding significant experiments, such as zero-shot classification on ImageNet and AudioSet.
- Comparisons with recent literature on Masked Autoencoders and CLIP-style approaches are insufficient, limiting the contextual understanding of the proposed methods.

### Suggestions for Improvement
We recommend that the authors improve the organization of the manuscript, particularly by enhancing the introduction with more promising proposals. Additionally, we suggest presenting a more comprehensive visualization in Figure 4. To strengthen the evaluation, we encourage the authors to include experiments on zero-shot classification for ImageNet and AudioSet. Furthermore, we advise incorporating comparisons with recent works in unified representation learning, such as Contrastive Audio-Visual Masked Autoencoder and ImageBind, to provide a broader context for their contributions.