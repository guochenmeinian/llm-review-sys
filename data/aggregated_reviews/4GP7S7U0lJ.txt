ID: 4GP7S7U0lJ
Title: Learnability Matters: Active Learning for Video Captioning
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an active learning algorithm for video captioning that addresses data learnability and collective outliers. The algorithm incorporates learnability, diversity, and uncertainty, leveraging off-the-shelf models like LVLM for approximating ground truths. The authors conduct extensive experiments on MSVD and MSRVTT datasets, comparing against various baselines, and report promising results. However, the paper's applicability is limited by the lack of cross-dataset experiments and a thorough limitation analysis.

### Strengths and Weaknesses
Strengths:
- The proposed acquisition function comprehensively captures learnability, diversity, and uncertainty.
- The paper is the first to explore collective outliers in video captioning, providing a unique perspective.
- The introduction of a caption-wise active learning protocol effectively integrates human knowledge, enhancing model performance.
- Experiments demonstrate clear improvements over multiple active learning baselines.

Weaknesses:
- The reliance on LVLM raises questions about the necessity of active learning if LVLM is sufficiently powerful or the reliability of its outputs if not.
- The computational costs associated with the extensive use of LVLM and scene graphs are significant, especially when cheaper human annotation tools exist.
- The paper lacks cross-dataset experiments, limiting its real-world applicability.
- The reported performance metrics are relative, obscuring the absolute performance of the proposed model, which is significantly below state-of-the-art (SOTA) benchmarks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their performance claims by providing absolute metrics alongside relative performance comparisons. Additionally, consider conducting cross-dataset experiments to enhance the generalizability of the findings. Addressing the computational cost concerns by justifying the trade-off between human annotation and computational expenses would strengthen the paper. Lastly, we suggest revising the writing for clarity, particularly in figures, to ensure visibility and consistency in presentation.