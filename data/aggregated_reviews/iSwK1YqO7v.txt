ID: iSwK1YqO7v
Title: Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 8, 8, 7, 9, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive evaluation framework, the Embodied Agent Interface, for benchmarking Large Language Models (LLMs) in embodied decision-making tasks. The authors formalize four critical ability modules—Goal Interpretation, Subgoal Decomposition, Action Sequencing, and Transition Modeling—and employ Linear Temporal Logic (LTL) to standardize goal specifications. The evaluation includes 15 LLMs across two simulators, providing insights into LLM capabilities and limitations in embodied AI systems.

### Strengths and Weaknesses
Strengths:
- The paper provides a significant contribution by evaluating LLMs' capabilities in embodied control tasks, filling a current gap in the literature.
- The analysis of LLM modules and error types is thorough and valuable, with high reproducibility due to detailed implementation information.
- The use of LTL for goal specification and the introduction of fine-grained evaluation metrics enhance the understanding of LLM performance.

Weaknesses:
- The discussion on Visual Language Models (VLMs) is limited, which could be addressed in future work.
- Key conclusions and discussions on LLM challenges should be more prominently featured in the main text.
- Some technical terms are introduced without prior explanation, potentially confusing readers.

### Suggestions for Improvement
We recommend that the authors improve the discussion and comparison of VLMs, as noted in the limitations section, and consider integrating this into future work. Key conclusions based on LLM evaluations and discussions on challenges should be highlighted in the main text. Additionally, clarify the terms "trajectory feasibility error" and "trajectory evaluation performance" upon their first mention to enhance readability. The example of hallucination error in Figure 3 should use more precise terminology to avoid ambiguity. Lastly, address the duplicate statements found in Appendix G.3 regarding the BFS searching algorithm.