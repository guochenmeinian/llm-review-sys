ID: pIXTMrBe7f
Title: What Makes Good Examples for Visual In-Context Learning?
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 6, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the in-context learning capabilities of large vision models, revealing that performance is highly sensitive to the selection of visual examples. The authors propose a retrieval framework for selecting prompts, utilizing both supervised and unsupervised methods, which significantly enhances performance over random selection across tasks such as foreground segmentation, single object detection, and image colorization.

### Strengths and Weaknesses
Strengths:
- The study addresses a novel area of research in visual in-context learning, which is of significant interest to the computer vision community.
- The proposed retrieval methods consistently outperform random selection across all evaluated tasks.
- The findings on distribution shift suggest that the supervised retrieval method effectively acquires robust in-context knowledge.
- The analysis and ablation studies provide valuable insights into the factors influencing visual in-context learning.

Weaknesses:
- The observation that semantically close examples yield better performance is not surprising, as it is already established in language models.
- The technical complexity of the proposed methods is relatively low compared to typical NeurIPS submissions.
- The analysis is limited to a single model, raising questions about the generalizability of the findings to other models and tasks.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by exploring a wider range of visual tasks beyond those currently studied, such as classification and standard segmentation tasks. Additionally, we suggest addressing the assumption of having a set of tagged examples for retrieval; incorporating an ensemble or supervised-trained baseline could strengthen the argument for retrieval over direct use of tagged examples. Furthermore, we encourage the authors to enhance the similarity function by exploring feature space similarities, potentially utilizing existing models like CLIP or DINO. Lastly, we recommend investigating the upper limits of in-context examples to determine if there are diminishing returns or negative impacts on performance with excessive examples.