ID: U78nBY8hRi
Title: DALE: Generative Data Augmentation for Low-Resource Legal NLP
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DALE, a generative data augmentation method tailored for low-resource legal NLP settings. DALE utilizes an encoder-decoder language model (BART) pre-trained on a large unlabeled legal corpus with a selective masking technique that focuses on co-occurring and highly correlated text spans. This approach preserves critical information about emerging entities and facts. The empirical evaluation across 13 datasets and six tasks demonstrates that models trained with DALE outperform baseline systems, yielding more diverse and coherent examples. The contributions include a novel augmentation method, strong empirical results, and qualitative evaluations.

### Strengths and Weaknesses
Strengths:
- Addresses the important issue of generating synthetic data for low-resource legal NLP scenarios.
- Well-motivated and clearly described approach, with extensive empirical results confirming its utility.
- Provides ablation experiments and qualitative examples that enhance reproducibility.

Weaknesses:
- Clarity issues in the methodology, particularly in the fine-tuning step, where the extraction of correlated spans is not clearly articulated.
- Some datasets used for experiments do not represent true low-resource legal NLP tasks, impacting the validity of conclusions.
- Presentation of experimental results is confusing, with some important details relegated to appendices.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology section, particularly in Section 3.2, by explicitly detailing the fine-tuning process and the rationale behind the selection of datasets. Additionally, we suggest conducting an empirical comparison of different template creation approaches during the pre-training stage. The authors should also consider including more experimental details in the main text rather than the appendix, and clarify the significance of the parameter R ("augmentation rounds"). Furthermore, we encourage the authors to explore the impact of training size on performance and to conduct tests for statistical significance, variance, and manual inspection of the augmented data.