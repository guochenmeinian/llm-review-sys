ID: ae6MmBuX6k
Title: MCC-KD: Multi-CoT Consistent Knowledge Distillation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Multi-CoT Consistent Knowledge Distillation (MCC-KD), which aims to enhance the reasoning capabilities of smaller models by addressing the limitations of previous chain-of-thought distillation methods. The authors propose a strategy that generates multiple diverse rationales for each question while ensuring consistency in answer predictions through similarity-based filtering and KL divergence. The method is validated through experiments on arithmetic reasoning and commonsense reasoning tasks, demonstrating improved performance over traditional knowledge distillation approaches.

### Strengths and Weaknesses
Strengths:
- The proposed MCC-KD method effectively enhances reasoning abilities in smaller models.
- The incorporation of diverse rationales and consistency enforcement through KL divergence is a sound approach.
- Empirical results indicate superior performance in mathematical and commonsense reasoning tasks, suggesting practical applicability.

Weaknesses:
- The organization of experimental results is confusing, with some results appearing unrelated to the proposed method's benefits.
- There is a lack of comparative analysis with other filtering methods, which could mislead practitioners regarding the effectiveness of rationale diversification.
- Some baselines are missing, and the rationale verification process is insufficiently thorough.

### Suggestions for Improvement
We recommend that the authors improve the organization and clarity of the experimental results to better highlight the advantages of MCC-KD. Additionally, a more profound analysis comparing the proposed method against other filtering techniques, such as the one in [1], would strengthen the paper. It would also be beneficial to include comparisons with self-consistency methods and to address the rationale verification process more rigorously, particularly for non-multiple-choice tasks. Lastly, consider discussing the choice of model sizes for distillation to inform readers about the performance limits of smaller models.