ID: qcQhBli5Ho
Title: Multi-Head Adapter Routing for Cross-Task Generalization
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 7, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new parameter-efficient few-shot fine-tuning method for pretrained language models, specifically introducing Multi-head Routing (MHR) to enhance cross-task generalization. The authors propose fine-tuning the routing function while freezing multi-head adapters, significantly reducing the number of updated parameters while maintaining comparable accuracy in downstream tasks. They also explore the training dynamics of MHR, noting improved gradient alignment that mitigates negative transfer during multi-task training.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important issue in efficient fine-tuning, achieving a better trade-off between updated parameters and accuracy.
- It is well-written, particularly the experimental section, which includes a comprehensive ablation study.
- The exploration of routing dynamics and the performance of MHR across various tasks is insightful and poses interesting questions for future research.

Weaknesses:
- The novelty of the work is limited, primarily differing from Poly by fine-tuning routing alone without adapters. More quantitative results are needed to clarify the necessity of routing fine-tuning.
- Some methodological aspects are unclear, such as the definitions of certain parameters in the discussion of (IA)^3.
- The gains over Poly are marginal, and the lack of statistical significance undermines the findings. Additionally, the adaptersoup baseline may not provide a fair comparison due to differing adaptation methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their methodology by providing explicit definitions for parameters discussed in the paper. Additionally, we suggest including more quantitative results to substantiate the importance of fine-tuning routing. Addressing the statistical significance of the performance gains over Poly would strengthen the claims made. Finally, consider discussing the limitations of the approach more explicitly, including the implications of routing during few-shot training and how it compares to averaging learned modules.