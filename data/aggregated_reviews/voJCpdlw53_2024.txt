ID: voJCpdlw53
Title: UltraPixel: Advancing Ultra High-Resolution Image Synthesis to New Peaks
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 8, 6, 4, -1, -1, -1, -1
Original Confidences: 4, 4, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents UltraPixel, an innovative architecture for ultra-high-resolution image generation that addresses semantic planning, detail synthesis, and high resource demands. The authors propose a method that utilizes cascade diffusion models to generate images at multiple resolutions, efficiently guiding high-resolution generation with lower-resolution semantics. The architecture incorporates implicit neural representations for continuous upsampling and scale-aware normalization layers, achieving high efficiency with less than a 3% increase in resource demands for high-resolution outputs.

### Strengths and Weaknesses
Strengths:
- The paper demonstrates impressive results, generating high-resolution images with remarkable detail and outperforming existing methods in speed and flexibility.
- The methodology is well-articulated, with clear evidence supporting the necessity of low-resolution guidance for optimal results.
- The introduction of implicit neural representations and scale-aware normalization layers is a creative solution to scalability challenges in image generation.

Weaknesses:
- The manuscript's layout requires refinement, with issues such as Figure 4 extending beyond page margins and overly condensed text near Figure 9.
- The explanation of implicit neural representations and scale-aware normalization lacks clarity, and further comparative analyses with state-of-the-art methods are needed to underscore the proposed framework's advantages.
- The paper would benefit from a more comprehensive set of visual results and ablation studies, particularly regarding LR guidance timesteps and scale-aware normalization.

### Suggestions for Improvement
We recommend that the authors improve the manuscript's layout for better readability and clarity. Additionally, the authors should expand their experimental comparisons to include state-of-the-art generative image upsamplers and provide visual results demonstrating the limitations mentioned in the paper. A more detailed explanation of the implicit neural representation and an in-depth analysis of the scale-aware normalization feature would enhance understanding. Finally, we encourage the authors to explore the potential for generating even higher resolutions and to clarify the availability of code and pre-trained models for reproducibility.