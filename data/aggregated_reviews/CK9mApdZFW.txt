ID: CK9mApdZFW
Title: DISCO: A Large Scale Human Annotated Corpus for Disfluency Correction in Indo-European Languages
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the creation of a disfluency correction dataset, named *DISCO*, comprising approximately 12,000 samples across four languages: English, Hindi, German, and French, with around 3,000 samples per language. The dataset is the first for German and French and contributes additional disfluency data for English and Hindi. Utterances were sampled from a dataset of dialogues with AI agents and annotated by a single annotator per language, using a sequence-labeling approach to classify words as fluent or disfluent with specific disfluency types. The authors evaluate various baseline systems, reporting high F1 scores for disfluent utterances but significantly lower scores for fluent ones in English and French, attributed to unbalanced utterance types. An MT experiment demonstrates that disfluency correction improves BLEU scores for translations, although the dataset's comparative utility to existing datasets remains unclear.

### Strengths and Weaknesses
Strengths:
- The paper introduces a valuable multilingual resource for disfluency correction, particularly for German and French.
- It provides a detailed description of the dataset and demonstrates the impact of disfluency correction on downstream MT performance.

Weaknesses:
- The dataset's comparative analysis with existing datasets for English and Hindi is insufficient, leaving its utility ambiguous.
- The annotation process's validity is questionable due to reliance on a single annotator per language, and the paper lacks clarity on the selection of sentences and the handling of fluent utterances.

### Suggestions for Improvement
We recommend that the authors improve the comparative analysis of the *DISCO* dataset with existing datasets for English and Hindi, detailing size, breadth, and coverage. Clarifying the selection process for sentences and addressing the impact of the dataset's skewed disfluency types on model performance is essential. Additionally, we suggest including multiple annotators to enhance annotation validity and providing a more organized results section with in-depth discussions on the performance improvements brought about by disfluency correction. Evaluating the performance of rule-based predictions on *DISCO* could also help contextualize the dataset's challenges.