ID: 96gXvFYWSE
Title: Pearls from Pebbles: Improved Confidence Functions for Auto-labeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 7, 6, 8, 4, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a threshold-based auto-labeling (TBAL) method called Colander, which aims to optimize TBAL performance by identifying the best labeling confidence function and thresholds. Colander formulates the auto-labeling objective as an optimization problem, maximizing coverage while adhering to label error constraints, utilizing a neural network for the confidence function and a surrogate for optimization via gradient methods. The authors evaluate Colander against existing methods across various datasets, demonstrating improvements in coverage and error rates.

### Strengths and Weaknesses
Strengths:
- The paper effectively transforms auto-labeling into an optimization problem, providing a structured solution solvable by gradient methods.
- The proposed optimization surrogate has potential applicability to other auto-labeling methods.
- Extensive experiments are conducted, yielding promising results compared to existing methods.

Weaknesses:
- The discussion of thresholds lacks clarity and detail, particularly regarding the definitions and roles of vectors and thresholds in the algorithms.
- There is insufficient discussion on the computational overhead associated with Colander.
- The paper's experimental content is thin, lacking details on hyperparameter searches and their effects on performance.
- Limitations and future work are not adequately addressed, leaving gaps in understanding the approach's constraints.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the threshold discussions, particularly in defining the vector _**t**_ and the set of thresholds _T_. Additionally, the authors should elaborate on the computational overhead of Colander and provide more comprehensive details on hyperparameter searches and their impacts on coverage and error rates. A more thorough discussion of limitations and potential future work would enhance the paper's depth. Furthermore, we suggest including comparisons with active learning methods in the experiments to clarify the distinctions between TBAL and self-training approaches.