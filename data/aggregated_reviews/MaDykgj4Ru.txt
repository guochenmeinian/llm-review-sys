ID: MaDykgj4Ru
Title: BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an algorithm that combines Low-Rank Adaptation (LoRA) and Bayes by Backprop (BBB) to learn a low-rank variational posterior distribution over a subset of LLM weights during fine-tuning. The authors introduce several modifications, including parameterizations and optimization strategies, to enhance practical implementation. The method is empirically evaluated against various benchmarks, showing competitive performance. Additionally, the paper proposes a method called Blob, which utilizes mean-field variational inference on LoRA parameters to achieve a richer posterior structure while maintaining computational efficiency. The authors also revise Theorem 3.1 and Theorem 3.2 to Proposition 3.1 and Proposition 3.2, reflecting the nature of their contributions more accurately. They propose that the theoretical foundation of Asymmetric Bayesianization serves as a springboard for developing new methods in the field and plan to reorganize Section 3.1 for clarity. Furthermore, they acknowledge the empirical trade-off between accuracy and calibration in mean prediction versus Bayesian prediction.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with a clear contextualization of the proposed algorithm within existing literature, supported by effective visual aids.
- The introduction of concepts is paced adequately, and the notation is clean, facilitating reader comprehension.
- The empirical evaluation indicates consistent gains over baseline approaches, and the modifications necessary for practical implementation are thoroughly discussed.
- The authors demonstrate responsiveness to feedback by revising key terms to better reflect their contributions.
- The theoretical insights provided have the potential to advance the field, and plans for reorganization and additional citations indicate a commitment to clarity and relevance.

Weaknesses:
- The proposed method incurs additional compute time and memory compared to standard LoRA, which should be transparently mentioned in the main text.
- Limitations of the method are not explicitly discussed, and the presentation is overly complex, detracting from clarity.
- The novelty of the approach is limited compared to existing methods, particularly in relation to the work by Yang et al., which employs similar ideas.
- The initial use of the term "Theorem" may have misrepresented the nature of the contributions.
- The discussion on the trade-off between accuracy and calibration could have been more explicit in the original submission.

### Suggestions for Improvement
We recommend that the authors improve transparency regarding the additional compute time and memory requirements by including this information in the main paper. It would also be beneficial to explicitly discuss the limitations of the proposed method in the conclusion. To enhance clarity, we suggest simplifying the presentation by starting from Equation 5 and deferring discussions about the prior on matrix B. Additionally, we encourage the authors to address the importance of the KL divergence term weights and clarify the rationale behind modeling $\mathbf{A}$ probabilistically while treating $\mathbf{B}$ deterministically. We also recommend improving clarity in Section 3.1 by starting with the calculation of the full weight matrix \( W_{ij}=W_{0,ij} + \sum_{k=1}^r B_{ik}A_{kj} \) before discussing the advantages of Asymmetric Bayesianization. Furthermore, ensure that the relevant ICML 2024 paper is adequately cited and discussed in the revised version. Finally, we suggest including a more explicit discussion of the empirical trade-off between accuracy and calibration in the context of mean prediction versus Bayesian prediction.