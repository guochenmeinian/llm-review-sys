ID: MWQjqtV1z4
Title: Restless Bandits with Average Reward: Breaking the Uniform Global Attractor Assumption
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 4, 7, 6, 6, 8, 6, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, 2, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a simulation-based framework, Follow-the-Virtual-Advice (FTVA), to address the infinite-horizon restless bandits problem without relying on the uniform global attractor property (UGAP). The authors propose a method to leverage a single-armed policy for multi-armed scenarios, which significantly simplifies the problem by removing the difficult-to-verify UGAP condition. The paper includes both discrete and continuous time variants and provides convergence guarantees.

### Strengths and Weaknesses
Strengths:
- The approach of coupling actual trajectories with virtual ones for each bandit is innovative.
- The removal of the UGAP assumption is a notable advancement, although it is not unique to this paper.
- The literature review is comprehensive, and the paper is well-structured and clear.

Weaknesses:
- The arguments regarding FTVA lack sufficient evidence and clarity, particularly concerning the synchronization assumption and its implications.
- The example provided does not convincingly support the claims made, particularly regarding the preferred actions and state distributions.
- Important related work is omitted, which also addresses similar challenges without UGAP.
- The precision of references is questionable, and some statements lack clarity in relation to the RMAB problem.

### Suggestions for Improvement
We recommend that the authors improve the clarity and rigor of the arguments in Section 3.3, particularly regarding the synchronization assumption and its implications for the real and virtual states. Additionally, we suggest providing more detailed explanations of the examples and ensuring that the claims made are well-supported by evidence. It would be beneficial to include a discussion of the omitted related work, specifically Ghosh et al. (2022), and to clarify the references cited throughout the paper. Finally, we encourage the authors to elaborate on how the FTVA policy can be implemented in a distributed manner and to provide more insights into the action policies compared to other non-asymptotically optimal policies.