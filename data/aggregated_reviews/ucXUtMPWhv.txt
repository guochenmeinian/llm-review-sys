ID: ucXUtMPWhv
Title: ElasTST: Towards Robust Varied-Horizon Forecasting with Elastic Time-Series Transformer
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 5, 5, -1, -1, -1, -1
Original Confidences: 5, 3, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ElasTST, a masked encoder-based transformer model designed for time series forecasting across varied horizons. The authors propose a non-autoregressive architecture that incorporates placeholders for forecasting horizons, structured self-attention masks, and a tunable rotary position embedding (RoPE). The model employs a multi-scale patch design to enhance adaptability and demonstrates robust performance in extrapolating beyond trained horizons.

### Strengths and Weaknesses
Strengths:  
- The paper addresses the critical issue of adapting to varied forecasting horizons, enhancing model flexibility.  
- The introduction of multi-scale patching effectively captures local patterns of different granularities.  
- The tunable RoPE module shows potential for broader application in time series forecasting.

Weaknesses:  
- The proposed approach closely resembles Moirai, with limited additional features and insufficient experimental evidence to support the benefits of the changes made.  
- The tunable RoPE lacks comparative analysis with existing positional embedding methods, which would clarify its advantages.  
- The literature review is inadequate, omitting significant foundational models and lacking a traditional encoder transformer as a baseline for comparison.  
- The paper does not sufficiently address the computational scalability of ElasTST for large datasets or long time series.  
- The modified metrics used may not provide fairer results, as the datasets are already scaled.

### Suggestions for Improvement
We recommend that the authors improve the comparative analysis of the tunable RoPE module by including comparisons with no positional embedding, vanilla positional encoding, and other relevant methods. Additionally, we suggest incorporating a traditional encoder transformer as a baseline to better demonstrate the proposed method's improvements. A more comprehensive literature review should include notable models like TimeGPT-1, Chronos, and others. Furthermore, the authors should provide a detailed discussion on the computational costs associated with longer sequence lengths and the implications of using multiple patch sizes. Lastly, we encourage the authors to clarify the necessity of the structured mask for varied horizon prediction and to address potential failure cases for a more balanced view of ElasTST's applicability.