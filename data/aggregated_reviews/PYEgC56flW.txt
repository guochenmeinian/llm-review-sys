ID: PYEgC56flW
Title: Feature Learning for Interpretable, Performant Decision Trees
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 5, 6, 5, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for learning interpretable decision trees by maintaining limited tree depth without compromising performance. The authors propose a process that involves multiple iterations of feature transformation and tree regrowth, utilizing linear and distance-to-prototype transformations. Regularization techniques are applied to enhance interpretability by increasing feature sparsity and refining the output range of features, leading to purer leaves. Empirical evaluations against decision trees, random forests, and ExtraTrees on UCI datasets indicate that the proposed method outperforms baselines in 4 out of 6 datasets.

### Strengths and Weaknesses
Strengths:
- Originality: The approach builds on existing literature and introduces a novel method of alternating feature transformation and tree regrowth, supported by sensible regularizations.
- Quality: The paper is generally well-structured, with clear motivation and comprehensive coverage of related literature. Experimental results are detailed, although some metrics are relegated to supplementary materials.
- Clarity: The writing is mostly clear, though some sections, particularly regarding MNIST datasets, lack clarity in comparing the proposed method to baseline approaches.

Weaknesses:
- Interpretation of interpretability: The reliance on feature transformation to control tree size may compromise interpretability, as seen in examples where decision criteria involve linear combinations of features.
- Limited competitive analysis: The absence of comparisons with linear methods like linear regression limits the evaluation of interpretability. The performance variability based on alpha values, which are not programmatically selected, raises concerns about the method's robustness.
- Lack of clarity on certain aspects: The rationale for using MNIST datasets and the justification for alpha values affecting performance need further elaboration.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the discussions surrounding the MNIST datasets and provide a more thorough justification for the choice of alpha values, particularly addressing the observed performance variability. Additionally, we suggest including comparisons with linear models to better contextualize the interpretability of the proposed method. It would also be beneficial to clarify how the regularization techniques are applied during tree learning and to provide guidelines on selecting transformations for different tasks. Finally, addressing the time complexity by reporting training time instead of inference time would enhance the evaluation of the proposed method's efficiency.