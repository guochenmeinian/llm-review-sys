ID: paa2OU5jN8
Title: Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel zero-shot text-to-video generation method that utilizes a large language model (LLM) as a director to create per-frame prompts for a pre-trained text-image model, such as Stable Diffusion. To ensure temporal coherence, the authors propose a noise distribution that interpolates global and local noise and incorporate prior frames in the self-attention layers during generation. Additionally, a dual interpolation method is introduced to enhance video frame rates.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a novel method that outperforms state-of-the-art (SOTA) zero-shot text-video methods in frame fidelity and visual semantics without requiring text-video training.
- The approach effectively leverages the LLM's understanding of language for video generation, resulting in high-quality per-frame outputs.
- The method is low-cost, as it employs zero-shot learning without additional training, and combines two pre-trained models efficiently.

Weaknesses:
- Quantitative evaluations are limited due to a small sample size (20 prompts, 4 videos each), and the ablation study lacks rigorous quantitative analysis, relying primarily on qualitative results.
- The coherence of generated videos is insufficient, with observable sudden changes in color and identity, potentially due to the technical design choices made regarding attention shifts.
- There is a lack of comparison with other large language models beyond ChatGPT, and the proposed frame interpolation algorithm is not numerically compared with SOTA methods.

### Suggestions for Improvement
We recommend that the authors improve the quantitative evaluation by increasing the sample size and providing more rigorous numerical comparisons in the ablation study. Additionally, we suggest including comparisons of video generation quality guided by different large language models. To enhance coherence, consider refining the technical design regarding attention shifts, particularly in early time steps, and explore the possibility of combining the LLM director with existing pretrained text-to-video methods. Furthermore, we encourage the authors to clarify experimental details, including prompt engineering and hyperparameters, to strengthen the overall presentation of the work.