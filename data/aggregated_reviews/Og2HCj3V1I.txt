ID: Og2HCj3V1I
Title: Attribute Based Interpretable Evaluation Metrics for Generative Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 4, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel evaluation metric for generative models that compares the distributions of real and generated images using a predefined set of attributes. The authors propose two metrics: Single attribute KL divergence (SaKLD) and Paired attribute KL divergence (PaKLD), which utilize interpretable embeddings derived from a CLIP model. The metrics aim to provide insights into how well the generated images align with the training data based on specified attributes, allowing for customizable evaluations. Additionally, the paper includes a comparative analysis of CLIP Score and Directional CLIPScore (DCS), highlighting DCS's suitability for measuring attribute strength. The authors discuss the implications of potential biases in CLIP and their impact on results.

### Strengths and Weaknesses
Strengths:
- The proposed metric is innovative, leveraging language-image models to assess the alignment of distributions based on user-defined attributes, enhancing task-specific evaluations.
- The introduction of DCS offers a more interpretable approach to measuring similarity between images and attributes.
- The authors provide clear clarifications regarding the experimental design and the rationale behind their metrics.
- The empirical exploration with a larger number of generative models is appreciated, enhancing the robustness of the findings.

Weaknesses:
- The paper lacks a thorough discussion of the metric's failure modes and limitations, particularly regarding the robustness of language-image models.
- Empirical validation of the metrics is insufficient, with results primarily presented as scalar values rather than detailed analyses that leverage the metric's interpretability.
- The methodology's reliance on text attributes may limit its generalizability and applicability compared to existing metrics that evaluate image quality directly.
- Figure 2 lacks clarity on whether it illustrates actual scores or is redundant with Figure 1.
- The ablation experiment in section 5.4 does not present results beyond 50k samples, limiting the support for the claim of score stabilization.
- The proposed metrics do not address quality and diversity aspects as covered by existing metrics like Precision-Recall or Density-Coverage.

### Suggestions for Improvement
We recommend that the authors improve the discussion of the metric's limitations, particularly regarding potential failure scenarios and the implications of using language-image models. Additionally, we suggest providing a more comprehensive empirical evaluation that includes a fine-grained analysis of the models rather than relying solely on scalar comparisons. Clarifying the calculation of Table 1 and addressing the ambiguity in the description of the attribute extraction methods would enhance the paper's technical soundness. We also recommend improving the clarity of Figure 2 by explicitly stating whether it represents actual scores or is merely illustrative. Furthermore, we encourage the authors to include results beyond 50k samples in the ablation experiment to substantiate their claims. Lastly, we recommend providing a plausible explanation for the exclusion of quality and diversity considerations in their proposed metrics and further exploration of attribute selection to highlight the advantages of their approach over pre-existing methods.