ID: jzkpwcj200
Title: Efficient multi-prompt evaluation of LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 8, 5, 6, -1
Original Confidences: 1, 3, 4, -1

Aggregated Review:
### Key Points
This paper presents PromptEval, a novel method for efficiently evaluating large language models (LLMs) across multiple prompt templates. The authors propose a statistical framework based on Item Response Theory (IRT) to estimate LLM performance distribution with limited evaluations, ensuring theoretical guarantees on the consistency of the estimators. The study includes a large-scale analysis of prompt sensitivity across 15 popular open-source LLMs using benchmarks such as MMLU, BBH, and LMentry.

### Strengths and Weaknesses
Strengths:
- Theoretical guarantees demonstrate that PromptEval has desirable statistical properties, including consistency in estimating performance distribution and quantiles.
- The paper addresses a significant problem in LLM evaluation, providing a solid theoretical foundation with clear assumptions and complete proofs in the appendix.
- Extensive experiments validate the effectiveness of PromptEval across multiple benchmarks and prompt variations.

Weaknesses:
- The authors do not explore alternative modeling approaches beyond IRT, limiting the scope of their comparisons.
- The computational requirements of the estimation process are not discussed, which is relevant for large prompt sets.
- Experiments are currently limited to open-source models; there is a need for testing on mainstream closed-source models.
- The method's effectiveness may depend on the availability of a large number of active prompt templates, which may not always be satisfied.

### Suggestions for Improvement
We recommend that the authors improve their exploration of alternative modeling approaches beyond IRT to enhance the robustness of their findings. Additionally, discussing the computational complexity of the estimation process in relation to the number of prompts and examples would provide valuable insights. Conducting experiments on mainstream closed-source models, such as the GPT or Claude series, would further validate the applicability of PromptEval. Lastly, comparing the baseline construction with relevant methods like TRIPLE would help highlight the advantages of PromptEval more effectively.