ID: 7UenF4kx4j
Title: SMART: Towards Pre-trained Missing-Aware Model for Patient Health Status Prediction
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 8, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SMART, a self-supervised representation learning approach aimed at addressing missingness in electronic health record (EHR) data. The authors propose a two-stage training strategy that includes a novel masked attention recurrent transformer (MART) block, which effectively captures temporal and variable interactions to reconstruct missing data representations. The pre-trained encoder can be fine-tuned with a label-specific decoder for various downstream classification tasks. The method demonstrates superior performance across multiple datasets, showcasing its robustness and generalization capabilities.

### Strengths and Weaknesses
Strengths:
1. The approach effectively addresses the critical issue of missingness in EHR data, making it impactful for the medical domain.
2. The MART block and pre-training paradigm are innovative, contributing to the model's performance improvements.
3. SMART outperforms previous baselines in both accuracy and training efficiency across all tested datasets.
4. Comprehensive ablation studies validate the effectiveness of different model components.
5. The paper is well-written, and the authors provide code for reproducibility.

Weaknesses:
1. The pre-training stage requires access to the full dataset, limiting its applicability when training data contains missing values.
2. The rationale for updating only the label decoder during the initial fine-tuning epochs is unclear; a quantitative explanation or additional ablation studies would enhance understanding.
3. The lack of comparative analysis with state-of-the-art models beyond the specific baselines raises questions about the model's relative performance.
4. The computational efficiency of SMART, particularly regarding inference time and resource requirements, is not adequately discussed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their methodology by providing a more in-depth discussion on the computational efficiency of SMART, particularly regarding inference time and resource requirements. Additionally, we suggest conducting evaluations specifically focused on the pre-training task to assess the model's ability to reconstruct and impute missing values. The authors should also consider including a broader comparison with state-of-the-art models in EHR prediction to establish a clearer benchmark. Furthermore, addressing the potential biases in predictions and the model's generalizability to diverse patient populations would enhance the paper's impact. Lastly, visualizations of the learned embeddings and a clearer explanation of the model's handling of different missing data patterns would provide valuable insights.