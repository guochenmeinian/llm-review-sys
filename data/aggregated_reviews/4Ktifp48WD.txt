ID: 4Ktifp48WD
Title: Differentially Private Optimization with Sparse Gradients
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 5, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents differentially private optimization under the data/gradient sparsity assumption, focusing on mean estimation and its implications for DP ERM/SO. It introduces new near-optimal bounds for sparse data, particularly in high-dimensional settings, and establishes a corresponding lower bound using a novel block-diagonal construction. The authors propose a bias-reduced, randomly stopped SGD method that achieves nearly dimension-independent risk rates in the sparse gradient context.

### Strengths and Weaknesses
Strengths:
- The paper is technically solid, addressing both ERM and SO with results that include lower bounds.
- The novel block-diagonal construction and analysis of randomly stopped noisy SGD will benefit future research.
- It contributes to developing dimension-free error rates under structural assumptions, which is crucial for large models.

Weaknesses:
- The organization of the paper could be improved, particularly by enhancing Section 6 and summarizing the proof in Section 5.
- Several related works are missing, which could better position this paper within the existing literature on dimension-independent rates for DP optimization.
- The presentation is dense, lacking summary or conclusion sections, making it difficult to parse some proofs and algorithmic choices.

### Suggestions for Improvement
We recommend that the authors improve the organization of the paper by adding more content to Section 6 and providing clearer summaries in Section 5. Additionally, we suggest including missing related works to contextualize their contributions better. Clarifying the algorithmic choices, particularly regarding the random batch size and its necessity, would enhance understanding. Finally, addressing the questions raised about the running times for Algorithms 2 and 3, as well as the implications of the sparsity assumption in practical applications, would strengthen the paper.