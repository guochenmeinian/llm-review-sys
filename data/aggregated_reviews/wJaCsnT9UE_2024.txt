ID: wJaCsnT9UE
Title: Sharpness-diversity tradeoff: improving flat ensembles with SharpBalance
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a training approach for ensemble learning called SharpBalance, which aims to balance sharpness and diversity within ensembles. The authors investigate the trade-off between sharpness and diversity, providing both theoretical and empirical evidence for the effectiveness of SharpBalance in improving ensemble performance, particularly in out-of-distribution scenarios.

### Strengths and Weaknesses
Strengths:
- The exploration of sharpness and diversity in deep ensembles is novel and well-motivated.
- The paper provides sufficient theoretical and empirical evidence for validation.
- The proposed method is simple, effective, and includes code for verification.

Weaknesses:
- The empirical improvements over existing methods are marginal.
- The evaluation is limited, focusing primarily on classification accuracy without considering uncertainty metrics.
- The authors should clarify how the training of individuals with full datasets is conducted and whether different initializations are used.
- Comparisons with stronger ensemble baselines, such as GSAM and OBF, are lacking.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by comparing SharpBalance with more ensemble baselines to strengthen their claims. Additionally, providing details on the scale of $D_{SAM}^i$ and its changes during training would enhance understanding. It is also crucial to discuss the differences between SharpBalance and the paper “Diversity-Aware Agnostic Ensemble of Sharpness Minimizers.” Furthermore, we suggest benchmarking with larger datasets, such as ImageNet-V1 for training and ImageNet-Real for testing, to validate the method's robustness. Lastly, incorporating uncertainty metrics like negative log-likelihood and expected calibration error in evaluations, especially for corrupted datasets, would provide a more comprehensive assessment of performance.