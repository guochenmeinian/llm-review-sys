ID: qYAp31KwU2
Title: Multi-Objective Intrinsic Reward Learning for Conversational Recommender Systems
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 5, 5, 6, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an algorithm for learning the intrinsic reward function to address the challenges posed by poorly designed reward functions in conversational recommendation systems (CRS). The authors propose a multi-objective bi-level optimization framework, where the inner layer optimizes the selection strategy using the learned reward function, while the outer layer updates the reward function to enhance system metrics. The effectiveness of the proposed method is validated across three CRS datasets, demonstrating significant performance improvements.

### Strengths and Weaknesses
Strengths:
- The research direction of designing an internal reward based on interaction outcomes is innovative and complements external rewards.
- The hindsight internal reward function effectively calculates reward scores post-interaction.
- The Recommendation Preference Matching module enhances decision-making efficiency by improving the likelihood of successful recommendations.
- The multi-objective bi-level optimization strategy is well-structured, with comprehensive experimental design and clear result analysis.

Weaknesses:
- The baseline comparisons are insufficient; notably, the CRIF model from SIGIR 2022 is not included, and results indicate inferior performance relative to it.
- The case study example lacks clarity, particularly regarding the negative scoring of pop music, raising questions about policy effectiveness.
- The paper focuses solely on the dialogue policy module without detailing how recommendation results are derived.
- There is a lack of significance testing to validate experimental results.
- The integration and interaction of the proposed techniques remain unclear, particularly concerning the intrinsic reward function and the RPM module.

### Suggestions for Improvement
We recommend that the authors improve the baseline comparisons by including the CRIF model to validate performance claims. Additionally, clarifying the case study example and addressing the policy failure concerns would enhance understanding. It is crucial to provide a detailed explanation of how recommendation results are achieved and to include significance tests for experimental validation. Furthermore, elucidating the interplay between the intrinsic rewards and policy learning, as well as the workings of the RPM module, will strengthen the paper's contributions. Lastly, discussing the convergence of the algorithm and its computational complexity in comparison to existing methods is essential for a comprehensive evaluation.