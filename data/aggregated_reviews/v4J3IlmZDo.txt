ID: v4J3IlmZDo
Title: Learning to Rewrite Prompts for Personalized Text Generation
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for enhancing personalized text generation by automatically revising prompts through a hybrid training paradigm that combines supervised learning (SL) and reinforcement learning (RL). The authors propose a prompt rewriter that modifies initial prompts to better summarize and synthesize personal context, validated across three distinct datasets, demonstrating superior performance compared to original prompts and those optimized by SL or RL alone.

### Strengths and Weaknesses
Strengths:
- The innovative combination of SL and RL for prompt rewriting narrows the action space for RL, leading to efficient training.
- The paper provides a comprehensive evaluation across multiple datasets, showing consistent performance improvements.
- The research addresses a common scenario in LLM usage, offering a practical and generalizable solution.
- There is an in-depth analysis of rewritten prompts, providing insights for manual prompt improvement.

Weaknesses:
- The method's efficacy is dependent on the quality of initial prompts, which may limit effectiveness if poorly constructed.
- The complexity and resource requirements of using both SL and RL could hinder accessibility for some users.
- There is a lack of detailed analysis regarding negative cases where the method fails or performs suboptimally.
- Potential biases in initial prompts or training data could be perpetuated or amplified by the rewriter.

### Suggestions for Improvement
We recommend that the authors improve the novelty discussion by addressing recent works on personalized text generation with LLMs and clarifying the motivation for focusing solely on summary and keyword generation modules in Section 3.2. Additionally, we suggest including comparisons with prompt optimization methods such as APE and conducting ablation experiments using other large-scale models like ChatGPT or GPT-4 to verify robustness. To reduce the time and cost associated with generating 65 documents per sample, we recommend analyzing the number of generated documents needed to find the best prompt. Furthermore, we encourage the authors to provide quantitative results demonstrating the extent to which the RL model generates new words compared to the original input prompt and context. Lastly, we suggest improving the layout of analysis results to enhance readability.