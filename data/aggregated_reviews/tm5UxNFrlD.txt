ID: tm5UxNFrlD
Title: Location-Aware Visual Question Generation with Lightweight Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel task, location-aware visual question generation (LocaVQG), aimed at generating engaging questions based on geographic coordinates and images of a user's surroundings. The authors introduce a dataset created using GPT-4 to generate questions from location prompts and image captions, filtered for engagement. They also propose a lightweight model, FDT5, which is a fine-tuned T5-Large model designed to produce these questions. The paper emphasizes the potential real-world applications of this task, particularly in enhancing driver engagement and reducing fatigue.

### Strengths and Weaknesses
Strengths:  
- The proposed task is innovative and has significant real-world applications.  
- The dataset and methodology demonstrate strong performance and can serve as a benchmark for future research.  
- The paper is well-motivated and presents a thorough analysis of the dataset, including human evaluations alongside automatic metrics.

Weaknesses:  
- The paper lacks details on the engaging question classifier, including dataset size, composition, and training specifics.  
- The evaluation metrics are unclear regarding the ground truth used for comparison, raising concerns about the validity of the automatic evaluation.  
- Some results appear trivial, as adding concrete address information leads to predictable increases in vocabulary size and question length without necessarily enhancing question quality.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the engaging question classifier by providing details on its dataset size and training methodology. Additionally, we suggest extending the evaluation section to clearly define the ground truth used for automatic evaluation and address how results are interpreted in light of this. It would also be beneficial to discuss the potential distractions posed by certain questions, categorizing them as "distracting" or "not distracting." Finally, we encourage the authors to include a discussion on the speed and latency of their model, particularly in the context of its application on edge devices.