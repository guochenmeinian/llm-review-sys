ID: jhs8F63xI6
Title: Adaptive Online Replanning with Diffusion Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 6, 5, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called Replanning with Diffusion Models (RDM), which utilizes the estimated likelihood of trajectories to determine when and how to replan in diffusion-based models. The authors propose strategies for replanning that aim to enhance motion planning efficiency, particularly in robotic control and stochastic environments. The method reportedly leads to a 38% performance improvement over previous diffusion planning approaches. The authors also address concerns regarding replanning at every time step and the computation cost associated with determining when to replan, providing visualizations and detailed computational costs. They indicate that frequent replanning can lead to inconsistent agent trajectories and worse performance, particularly in longer sequence tasks.

### Strengths and Weaknesses
Strengths:
- The topic of replanning is crucial, especially within the diffusion model community, and the proposed methodology is compelling.
- The paper is well-written and clear, with promising experimental results demonstrating significant performance improvements in robotic control tasks.
- The originality of the approach to infer replanning timing is noteworthy, as it addresses a gap in existing literature.
- The authors effectively address reviewer concerns, providing visual examples and detailed computational analysis.
- The RDM method shows clear advantages in longer sequence tasks, enhancing the understanding of replanning dynamics.

Weaknesses:
- The experimental results lack comprehensive comparisons with relevant baselines that incorporate replanning strategies, making it difficult to evaluate the effectiveness of the proposed method.
- The clarity of the problem statement is insufficient, leading to confusion regarding whether RDM is an imitation learning or reinforcement learning method.
- The paper does not adequately discuss the accuracy of the likelihood estimation or its impact on performance, and there are language issues and typos that need correction.
- Performance in highly stochastic environments remains an area for improvement.
- The optimization of replanning timing, while insightful, does not significantly change initial impressions of the method's effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by including comparisons with "no replanning baselines" and other replanning methods to provide a clearer evaluation of RDM's effectiveness. Additionally, clarifying the problem statement to explicitly define whether RDM is an imitation learning or reinforcement learning method would enhance reader understanding. It would also be beneficial to provide details on how the replanning thresholds were determined and to include error bars in the experimental results for better statistical significance. Furthermore, we recommend that the authors improve the performance of the RDM method in environments with high levels of stochasticity. Lastly, addressing the language issues and typos throughout the paper will improve overall presentation quality, and providing more intuitive examples regarding the implications of replanning at very small intervals could enhance clarity for readers.