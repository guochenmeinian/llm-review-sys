ID: NxOeOxe6qs
Title: Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Variator, a parameter-efficient acceleration method aimed at enhancing the computational efficiency of large language models (LLMs) without compromising performance. The authors propose using compression plugins to consolidate multiple hidden vectors into a single representation, thereby shortening sequence length. Experimental results on seven datasets indicate a 53% reduction in computational costs with only a 0.9% increase in parameters and less than a 2% performance drop.

### Strengths and Weaknesses
Strengths:
- The idea is novel and interesting, with limited additional overhead.
- The method is technically sound and demonstrates effectiveness across various datasets.
- The paper is well-written and provides sufficient details for reproducibility.

Weaknesses:
- The method bears similarities to existing approaches, such as convolutional layers with stride > 1 and token merging, necessitating comparisons with these methods.
- The experimental evaluation lacks clarity and requires elaboration, including comparisons with other compression models and broader applicability to various LLMs.
- The proposed method may not be suitable for large language models that require pre-training, potentially conflicting with the acceleration objective.
- The suitability of the method for generation models needs further exploration.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by including results from additional LLMs, such as LLaMa and BERT, to demonstrate the generalizability of the proposed method. Additionally, the authors should discuss the similarities with prior work, particularly the paper by Zihang Dai et al., and clarify how Variator differs from existing methods. We also suggest providing measured latency on hardware to complement the #FLOPs metric and enhance the evaluation of real-world performance. Finally, the authors should address the applicability of their method to generation models and consider comparing it with convolutional layers to further validate their approach.