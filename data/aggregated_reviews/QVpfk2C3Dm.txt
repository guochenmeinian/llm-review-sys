ID: QVpfk2C3Dm
Title: Bottleneck Structure in Learned Features: Low-Dimension vs Regularity Tradeoff
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 6, 8, 6, -1, -1, -1, -1
Original Confidences: 4, 2, 2, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the representation cost $R(f)$ of deep neural networks, focusing on the Taylor expansion of this cost as the depth $L$ approaches infinity. The authors introduce two corrections, $R^{(1)}(f)$ and $R^{(2)}(f)$, which address the limitations of the previously established rank term $R^{(0)}(f)$. They argue that these corrections help prevent underestimation of the true bottleneck rank and provide insights into the dynamics of hidden representations in deep networks.

### Strengths and Weaknesses
Strengths:  
- The paper presents a novel theoretical framework that enhances understanding of representation costs in deep neural networks.  
- The exploration of new terms $R^{(1)}(f)$ and $R^{(2)}(f)$ is thorough and contributes significantly to the field.  
- The theoretical results are well-supported with clear explanations, particularly regarding the implications of learning rates and representation geodesics.  

Weaknesses:  
- The justification for treating the representation cost $R(L)$ as Taylor-expandable is insufficiently emphasized, particularly regarding the continuity of $L$.  
- The study of representation costs at finite weight decay for nearly infinite depth networks raises concerns about the differentiability of the cost function.  
- The paper lacks empirical validation; the authors should conduct experiments to directly compute and illustrate the first and second order terms of $R$ in $1/L$.  
- The presentation is disjointed, requiring more cohesive discussion and clarity on the high-level picture and the role of learning rates.

### Suggestions for Improvement
We recommend that the authors improve the justification for the Taylor expansion of $R(L)$, particularly addressing the continuity of $L$ and the implications of finite weight decay on the representation cost. Additionally, we suggest that the authors conduct numerical experiments to validate their theoretical predictions, specifically by computing the first and second order terms of $R$ in $1/L$ for real neural networks. Furthermore, enhancing the paper's presentation by providing more integrated discussions and clarifying the concept of regularity would significantly improve readability.