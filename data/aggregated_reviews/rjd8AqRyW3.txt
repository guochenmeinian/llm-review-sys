ID: rjd8AqRyW3
Title: OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a dataset for multi-document open aspect-based summarization (OPENASP), which includes 1,310 instances and 1,266 unique aspects. The authors propose a novel protocol that simplifies the manual annotation process by allowing annotators to derive aspects from existing multi-document summarization datasets, specifically the DUC and MultiNews datasets. The paper includes benchmark experiments with various models, including chatGPT, to validate the dataset's effectiveness.

### Strengths and Weaknesses
Strengths:
- The proposed dataset is significant for advancing research in aspect-based summarization.
- The protocol for dataset creation is efficient and cost-effective, allowing for high-quality annotations.
- The paper is well-structured, with a comprehensive analysis of the dataset and robust benchmark experiments.

Weaknesses:
- The methodology lacks clarity regarding the quality of aspects generated from generic summaries, with no experiments demonstrating their alignment with document topics.
- There is no comparison to similar datasets like AnyAspect and OASUM, which would provide context for the dataset's uniqueness.
- The verification process of the dataset curation is insufficient, lacking objective measures and inter-rater agreement statistics.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology by including experiments that assess the quality of the generated aspects and their relevance to the original documents. Additionally, a comparison with similar datasets such as AnyAspect and OASUM should be included to contextualize the contributions of OPENASP. We also suggest incorporating an external expert in the verification process to mitigate potential biases and providing inter-rater agreement statistics to enhance data quality transparency. Finally, consider evaluating fine-tuned models like BART and PRIMERA in the recursive summarization setting to strengthen the experimental results.