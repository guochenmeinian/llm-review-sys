ID: 3gamyee9Yh
Title: QuantSR: Accurate Low-bit Quantization for Efficient Image Super-Resolution
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 8, 7, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel quantization method for super-resolution (SR) models, introducing the Redistribution-driven Learnable Quantizer (RLQ) and Depth-dynamic Quantized Architecture (DQA). The authors propose that RLQ mitigates weight homogeneity during quantization, enhancing representation and gradient information without additional inference overhead. DQA aims to optimize performance-efficiency trade-offs, allowing flexible resource adaptation in real-world applications. The experimental results demonstrate that QuantSR achieves state-of-the-art (SOTA) performance across various bit-widths, surpassing existing quantized SR methods. Additionally, the authors compare the QuantSR method against the DDTB algorithm, claiming that QuantSR achieves higher accuracy with fewer training epochs, specifically 300K iterations (equivalent to 26 epochs), compared to DDTB's 60 epochs. However, concerns arise regarding the fairness of this comparison due to differing training settings, including the number of iterations and batch sizes.

### Strengths and Weaknesses
Strengths:
- The proposed RLQ and DQA technologies effectively enhance the performance and efficiency of quantized SR models.
- Experimental results indicate significant performance improvements over existing methods, with clear visualizations supporting the findings.
- The authors provide clarifications regarding the training iterations for QuantSR, emphasizing its efficiency.
- The paper is well-organized and presents a clear motivation for the proposed methods.

Weaknesses:
- The function of RLQ, particularly the role of the \(\phi\) function during forward propagation, requires further clarification, as does the meaning of the equations in backward propagation.
- The comparison of QuantSR with state-of-the-art quantized transformer SR methods is lacking; additional experiments are needed.
- The comparison between QuantSR and DDTB lacks fairness due to significant differences in training settings, such as the inclusion of a pre-training phase for DDTB.
- The presentation of results may mislead readers regarding the performance claims, particularly in relation to state-of-the-art benchmarks.
- The writing contains grammatical errors and unclear symbols that need thorough revision.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the RLQ function, specifically addressing the role of \(\phi\) in forward propagation and discussing the implications of the equations in backward propagation. Additionally, we suggest implementing comparisons with state-of-the-art quantized SR methods for transformer networks to strengthen the experimental section. We also recommend that the authors improve the presentation of results by clearly detailing the training settings for each method, including the number of iterations and batch sizes, to avoid misleading claims. Furthermore, we suggest that the authors tone down assertions of outperforming state-of-the-art algorithms unless comparisons are made under identical conditions. Including discussions on the impact of training details, as seen in the referenced works, would enhance the paper's rigor and completeness. Lastly, we encourage the authors to thoroughly revise the manuscript for grammatical errors and clarify any ambiguous symbols to enhance readability.