ID: IZtX4RNBeH
Title: How Good is my Video LMM? Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 7, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark called CVRR-ES, designed to evaluate the reasoning ability and textual robustness of Video-Language Models (Video-LMMs) across 11 diverse dimensions. It collects 214 videos and generates 2,400 question-answer (QA) pairs through collaboration between human annotators and ChatGPT. The evaluation reveals significant limitations in complex video reasoning and robustness to confusing user questions. The authors emphasize that advanced contextual understanding is critical for reasoning ability, involving the interpretation of motivations behind actions in videos, and clarify the concept of "surrounding context," which includes environmental cues and character interactions. To address the identified issues, the authors propose a novel prompting methodology—Dual-Step Contextual Prompting (DSCP)—which enhances the performance of multiple Video-LMMs on the CVRR-ES benchmark.

### Strengths and Weaknesses
Strengths:
- The work is well-motivated, addressing the lack of appropriate benchmarks for evaluating Video-LMMs' complex reasoning and robustness.
- The authors provide a clear distinction between basic comprehension and advanced contextual understanding, which is crucial for reasoning in video comprehension.
- Evaluation results indicate that existing state-of-the-art Video-LMMs struggle with complex reasoning, providing guidance for future model development.
- The inclusion of concrete examples of reasoning and robustness QA pairs enhances clarity and understanding of the proposed benchmark.
- The LMM-based automatic evaluation procedure achieves a 95.36% alignment rate with human judgment.
- The DSCP prompting technique is effective, consistently improving the performance of various Video-LMMs.

Weaknesses:
- The definition of “reasoning” is vague, and the relationship of the 11 dimensions to reasoning is unclear.
- The rationale for selecting the specific 11 dimensions is inadequately discussed.
- Some QA pairs do not align well with the expected dimensions, raising concerns about their relevance.
- The rationale behind the importance of "surrounding context" could be further elucidated for clarity.
- Certain technical details are unclear, such as the absence of prompts for dimensions other than “understanding of emotional context” and how to ensure QA pairs involve complex reasoning.
- The performance of Video-LMMs on robustness-based QA pairs is notably lower, indicating potential limitations in training for such scenarios.
- Related works on Video-LMM benchmarks are not adequately cited or discussed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of “reasoning” and specify how each of the 11 dimensions relates to it. Additionally, the rationale for selecting these dimensions should be more thoroughly discussed. We suggest ensuring that all QA pairs are relevant to their respective dimensions and providing prompts for all dimensions in the main text. It would be beneficial to include a comprehensive overview of the proposed tasks in a single figure for better clarity. Furthermore, we recommend including a top-down diagram similar to MMBench Figure 2 to enhance the understanding of evaluation dimensions. A more detailed analysis of the DSCP technique, including its efficiency and performance across videos of varying lengths, should be included. Lastly, we encourage the authors to expand the related works section to include relevant benchmarks and discuss their implications in relation to the proposed benchmark, as well as report the respective results for reasoning and robustness questions separately to provide valuable insights into the performance of Video-LMMs.