ID: Tnx0922coo
Title: Disentangling Transformer Language Models as Superposed Topic Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to topic modeling using decoder-only Transformer-Based Language Models (TLMs), specifically focusing on disentangling topics through a weight-based, model-agnostic, and corpus-agnostic method that converts the topic disentanglement problem into a graph optimization task. The authors propose a three-step disentanglement process, which includes projecting to obtain corpus statistics, shortlisting using heuristics, and exact-solving to locate subgraphs and return coherent topics. The experimental results demonstrate the effectiveness of this method on widely used models like GPT2 and Llama.

### Strengths and Weaknesses
Strengths:
- The interpretation of TLMs as superposed Neural Topic Models (NTMs) is novel.
- The three-step disentanglement process is clearly articulated.
- Comprehensive experimental evaluation using popular language models.
- The method shows potential benefits for the topic modeling field.

Weaknesses:
- The concept of "projecting to a corpus" may be difficult for some readers to comprehend.
- The necessity of creating word pools for the algorithm's effectiveness is unclear, raising questions about redundancy.
- Evaluation metrics are limited, particularly the use of NPMI for both topic creation and model evaluation, which may be perceived as unfair.
- The scalability of the method on larger datasets remains uncertain.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the "projecting to a corpus" concept to enhance reader comprehension. Additionally, it would be beneficial to analyze the computational complexity of the method on larger datasets to address scalability concerns. We suggest that the authors consider incorporating a broader range of evaluation metrics beyond NPMI to provide a more comprehensive assessment of their model's performance. Furthermore, the authors should clarify the necessity of the word pool creation step and explore whether graph construction can be achieved without it. Lastly, including more preliminary introductions to disentangling superpositions could make the paper more accessible to a wider audience.