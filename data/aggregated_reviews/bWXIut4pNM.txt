ID: bWXIut4pNM
Title: INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework named INGENIOUS for enhancing the training efficiency of pre-trained language models (PTLMs) by selecting highly informative subsets of the training data through submodular optimization. The authors demonstrate that models trained on these subsets can achieve performance comparable to those trained on the full dataset, with significant reductions in training costs. The approach is validated across various architectures and downstream tasks, showcasing its potential to address the high computational costs associated with pre-training language models.

### Strengths and Weaknesses
Strengths:
- The INGENIOUS method significantly advances the efficiency of language model pretraining by extracting crucial information subsets.
- The experimental design is comprehensive, covering various architectures and sampling methods, and includes a thorough evaluation of performance and cost trade-offs.
- The paper is well-structured, with detailed sections on experiments and analysis.

Weaknesses:
- The paper lacks sufficient discussion on the implications of training data quality, including issues related to values, toxicity, and biases.
- Validation is limited to smaller models (~110M parameters), raising questions about the method's effectiveness with larger models.
- Some results, particularly in Figures 3 and 4, are confusing and require clarification regarding the rationale behind performance comparisons.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the choice of the similarity kernel used in their method, including what types of kernels work best and whether simpler metrics, like SentenceBERT embeddings, could be effective. Additionally, including comparisons to methods from the computer vision domain would enhance the paper's relevance. We also suggest providing a clearer analysis of why increasing the subset size sometimes negatively impacts performance, as indicated in Table 4. Finally, addressing the ambiguities in the results and clarifying the differences between informative and non-informative texts would strengthen the paper's contributions.