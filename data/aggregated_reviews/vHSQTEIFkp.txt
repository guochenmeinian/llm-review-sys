ID: vHSQTEIFkp
Title: Accelerated Zeroth-order Method for Non-Smooth Stochastic Convex Optimization Problem with Infinite Variance
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 5, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel gradient-free (zeroth-order) clipped version of the stochastic similar triangles method for solving non-smooth stochastic convex optimization problems under a weaker infinite variance assumption. The authors derive optimal iteration and oracle complexity bounds for both convex and strongly convex cases. The work builds upon previous algorithms adjusted for zero-order oracles, aiming to optimize non-smooth stochastic convex problems with infinite variance. The paper also analyzes a zeroth-order method for non-smooth stochastic optimization under heavy-tailed and adversarial noise, combining ball-averaging-based smoothing and gradient clipping techniques.

### Strengths and Weaknesses
Strengths:
- The problem addressed is well-motivated, with a clear write-up that highlights the novelty and contributions.
- The paper introduces concepts with clarity and is well-organized, aiding reader comprehension.
- The topic is significant, particularly in handling heavy-tailed/adversarial noise, and the theoretical bounds appear close to optimal.

Weaknesses:
- The absence of experimental results limits the demonstration of the method's feasibility and effectiveness in practical applications.
- The paper lacks a Conclusion section, making it feel incomplete.
- Minor typographical errors and unclear definitions in equations detract from clarity.
- The motivation for the research is insufficiently detailed, relying on citations without concrete examples.

### Suggestions for Improvement
We recommend that the authors improve the paper by including experimental results to demonstrate the practical applicability of the proposed methods. Additionally, we suggest adding a Conclusion section to provide a definitive end to the work. The authors should clarify the definitions in equations, particularly regarding the distribution of variables, and ensure that all terms are consistently defined. Furthermore, we encourage the authors to elaborate on the motivation for their research by including specific examples of non-smooth stochastic convex optimization problems. Lastly, a dedicated section for related work would enhance the paper's depth.