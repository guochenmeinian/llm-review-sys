ID: 0QpwcDPiHT
Title: Language Models Implement Simple Word2Vec-style Vector Arithmetic
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 8, 5, 4, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents evidence that large language models (LLMs) utilize a computational mechanism akin to traditional word embeddings, specifically employing vector arithmetic to encode abstract relations. The authors investigate this mechanism through experiments on three tasks: retrieving country capitals, converting words to uppercase, and forming past tense verbs. They find that the information flow in LLMs can be decomposed into stages: argument preparation, function application, and saturation. Additionally, the paper analyzes how LLMs recall information during in-context learning, focusing on the role of the feedforward network (FFN) layer. The authors propose that LLMs implement a basic vector-addition mechanism, which is significant for certain tasks, particularly one-to-one relations like mapping countries to their capitals. However, the generalizability of this conclusion to other functions and the effectiveness of vector addition compared to other methods, such as vector rotation, remain unclear.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, making it a pleasure to read.
- It employs a range of LMs to explore the arithmetic mechanism, enhancing the robustness of the findings.
- The novel approach to isolating function application through vector addition demonstrates the model's ability to generate intended outputs.
- The experiments are well-conducted, contributing valuable insights into LLM operations.
- The authors have effectively addressed previous concerns raised by reviewers, demonstrating a commitment to improving the clarity and accuracy of their work.

Weaknesses:
- The focus on the country-capital task, while interesting, limits the exploration of other tasks; a layer-by-layer analysis of early decoding outputs for all tasks is encouraged.
- The paper lacks a thorough discussion of incorrect outputs from LMs, which is crucial for understanding their limitations; examples of failures should be included.
- The analysis does not sufficiently address the generalizability of findings across different models and tasks, raising concerns about the robustness of the conclusions.
- The reliance on the analogy to word2vec has been criticized as potentially misleading, detracting from the paper's primary contributions.
- There is ambiguity in interpreting experimental results, particularly regarding the argument formation stages and their relationship to function application.

### Suggestions for Improvement
We recommend that the authors improve the discussion of when and why LMs produce incorrect outputs by including examples of failures in tasks such as identifying capitals or converting words. Additionally, we suggest expanding the analysis to include a systematic study of various relations beyond the selected tasks to strengthen the claims made. Clarifying the early decoding process with precise descriptions and equations would enhance the paper's self-containment. Furthermore, we recommend conducting broader experiments and/or theoretical analyses to address concerns about the generalizability of their conclusions and the effectiveness of vector addition versus other methods. Clarifying the interpretation of experimental results, particularly in figures that illustrate argument formation and function application, is also essential. Finally, revising the title, abstract, and introduction to remove the word2vec connection and instead emphasize the interpretability aspects of the FFN layer could better align with the paper's core contributions.