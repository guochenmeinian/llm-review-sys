ID: 56UYArtXyA
Title: FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FreeAL, a collaborative learning framework where a large language model (LLM) acts as an active annotator to generate coarse-grained annotations, while a student language model (SLM) is trained on these pseudo labels. FreeAL operates in an unsupervised setting, enhancing the performance of both LLMs and SLMs. The process involves generating an initial demonstration set, followed by an iterative loop of LLM annotation and SLM training using the DivideMix technique to filter high-quality samples.

### Strengths and Weaknesses
Strengths:
- The method provides a practical approach for LLM-based unsupervised learning, addressing real-world annotation costs.
- The self-generation of an initial demonstration set is both interesting and effective.
- The integration of various techniques, such as DivideMix and small loss selection, is well-motivated and clearly ablated.

Weaknesses:
- FreeAL's comparison with active learning methods is problematic, as it annotates the entire dataset rather than a subset, which may misrepresent its efficiency.
- The paper lacks comparisons with other zero-shot data generation methods, which could better illustrate FreeAL's novelty.
- Concerns exist regarding the reliability of performance improvements attributed to better labels, as random labeled demonstrations may yield similar results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the comparison with active learning methods by either removing it or relocating it to the appendix. It is crucial to display the size of the to-be-labeled set to understand the cost implications of FreeAL at each iteration. Additionally, we suggest including comparisons with zero-shot data generation methods like ZeroGen, ProGen, and SunGen to provide a more comprehensive evaluation. Lastly, addressing the potential for LLM/SLM self-improvement without interaction and clarifying the use of equations in the methodology would enhance the paper's rigor.