ID: mlxRLIy7kc
Title: Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Language-Quantized AutoEncoder (LQAE), a method for unsupervised image-text alignment that quantizes image embeddings using a language model's codebook. The authors aim to enhance few-shot multi-modal learning by reconstructing images as sequences of text tokens, leveraging a frozen BERT model to impose language structure on the learned representations. The study includes extensive experiments and ablation analyses, demonstrating competitive performance in tasks such as image classification and visual question answering. The authors conducted experiments comparing LQAE with MiniGPT-4 on few-shot image classification and FastVQA benchmarks, showing that LQAE outperforms baseline methods without access to text-image pairs and approaches MiniGPT-4's performance with minimal few-shot examples. They emphasize LQAE's potential for applications in domains with limited paired data, such as audio-language understanding.

### Strengths and Weaknesses
Strengths:
- The paper features a clear presentation and well-structured writing, effectively demonstrating the proposed method and experimental results.
- The novel idea of using a BERT model to guide latent representations towards a language-like structure is intriguing.
- A comprehensive ablation study provides valuable insights into the method's components and their roles.
- The additional experiments comparing LQAE with MiniGPT-4 provide compelling evidence of its competitive performance, particularly in unsupervised training.
- The authors effectively highlight the importance of data efficiency and the challenges associated with acquiring paired data.

Weaknesses:
- The experimental results lack robustness, with the best performance reaching only 54% top-1 accuracy in 2-way image classification, and performance does not scale with increased training data.
- LQAE does not outperform MiniGPT-4, which may limit its perceived effectiveness.
- The method's reliance on quantized tokens may lead to loss of visual information compared to models that use soft prompts, such as MiniGPT-4 or LLaVa.
- Some methodological details, particularly regarding the loss function and gradient propagation, are unclear, raising questions about the model's operational mechanics.
- The motivation regarding the acquisition cost of paired data could be more convincingly articulated.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their experimental results by conducting multiple runs for each setting to assess statistical significance. Additionally, evaluating LQAE on a broader range of vision-language tasks, such as visual question answering or captioning, would strengthen the contribution. We suggest that the authors update the motivation to better portray the acquisition cost of paired data and incorporate more in-depth discussions on the data-efficiency benefits of LQAE to enhance clarity. Clarifying the gradient propagation process, particularly how the reconstruction loss interacts with the frozen BERT model, is essential for understanding the model's mechanics. Finally, while the comparison with MiniGPT-4 is strong, the authors might consider including a brief mention of OpenFlamingo for completeness, though it is not necessary to include it in detail.