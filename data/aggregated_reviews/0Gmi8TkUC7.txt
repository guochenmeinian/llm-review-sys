ID: 0Gmi8TkUC7
Title: GenAI Arena: An Open Evaluation Platform for Generative Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 7, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GenAI Arena, an innovative platform for evaluating generative AI models across text-to-image, image editing, and text-to-video tasks, utilizing community-driven voting for performance assessment. The authors report that 27 open-source generative models have been implemented, with over 6000 votes collected, demonstrating user engagement. The study also highlights the limitations of existing automated evaluation methods, suggesting that human rankings may provide a more accurate assessment of model performance. Additionally, the paper analyzes preference data quality through an expert review process, evaluating 350 items from GenAI-Bench, resulting in 303 valid evaluations. The authors report that 76.24% of votes are clearly reasonable, while 16.83% are vague, and only 6.94% are deemed wrong. They argue that runtime differences do not introduce significant bias, as personal preferences regarding wait times vary, and suggest that users can utilize a "Random Sample" button for quicker results.

### Strengths and Weaknesses
Strengths:
- The platform's user-centric approach to model evaluation offers a more nuanced understanding of performance compared to traditional metrics.
- A significant number of models are implemented, and the platform is operational, having collected substantial user data.
- The use of an Elo rating system for ranking models based on user votes enhances the evaluation process's transparency and dynamism.
- The expert review process is well-documented, providing clear statistics on the quality of votes, with the majority classified as reasonable, indicating a high level of reliability in the preference data.

Weaknesses:
- The claim that standard assessment methods are ineffective lacks supporting evidence, and the paper does not provide a comparison of user rankings with traditional metrics.
- There is insufficient clarity on how new models can be integrated into the benchmarking process.
- The dataset's balance and representativeness are questionable, and the paper does not address potential biases in user voting.
- The explanation regarding the potential bias from long wait times is insufficient, as it does not fully address concerns about user dropout during voting.
- The reliance on the "Random Sample" button could introduce bias if overused, and the authors do not adequately discuss strategies to mitigate this risk.

### Suggestions for Improvement
We recommend that the authors improve the validation of their claim regarding the inadequacy of standard evaluation methods by providing comparative performance metrics. Additionally, clarifying the process for contributing new models to the platform would enhance user understanding. It would be beneficial to supplement user-generated datasets with a more balanced dataset to ensure comprehensive evaluation coverage. We suggest exploring methods to ensure fair comparisons across models, such as standardizing input conditions and addressing the impact of varying inference speeds on user voting. Furthermore, we recommend that the authors improve the discussion surrounding the expert review process to clarify how subjective votes were identified and addressed. A more thorough analysis of the potential biases introduced by varying runtimes, specifically addressing how user dropout could affect data quality, would also be valuable. Lastly, we encourage the authors to elaborate on strategies to manage the use of the "Random Sample" button to prevent bias in the collected data.