ID: FCwfZj1bQl
Title: Anytime-Competitive Reinforcement Learning with Policy Prior
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Anytime-Constrained Markov Decision Process (A-CMDP), focusing on optimizing expected rewards while ensuring that cumulative costs do not exceed a relaxed version of a policy prior's cost at each step. The authors propose a projection-based algorithm, Anytime-Constrained Decision-making (ACD), which guarantees these constraints by projecting outputs into safe action sets. They further develop the Anytime-Constrained Reinforcement Learning (ACRL) algorithm, providing theoretical analysis on regret performance and its dependence on relaxation parameters and the policy prior.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant and novel problem relevant to safety-critical applications, introducing the A-CMDP framework.
- ACD is a novel algorithm that ensures anytime constraints by projecting machine learning policy outputs into safe action sets.
- The theoretical analysis of ACRL's regret performance is rigorous and insightful.
- Empirical results are presented, although they are relegated to the appendix.

Weaknesses:
- The strong assumptions regarding Lipschitz continuity and the telescoping property of the policy prior may limit applicability.
- The reliance on querying the safe policy prior for unvisited states may not be feasible in practice.
- The exploration-exploitation trade-off and the impact of relaxation parameters on ACRL's robustness are insufficiently discussed.
- The writing quality needs improvement, with some definitions and assumptions being unclear.

### Suggestions for Improvement
We recommend that the authors include experimental results in the main body of the paper to enhance clarity and impact. Additionally, addressing the scalability issues more explicitly in the text would be beneficial. The authors should clarify the definitions and assumptions, particularly regarding the initialized distribution and the linear kernel assumption, to avoid confusion. It would also be advantageous to discuss the differences between this work and relevant literature, such as Ref-1, and to consider evaluating the method on standard benchmark problems like Safety-Gym. Lastly, exploring how to relax strong assumptions or estimate them online could broaden the applicability of their work.