ID: jOuPR9IH00
Title: Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 4, 5, 3, -1, -1, -1, -1
Original Confidences: 3, 5, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies offline reinforcement learning (RL) with non-linear function approximation, motivated by the suboptimal dependency on function class complexity in existing sample complexity guarantees. The authors propose an oracle-efficient algorithm that achieves minimax optimal problem-dependent regret when bounds are specialized to the linear case and introduce a new coverage definition. Additionally, the paper considers variance-weighted least-squares regression under a uniform data coverage assumption, showing that the proposed algorithm obtains a sub-optimality bound scaling with the $D^2$-divergence of the offline dataset. The authors also present a pessimistic nonlinear least-squares value iteration algorithm, claiming it is both computationally efficient and optimal regarding the complexity of the nonlinear function class.

### Strengths and Weaknesses
Strengths:
- The paper is technically sound, presenting new ideas in algorithm design and dataset coverage formulation.
- The proposed approach achieves a minimax optimal rate in non-linear function approximation when bounds are translated to linear.
- Clear presentation, with relevant results for the offline RL community.

Weaknesses:
- The requirement for uniform coverage or a non-linear bonus oracle is a significant limitation; the latter simplifies the challenges of pessimism in offline RL, while the former is too strong for fair comparisons.
- A lack of clear comparisons to prior work hinders evaluation, with no discussion of various axes such as dependency on $\epsilon$, function classes, and computational efficiency.
- The paper's presentation needs improvement, particularly in terminology consistency and clarity of definitions.
- Claims regarding computational efficiency and novelty of the proposed algorithm may be overstated, as it shifts computational burdens to the oracle for bonus function optimization.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their presentation, particularly in defining terms consistently and explaining the $D^2$-divergence and bonus function in more detail. Additionally, we suggest providing a comprehensive comparison to prior work across multiple axes, including the dependency on $\epsilon$ and function classes, possibly through a comparative table. The authors should also address the strong assumptions regarding uniform data coverage and clarify the necessity of pessimism under this assumption. Finally, we encourage the authors to critically evaluate the computational efficiency of their algorithm and substantiate claims about its novelty and optimality in the broader context of nonlinear function classes.