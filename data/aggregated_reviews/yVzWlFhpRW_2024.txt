ID: yVzWlFhpRW
Title: Excluding the Irrelevant: Focusing Reinforcement Learning through Continuous Action Masking
Conference: NeurIPS
Year: 2024
Number of Reviews: 24
Original Ratings: 6, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents methods for continuous action masking in reinforcement learning (RL) to enhance convergence stability and sample efficiency. The authors propose three action masking techniques—ray masks, hypercube transform masks, and distributional masks—aimed at refining the action space based on state-specific relevance. They discuss the implications of using a modified policy gradient approach, particularly focusing on the mapping function $\mathcal{F}$ that relates the action space to the relevant action space. The authors argue that using $\nabla \log \pi^r_\theta(a^r \| s)$ instead of $\nabla \log \pi_\theta(a \| s)$ is essential for accurately capturing the gradient of the modified distribution. Empirical evaluations using Proximal Policy Optimization (PPO) across control tasks like Walker2D and Ant demonstrate improved final rewards and faster convergence compared to baseline methods without action masking. The paper also explores the effects of non-bijective mapping functions and the implications of action masking on policy updates.

### Strengths and Weaknesses
Strengths:
- The proposed action masking methods cover a broad spectrum of action mask definitions, potentially improving agent performance in complex environments.
- The paper is well-written, with clear mathematical formulations and sound empirical derivations.
- The originality of the approach is notable, as it applies continuous action masking to RL, a topic that has not been extensively explored.
- The use of illustrative examples, including numerical evaluations, enhances the clarity of complex concepts.
- Empirical results from established benchmarks support the proposed method's effectiveness.
- The authors acknowledge the limitations of their approach and the need for further investigation into gradient approximation.

Weaknesses:
- The ease of recovering action masking criteria, particularly under complex schemes, is unclear, raising concerns about practical applicability.
- Experimental results are limited to similar control tasks, which may not convincingly demonstrate the methods' effectiveness across diverse scenarios.
- The paper's assumptions, such as the definition of action relevance and the bijectivity of certain mappings, require further clarification and justification.
- The analogy involving beta distributions is criticized as being incorrect or overly abstract.
- The paper lacks experimental evidence comparing policy-masking to environment-masking, which raises questions about the practical benefits of the proposed approach.
- Concerns are raised about the off-policy nature of the distributional mask and its implications for practical applications.
- The gradient approximation may introduce bias, raising concerns about its empirical triviality.
- The handling of non-convex relevant action sets is not convincingly addressed, potentially oversimplifying complex scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of action relevance in Section 3 and provide motivation for the state-dependent nature of A(s) in the generator mask. Additionally, justifying the design choices for action relevance in the experiments and comparing the proposed methods against simpler action masking baselines would strengthen the evaluation. We suggest conducting experimental comparisons between policy-masking and environment-masking to validate the claimed advantages of the proposed approach. Addressing the potential off-policy nature of the distributional mask and its implications on performance should also be prioritized. Lastly, we encourage the authors to provide more nuanced insights into the impact of relevant action sets on decision-making and to address the complexities of non-convex relevant action sets in more detail to enhance the robustness of the paper's claims.