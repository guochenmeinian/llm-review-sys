ID: xSziO6gQgG
Title: Implicit Optimization Bias of Next-token Prediction in Linear Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the implicit bias of gradient descent in the Next-Token Prediction (NTP) problem, formulating it as minimizing the cross-entropy (CE) loss over distinct contexts with sparse conditional probabilities. The authors propose necessary conditions for achieving the entropy lower bound, specifically the NTP-compatible and NTP-separable conditions, and demonstrate that overparameterization (where the embedding dimension \(d\) exceeds the number of distinct contexts \(m\)) is a sufficient condition for these to hold. They also prove the directional convergence of the CE loss minimizer and the GD iterate towards the solution of an NTP-SVM.

### Strengths and Weaknesses
Strengths:
1. The investigation of the optimization path and implicit bias of NTP is both interesting and significant.
2. The formulation of NTP as CE minimization over distinct contexts is novel.
3. The theoretical results presented are rigorous, and the proofs are solid.

Weaknesses:
1. The requirement that \(d > m\) for the NTP-compatible and separable conditions raises concerns about practical applicability, as \(d\) is often smaller than the number of training data.
2. Some sections lack clarity, particularly regarding the constraints in equation 4 and the uniqueness of the solution \(W^*\). The claim that (3a) holds if and only if the data satisfies the NTP-compatible condition needs a more rigorous proof.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the text, particularly in sections where complex concepts are introduced, such as lines 154-157 and the proof regarding the uniqueness of \(W^*\). Additionally, we suggest providing a more thorough discussion on the practical implications of the overparameterization condition and clarifying the relationship between the structure of weights and generalization in NTP, as this connection is currently unclear. Finally, consider moving some dense discussions from Sections 6 and 7 to the Appendix to enhance the overall flow of the paper.