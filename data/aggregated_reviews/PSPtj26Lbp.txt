ID: PSPtj26Lbp
Title: L4GM: Large 4D Gaussian Reconstruction Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents L4GM, an efficient large 4D Gaussian reconstruction model designed to generate animated objects from videos using a single feed-forward approach. L4GM utilizes ImageDream and LGM to create multiview images from the first frame, incorporating cross-view and temporal attention blocks. The model introduces autoregressive reconstruction and 4D interpolation, enhancing the smoothness of the generated 4D outputs. The authors trained L4GM on animated objects from Objaverse and conducted extensive experiments to validate its effectiveness.

### Strengths and Weaknesses
Strengths:
1. L4GM demonstrates good efficiency and performance as a feed-forward technique, particularly for high-resolution 4D generation.
2. The overall pipeline is convincing, and the paper is clearly articulated.
3. The contribution of 12M videos rendered from Objaverse adds significant value to large-scale 4D data.

Weaknesses:
1. The novelty of the work is questionable, as it appears to extend LGM for 4D generation with straightforward techniques like temporal and cross-view attention.
2. The approach of repeating multiview images from the initial timestep for subsequent frames is seen as a temporary fix rather than a robust solution, particularly in scenarios involving motion changes. The authors should consider using ImageDream+LGM for more diverse multiview inputs. Additionally, more quantitative and qualitative ablation studies are needed for different input settings.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the work by exploring more innovative techniques beyond the straightforward extensions of existing methods. Additionally, addressing the limitations of using repeated multiview images is crucial; consider employing ImageDream+LGM for enhanced multiview input diversity and conducting comprehensive ablation studies on various input configurations. Clarifying the dataset's open-source status and providing more empirical support for claims of generalization to in-the-wild videos would strengthen the paper. Finally, we suggest including more details about the architecture and training process to facilitate reproducibility.