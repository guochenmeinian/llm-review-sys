ID: JlWn80mTJi
Title: The Implicit Bias of Gradient Descent on Separable Multiclass Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 7, 4, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the implicit bias of gradient descent on separable multiclass data using Permutation Equivariant and Relative Margin-based (PERM) losses. The authors extend the exponential tail property, commonly applied in binary classification, to multiclass classification, demonstrating that, under certain conditions, gradient descent converges directionally towards the hard-margin multiclass SVM solution.

### Strengths and Weaknesses
Strengths:  
- The paper is well-structured and clearly written, making it accessible to readers.  
- It effectively addresses the gap in the literature regarding the implicit bias of multiclass classification, extending beyond cross-entropy loss.  
- The theoretical results are convincing, with a clear presentation of proof ideas.

Weaknesses:  
- The paper lacks sufficient detail in several technical steps, leading to potential misunderstandings regarding the soundness of the proof.  
- There is a notable absence of numerical results to validate theoretical claims, which could illustrate the practical implications of the findings.  
- Some results do not significantly advance the understanding of implicit bias compared to previous works focused on cross-entropy loss.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proof by providing more detailed explanations of the technical steps, particularly those that raised questions among reviewers. Additionally, including numerical experiments would enhance the paper's contribution by demonstrating the practical relevance of the theoretical results. A discussion on the restrictive nature of the assumptions on the loss function would also be beneficial.