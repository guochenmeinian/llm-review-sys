ID: dWDEBW2raJ
Title: Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 7, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel modification of the neural tangent kernel (NTK) termed modular NTK (mNTK), which decomposes the NTK into tangent kernels for disjoint modules within a network. This decomposition facilitates module-level analysis of training dynamics. The authors find that the principal eigenvalue of mNTKs is significantly larger than others, and its variation is synchronous across modules. This insight allows for selective updates of module subsets during training, optimizing computational resource allocation. The authors introduce modular adaptive training (MAT), which halts backpropagation for modules with principal eigenvalues below a certain threshold, demonstrating that MAT achieves comparable performance to vanilla models while requiring fewer FLOPs and improving generalization.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a novel, theoretically motivated method supported by experiments.
- The approach addresses an important topic in reducing computational resources for large model training.
- The method shows potential for high impact and is applicable across various architectures.

Weaknesses:
- The evaluation primarily focuses on perplexity scores of pretrained models, lacking comparisons on fine-tuned downstream tasks, which are more relevant for practitioners.
- The connection between low principal eigenvalues and nuisance features requires further exploration, particularly regarding their impact on downstream task performance.
- The computational cost of calculating the NTK and its eigenvalues may offset the claimed speedups, and the performance improvements are not substantial enough to guarantee widespread adoption.

### Suggestions for Improvement
We recommend that the authors improve their evaluation by comparing the performance of MAT on fine-tuned downstream tasks against vanilla pretrained models. Additionally, exploring the effects of MAT on the fine-tuning process could provide valuable insights into its efficacy. Clarifying the stability of the hyperparameter $\alpha$ mentioned in line 221 is necessary, as it appears to be a potential typo. Finally, providing more evidence to support the interpretation of mNTK and its implications across different architectures would strengthen the paper's contributions.