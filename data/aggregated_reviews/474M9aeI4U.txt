ID: 474M9aeI4U
Title: COVE: Unleashing the Diffusion Feature Correspondence for Consistent Video Editing
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 7, 6, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents COrrespondence-guided Video Editing (COVE), a method that enhances video editing using pretrained text-to-image (T2I) diffusion models. It addresses the challenge of maintaining temporal consistency by leveraging diffusion feature correspondence and employs a sliding-window strategy to efficiently track features across frames. COVE reduces GPU memory usage through a temporal-dimensional token merging strategy and integrates seamlessly into existing T2I models without additional training, demonstrating superior performance in various scenarios.

### Strengths and Weaknesses
Strengths:
- The method is reasonable, easy to follow, and well-presented.
- The sliding-window strategy is useful for video editing, and the results indicate state-of-the-art performance in comparison to existing methods.
- The paper is well-written with clear motivations and extensive ablations to evaluate effectiveness.

Weaknesses:
- The experimental performance improvement appears limited, primarily showcasing style/color changes that are easily achievable with prior methods, raising doubts about the claim of achieving state-of-the-art performance.
- The reliance on diffusion feature correspondences may pose challenges for videos with significant motion changes, necessitating a more detailed discussion on ensuring correspondence accuracy.
- The novelty of the sliding-window approach is somewhat limited, as similar methods can be applied using optical flow, and the paper lacks a critical ablation study comparing these approaches.

### Suggestions for Improvement
We recommend that the authors improve the experimental analysis by including comparisons with methods like VideoSwap, particularly for cases involving significant shape changes. Additionally, a more detailed discussion on the accuracy of correspondences in videos with large content motions is necessary. The authors should also consider including a comprehensive comparison with optical flow-based methods and other correspondence-based techniques to clarify the advantages of their approach. Lastly, addressing the limitations of the sliding-window strategy in long video edits and providing quantitative metrics for evaluating diffusion feature correspondences would strengthen the paper.