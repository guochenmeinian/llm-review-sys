ID: J8m0DEjxEO
Title: AttnGCG: Enhancing Adversarial Attacks on Language Models with Attention Manipulation
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 5, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel adversarial attack method for large language models (LLMs) called AttnGCG, which incorporates a regularization term to maximize attention scores on adversarial suffixes, thereby minimizing attention on other tokens. The authors claim that this modification leads to improved attack success rates compared to existing methods like GCG, AutoDAN, and ICA. The experiments conducted cover various models and evaluation metrics, demonstrating the effectiveness of the proposed approach.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant research question regarding adversarial attacks on LLMs, contributing to interpretability and potential mitigation strategies.
- The experimental design is comprehensive, comparing the proposed method against multiple state-of-the-art attacks and covering a range of models.

Weaknesses:
- The measurement and interpretation of attention scores are problematic, as they may not be normalized, complicating comparisons across attacks and undermining the paper's conclusions.
- The transferability of the proposed attack across different goal prompts is unclear, raising concerns about its generalizability.
- The rationale behind maximizing attention on suffix tokens as a means to strengthen attacks is questioned, particularly in light of conflicting results in the data.

### Suggestions for Improvement
We recommend that the authors improve the normalization of attention scores to ensure meaningful comparisons across different attacks. Additionally, conducting controlled experiments to clarify the transferability of the proposed attack across various goal prompts would strengthen the findings. It would also be beneficial to analyze the potential for harmful outputs that may arise from minimizing attention on goal tokens. Finally, we suggest including evaluations of defenses against adversarial attacks, particularly those based on perplexity, to provide a more comprehensive understanding of the proposed method's effectiveness.