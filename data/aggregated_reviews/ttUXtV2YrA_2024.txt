ID: ttUXtV2YrA
Title: Revisiting the Integration of Convolution and Attention for Vision Backbone
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 4, 5, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to address scalability issues in vision transformers by integrating convolutions (Convs) and multi-head self-attentions (MHSAs) at different granularities. The authors propose using Convs for fine-grained feature extraction and MHSAs for coarse-grained semantic slots in parallel. They introduce fully differentiable soft clustering and dispatching modules to facilitate local-global fusion, resulting in the GLMix framework. Extensive experiments demonstrate the method's efficiency and effectiveness across various vision tasks, indicating improved performance and interpretability, particularly in semantic segmentation with weak supervision.

### Strengths and Weaknesses
Strengths:
1. Innovative Integration Approach: The integration of Convs and MHSAs at different granularities effectively addresses scalability issues in vision transformers.
2. Efficient Local-Global Fusion: The soft clustering and dispatching modules enable significant advancements in local-global feature fusion.
3. Extensive Empirical Validation: The method achieves state-of-the-art results on benchmarks like ImageNet-1k, COCO, and ADE20K, showcasing a favorable performance-computation trade-off and enhanced model interpretability.

Weaknesses:
1. Complexity of Implementation: The introduction of soft clustering and dispatching modules complicates the implementation, potentially hindering practical deployment.
2. Static Number of Semantic Slots: Utilizing a static number of semantic slots may lead to inefficiencies and redundancy.
3. Limited Scope of Clustering Strategy: The current clustering strategy could be optimized further, as it may still be computationally intensive for real-time applications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the GLMix block's components and their learning objectives, particularly regarding the dispatching module's role. Additionally, the authors should explore the sensitivity of the GLMix model's performance to the choice of clustering strategy and consider testing alternative methods. We suggest investigating the implications of using a dynamic number of semantic slots based on input image complexity to enhance efficiency. Furthermore, providing visualizations of the slots' evolution during training and clarifying the architecture of GLNet would be beneficial. Lastly, we encourage the authors to address the scalability of their method with very high-resolution images or videos and to provide throughput details for the performance tables.