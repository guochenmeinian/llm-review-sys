ID: 5tIG2KZogL
Title: Supervised Kernel Thinning
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a kernel thinning approach to non-parametric regression, specifically for Nadaraya-Watson (NW) and Kernel Ridge Regression (KRR) estimators. The authors propose using carefully selected coresets to enhance computational efficiency while maintaining statistical properties. Theoretical results on approximation error are established, and numerical experiments are conducted using synthetic data and the California housing dataset.

### Strengths and Weaknesses
Strengths:
1. The paper is well-organized and clearly motivated, providing sufficient background.
2. Solid theoretical results support the proposed method.
3. Experimental results are included to validate the method.

Weaknesses:
1. The significance of the method's contribution remains unclear; more comparisons with existing efficient non-parametric regression methods are needed.
2. Applications are limited to small datasets, raising questions about the method's effectiveness on larger datasets.
3. The choice of baseline estimators (NW/KRR with naive thinning) could be improved by including more competitive, state-of-the-art methods.

### Suggestions for Improvement
We recommend that the authors improve the empirical analysis by including comparisons with a broader range of large-scale kernel methods beyond RPCholesky. Additionally, addressing the limitations of the method on real-world data and discussing potential failure cases would enhance the paper's robustness. Clarifying the theoretical assumptions, particularly regarding kernel selection and generalization to multi-dimensional response data, would also strengthen the manuscript. Furthermore, we suggest providing an algorithmic description of the Kernel Thinning process and correcting any unclear notations or incomplete sentences in the text.