ID: yEewbkBNzi
Title: Convergence of Adam Under Relaxed Assumptions
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 6, 6, 8, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive study on the convergence of the Adam algorithm under relaxed assumptions, specifically targeting non-convex objectives. The authors propose a new proof strategy that eliminates the bounded gradient assumption, introducing a non-uniform smoothness condition termed $(\rho, L_0, L_1)$ smoothness. They demonstrate that Adam achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$ under this condition, while also introducing a variance-reduced version, VRAdam, which achieves a convergence rate of $\mathcal{O}(1/\sqrt[3]{T^2})$. The results are significant as they provide theoretical insights into the performance of Adam without the need for bounded gradients.

### Strengths and Weaknesses
Strengths:
1. The paper effectively relaxes the bounded gradient assumption, a notable challenge in optimization analysis.
2. The new analysis framework is clear and provides a solid understanding of the convergence behavior of Adam.
3. The introduction of the variance-reduced variant of Adam is a valuable contribution, enhancing the algorithm's efficiency.

Weaknesses:
1. The requirement for almost surely bounded noise remains a strong assumption, potentially limiting practical applicability.
2. The convergence results depend on specific parameter settings, which may not be easily obtainable in real-world scenarios.
3. The lack of numerical demonstrations for the variance-reduced version raises questions about its practical effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the paper by including numerical demonstrations to validate the theoretical results, particularly for the variance-reduced version of Adam. Additionally, we suggest clarifying the assumptions regarding bounded noise, as this remains a significant limitation. It would also be beneficial to provide a formal theorem for the deterministic setting and summarize recent results on Adam convergence in a comparative table. Lastly, addressing the implications of the momentum term in the convergence analysis could enhance the theoretical understanding of the algorithm's performance.