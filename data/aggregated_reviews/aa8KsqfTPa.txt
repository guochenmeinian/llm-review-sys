ID: aa8KsqfTPa
Title: MarioGPT: Open-Ended Text2Level Generation through Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 6, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MarioGPT, a method for generating new Super Mario Bros levels using novelty search and fine-tuning from GPT-2. The novelty search involves mutating generated levels based on a novelty criterion, specifically the mean distance to existing levels. Experimental results indicate that MarioGPT achieves better reconstruction accuracy than LSTM, with 88.33% of levels being playable by an A* agent. The authors also propose two algorithms: MarioGPT for predicting game slices and a novelty search for enhancing diversity in generated levels.

### Strengths and Weaknesses
Strengths:
- The idea of generating Super Mario Bros levels is intriguing and beneficial for both the reinforcement learning and gaming communities.
- The finetuned GPT demonstrates the capability to generate playable levels, providing valuable insights into procedural content generation.

Weaknesses:
- The evaluation criteria, particularly "playable by an A* agent," inadequately measure level quality, primarily reflecting simplicity. The empirical evaluation in section 4.4 lacks objectivity and baseline comparisons.
- The baseline comparison with LSTM is insufficient, and the performance of a non-finetuned GPT-2 or GPT-3/3.5/4 is not explored. The claim that LSTM is not promptable is questionable.
- The simplicity of generated levels diminishes the work's significance, as they lack complexity and variety, reducing their relevance in AI and gaming.

### Suggestions for Improvement
We recommend that the authors improve the evaluation criteria to include more robust measures of level quality beyond playability by an A* agent. Additionally, providing explicit demonstrations of controllability in level generation would enhance the paper's contributions. The authors should also consider including comparisons with non-finetuned GPT-2 and other models like GPT-3/3.5/4 to strengthen the baseline evaluations. Finally, exploring more complex level designs and addressing the simplicity of generated levels would significantly enhance the impact of this work.