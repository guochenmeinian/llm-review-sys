ID: Z28nPtAVxx
Title: Optimal Extragradient-Based Algorithms for Stochastic Variational Inequalities with Separable Structure
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the stochastic monotone variational inequality problem and proposes the Accelerated Gradient - Extragradient (AG-EG) algorithm, which achieves optimal convergence rates for strongly monotone VI problems. The authors also extend the formulation to cases with convex but unbounded constraint sets, matching known convergence results. The analysis provides optimal error bounds in various settings and introduces two variants of the AG-EG algorithm, including one with scheduled restarting.

### Strengths and Weaknesses
Strengths:  
- The proposed algorithm matches several lower bounds simultaneously.  
- The analysis yields optimal error bounds, contributing significantly to the field.  
- The paper is generally well-written and easy to follow, with excellent technical details.  

Weaknesses:  
- There is a dependence on the initial distance of the iterate to the solution, contradicting the claim of independence from the constraint set.  
- The optimal dependence on the noise term appears suboptimal compared to existing works.  
- The notation in Algorithm 1 is confusing, and the efficacy of the approach lacks empirical demonstration.  
- The absence of a related work section and a comparative table of error bounds hinders understanding of the paper's position in the literature.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the notation in Algorithm 1 and provide empirical studies to demonstrate the efficacy of the AG-EG algorithm in applications such as Reinforcement Learning or quadratic games. Additionally, we suggest including a table comparing the complexity results with state-of-the-art algorithms for solving stochastic VI problems and adding a related work section to contextualize their contributions better. Furthermore, the authors should clarify the assumptions regarding the deterministic and stochastic settings to avoid confusion.