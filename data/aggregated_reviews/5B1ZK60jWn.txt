ID: 5B1ZK60jWn
Title: A Spectral Theory of Neural Prediction and Alignment
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 4, 7, 9, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a spectral theoretical framework to explore how the geometrical properties of deep neural networks (DNNs) affect their performance in predicting neural responses. The authors analyze the generalization error as a measure of fit, providing insights into the relationship between DNN representations and neural activity in visual cortical areas. They conduct experiments to investigate the influence of layer depth, dataset size, and training approaches on prediction performance. Additionally, the authors utilize a method that examines the geometry of prediction error by assessing the radius and dimensionality of error along eigenvectors of input data. They reveal that trained networks exhibit reduced error radius but increased error dimensionality for V2 - IT data, and explore the impact of adversarial training on model predictivity and generalization error in relation to training set size.

### Strengths and Weaknesses
Strengths:
- The authors establish a novel link between the predictivity of neural responses and the geometry of DNN representations.
- The paper includes a thorough literature review and employs both theoretical and empirical analyses.
- The paper effectively applies a recent methodology to enhance understanding of DNN-neural data representational alignment, with well-structured experiments that provide valuable insights into existing literature.

Weaknesses:
- The clarity and accessibility of the paper are lacking, making it difficult for readers to grasp the main arguments and results.
- The motivation for the chosen methodology lacks clarity, particularly regarding the significance of error radius and dimensionality in representational alignment.
- The distinction between artificial and biological neural networks is often unclear, leading to ambiguity in the discussion.
- The conclusions drawn from experiments are not clearly articulated, requiring readers to engage in extensive interpretation.
- Some key references are missing, and the assumptions regarding model and brain responses may be unrealistic.
- The standard approach for interpreting DNN-neural data alignment is inadequately explained, which may hinder comprehension.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing, particularly in Section 3, by summarizing key conclusions at the beginning or end of each paragraph. Additionally, we urge the authors to provide more background information on the spectral theoretical framework and clarify the meaning of key terms such as "neural predictivity." It would be beneficial to discuss the limitations and assumptions in greater detail, particularly regarding the deterministic nature of model and brain responses. We also suggest including more references, particularly those that address different prediction performances based on pretrained representations. Furthermore, we recommend that the authors improve the clarity of the motivation behind analyzing the radius and dimensionality of error, explicitly stating the insights this provides into representational alignment. The authors should clearly and simply present the conclusions from their experiments, possibly in a list format, and consider creating more intuitive graphical figures to emphasize key points. Lastly, a schematic plot illustrating the standard approach to DNN-neural data alignment versus their method would enhance understanding, and greater effort should be made to elucidate the significance of quantified values such as error radius and dimension along eigenvalues of the data's Gram matrix.