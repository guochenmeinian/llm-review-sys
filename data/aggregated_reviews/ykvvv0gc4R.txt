ID: ykvvv0gc4R
Title: Deep Momentum Multi-Marginal Schrödinger Bridge
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 2, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an algorithm, DMSB (Deep Multi-Marginal Momentum Schrödinger Bridge), aimed at efficiently solving the multi-marginal momentum Schrödinger Bridge problem in phase space. The authors propose a computationally tractable solver that adapts the iterative proportional fitting (IPF) algorithm to handle multiple marginal constraints, ensuring smooth trajectories by introducing stochasticity solely in the velocity component. The algorithm demonstrates improved performance in reconstructing lower-variance trends and recovering velocity fields in high-dimensional single-cell RNA sequencing data. Additionally, the paper addresses optimization problems involving path measures and constraints, emphasizing the importance of discussing the practical implications of the constraint \(K_{bridge}\) and the necessity of conducting experiments in real high-dimensional settings to strengthen the contribution. The authors also acknowledge the need for clearer presentation and technical details, including time discretization, the number of Bregman iterations, and the regularization term \(L_{reg}\).

### Strengths and Weaknesses
Strengths:  
- The paper builds on established results in Schrödinger bridges and convex optimization, effectively addressing the multi-marginal SB problem while ensuring smooth trajectories.  
- It introduces a novel formulation of marginal constraints and extends the IPF to the momentum SB, demonstrating significant improvements over baseline algorithms.  
- The experimental results convincingly show the algorithm's effectiveness in high-dimensional contexts, supported by detailed proofs and a clear presentation of the algorithm in the appendix.  
- The authors' acknowledgment of the need for clearer presentation and technical details is a positive step towards improving the paper's readability.  
- The discussion around the implications of the constraint \(K_{bridge}\) adds depth to the understanding of the methodology.  

Weaknesses:  
- The rationale for decoupling constraints and the avoidance of the “geometric averaging issue” needs clearer exposition.  
- The section on neural network parametrization lacks clarity, particularly in linking log-likelihood minimization to the mean matching objective.  
- The paper does not provide a complexity analysis of the algorithm concerning dimensions and the number of marginals, which raises concerns about scalability.  
- The lack of experiments in real high-dimensional settings is identified as a significant limitation.  
- The explanation of Proposition 4.4 remains unconvincing to some reviewers, particularly regarding the inclusion of the boundary condition \(K_{t_0}\).  
- Notation inconsistencies, particularly between \(\mu_0\) and \(\hat{\mu}_0\), have led to confusion and require clarification.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the rationale behind the decoupling of constraints and explicitly explain the avoidance of the “geometric averaging issue.” Additionally, we suggest providing a more detailed discussion on the differences between constraints in Eqs. 9b and 10b following Proposition 4.5. 

To enhance the section on neural network parametrization, we advise elaborating on how log-likelihood minimization ties to the mean matching objective, as briefly mentioned in appendix B.5. Furthermore, we encourage the authors to include a complexity analysis of the algorithm to address scalability concerns.

We also recommend conducting experiments in real high-dimensional settings to strengthen their contribution. It is crucial to include a discussion on the practical importance of the constraint \(K_{bridge}\) and to clarify the notation, especially the distinction between \(\mu_0\) and \(\hat{\mu}_0\). Lastly, we suggest open-sourcing the code to facilitate reproducibility and accessibility for practitioners, and providing a clearer explanation of the training scheme and discretization method in section 4.5.