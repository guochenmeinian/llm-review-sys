ID: gmVoaAxB1R
Title: Structure of universal formulas
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies function approximation in the context of the Kolmogorov-Arnold representation theorem, proposing definitions of expressivity classes that differentiate between pointwise and uniform approximation. The authors introduce function families resembling neural networks and provide approximation results. They explore the expressive power of fixed-complexity function families on [0, 1], defining three classes: $\mathcal{G}_0$, $\mathcal{G}_1$, and $\mathcal{G}_2$, with a focus on the difference sets $\mathcal{G}_0 \setminus \mathcal{G}_1$ and $\mathcal{G}_1 \setminus \mathcal{G}_2$. Additionally, the paper presents a critical examination of existing network approximation theorems, particularly highlighting the limitations of Cybenko's theorem and its applicability to real-world problems. The authors argue that while theoretical results exist, they often do not provide practical estimates for network size required to achieve specific accuracy levels, such as 90% on ImageNet. They emphasize that many quantitative results involve implicit constants that diminish their utility in practical applications and clarify that unbounded precision of parameters is a common assumption in theoretical studies of neural networks.

### Strengths and Weaknesses
Strengths:
- The definitions are systematic, and the arguments are clear.
- The mathematical framework is rigorously defined, and the paper is well-written.
- The exploration of expressiveness in largely unexplored areas is original and significant.
- The authors provide a thorough critique of existing approximation theorems, effectively highlighting the gap between theory and practical application.
- They clarify the necessity of unbounded precision in theoretical analyses, contributing to a deeper understanding of neural network capabilities.

Weaknesses:
- The reliance on infinite precision arithmetic limits the applicability of results to finite precision contexts, which may not interest the machine learning community.
- Many proofs lack mathematical rigor, containing vague statements and missing references.
- The focus on one-dimensional functions restricts the immediate applicability of results to machine learning.
- The paper lacks quantitative guarantees connecting theoretical claims to real-world implementations, which diminishes its practical relevance.
- The definitions and interpretations of key concepts, such as generalization, may lead to confusion and require clearer articulation.

### Suggestions for Improvement
We recommend that the authors improve the rigor of all proofs by eliminating vague statements and providing clear definitions and references. Specifically, clarify the limits in equations and provide precise citations for classical theorems referenced. Additionally, we suggest removing speculative claims regarding generalization and revising the misleading hierarchy of classes $G_1 \subset G_2$. The authors should also consider expanding the scope beyond one-dimensional functions to enhance relevance to machine learning. Furthermore, we recommend improving the clarity and precision of Theorem 2 and the mathematical formulation of the Pfaffian class to align with established definitions. We also suggest that the authors compare their non-standard notions of generalization with standard concepts from statistical learning theory to address potential inaccuracies. Finally, we encourage the authors to provide quantitative guarantees that link their theoretical results to practical applications, thereby enhancing the utility of their findings.