ID: YRemB4naKK
Title: Deterministic Policies for Constrained Reinforcement Learning in Polynomial Time
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 4, 5, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an algorithm that efficiently computes near-optimal deterministic policies for constrained reinforcement learning (CRL) problems. The authors propose three key ideas: 1) value-demand augmentation, 2) action-space approximate dynamic programming, and 3) time-space rounding. Under certain assumptions, the algorithm constitutes a fully polynomial-time approximation scheme (FPTAS) for various cost criteria, requiring recursive computation of policy costs over time and state space. The paper also addresses constrained MDPs with time-restricted (TR) properties, converting the original problem into an unconstrained MDP and accelerating the Bellman update for these constraints.

### Strengths and Weaknesses
Strengths:
- The paper introduces novel insights into safe reinforcement learning and constrained MDPs, providing unique perspectives on solving CRL problems.
- The algorithm is technically sound and demonstrates the ability to solve diverse problems efficiently.
- The analysis is comprehensive, and the writing is clear, particularly regarding mathematical notations.

Weaknesses:
- The paper lacks clarity in distinguishing its contributions from existing work, failing to adequately survey prior research in safe RL and constrained RL.
- The theoretical results, while promising, need comparison with existing algorithms to clarify their advantages.
- Empirical evaluation is necessary to validate the algorithm's efficiency, as the complexity remains high and its practical applicability is questionable.

### Suggestions for Improvement
We recommend that the authors improve the literature review to clearly articulate the differences and contributions of their work compared to existing research in safe RL and constrained RL. Specifically, they should reference relevant papers, such as those by Sootla et al. and Wachi et al., to contextualize their contributions. Additionally, we suggest incorporating comparisons of theoretical results with existing algorithms to highlight advantages. Furthermore, empirical evaluations should be included to substantiate claims of efficient computation. Lastly, clarifying the definitions and assumptions in the algorithm would enhance understanding and accessibility for readers.