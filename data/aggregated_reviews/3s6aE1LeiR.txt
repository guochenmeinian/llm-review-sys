ID: 3s6aE1LeiR
Title: From One to Zero: Causal Zero-Shot Neural Architecture Search by Intrinsic One-Shot Interventional Information
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 2, 3, 3, 3, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a 0-shot NAS method that identifies latent factors influencing architecture search, arguing that validation accuracy in one-shot NAS is unreliable. The authors propose using Gaussian intervention on data to evaluate operation performance, aiming to mitigate bias from validation dataset sampling. Experiments on CIFAR-10, NAS-Bench-201, and ImageNet demonstrate the method's efficiency in architecture searching.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow.
2. It considers latent factors affecting NAS validation, a topic often overlooked in existing literature.
3. The method employs causal inference techniques to address the NAS problem.

Weaknesses:
1. Some definitions and connotations are unclear, particularly regarding latent factors.
2. The algorithm appears overly simplistic, applying intervention techniques without addressing specific NAS characteristics, limiting its contribution.
3. The authors claim efficiency but do not compare their method with other 0-shot NAS approaches, which typically exhibit high efficiency.
4. The novelty is incremental compared to baseline methods like Zen-NAS and Shapley-NAS, and the performance improvement is not significant.
5. The evaluation lacks comprehensive comparisons with recent state-of-the-art methods, including Zen-NAS and ZiCo.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the connotation of latent factors and provide a more detailed explanation of why validation accuracy follows a Gaussian distribution. Additionally, the authors should clarify the definition of "true validation accuracy" and address the limitations of their method, particularly regarding its applicability to different search spaces, including transformer architectures. We also suggest including comparisons with baseline methods and recent works, such as GradSign and ZiCo, to strengthen the experimental evaluation. Finally, the authors should consider releasing their code to enhance reproducibility and address the correlation coefficient of predicted rankings with true rankings.