ID: M1dTz6QmuM
Title: The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 6, 6, 6, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 1, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical framework for understanding the learning dynamics of policy gradient reinforcement learning (RL) algorithms in high-dimensional settings. The authors propose the RL perceptron model, which employs a student-teacher design to analyze generalization dynamics in sequential decision-making tasks. They derive a set of ordinary differential equations (ODEs) that describe learning behaviors and explore various reward settings, optimal hyperparameters, and the speed-accuracy trade-off. The authors validate their theoretical findings through experiments in the Procgen Bossfight environment and Atari Pong, demonstrating that similar dynamics arise in practice. Additionally, the paper introduces a new problem definition and solution methods for decision-making in RL, arguing that existing methods lack theoretical results that align with practical applications. The authors claim their model exhibits similar behavior to existing methods concerning hyperparameters, supported by empirical illustrations in a specially designed environment called “Bossfight.” Furthermore, the paper analyzes the REINFORCE algorithm, emphasizing its foundational role in understanding learning dynamics within deep RL systems, and asserts that average-case performance provides more precise predictions about learner behavior than worst-case analyses.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured, clearly written, and offers a novel approach to analyzing high-dimensional RL systems.
- The theoretical framework effectively connects learning dynamics to practical scenarios, providing insights into hyperparameter optimization and reward structures.
- Extensive experiments illustrate the learning dynamics under different conditions, enhancing the paper's contributions to the field.
- The authors introduce a novel problem definition and solution methods for RL, particularly in policy optimization, and provide a new perspective inspired by statistical mechanics and dynamical systems.
- The focus on average-case performance allows for practical predictions, such as speed-accuracy trade-offs, validated through experiments on procedurally generated problems.

Weaknesses:
- The analysis of the latent feature space dimension $D$ is limited, with a fixed value of 900 in most cases, necessitating further exploration of its impact on learning dynamics.
- Some experimental results, particularly regarding episode lengths, may be misleading, as the claim about slower learning in shorter episodes does not align with total time step comparisons.
- The paper lacks a comprehensive evaluation across multiple environments, limiting the generalizability of the findings.
- While the motivation for theoretical guarantees in RL is crucial, the authors overlook existing global convergence results for policy-gradient methods with neural function classes, which contradicts their claims.
- The paper lacks context regarding its relation to standard RL models, failing to clarify limitations of previous methods and how their approach improves upon them.
- The empirical analysis lacks depth in comparing their model's performance against more complex architectures and real-world methods.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the latent dimension $D$ by providing empirical evidence on its influence on the proposed ODE-based learning dynamics. Additionally, clarify the experimental results related to episode lengths to avoid potential misinterpretations. To strengthen the manuscript, we suggest conducting further experiments across diverse environments to validate the applicability of the proposed model. We also recommend improving the clarity of their motivation by addressing existing global convergence results for policy-gradient methods. Furthermore, the authors should clearly articulate the limitations of previous RL models and how their approach offers advantages. It would be beneficial to formalize the problem definition and clarify the learning setting, including whether it pertains to finite or infinite horizons. Finally, we encourage the authors to define key terms and concepts used throughout the paper to improve overall clarity and understanding, and to include more comprehensive empirical comparisons with existing methods to better illustrate the practical implications of their findings.