ID: 6b6TfDBDOO
Title: Diffusion Imitation from Observation
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Diffusion Imitation from Observation (DIFO), an adversarial imitation learning method that employs a conditional diffusion model as the discriminator for policy learning. DIFO learns a diffusion model on expert and agent state transitions, incorporating an auxiliary binary classification objective to differentiate between the expert and agent. Experimental results indicate that DIFO can learn an imitation policy online using only expert demonstration observations, outperforming relevant learning from observation (LfO) baselines. The ablation studies confirm the necessity of both loss terms for effective diffusion model training.

### Strengths and Weaknesses
Strengths:
- The integration of conditional diffusion models into adversarial imitation learning is a novel approach.
- The loss objectives for the diffusion model are well-motivated and essential for downstream policy performance.
- The diffusion model effectively captures the expert distribution and shows generalizability in generated trajectories.
- DIFO demonstrates superior performance across various state-based and image-based environments compared to previous methods.

Weaknesses:
- The method requires online interactions for training the diffusion discriminator and policy, lacking comparisons with optimal transport-based RL methods that also utilize expert demonstration observations.
- The experimental evaluation is limited, particularly in image-based environments.
- The performance gain attributed to the diffusion model is questioned, as the binary cross-entropy (BCE) loss appears to be the primary contributor to improvements.
- The LfO baselines used in experiments are outdated, necessitating comparisons with more recent methods.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by including comparisons with optimal transport-based RL methods to strengthen the claims regarding DIFO's effectiveness. Additionally, expanding evaluations on image-based environments would enhance the robustness of the findings. The authors should clarify the role of the diffusion model in performance improvements, possibly by discussing alternative regularization techniques. Furthermore, updating the LfO baselines to include more recent methods would provide a clearer context for DIFO's contributions. Lastly, addressing minor presentation issues, such as symbol clarity and figure distinctions, would improve the overall readability of the paper.