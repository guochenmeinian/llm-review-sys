ID: UWUUVKtKeu
Title: Diffusion-based Reinforcement Learning via Q-weighted Variational Policy Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents QVPO, a novel model-free algorithm for online reinforcement learning (RL) that utilizes a diffusion policy. It introduces a Q-weighted VLO loss, modifying the original diffusion model objective by incorporating Q-values and addressing negative Q-values through advantage weighting. The algorithm promotes exploration by mimicking a uniform distribution and aims to reduce inference variance by favoring actions with higher Q-values. The authors clarify that $\rho_0$ represents the distribution of the initial state $s_0$, not the discounted state distribution denoted by $d_\pi$. They justify their policy update assumption, indicating that the degree of updating is proportional to the weight $A_{\pi_{old}}(s, a^\star)$. The paper discusses the limitations of using $\exp(A)$ for weight functions in AWR-style methods, citing slow convergence and numerical instability. Additionally, QVPO's ability to leverage GPU parallelism for efficient action sampling and selection during policy optimization is highlighted. Experimental validation on MuJoCo benchmarks shows promising results, indicating state-of-the-art performance in cumulative reward and sample efficiency.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant problem in online RL by integrating diffusion models, an area that has seen limited exploration.
- The QVPO algorithm is theoretically sound, with the Q-weighted variational loss established as a tight lower bound of the policy objective.
- The method demonstrates strong performance on MuJoCo benchmarks and includes clear discussions of its components.
- The paper provides a clear explanation of the assumptions underlying the policy update mechanism.
- The authors effectively address potential concerns regarding the use of $\exp(A)$, highlighting its drawbacks in practical applications.
- QVPO's ability to leverage GPU parallelism for action sampling is a notable advantage over previous methods.

Weaknesses:
- Several claims lack sufficient empirical support; for example, the effectiveness of the entropy term would be better substantiated with experimental results rather than illustrations.
- The claim regarding the multimodality of diffusion policies is not sufficiently substantiated within the paper.
- The experimental scope is limited to D4RL datasets, raising questions about the generalizability of the method to other environments, particularly those with discrete action spaces or sparse rewards.
- The initial diffusion policy pre-training procedure may complicate convergence, as noted by the authors.
- The computational demands of the algorithm are high due to its reliance on action samples for both optimization and action selection, which may hinder practical application.

### Suggestions for Improvement
We recommend that the authors improve the empirical support for their claims, particularly by providing experimental results on toy examples to validate the effects of the entropy term. Additionally, we suggest improving the clarity and substantiation of the claim regarding the multimodality of diffusion policies, possibly by including experimental verification. Expanding the experimental scope to include a wider variety of tasks, such as those with discrete action spaces or sparse rewards, would strengthen the paper's contributions. We recommend conducting more exhaustive ablation studies to isolate the contributions of individual components like the Q-weighted variational loss and the entropy regularization term. Furthermore, addressing the potential trade-off between exploration and exploitation due to the variance reduction method would enhance the discussion. Lastly, clarifying the relationship between training epochs and environment steps in the paper would improve reader comprehension.