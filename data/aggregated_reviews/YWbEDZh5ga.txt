ID: YWbEDZh5ga
Title: On Robustness of Finetuned Transformer-based NLP Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper explores the robustness of finetuned Transformer-based NLP models, specifically addressing three questions: (i) the consistency of finetuning effects across models for various NLP tasks, (ii) the effectiveness of these models in handling input text perturbations, and (iii) the varying robustness levels when finetuned for different tasks. The authors analyze BERT, GPT-2, and T5 using two metrics to characterize changes in model representations and study their robustness against various text perturbations in classification and generation tasks. Key contributions include a systematic examination of robustness and the impact of finetuning on these models.

### Strengths and Weaknesses
Strengths:
1. The paper presents interesting analyses on the robustness of transformer-based models affected by input perturbations.
2. Layer-wise finetuning analysis provides insights for selecting specific layers for task-specific fine-tuning.
3. The analyses are broad and useful for understanding model behavior across different tasks.

Weaknesses:
1. The study is limited to three models (BERT, GPT-2, T5), lacking exploration of larger models like LLaMA 7B/13B.
2. The findings are not significantly impactful, and the authors do not adequately explain why GPT-2 outperforms BERT and T5.
3. The focus on text perturbations is narrow; additional robustness evaluations, such as perturbing hidden states, would enhance the analysis.

### Suggestions for Improvement
We recommend that the authors improve the depth of their analysis by exploring additional models, particularly larger language models like LLaMA 7B/13B. Further investigation into the reasons behind GPT-2's superior robustness compared to BERT and T5 is necessary. Additionally, we suggest including comparisons of model performance on new data versus training data, as this is crucial for assessing robustness. Expanding the scope of robustness evaluations to include methods like dropping hidden states or perturbing representations across layers would also strengthen the paper.