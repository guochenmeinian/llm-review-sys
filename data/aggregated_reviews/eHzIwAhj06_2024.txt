ID: eHzIwAhj06
Title: The Group Robustness is in the Details: Revisiting Finetuning under Spurious Correlations
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 5, 4, 2, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the over-reliance on spurious correlations in machine learning, particularly focusing on class balancing techniques and their impact on worst-group accuracy (WGA). The authors propose a class balancing scheme that combines discarding samples from majority classes with upsampling to enhance robustness. They identify two failure modes of common class-balancing techniques: (1) catastrophic collapse during class-balanced mini-batch finetuning with standard hyperparameters, and (2) detrimental effects on WGA when finetuning on a balanced subset that includes a small minority group within the majority class. The authors conduct experiments across vision and NLP datasets, revealing nuances in class-balancing methods and their effects on model performance.

### Strengths and Weaknesses
Strengths:
1. The authors provide valuable insights into subsetting and upsampling, demonstrating that subsetting can be effective unless a small minority group is present in the majority class.
2. The decision to investigate model scaling's impact on WGA in a practical finetuning context is a notable contribution.
3. The finding that the largest eigenvalue in each dataset belongs to a minority group, with minority group eigenvalues generally larger than those of majority groups, extends existing literature.

Weaknesses:
1. The authors overstate their method's strengths in Section 3.2, with insufficient investigation of prior work, particularly regarding Deep Feature Reweighting and Automatic Feature Reweighting, which are not adequately compared.
2. Limited empirical evidence is provided, as claims are based on experiments from only three settings, raising questions about the generalizability of results.
3. The relevance of spectral imbalance analysis is unclear, lacking a discussion on how spectral features differ between their method and baseline methods.
4. The choice of mixture ratio in their method is not thoroughly discussed, and the supplementary ablation study is limited in scope.
5. The paper primarily focuses on ConvNext, neglecting other popular pre-trained models like ViTs, which could provide a broader understanding of the proposed method's efficacy.

### Suggestions for Improvement
We recommend that the authors improve the clarity and depth of their investigation in Section 3.2 by providing a more comprehensive comparison with prior work, particularly addressing the relevance of Automatic Feature Reweighting. Additionally, we suggest including a section that articulates the motivation behind the emphasis on subsetting and oversampling, as well as a more detailed exploration of spectral imbalance. To strengthen empirical claims, we encourage the authors to conduct further experiments across a wider range of settings and datasets, including multi-class scenarios. Finally, we advise the authors to discuss the choice of mixture ratio in greater detail and to consider incorporating results from other model architectures, such as ResNet50, to enhance the robustness of their findings.