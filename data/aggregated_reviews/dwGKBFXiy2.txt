ID: dwGKBFXiy2
Title: NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a pruning strategy for encoder-decoder models, focusing on increasing encoder sparsity and reducing decoder layers. The authors propose NASH, which is supported by empirical evidence demonstrating the trade-offs among model size, accuracy, and latency. Extensive experimental results validate the effectiveness of the proposed method.

### Strengths and Weaknesses
Strengths:
- Sufficient empirical evidence motivates the proposal.
- It is the first systematic strategy for pruning encoder-decoder models.
- The paper is well-written, with clear illustrations of motivation and methods.
- Comprehensive experiments confirm the effectiveness of NASH.

Weaknesses:
- The contribution of increasing encoder sparsity over solely pruning the decoder is unclear.
- Observations regarding decoder layers and efficiency are somewhat trivial and have been previously noted.
- Experiments are limited to the T5 architecture, lacking ablation studies on other models like BART.
- The mathematical presentation requires improvement, including a notation table.
- There is a lack of qualitative analysis regarding the feasibility of the approach in various contexts.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the contribution by quantitatively assessing the impact of decoder layers on semantic relationships and inference speed. Additionally, conducting experiments on non-T5 models would enhance the robustness of the findings. The authors should also include a notation table to clarify mathematical expressions and provide charts or graphs to visually depict the method's strengths compared to state-of-the-art approaches. Finally, incorporating a qualitative analysis of the approach's feasibility in relation to task complexity, data volume, and model robustness would strengthen the paper.