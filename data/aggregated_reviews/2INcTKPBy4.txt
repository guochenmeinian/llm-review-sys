ID: 2INcTKPBy4
Title: The Sample Complexity of Gradient Descent in Stochastic Convex Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 3, 7, 8, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 3, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new lower bound for the generalization error of gradient descent (GD) for Lipschitz functions, demonstrating a linear dependency on dimension that bridges the gap between lower and upper bounds in sample complexity across various regimes. The authors utilize a block version of a variation of the Nemirovski function and transition from a sample-independent oracle to a sample-dependent oracle. Additionally, the authors propose open problems related to the generalization error of GD. 

### Strengths and Weaknesses
Strengths:
- The paper cleverly leverages Nemirovski and Feldman's functions to control sub-differential dynamics.
- The authors provide a solid intuition for the function construction.
- The technical challenges are well-explained, with no apparent issues in the proofs.

Weaknesses:
- The presentation of results is unclear, requiring multiple readings to grasp their significance; a more concise format with explicit regimes, possibly in a table, would enhance clarity.
- There are several typographical errors and unclear references, such as the need to mention that Eq 16 will be proven in Appendix D.
- The structure is non-standard, lacking a conclusion, and the notation section is poorly written, leading to confusion regarding the main contributions and their proofs.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by restructuring the results and explicitly outlining the improvements under various regimes, potentially using a table. Additionally, addressing the typographical errors and ensuring that all relevant proofs are clearly located would enhance the manuscript. Including a discussion on the implications of the analysis for stochastic gradient descent (SGD) would also be beneficial.