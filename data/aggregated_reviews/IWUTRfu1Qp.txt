ID: IWUTRfu1Qp
Title: Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 9
Original Confidences: 3, 4

Aggregated Review:
### Key Points
This paper presents an argument that LLM auto-eval systems are susceptible to cheating through adversarially-designed prompts. The authors demonstrate that generic persuasive responses can achieve high win rates in state-of-the-art evaluations, highlighting the limitations of existing benchmarks. They conduct various ablations to assess the impact of their setup across multiple evaluators and benchmarks, providing a well-motivated context and acknowledging relevant limitations.

### Strengths and Weaknesses
Strengths:
* The paper is clear, and the experimental setup is sound.
* The authors perform numerous ablations to understand the effects on a wide range of evaluators and benchmarks.
* The background indicates a strong understanding of the field, and the argument regarding the ease of cheating in auto-evals is compelling.

Weaknesses:
* The main limitation is the applicability of findings to real-world scenarios, as results primarily demonstrate the creation of adversarial suffixes that prompt specific responses. In practice, an adversary might aim to deceive judges while maintaining the appearance of benign responses. Additionally, the adversarial random search method lacks detail, raising concerns about its effectiveness.

### Suggestions for Improvement
We recommend that the authors explore ways to incorporate their findings into a model optimized to deceive automated judges during training. Additionally, consider including an example of what is meant by "constant outputs" earlier in the paper to provide readers with an intuitive understanding, potentially by moving Figure 1 to the second page. More detail on the adversarial random search method would also enhance the paper's rigor.