ID: cuO0DenqMl
Title: Wasserstein Gradient Boosting: A Framework for Distribution-Valued Supervised Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 3, 3, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Wasserstein Gradient Boosting (WGBoost), a novel gradient boosting framework that utilizes the Wasserstein gradient of a loss functional over probability distributions for probabilistic predictions. The authors propose an algorithm that approximates the Wasserstein gradient using a particle method and kernel approximation, demonstrating its application in posterior regression tasks with empirical evaluations on various datasets. WGBoost aims to sample from a conditional distribution \( P(\theta \mid x) \) for any input \( x \), contrasting with SVGD, which samples from an arbitrary distribution \( P(\theta) \). The paper introduces a second-order WGBoost algorithm based on the approximate Wasserstein gradient and Hessian of the KL divergence, emphasizing its conditional nature and asserting superior performance on small to medium-large datasets.

### Strengths and Weaknesses
Strengths:
- The application of Wasserstein gradient flows to gradient boosting is a novel and promising direction.
- The proposed method shows superior performance in empirical evaluations on real-world datasets for regression and out-of-distribution detection tasks.
- The theoretical concepts are solid, with detailed derivations of the Wasserstein gradient and Hessian approximations.
- The novelty of WGBoost is recognized, particularly its conditional sampling approach.
- The authors provide a clear distinction between WGBoost and SVGD, enhancing understanding of their methodology.

Weaknesses:
- The paper lacks a comprehensive comparison with existing methods, particularly SVGD, and does not clarify the fundamental differences between them.
- Some reviewers perceive WGBoost as fundamentally similar to SVGD, indicating a potential lack of clarity in the explanation of their differences.
- There is no theoretical analysis or discussion regarding the computational cost and convergence speed of the proposed algorithm.
- Experiments primarily focus on KL divergence functionals, limiting the exploration of other potential functionals.
- The performance of the entropy of multi-class classifiers has not been adequately addressed in the context of the paper.
- The clarity of the algorithm's implementation and the interaction of particles during the gradient step is insufficiently explained.

### Suggestions for Improvement
We recommend that the authors improve the comparison with existing methods, particularly SVGD, by providing qualitative and quantitative analyses to clarify the differences. Additionally, we suggest incorporating a comparison with the entropy of multi-class classifiers in future experiments to strengthen the paper's findings. We also recommend including a discussion on the computational cost and convergence speed of WGBoost, as well as exploring other functionals beyond KL divergence. To enhance clarity, we advise revising the presentation of Algorithm 1 to better illustrate the interaction of particles during the gradient step and specifying the parameters being estimated in the loss function. Lastly, we encourage the authors to conduct larger-scale experiments to validate the performance of WGBoost against established probabilistic methods.