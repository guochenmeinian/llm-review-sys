ID: hv3VpXDIh8
Title: CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive multilingual benchmark for code translation tasks, named "CodeTransOcean," which includes four new datasets: MultilingualTrans for translations between popular programming languages, NicheTrans for niche and popular languages, LLMTrans for evaluating compilability of translated code by large language models (LLMs), and DLTrans for translating deep learning code across different frameworks. The authors propose a novel evaluation metric, Debugging Success Rate@K, for program-level code translation and evaluate both open LLMs like CodeT5+ and closed-source models such as ChatGPT. The benchmark aims to enhance the measurement of models' code translation capabilities.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel and comprehensive benchmark that significantly contributes to the field of code translation.
- It includes detailed evaluations of both open and closed-source LLMs, providing valuable insights into their performance.
- The datasets cover a wide variety of programming languages, including niche languages, which is beneficial for real-world applications.
- The structure of the paper is well-organized and easy to follow.

Weaknesses:
- Important experimental results are relegated to the Appendix, which undermines the self-contained nature of the main content.
- The evaluation primarily focuses on ChatGPT, neglecting other significant models, which limits the benchmark's applicability.
- The proposed evaluation metric still relies on the compilation environment, which may not fully address the core issues in code translation.

### Suggestions for Improvement
We recommend that the authors improve the presentation by including key experimental results in the main body of the text rather than the Appendix to enhance self-containment. Additionally, we suggest expanding the model evaluation to include other significant models such as Transformer, CodeBERT, and open-source LLMs like LLaMa for a more comprehensive comparative analysis. Furthermore, we encourage the authors to clarify the selection criteria for the evaluated models and to reference key works that could provide important comparative insights.