ID: bescO94wog
Title: Active Learning with LLMs for Partially Observed and Cost-Aware Scenarios
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 7, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel problem setting in Active Learning (AL) called Partially Observable Cost-Aware Active-Learning (POCA), addressing the challenge of acquiring both features and labels in partially observed datasets with associated costs. The authors propose µPOCA, which utilizes Generative Surrogate Models (GSMs), specifically Large Language Models (LLMs), to impute missing features and compute uncertainty-based active learning metrics. The theoretical framework demonstrates that acquiring both labels and features leads to greater uncertainty reduction compared to traditional AL methods, with empirical validation across various synthetic datasets.

### Strengths and Weaknesses
Strengths:  
- The paper tackles a practical problem in AL that has been largely overlooked, formalizing the POCA problem setting as a valuable contribution.  
- The use of LLMs for feature imputation is innovative, enhancing the estimation of uncertainty reduction metrics crucial for the proposed method's success.  
- The empirical results support the effectiveness of µPOCA, demonstrating its potential to outperform traditional AL methods in specific scenarios.  

Weaknesses:  
- Clarity and flow could be improved, particularly in explaining the use of GSMs and the notation, which can be confusing.  
- The theoretical contributions are somewhat limited, with the problem resembling experimental design, and the implications of partial observability on predictive performance are not adequately explored.  
- The Monte-Carlo-based approach may incur high computational costs, and the model's performance can sometimes be inferior to random sampling, depending on dataset characteristics.  

### Suggestions for Improvement
We recommend that the authors improve the clarity and flow of the paper, particularly in the explanation of GSMs and the notation used. Additionally, we suggest a deeper exploration of how partial observability impacts predictive performance and a discussion on the independence assumption's practicality. The authors should also consider evaluating µPOCA on real-world datasets with varying degrees of missingness and include more active learning methods as baselines beyond BALD and random sampling. Finally, we advise a thorough proofreading to correct any typos present in the manuscript.