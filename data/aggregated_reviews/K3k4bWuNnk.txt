ID: K3k4bWuNnk
Title: Even Sparser Graph Transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 4, 3, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a two-phase training process for Graph Transformers aimed at mitigating memory and computational inefficiencies associated with large graph datasets. The first phase involves training a narrow network to estimate attention scores, which are then utilized to construct sparse interaction graphs for a larger network in the second phase. The authors assert that this method preserves performance while significantly reducing memory requirements, supported by theoretical justifications and experimental validation across various graph datasets. Additionally, the paper introduces a novel approach to sparsification in Exphormer-like architectures, addressing the memory demands of large datasets such as Amazon2M, and claims to achieve a memory-time trade-off while maintaining competitive performance. However, concerns remain regarding the experimental validation, particularly the absence of SGFormer in key comparisons, which raises questions about the robustness of the results.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and articulates the methodology and experimental setups clearly, enhancing reader comprehension.
- The introduction of a novel two-phase training methodology is creative and addresses significant scalability and memory usage limitations in current graph transformer models.
- The theoretical analysis provides a solid foundation for the proposed method.
- The proposed model effectively addresses memory efficiency in large graph datasets and acknowledges the need for further sparsification in existing graph transformers, highlighting a significant contribution to the field.

Weaknesses:
- The innovation of the two-phase training process is somewhat incremental, primarily building on existing Exphormer by adjusting the expander graph's flexibility and incorporating a sampled attention mechanism.
- The reported performance gap between Graph Transformers and MPNNs on node classification may be overestimated due to suboptimal hyperparameter choices, raising questions about the necessity of global information propagation in large graphs.
- The justification for not comparing with stronger baselines, including SGFormer and Polynormer, is unconvincing, and the experimental results lack comprehensive validation across a wider range of datasets, which undermines their credibility.
- The authors should include an ablation study to clarify the necessity of the two-phase training process and extend the theoretical analysis to deeper layers for a more comprehensive understanding.
- Some sections, particularly lines 268-277, are confusing and may contain inaccuracies.

### Suggestions for Improvement
We recommend that the authors improve the novelty aspect by clearly differentiating their contributions from existing methods like Exphormer. Additionally, a comparison of memory usage and runtime between MPNNs and the proposed method is essential to demonstrate efficiency. We suggest including a thorough comparison with SGFormer and other scalable transformers, focusing on performance, memory usage, and runtime across a broader range of datasets. Clarifying the rationale for excluding certain baselines in the paper would strengthen the argument for the model's contributions. Including an ablation study to illustrate the necessity of the two-phase training process would enhance clarity. The theoretical analysis should be extended to deeper layers, and the authors should address the confusing sections for better readability. Lastly, a more in-depth discussion on the trade-offs between memory and runtime performance, as well as the implications of using a higher expander degree, is warranted.