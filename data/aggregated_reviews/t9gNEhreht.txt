ID: t9gNEhreht
Title: SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 7, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a workflow for generating synthetic Text To Image (T2I) data to enhance the faithfulness of T2I models. The authors propose a four-stage process: (1) collecting skill-specific prompts using LLMs, (2) generating image-text samples without human annotation, (3) fine-tuning expert T2I models on these datasets, and (4) merging the experts for efficient adaptation. The results indicate that the proposed model can achieve superior performance based solely on LLM-generated prompts and images.

### Strengths and Weaknesses
Strengths:
- The paper addresses the important issue of faithfulness in T2I models, which is increasingly relevant in both industry and academia.
- Experimental results demonstrate consistent improvements over multiple base models, with qualitative results appearing convincing.
- The approach shows that T2I models can be trained effectively using only generated data, alleviating the need for human annotation.

Weaknesses:
- The motivation of faithfulness is somewhat misleading, as the proposed method treats it as a side effect rather than a primary focus, which may detract from clarity.
- The technical novelty is limited; LoRA fine-tuning and expert merging are not original concepts, and the prompt generation appears straightforward.
- Key comparisons, such as those in Table 2 regarding single LoRA versus multi-LoRA, lack detailed explanations, and the fusion mechanism among multiple LoRAs requires more thorough exploration.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by focusing on the multi-task fine-tuning aspect rather than the side effects related to faithfulness. Additionally, a comprehensive discussion of limitations should be included in the main text rather than relegated to the appendix. To enhance the robustness of the findings, we suggest conducting ablation studies on the sensitivity of various components in the workflow, such as LLM choice and LORA parameters. Finally, providing detailed comparisons of optimization time and training steps for the presented approach versus baselines would strengthen the paper's contributions.