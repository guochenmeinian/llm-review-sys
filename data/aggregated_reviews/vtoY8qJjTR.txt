ID: vtoY8qJjTR
Title: Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 7, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for offline-to-online reinforcement learning (RL) that introduces a state-adaptive improvement-constraint balance. The authors propose the Family Offline-to-Online RL (FamO2O), which trains a diverse policy family to adaptively select policies based on the quality of actions in the dataset. The method aims to maintain a balance between conservative and radical policy improvements depending on the state. Additionally, the authors provide a methodology for selecting the range of $\beta$ when applying FamO2O to new algorithms, guided by principles that ensure the range includes the original algorithm's $\beta$ and maintains reasonable performance. The necessity for different $\beta$ intervals across environments is well-articulated, and the authors utilize a $\tanh$ activation layer in the balance model to enforce valid $\beta$ values. Experimental results demonstrate that FamO2O outperforms existing offline-to-online RL methods across various tasks.

### Strengths and Weaknesses
Strengths:
1. The approach enhances the performance of multiple RL algorithms and is applicable to various offline RL settings.
2. The paper is well-written, theoretically justified, and includes thorough empirical evaluations and ablation studies.
3. The experiments are extensive, covering diverse datasets and tasks, validating the algorithm's design choices.
4. The authors provide clear principles for selecting $\beta$ ranges, enhancing the understanding of their methodology.
5. The explanation of the necessity for different $\beta$ intervals across environments is well-articulated and aligns with established practices in the field.
6. The use of a $\tanh$ activation layer for enforcing valid $\beta$ values is a technically sound approach.

Weaknesses:
1. The method may require abundant diverse data, which could be challenging in more complex environments, such as the Adroit domain.
2. The balance between policy improvement and constraints may be overly complicated, leading to potential confusion.
3. The proposed method shares similarities with existing decision-transformer-style methods, raising questions about its novelty.
4. The paper could benefit from further exploration of clipping as an alternative method for enforcing valid $\beta$ values, which is only briefly mentioned.

### Suggestions for Improvement
We recommend that the authors clarify how the modification affects the compute time required to train algorithms. Additionally, we suggest that the authors simplify the explanation of the balance model to enhance clarity, particularly regarding the relationship between the balance coefficients and data quality. It would be beneficial to include a discussion on the limitations of the method, especially concerning the performance after online fine-tuning, as indicated by the results in Figure 10. Lastly, we encourage the authors to consider incorporating a comparison with related works that address similar state-adaptive policy changes to strengthen the novelty of their contribution. Furthermore, we recommend that the authors improve the discussion on the potential of clipping as an alternative method for enforcing valid $\beta$ values, providing more detail on its feasibility and implications.