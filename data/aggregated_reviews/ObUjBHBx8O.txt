ID: ObUjBHBx8O
Title: Mitigating Spurious Correlations via Disagreement Probability
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 5, 6, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called Disagreement Probability based Resampling for debiasing (DPR), which aims to mitigate spurious correlations in machine learning models without requiring explicit bias labels. The authors propose a new training objective that encourages consistent model performance across bias-aligned and bias-conflicting groups while reducing the performance gap between them. They derive sampling weights based on the disagreement probability between biased model predictions and target labels and demonstrate that DPR outperforms existing methods on various benchmark datasets. The authors report performance using a bias-aligned validation set to ensure a fair comparison with previous work by Ahn et al., despite acknowledging that DPR's performance could be further validated using an unbiased or bias-conflicting validation set.

### Strengths and Weaknesses
Strengths:
1. The paper is well-organized and clearly written, making it accessible.
2. It provides a comprehensive theoretical and empirical analysis, with extensive experiments across diverse datasets.
3. The introduction of a new training objective and the innovative use of disagreement probability are noteworthy contributions.
4. The authors effectively address reviewer concerns and demonstrate responsiveness by conducting additional experiments.
5. DPR outperforms other baselines, indicating its potential effectiveness as a debiasing method.

Weaknesses:
1. The proposed method, DPR, lacks novelty as it extends existing approaches like Just Train Twice (JTT) and Environment Inference for Invariant Learning (EIIL) without significant innovation.
2. The experimental section lacks coherence, with inconsistent metrics and datasets, and the results on CivilComments-WILDS contradict theoretical claims.
3. The reliance on color jitter for data augmentation raises concerns about fair comparisons with baseline models that did not use similar techniques.
4. The reliance on a bias-aligned validation set raises questions about the generalizability of the reported performance.
5. There is a need for further validation using unbiased or bias-conflicting validation sets to strengthen the findings.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their method by clearly distinguishing it from existing approaches like JTT and EIIL, and by discussing relevant literature in the related works section. Additionally, we suggest focusing on the worst-performing groups in experiments to better illustrate DPR's effectiveness. To enhance the experimental rigor, please provide consistent metrics and datasets, and clarify the impact of data augmentation techniques on all models compared. We also recommend improving the robustness of validation by incorporating evaluations using an unbiased validation set or a bias-conflicting validation set with the same distribution as the test set. Finally, we encourage the authors to address the hyperparameter selection process, particularly in the context of biased validation sets, to ensure reliable results and to include the issues discussed during the discussion period regarding color augmentation in the revision to guide future research on spurious correlations more rigorously.