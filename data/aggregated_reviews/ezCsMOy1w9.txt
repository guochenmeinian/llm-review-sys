ID: ezCsMOy1w9
Title: $\texttt{TACO}$: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TACO, a temporal contrastive learning approach designed to encode high-dimensional observations and inputs for reinforcement learning. The authors propose a loss function (TACO) that maximizes mutual information between current state-action pairs and future states. By jointly optimizing the TACO loss with CURL loss and reward prediction loss, the method shows superior performance compared to state-of-the-art (SOTA) representation learning algorithms in both on-policy and off-policy frameworks, as well as in model-based settings.

### Strengths and Weaknesses
Strengths:
- The proposed method outperforms SOTA algorithms in both model-free and model-based frameworks.
- Extensive experiments across various environments with multiple baselines, including on-policy, off-policy, and model-based methods, were conducted.
- An appropriate ablation study was performed for each loss term.

Weaknesses:
- The novelty of TACO is limited as it closely resembles DRIML, with the main difference being the consideration of the entire action sequence rather than a single action. In some environments, TACO performs better with K=1 than K=3, suggesting potential for DRIML's extension to continuous action spaces.
- The implementation details of baselines based on DrQ-v2 lack clarity, complicating fair comparisons.
- The empirical evaluation does not isolate the contrastive objective from reward prediction and CURL objectives, making it difficult to assess the true contribution of TACO.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the implementation details regarding the baselines based on DrQ-v2 to ensure fair comparisons. Additionally, we suggest conducting experiments that evaluate the TACO objective in isolation from the reward prediction and CURL objectives to substantiate the empirical gains. Clarifying how action inputs were integrated into existing methodologies and addressing the potential impact of using TACO for pretraining would also enhance the paper. Finally, we encourage the authors to provide ablation studies for offline experiments similar to those presented in Figure 6b to better understand TACO's performance advantages.