ID: 1ae108kHk2
Title: Delays in generalization match delayed changes in representational geometry
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 6, 9, 3, 7, -1
Original Confidences: 4, 4, 4, 4, -1

Aggregated Review:
### Key Points
This paper presents an empirical study on grokking in neural networks, particularly focusing on image classification tasks using MNIST and EMNIST datasets. The authors propose that changes in the Neural Tangent Kernel (NTK) occur before grokking, but assert that the critical factor for grokking is a sudden enhancement in data separation, as measured by representational geometry and manifold capacity. The study aims to provide a deeper understanding of grokking through these geometric metrics, suggesting that manifold capacity offers a more accurate measure of generalization than traditional NTK methods.

### Strengths and Weaknesses
Strengths:
- The authors build on existing theories and provide new insights into the grokking process through representational geometry, which is particularly relevant for image classification tasks. 
- The writing is generally clear, and the visualizations effectively illustrate the relationship between manifold capacity and grokking.

Weaknesses:
- The paper lacks a comprehensive discussion on the significance of neural network training dynamics and their impact on generalization, particularly in distinguishing grokking from double descent.
- The use of toy models raises concerns about the applicability of findings to larger datasets and more complex models.
- Justification for the choice of manifold capacity and other geometric metrics as tools for analyzing generalization is insufficiently addressed.
- The core claim regarding the relationship between NTK changes and generalization is not convincingly supported by the figures presented, particularly concerning the timing of these changes relative to grokking.
- The characterization of out-of-distribution (OOD) performance is flawed, as the MNIST task is a subset of EMNIST, and the methodology for linear probing lacks clarity.

### Suggestions for Improvement
We recommend that the authors improve the explanation of neural network training dynamics and their relevance to grokking, particularly in relation to double descent. Additionally, we suggest exploring the behavior of their findings on larger datasets and more complex models to validate their claims. The authors should provide a stronger justification for the selected geometric metrics and clarify the connection between manifold capacity and grokking. Furthermore, we advise revising the figures to ensure they accurately support the core claims, particularly regarding the timing of NTK changes and generalization. Lastly, we recommend refining the characterization of OOD performance and clarifying the methodology used for linear probing to enhance the study's rigor.