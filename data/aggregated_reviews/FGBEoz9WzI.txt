ID: FGBEoz9WzI
Title: Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Automate-CoT strategy, which aims to enhance the reasoning capabilities of large language models (LLMs) by automatically augmenting and optimizing chain-of-thought (CoT) prompts from a small labeled dataset. The approach involves three steps: augmenting rationale chains, pruning low-quality chains, and selecting optimal combinations using a variance-reduced policy gradient strategy. The authors evaluate the method across various reasoning and non-reasoning tasks, demonstrating competitive performance improvements compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated and clearly written, providing a useful alternative for automatic CoT prompting.
- Impressive experimental results show significant performance gains over baselines, supported by thorough analysis and ablation studies.
- The approach effectively leverages labeled data to optimize CoT prompts, achieving noticeable improvements over manually crafted prompts.

Weaknesses:
- The reliance on hundreds of labeled data points limits the applicability of the proposed approach in true few-shot settings.
- The method incurs substantial computational overhead for optimizing prompts, which could be elaborated in terms of cost.
- Some claims lack adequate references to existing studies, and the relationship between the proposed method and the four motivation factors remains unclear.
- The paper does not include comparisons with recent closed-source LLMs, such as ChatGPT, which may perform better.

### Suggestions for Improvement
We recommend that the authors improve the clarity of notations in Section 3.2 to enhance readability. Additionally, it would be beneficial to include strong prompt selection baselines for few-shot learning and to compare the selection method with clustering techniques. The authors should elaborate on the cost of optimizing prompts in terms of monetary value or token usage. Furthermore, we suggest conducting experiments with advanced models like gpt-3.5-turbo and providing results for other related methods, such as turbo results of AutoCoT, to strengthen the comparative analysis. Lastly, consider renaming Automate-CoT to avoid confusion with Auto-CoT, as the current name may be misleading given the method's reliance on extensive training data.