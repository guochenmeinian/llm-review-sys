ID: bFrNPlWchg
Title: Extending Video Masked Autoencoders to 128 frames
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 6, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the MAE pretraining of long videos, proposing an effective decoder masking strategy that utilizes a MAGVIT-based tokenizer to prioritize important tokens and use quantized tokens as reconstruction objectives. The method demonstrates improved performance on downstream tasks, specifically on EK-100 and Diving48, when using 128 frames for pretraining compared to 32 frames. The approach allows for significant memory savings, enabling the processing of longer video clips.

### Strengths and Weaknesses
Strengths:
1. The adaptive decoder masking strategy effectively addresses the out-of-memory (OOM) problem associated with long-video training and outperforms previous masking strategies.
2. The paper is well-written, with clear explanations and a focus on an important issue in video understanding, namely the scalability of video masked modeling for longer sequences.
3. The proposed module is trained offline, reducing training difficulties and overhead, while providing both decoder masking and semantically informative tokens.

Weaknesses:
1. The experimental results are limited to only two input frame sizes (32 and 128 frames), lacking a gradual exploration of input frame numbers from 16 to 128.
2. There is insufficient theoretical analysis or experiments explaining the conversion of MAGVIT to FSQ-MAGVIT for reconstruction targets and adaptive token selection.
3. The scope of downstream tasks is narrow, with only two tasks evaluated, raising questions about the method's generalizability.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by including results for a range of input frame sizes from 16 to 128 frames to better identify optimal settings. Additionally, we suggest providing theoretical analysis or experiments to clarify the rationale behind converting MAGVIT to FSQ-MAGVIT for reconstruction targets and adaptive token selection. Expanding the evaluation to include a broader range of downstream tasks, such as action detection and temporal action localization, would also enhance the robustness of the findings.