ID: Y841BRW9rY
Title: AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 4, 3, 8, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AGENTPOISON, a novel red-teaming approach targeting vulnerabilities in large language model (LLM) agents by poisoning their long-term memory or retrieval-augmented generation (RAG) knowledge base. The authors propose a constrained optimization method to inject backdoor triggers that lead to the retrieval of malicious demonstrations when specific triggers are present in user queries. The method is validated through extensive experiments on three types of LLM agents, demonstrating high attack success rates with minimal impact on benign performance.

### Strengths and Weaknesses
Strengths:
- The problem addressed is novel, focusing on the safety of LLM agents, which is a significant and timely topic.
- The paper is well-structured and clearly written, with effective visualizations that enhance understanding.
- The trigger optimization process is designed to be highly transferable, stealthy, and coherent, ensuring minimal disruption to normal agent functions.
- Extensive experiments validate AGENTPOISON's effectiveness, achieving high attack success rates and demonstrating robustness across different LLM agents.

Weaknesses:
- The threat model is vague, particularly regarding how an attacker can add triggers to user queries during inference.
- The motivation for using constrained optimization over unconstrained methods is unclear, especially given the difficulty in setting intuitive values for parameters.
- There is a lack of optimization for poisoned passages, which are critical for influencing LLM output.
- The explanation of transferability is insufficient, and the authors do not provide a convincing rationale for their claims regarding it.
- The paper briefly mentions potential defenses but does not explore them in depth.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the threat model by explicitly detailing how triggers can be integrated into user queries. Additionally, we suggest providing a more thorough explanation of the motivation behind using constrained optimization, particularly addressing the selection of parameters like $\eta_\text{tar}$ and $\eta_\text{coh}$. It would be beneficial to include optimization for poisoned passages to ensure that triggered queries can effectively retrieve them. Furthermore, we encourage the authors to enhance the discussion on transferability, providing deeper insights into the factors affecting it. Lastly, a more comprehensive exploration of potential defenses against AGENTPOISON would strengthen the paper's contributions.