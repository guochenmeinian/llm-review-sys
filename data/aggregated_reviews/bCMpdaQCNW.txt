ID: bCMpdaQCNW
Title: Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 7, 7, 8, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents YESBUT, a benchmark designed to evaluate large vision-language models' (VLMs) understanding of humor and contradictions in comics through juxtaposed panels. The benchmark includes two-panel comics with contradictory narratives and annotations for literal descriptions, contradiction explanations, underlying philosophies, and titles. The authors propose four tasks of increasing complexity: literal description writing, contradiction generation, underlying philosophy selection, and title matching. The evaluation of various commercial and open-source VLMs reveals that even state-of-the-art models struggle with these tasks, particularly in reasoning about contradictions and abstractions. Additionally, the authors introduce a dataset constructed from publicly available social media content, adhering to copyright laws and ethical considerations regarding data privacy and consent. They acknowledge the subjective nature of humor and the potential for inherent biases in their annotations, committing to address these issues in their revisions.

### Strengths and Weaknesses
Strengths:
- Originality: The paper tackles a unique area in AI researchâ€”understanding humor in comics through contradictory narratives, pushing the boundaries of VLM capabilities.
- Quality: The rigorous data collection and annotation process involves multiple stages of human-AI collaboration and quality checks, leading to comprehensive experimental design.
- Clarity: The paper is well-structured, clearly explaining motivation, dataset creation, task designs, and experimental results, supported by effective figures and examples.
- Significance: The research provides valuable insights into AI limitations in humor comprehension, crucial for developing socially intelligent systems.
- The dataset explores a novel task of humor understanding through juxtaposition, which has not been previously examined.
- A rigorous annotation process involving diverse annotator backgrounds helps mitigate biases.

Weaknesses:
- The dataset size is limited (348 comics), potentially affecting the generalizability of findings.
- The annotation process relies heavily on human judges and GPT-4, which may introduce biases not thoroughly explored in the paper.
- The investigation into why decomposing tasks for VLMs does not yield consistent performance improvements is insufficient.
- While the paper identifies limitations of current VLMs, it lacks concrete suggestions or experiments for overcoming these challenges.
- The novelty of the dataset is questioned, as similar reasoning abilities have been studied in other multi-image tasks.
- The authors do not provide inter-human annotator agreement, raising concerns about the handling of subjectivity in humor interpretation.
- Cultural bias remains a concern, with insufficient explanation of measures taken to ensure cultural sensitivity in the dataset.
- The chosen evaluation metrics may not fully capture the depth of understanding required for humor comprehension.

### Suggestions for Improvement
We recommend that the authors improve the dataset size to enhance generalizability and robustness of findings. Additionally, exploring bias detection and mitigation techniques in the annotation process would strengthen the paper. The authors should consider expanding the benchmark to include comics with more than two panels to evaluate models' understanding of complex narratives. We also suggest providing inter-human annotator agreement statistics to address concerns about subjectivity. Furthermore, enhancing the discussion on cultural bias and the measures taken to ensure cultural sensitivity in the dataset is essential. Finally, we encourage the authors to justify their choice of evaluation metrics more robustly, addressing how these metrics align with the depth of understanding necessary for humor comprehension.