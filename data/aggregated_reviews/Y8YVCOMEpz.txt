ID: Y8YVCOMEpz
Title: MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 8, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unifying framework for recent linear attention models, including LinFormer, SSM, and LinRNN, and introduces a novel linear attention model called MetaLA. The authors propose enhancements such as the elimination of the Key matrix, dynamic decay for memory management, and self-augmentation with convolution to improve the approximation accuracy of softmax attention. The paper evaluates MetaLA across various tasks, including MQAR, language modeling, image classification, and LRA, demonstrating its effectiveness.

### Strengths and Weaknesses
Strengths:
- The paper provides a comprehensive unification of linear attention models, enhancing understanding of their mechanisms.
- MetaLA is theoretically grounded and empirically evaluated, showcasing its robustness across NLP tasks.
- The systematic comparison of existing models against proposed criteria for optimal linear attention offers valuable insights.

Weaknesses:
- The analysis of MQAR is insufficient, lacking challenging task variations and attention baselines for clearer comparisons.
- The empirical results primarily focus on classification tasks, leaving performance in text generation scenarios unclear.
- There is no discussion of low-precision formats like FP16 and BF16, which are critical for assessing practical implementation.
- The absence of code for MetaLA complicates the evaluation of its practicality and ease of implementation.
- The presentation of theoretical concepts, particularly in Section 2, is dense and could obscure contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical analysis by moving Proposition statements to the main text and reorganizing Section 4 around these propositions. Additionally, we suggest including more challenging MQAR task variations and a softmax attention baseline for better comparative analysis. The authors should also explore the performance of MetaLA in text generation tasks and provide empirical results in low-precision formats. Finally, including code for MetaLA would enhance the assessment of its practicality and implementation.