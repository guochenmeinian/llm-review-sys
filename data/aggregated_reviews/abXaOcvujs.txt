ID: abXaOcvujs
Title: WikiDBs: A Large-Scale Corpus Of Relational Databases From Wikidata
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 8, 9, 7, 8, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents WikiDBs, an open-source corpus of 100K relational databases and 1.4M tables derived from Wikidata, aimed at serving as a foundation for Large Database Models (LDMs). The authors provide detailed construction methodologies and preliminary experimental results showcasing potential applications of LLMs on the dataset. The work is notable for its scale and the connectivity between tables, which is crucial for tasks like table representation learning. Additionally, the paper introduces a benchmark dataset aimed at evaluating the performance of language models (LLMs) in predicting column names, proposing the generation of new names for columns to enhance benchmarking accuracy. However, there is a disagreement regarding the effectiveness of this approach, as original WikiData column names may be more recognizable to LLMs. The dataset could also be utilized for various tasks beyond those suggested by the authors, particularly in corporate database management systems (DBMS) where meaningful naming is often lacking. The availability of both original and renamed versions of the dataset is appreciated, as it allows users to select the variant that best suits their needs.

### Strengths and Weaknesses
Strengths:
- The dataset is large and represents a significant improvement over previous baselines in terms of the number of tables and databases.
- The construction methodology is well-designed and detailed, making it a valuable resource for the table representation learning community.
- Preliminary experiments indicate promising results, highlighting the dataset's potential impact on research.
- The authors effectively address reviewer concerns and clarify less-clear sections of the work.
- The publication of both versions of the dataset enhances its usability and applicability across different scenarios.
- The benchmark has the potential for significant impact on training LLMs and advancing data lake-oriented research.

Weaknesses:
- The average size of the tables is small, which may not be representative of typical relational databases.
- The experimental section lacks depth and compelling justification for the chosen tasks, limiting the demonstration of the dataset's usefulness.
- There is insufficient information on the computational resources required for dataset preparation and usage.
- Limited detail on the Graph Neural Network (GNN) and benchmark usage requirements in the main body of the paper.
- The future work section lacks an expansive discussion that could address additional reviewer points.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including comparisons that demonstrate how training or fine-tuning with WikiDBs enhances model performance on specific tasks, such as those in the SemTab dataset. Additionally, generating a more diverse collection of relational databases, particularly those with full numerical values, would enhance the dataset's applicability. 

We suggest providing more detailed information on the computational resources needed to work with WikiDBs, including dataset size and resource requirements for preprocessing. Furthermore, we advise the authors to clarify the rationale behind using cosine similarity for table matching and to explore alternative methods for evaluating join accuracy.

We recommend improving the main body of the paper by providing additional detail on the GNN and the requirements for using the benchmark. Further elaboration in the appendix could also be beneficial. Additionally, expanding the future work section with a more comprehensive discussion would address the points raised by reviewers. Lastly, we recommend considering a version of the dataset without GPT-generated column names to assess the impact of column name informativeness on model performance, which could provide insights into the real-world applicability of the dataset and its methods.