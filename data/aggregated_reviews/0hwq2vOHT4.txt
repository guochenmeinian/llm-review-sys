ID: 0hwq2vOHT4
Title: Described Object Detection: Liberating Object Detection with Flexible Expressions
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 7, 4, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new multi-modal computer vision task called Described Object Detection (DOD), which extends existing Open-Vocabulary Object Detection (OVD) and Referring Expression Comprehension (REC) tasks. The DOD task aims to detect multiple instances in images based on textual descriptions, including both presence and absence of objects. The authors introduce a new dataset, D3, designed for evaluation in the DOD domain, and evaluate existing methods alongside a proposed baseline method, OFA-DOD, which shows improved performance. The authors argue that DOD is a superset of OVD, although concerns arise regarding the lack of evaluation results on base classes, which are crucial for generalization. They acknowledge limitations in their dataset regarding class number and express a willingness to explore federated annotation in future work. The authors clarify that their dataset serves as an evaluation benchmark rather than a training set, emphasizing the importance of complete annotation.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant problem in language-driven object detection and presents a novel task that is important for real-world applications.
- The authors provide a comprehensive dataset and benchmark results, demonstrating high experimental quality and a thorough investigation of existing methods.
- The authors provide a clear rationale for their dataset's design and its focus on complete annotation.
- They effectively address concerns regarding the suitability of COCO-style mAP for multi-label evaluation.
- The dataset is positioned as a valuable benchmark for evaluating current models in the DOD domain.

Weaknesses:
- Clarity issues are prominent, particularly in the dataset creation process and the description of the OFA-DOD model. The annotation process lacks detail, and the baseline method is poorly described, making it difficult for readers to replicate the model.
- The argument that DOD is a superset of OVD is contested, as the evaluation lacks results on base classes, which are critical for validating this claim. Additionally, the term "little overlap" regarding category definitions lacks rigor and quantitative support.
- The avoidance of hierarchical relationships in category definitions may not reflect real-world complexities, limiting the dataset's applicability.
- The lack of a training dataset is seen as a significant limitation by some reviewers, and concerns remain about the real-world applicability of the evaluation pipeline due to its restrictions.
- The visualizations may not adequately represent all scenarios, leading to potential misunderstandings about the annotation process.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset creation process by providing detailed explanations of how CLIP was utilized for annotations, including the handling of multiple annotations and the creation of negative image annotations. A diagram illustrating the annotation process would enhance understanding. Additionally, the authors should clarify the architecture of the OFA-DOD model with a figure and ensure that the baseline method is described more comprehensively. To strengthen the argument for DOD as a superset of OVD, we suggest including results on base classes in the evaluation and providing a more rigorous quantification of the overlap between categories in D3 and the training datasets. To enhance the dataset's real-world applicability, consider incorporating hierarchical relationships in the category definitions and investigating more complex relationships in the evaluation pipeline. Lastly, we encourage the authors to revise the title to better reflect the content and significance of the proposed dataset.