ID: kjMGHTo8Cs
Title: Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 6, 6, 6, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of various pretraining objectives for imitation learning, specifically focusing on the effectiveness of inverse dynamics modeling (IDM) compared to other methods such as behavioral cloning and forward dynamics. The authors provide empirical evidence and theoretical arguments supporting the superiority of IDM, particularly in terms of out-of-distribution generalization and robustness in environments with latent context. The study includes a comprehensive evaluation across multiple tasks and datasets, demonstrating that IDM outperforms other pretraining objectives, especially in small data regimes.

### Strengths and Weaknesses
Strengths:
- The authors support their claims with compelling empirical evidence and novel theoretical arguments.
- The paper is clearly written and accessible, with extensive experimental results that illustrate the effectiveness of IDM.
- It provides valuable insights into the performance of different pretraining objectives and their ability to recover true ground truth states.

Weaknesses:
- The superior performance of IDM may be somewhat expected, as the architecture examined appears more suited for inverse dynamics than forward dynamics.
- The theoretical arguments lack formal claims and proofs, feeling more like intuitions rather than solid results.
- The paper does not clarify the objective used during the policy finetuning phase, leading to potential confusion regarding the evaluation metrics and action value constraints.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the finetuning phase objectives and the evaluation metrics used, particularly regarding the mean-squared error loss for inverse dynamics and the binary cross-entropy loss for action predictions. Additionally, addressing the limitations of relying on expert trajectories for pretraining and discussing the implications of using suboptimal data would enhance the paper's robustness. Including experiments in more complex environments and providing insights into the performance of IDM in such settings could further strengthen the findings. Finally, we suggest that the authors formalize their theoretical arguments to provide a more solid foundation for their claims.