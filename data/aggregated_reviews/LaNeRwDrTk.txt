ID: LaNeRwDrTk
Title: Residual Q-Learning: Offline and Online Policy Customization without Value
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 4, 8, 7, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel problem setting called policy customization, which aims to train a policy that inherits characteristics from a prior policy while meeting additional requirements from a downstream task. The authors propose a new framework, Residual Q-learning, which formulates the customization problem as a Markov Decision Process (MDP) with a reward function that integrates the inherent reward of the demonstration and the additional reward from the downstream task. The authors argue that residual Q-learning consistently outperforms traditional RL fine-tuning, particularly in environments like Parking and Ant, and emphasize the theoretical foundation of their method in interpreting the trade-off between imitative and add-on objectives. They derive a family of residual Q-learning algorithms that demonstrate effectiveness in various environments, achieving policy customization tasks. The framework's flexibility allows it to integrate with existing maximum-entropy RL algorithms without the need for inferring inherent rewards from demonstrations, distinguishing it from methods like AIRL.

### Strengths and Weaknesses
Strengths:
- The introduction of the policy customization problem addresses a significant issue in imitation learning, enhancing the applicability of imitation policies in real-world scenarios.
- The residual Q-learning framework shows significant performance improvements over RL fine-tuning in various environments, supporting the effectiveness and adaptability of the proposed algorithms.
- The theoretical grounding of the framework provides a principled approach to balancing imitative and add-on objectives.
- The methodology is feasible and scalable, applicable to various tasks without specific assumptions or constraints.
- The simplicity and flexibility of the algorithm facilitate integration with existing RL methods.

Weaknesses:
- The derivation process in Equation 5 lacks clarity, particularly the transitions between equations, necessitating a more detailed explanation to enhance reader understanding.
- The rationale behind the chosen measure for policy customization needs further clarification, including how policy expansion can be utilized when a prior policy is available.
- The performance gap between residual Q-learning and RL fine-tuning can depend heavily on the nature of the add-on rewards and their impact on the optimal policy.
- The authors acknowledge challenges in determining the trade-off between different rewards, which may complicate practical applications.
- The experimental evaluation is limited in scope; more comprehensive experiments with diverse scenarios and benchmark datasets are recommended to substantiate claims.
- The derived forms of Residual SQL and SAC appear similar to original formulations, requiring clearer explanations of their interpretations and implications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the derivation process in Equation 5 by providing a detailed, step-by-step explanation of the transitions between equations. Additionally, the authors should clarify the rationale for the chosen measure in the policy customization setting and elaborate on how policy expansion can be employed with an available prior policy. We also suggest that the authors improve clarity on how the choice of add-on rewards influences the performance of residual Q-learning, particularly in environments where the optimal policy is significantly altered. Providing more detailed guidance on selecting appropriate values for $\beta$ and $\lambda$ could enhance the reproducibility of results across different environments. To strengthen the experimental evaluation, we suggest conducting more extensive experiments across a wider range of environments and including comparisons with relevant baselines, such as RL fine-tuning methods. Finally, the authors should provide a clearer distinction between the derived forms of Residual SQL and SAC and their original counterparts to enhance understanding of the proposed modifications.