ID: MzDakXdBbM
Title: Can Large Language Models Fix Data Annotation Errors? An Empirical Study Using Debatepedia for Query-Focused Text Summarization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a methodology for enhancing the Debatepedia dataset using large language models (LLMs) to address query-summary mismatches in query-focused abstractive summarization. The authors propose two approaches: a rule-based method for filtering low-quality examples and LLMs for rewriting queries. The results indicate improved performance of summarization models fine-tuned on the LLM-annotated dataset.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant issue with the Debatepedia dataset and demonstrates effective use of LLMs for data cleaning.
- It provides a clear case study that highlights the potential of LLMs in improving noisy datasets.

Weaknesses:
- The scope is narrow, focusing solely on the Debatepedia dataset, limiting broader applicability.
- The evaluation lacks sufficient human comparison between original and LLM-cleaned queries, which is crucial for validating the effectiveness of the proposed methods.
- There are concerns regarding the clarity of the methodology, including the nature of the generated queries and the dataset sizes.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including a detailed human assessment comparing the quality of queries from the original and LLM-cleaned datasets. Additionally, clarifying the methodology regarding whether it involves fixing or filtering queries is essential. Providing statistics on dataset sizes post-regeneration and measuring the overlap between generated queries and reference summaries would enhance the paper's rigor. Finally, addressing the concerns about the marginal improvements and their statistical significance is crucial for validating the results.