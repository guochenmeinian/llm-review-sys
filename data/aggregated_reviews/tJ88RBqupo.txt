ID: tJ88RBqupo
Title: Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 6, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a model evaluation framework called 3S-Testing, which utilizes (conditional) deep generative models to create synthetic test sets, particularly aimed at improving performance estimation for underrepresented subgroups and addressing distributional shifts. The authors empirically validate their approach using CTGAN on tabular data, demonstrating its effectiveness in scenarios with limited real test sets.

### Strengths and Weaknesses
Strengths:  
- The motivation for targeting subdominant classes is clear, and the use of generative models for uncertainty estimation is an encouraging approach.  
- The experiments are well-organized and provide a solid foundation for the proposed methodology.  
- The paper is well-written, with a comprehensive examination of relevant literature and clear insights into the application of synthetic data for model evaluation.  

Weaknesses:  
- The introduction of uncertified models for uncertainty estimation raises concerns about the evaluation process, particularly regarding biases introduced by generative models.  
- The clarity of Section 4.2 is compromised by excessive notations and assumptions that lack sufficient explanation.  
- The focus on small datasets limits the generalizability of the findings, and the paper does not adequately address potential failure modes of the proposed method.  
- The empirical benefits of synthetic data are not theoretically evaluated, and the paper lacks visualizations to illustrate the performance of generative models compared to real data.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 4.2 by minimizing notations and assumptions, ensuring that each is clearly explained. Additionally, the authors should provide a theoretical evaluation of the inequation presented in Line 155 and include visualizations that compare synthetic and real data manifolds. To enhance the robustness of their findings, we suggest testing their method on larger datasets and discussing potential failure cases more thoroughly. Finally, addressing the biases of generative models on subgroups and clarifying the use of the trained generative model in the evaluation process would strengthen the paper.