ID: DV15UbHCY1
Title: Are Language Models Actually Useful for Time Series Forecasting?
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 8, 6, 8, -1
Original Confidences: 4, 4, 4, 4, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the effectiveness of LLMs in time series forecasting, challenging the prevailing belief in their utility. Through a series of ablation studies on three LLM-based forecasting methods, the authors conclude that removing or replacing the LLM component often yields better forecasting results. They assert that simpler models or encoders with basic attention mechanisms can perform equally well, questioning the trend of employing LLMs in this domain.

### Strengths and Weaknesses
Strengths:
- The paper provides a compelling perspective on LLMs in time series forecasting, challenging established assumptions.
- The empirical analysis is robust, featuring well-executed ablation studies and comparisons across multiple methods and datasets.
- The writing is clear and organized, with effective use of figures and tables to convey results.
- Findings may redirect research efforts towards more efficient forecasting methods.

Weaknesses:
- The scope is limited to time series forecasting, neglecting other applications of LLMs in time series analysis, such as imputation or anomaly detection.
- The datasets utilized consist solely of evenly spaced time series, which may not reflect the diversity of real-world data.
- There is insufficient discussion on why LLMs underperform compared to simpler models, lacking deeper theoretical insights.
- Some experiments are only presented for specific models, and comparisons with existing state-of-the-art models are lacking.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the theoretical reasons behind LLMs' underperformance in time series forecasting. Additionally, the authors should consider including a broader range of time series applications and datasets, particularly those with irregular intervals. It would also be beneficial to present experiments for all models consistently and to address the performance of pre-trained weights versus random initialization. Lastly, clarifying the implementation and differences of the "LTrsf" model compared to existing models would enhance the paper's comprehensiveness.