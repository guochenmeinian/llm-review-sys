ID: 8qEkjSEdls
Title: Off-policy estimation with adaptively collected data: the power of online learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 5, 5, -1, -1, -1, -1
Original Confidences: 4, 2, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to estimating linear functionals of the reward function in contextual bandit settings using adaptively collected data. The authors focus on augmented inverse propensity weighted (AIPW) estimators, proving guarantees regarding the estimator's quality based on the plugin estimator of the mean reward. They characterize the finite sample mean squared error (MSE) of the AIPW estimator and relate online learning of the plugin estimator to online non-parametric regression, establishing a regret bound linked to the quality of the plugin estimator learned online.

### Strengths and Weaknesses
Strengths:  
- The presentation is technically precise and mathematically thorough.  
- The paper extends the non-asymptotic theory of AIPW estimators to adaptively collected data and provides both upper and local minimax lower bounds on the MSE of the off-policy value.

Weaknesses:  
- There is a significant lack of discussion on relevant related work in finite sample approaches for off-policy evaluation in contextual bandits, such as the works by Waubly-Smith et al., Karampatziakis et al., and Wang et al. A comparison to these papers, both theoretically and experimentally, is recommended.  
- The contribution is limited to guarantees for specific types of plug-in estimators, primarily focusing on the MSE upper bound for the AIPW estimator.  
- The assumption of a constant exploration rate (Assumption 1) is restrictive, as other works have shown results with decaying exploration rates.  
- The writing is dense and lacks sufficient context, making it difficult to parse certain results, such as the final result in Section 4 and the rationale behind introducing the perturbed IPW estimator in Section 3.1.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related work by including comparisons to relevant literature on finite sample approaches for off-policy evaluation. Additionally, the authors should broaden the types of plug-in estimators for which they provide guarantees. Clarifying the assumptions regarding exploration rates and providing results for decaying exploration would enhance the paper's robustness. The authors should also strive to simplify the writing for better clarity, particularly in Section 4 and in explaining the connection between the perturbed IPW estimator and the AIPW estimator. Finally, including simulation experiments to verify theoretical results would significantly enhance the practical utility of the work.