ID: 3k5GFJEGem
Title: ParroT: Translating during Chat using Large Language Models tuned with Human Translation and Feedback
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ParroT, a framework aimed at enhancing machine translation performance of pre-trained LLMs by leveraging human-written translations and quality information about translations, specifically through error annotations. The authors propose various instruction types, including standard translation instructions, contrastive instructions, and error-guided instructions. The evaluation involves two LLMs, LLaMA and BLOOMZ-7B, showing that error-guided instructions yield improvements over vanilla approaches and instruction tuning with the Alpaca dataset. The authors also compare full fine-tuning with LoRA-based fine-tuning, revealing interesting performance differences across language pairs.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- The approach is interesting, with experiments indicating gains over vanilla baselines and providing valuable analyses.
- Different fine-tuning methods are explored, revealing significant performance variations based on language data sources.

Weaknesses:
- The approach relies heavily on the availability of human-labeled data, which is limited to a few datasets and languages, making scalability challenging.
- The experimental evaluation is constrained to a few language pairs, raising questions about the generalizability of the findings.
- The effectiveness of contrastive instructions is not supported by the results, and the paper lacks thorough quantitative analysis to convincingly demonstrate significant advantages over existing methods.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by expanding the evaluation to include a broader range of languages. Additionally, we suggest providing more baseline comparisons, particularly with larger models, to strengthen the claims of novelty and effectiveness. Clarifying the differences between error-guided and chain-of-thought-based methods could also enhance the paper's contributions. Finally, addressing the limitations of contrastive instructions with further empirical evidence would bolster the overall argument.