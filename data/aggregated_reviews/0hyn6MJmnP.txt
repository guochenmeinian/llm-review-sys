ID: 0hyn6MJmnP
Title: TADI: Topic-aware Attention and Powerful Dual-encoder Interaction for Recall in News Recommendation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a model called TADI (Topic-aware Attention and powerful Dual-encoder Interaction) for news recommendation, aiming to improve recall by addressing irrelevant word distraction through Topic-aware Attention (TA) and enhancing dual encoder interactions with Dual-encoder Interaction (DI). The authors claim that TADI achieves effective embedding learning and maintains high efficiency during inference, with experimental results demonstrating its effectiveness and robustness.

### Strengths and Weaknesses
Strengths:  
1. The proposed method is practical, solid, and reasonable, with promising and convincing experimental results.  
2. The writing is clear and easy to understand, and the model's modular design allows for adaptation to larger content-based encoders.  
3. The motivation for the model design is clear, and it achieves the highest performance compared to baseline models.  

Weaknesses:  
1. The paper relies on only two versions of the MIND dataset, which limits the robustness of the experimental validation.  
2. There is a significant discrepancy between the training and inference ranking objectives, which may negatively impact model performance.  
3. The efficiency comparisons with baselines in inference are insufficiently detailed, lacking rigorous time comparisons.  
4. The analysis of topic-aware attention is inadequate, and the paper does not address important related work that outperforms the proposed model.  

### Suggestions for Improvement
We recommend that the authors improve the experimental validation by incorporating more datasets beyond MIND, such as Addressa, to enhance robustness. Additionally, the authors should address the training/inference discrepancy by considering gradient surgery to mitigate potential conflicts. It is crucial to provide detailed efficiency comparisons with baselines to substantiate claims of interaction efficiency. Finally, we suggest including a discussion of the related work that demonstrates higher performance on the MIND-large dataset to provide a comprehensive context for the proposed approach.