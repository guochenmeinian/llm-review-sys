ID: XXPzBhOs4f
Title: Have it your way: Individualized Privacy Assignment for DP-SGD
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two variants of Differentially Private Stochastic Gradient Descent (DP-SGD) designed to achieve personalized differential privacy (PDP) by allowing different privacy budgets for distinct groups. The authors propose the **Sample** method, which adjusts sampling rates based on privacy budgets, and the **Scale** method, which modifies the noise multiplier by scaling the clipping norm of gradients. The paper provides theoretical privacy guarantees for these methods and demonstrates their effectiveness through experiments on common vision datasets, including the MNIST dataset, showing improvements in accuracy compared to traditional DP-SGD. The combination of Sample and Scale methods allows for arbitrary weighting, optimizing privacy parameters and addressing utility loss for groups with lower privacy budgets.

### Strengths and Weaknesses
Strengths:
1. The proposed methods address an important issue of varying privacy budgets among different user groups, enhancing overall utility.
2. Both **Sample** and **Scale** methods are easy to implement and come with theoretical guarantees.
3. The experimental evaluation is comprehensive, showcasing the advantages of the proposed methods with well-organized results, particularly for underrepresented data points.
4. The authors effectively address reviewer concerns and enhance clarity in their responses.

Weaknesses:
1. The privacy expectations are independent of model accuracy, which may not reflect real-world scenarios where minority groups might prefer stronger privacy.
2. The paper lacks a detailed theoretical analysis of the proposed individual privacy notion and its adaptation to DP-SGD.
3. The empirical evaluation is limited to convolutional architectures, raising questions about the generalizability of the results to other models and tasks.
4. The claims regarding the confidentiality of privacy preferences are unsupported by proof.
5. There remains a concern regarding the characterization of utility loss for groups with lower privacy budgets, particularly the slower convergence of DP-SGD for these groups.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis by providing a more specific discussion of the individual privacy notion and its implications for DP-SGD. Additionally, the authors should extend the experiments to include different model architectures, such as ResNets or transformers, to assess the robustness of their findings. It would also be beneficial to report accuracy metrics for different groups rather than just average accuracy, as this could reveal disparities in performance related to privacy assignments. Furthermore, we suggest providing a proof of the confidentiality of privacy preferences to substantiate the claims made regarding the security of the proposed methods. Lastly, we recommend improving the characterization of utility loss for groups with lower privacy budgets, specifically addressing the convergence issues related to DP-SGD, and clarifying that Class 0 is the underprivileged group in Appendix D.2 to enhance understanding.