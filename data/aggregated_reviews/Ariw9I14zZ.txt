ID: Ariw9I14zZ
Title: XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents XLM-V, a multilingual language model that enhances the previous XLM-R by expanding the vocabulary size to one million tokens. The authors propose a novel method for constructing this large vocabulary through lexical clustering, which facilitates vocabulary sharing among languages with high lexical overlap while minimizing sharing among those with less overlap. Experimental results across various benchmarks, including XNLI, FloRes-200, and MLQA, demonstrate the model's superiority over previous iterations.

### Strengths and Weaknesses
Strengths:
- The paper addresses the vocabulary bottleneck in multilingual models and presents a straightforward, well-written approach.
- Experimental results indicate improved performance on low-resource languages and various tasks, showcasing the model's effectiveness.
- The methodology is carefully executed, with thorough reporting and analysis.

Weaknesses:
- Some claims regarding the qualitative improvements in tokenization lack convincing evidence and could benefit from more extensive analysis.
- Minor inconsistencies in result presentation and clarity issues in figures and tables were noted.

### Suggestions for Improvement
We recommend that the authors improve the analysis of tokenization quality by comparing outputs with morphological analyzers and word segmentation systems for various languages. Additionally, please clarify the clustering approach's scalability as more languages are included and provide a comprehensive table detailing the languages covered in each dataset. Address the inconsistencies in bold-faced results across tables and ensure that all figures are visually interpretable. Lastly, correct any minor typographical and grammatical issues throughout the paper.