ID: D94QKZA7UP
Title: A One-Size-Fits-All Approach to Improving Randomness in Paper Assignment
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 6, 8, 5, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an enhancement to the Probability Limited Randomized Assignment (PLRA) algorithm for peer-review paper assignments, focusing on improving randomness while maintaining assignment quality. The authors propose the Perturbed Maximization framework, which replaces the linear objective of PLRA with a concave objective that discounts higher probability matchings. Theoretical and empirical evaluations demonstrate that the proposed algorithms, PM-E and PM-Q, outperform PLRA in terms of additional randomness metrics while achieving comparable assignment quality. However, the theoretical analysis regarding the approximation of the PM problem is insufficient.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written, with clear motivations and justifications for the proposed method.
2. The identification of additional randomness properties could inspire future research in randomized paper assignments.
3. The proposed algorithms are practical and can be efficiently implemented using convex optimization techniques.
4. The empirical results show superior performance of the proposed methods on real-world datasets.

Weaknesses:
1. The theoretical analysis lacks clarity, particularly regarding the approximation of the PM problem and the implications of relaxing the L2 norm constraint.
2. The motivations for randomness primarily reference PLRA, which may limit the novelty of the proposed approach.
3. The paper does not sufficiently compare the proposed methods against other baselines, including deterministic ones, and lacks a discussion on hyper-parameter sensitivity.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis by providing a clearer explanation of the approximation ratio and the implications of the soft constraint on the L2 norm. Additionally, we suggest including comparisons of the proposed methods against a broader range of baselines, including deterministic algorithms, to strengthen the evaluation. It would also be beneficial to discuss hyper-parameter sensitivity and its impact on the performance of PM-E and PM-Q. Finally, we encourage the authors to provide a more detailed motivation for the randomness metrics used and their relevance to the proposed objectives.