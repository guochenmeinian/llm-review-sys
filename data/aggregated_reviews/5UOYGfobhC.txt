ID: 5UOYGfobhC
Title: Uni3DETR: Unified 3D Detection Transformer
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 6, 5, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Uni3DETR, a unified 3D object detection framework designed to handle both indoor and outdoor scenes. The authors propose a transformer-based method that utilizes a mixture of learnable and non-learnable query points to effectively capture local and global information. Additionally, the introduction of a decoupled Intersection over Union (IoU) loss aims to enhance localization accuracy. The experimental results indicate that Uni3DETR performs well across various datasets, including SUN RGB-D, ScanNet, S3DIS, nuScenes, and KITTI. The paper also includes cross-dataset experiments that enhance its uniqueness, and the authors express a commitment to improving the quality of their work based on reviewer feedback.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant problem by proposing a unified architecture for 3D object detection across different environments.
- The method is straightforward and well-structured, making it easy to follow.
- Empirical results demonstrate strong performance and generalization ability in heterogeneous conditions.
- The inclusion of cross-dataset experiments is a notable feature that distinguishes Uni3DETR.
- The authors have shown responsiveness to reviewer feedback, indicating a willingness to enhance the paper's quality.

Weaknesses:
- The novelty of the approach is limited, primarily resembling a combination of existing architectures without substantial innovation.
- The experimental validation lacks comparisons with state-of-the-art methods, particularly on the KITTI dataset, where comparisons with 'PV-RCNN++' are missing.
- There is insufficient analysis of the computational complexity and memory footprint of the proposed method compared to existing techniques.
- The paper lacks comprehensive documentation of pre-processing steps and dataset preparation.
- There is a need for the release of all codes, including a single bash script to reproduce numbers on all combinations of train datasets, test datasets, models, and metrics.
- The performance of models with the same and different voxel sizes is not quantitatively reported in the tables.
- Cross-dataset experiments are not adequately detailed in the main paper.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the work by providing a more comprehensive analysis of the proposed architecture's unique contributions. Additionally, the authors should include comparisons with state-of-the-art methods, particularly on the KITTI dataset, to strengthen their claims. It would be beneficial to report test set results on KITTI and nuScenes, as well as a detailed breakdown of the computational complexity, including inference latency and FLOPS. Furthermore, we suggest exploring alternative designs, such as a dual-path architecture, to better capture local and global information. The authors should clarify the initialization process for the learnable query points and consider using a more robust evaluation metric for performance assessment. We also recommend improving the documentation of all pre-processing steps, from downloading datasets to preparing the datasets and environment. Additionally, they should release all codes, including training, inference, and cross-dataset inference, along with a single bash script to reproduce numbers on all combinations of train datasets, test datasets, models, and metrics. It is essential to release all pre-trained models trained individually on one dataset and on combinations of datasets with different voxel sizes and unified voxel sizes. Lastly, the authors should quantitatively report the performance of models with the same voxel size and different voxel sizes in the tables, and ensure that cross-dataset experiments are reported in the main paper.