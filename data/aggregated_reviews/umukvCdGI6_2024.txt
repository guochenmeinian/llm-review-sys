ID: umukvCdGI6
Title: DOFEN: Deep Oblivious Forest ENsemble
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 7, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DOFEN, a deep learning method for tabular data that integrates concepts from oblivious decision trees and random forests. DOFEN employs a novel differentiable oblivious tree model and a forest ensembling stage, achieving competitive performance on established benchmarks, closely matching boosted trees and outperforming other deep learning models. The authors compare DOFEN's training time with other deep learning methods, including Trompt, FT Transformer, and NODE, revealing that DOFEN's training time is approximately twice as long as Trompt's when using optimal hyperparameters. The inefficiency of group operations in DOFEN is identified as a potential reason for this extended training time. The authors also emphasize the importance of standard scaler normalization for improving model performance.

### Strengths and Weaknesses
Strengths:
- The model is evaluated on a recognized benchmark, demonstrating strong results.
- The ensembling stage is beneficial and could enhance other deep tree-based models.
- Comprehensive results are provided, including detailed assessments of model stability and hyperparameter effects.
- The paper is well-written and accessible, with sufficient explanations of formulas.
- The paper provides a thorough comparison of training times across various models, contributing valuable insights into the efficiency of DOFEN.
- The authors acknowledge the importance of data preprocessing and offer a link to the code for reproducing results, enhancing transparency.

Weaknesses:
1. The method's description can be confusing, particularly the mathematical notation; presenting it as an algorithm may enhance clarity. Details about the sub-networks $\Delta_i$ are lacking.
2. Training times are not reported, and the inference time is relatively long, which may hinder practical applications.
3. Randomization steps may slow convergence, requiring more training steps for optimal performance.
4. The training time for DOFEN is significantly longer than for other models, which may hinder its practical application.
5. Some reviewers reported difficulties in achieving the performance metrics claimed by the authors, suggesting potential issues with hyperparameter settings or data preprocessing.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the method's description, possibly by presenting it as an algorithm and providing more details on the sub-networks $\Delta_i$. Additionally, we suggest including training time metrics to better inform readers about the model's efficiency. To address the slow convergence speed, the authors should consider implementing optimization measures and discussing how randomness affects model stability and reproducibility. We also recommend that the authors improve the efficiency of group operations in DOFEN to reduce both training and inference times. Furthermore, we encourage the authors to ensure that all data preprocessing settings used in NODE are implemented in DOFEN, including standard scaler normalization of the target and the quantile noise setting, to address the discrepancies in performance. Finally, we encourage the authors to explore the interpretability of the DOFEN model and its performance across different data distributions.