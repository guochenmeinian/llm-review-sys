ID: GJpkvHMd7H
Title: Normalised Precision at Fixed Recall for Evaluating TAR
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: 2, 2, 2, 0, -1
Original Confidences: 4, 3, 4, 4, 5

Aggregated Review:
### Key Points
This paper presents a critical analysis of the measure ‘precision at r% recall’ for technology-assisted review (TAR) systems, highlighting its dependency on the dataset and the need for caution when using it across different topics. The authors propose normalized metrics, nP@r% and snP@r%, which offer a precision score scaled from 0 to 1, allowing for unbiased averaging across multiple queries. The paper demonstrates decreased variance and low correlation with the percentage of relevant documents, making it a valuable contribution to the field.

### Strengths and Weaknesses
Strengths:  
- The problem is clearly articulated, and the literature review is comprehensive.  
- The proposed metrics are intuitive and well-explained, with a solid theoretical foundation.  
- The writing is clear and concise, making the paper easy to follow.  

Weaknesses:  
- The insights derived from experiments are somewhat limited and lack statistical significance in certain comparisons.  
- There are inconsistencies in notation and definitions, particularly regarding the use of symbols in Section 2.  
- Some edge cases, such as scenarios with no non-relevant documents, are not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the discussion of edge cases, particularly regarding scenarios where there are no non-relevant documents, as this could lead to division by zero in nP@r%. Additionally, we suggest providing further explanations for Equation 8 to enhance clarity. Correcting typos, grammatical errors, and notation inconsistencies will also improve the overall presentation of the paper. Finally, defining the performance metrics listed in the related work section would aid in the paper's readability.