ID: WvoKwq12x5
Title: PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications
Conference: NeurIPS
Year: 2024
Number of Reviews: 41
Original Ratings: 7, 6, 4, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PediatricsGPT, the first Chinese pediatric LLM assistant, developed using PedCorpus, a comprehensive dataset with over 300,000 pediatric instructions. The authors propose a robust training pipeline that includes continuous pre-training, full-parameter supervised fine-tuning, and Direct Following Preference Optimization (DFPO). Performance evaluations indicate that PediatricsGPT outperforms existing Chinese medical AI models in various pediatric tasks. The authors emphasize the distinctiveness of their approach compared to existing models like DISC-MedLLM, particularly in dataset quality and model structure. The PedCorpus dataset incorporates diverse tasks, rich sources, and extensible instructions, addressing gaps in existing datasets.

### Strengths and Weaknesses
Strengths:
- The establishment of PedCorpus provides a high-quality, extensive dataset tailored for pediatric applications.
- The innovative training pipeline enhances the model's ability to generate responses that align with pediatric expertise.
- Thorough evaluation methods, including real doctor assessments, validate the model's effectiveness.
- The paper introduces a comprehensive training pipeline that effectively adapts LLMs to pediatric healthcare.
- The innovative mechanisms (DFPO and a mixture of universal-specific experts) enhance model performance and robustness.

Weaknesses:
- The techniques employed, such as DFPO, lack novelty as they adapt existing methods rather than introduce fundamentally new approaches.
- The paper does not sufficiently address the model's deployment in real-world settings or include evaluations from actual users, which is critical for pediatric applications.
- Ethical considerations regarding data usage and privacy are inadequately addressed, particularly concerning the use of real doctor-patient conversations.
- Concerns were raised regarding the dataset's alignment with real-world pediatric practice, suggesting it may not meet the practical needs of healthcare professionals.
- The reliance on previous works like DISC-MedLLM without sufficient differentiation in dataset quality may undermine the perceived novelty of the contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset construction process, detailing how manual sampling and automatic extraction were conducted. Additionally, we suggest including standard deviations for reported results and comparing the model's performance against leading medical LLMs, such as Meditron and Me-LLaMA. It is crucial to discuss the plan for real-world deployment and user evaluations, particularly from patients and their parents. Furthermore, we recommend that the authors consult with pediatric specialists to ensure that the dataset reflects the practical requirements of real-world clinical practice. It is also important to provide clearer distinctions between their dataset and existing datasets, particularly in terms of practical applicability and relevance to pediatric practitioners. Lastly, the authors should ensure that ethical guidelines are followed, particularly regarding data privacy and the use of human subjects, and clarify the licensing and open-source details of the datasets used.