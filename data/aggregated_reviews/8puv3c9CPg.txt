ID: 8puv3c9CPg
Title: Beyond the Doors of Perception: Vision Transformers Represent Relations Between Objects
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into how Vision Transformers (ViTs) perform visual relational reasoning, specifically through two tasks: identity discrimination and relational match-to-sample (RMTS). The authors propose that ViTs process information in two stages: a perceptual stage that extracts object features and a relational stage that compares these features. Mechanistic interpretability techniques validate these findings, revealing that failures in either stage hinder the model's ability to learn generalizable solutions.

### Strengths and Weaknesses
Strengths:
- The study addresses a significant issue in generalist vision models, contributing valuable insights to both psychology and AI.
- Clear definitions and demonstrations of the tasks are provided, supported by thorough analyses and visualizations.
- The proposed dataset and tasks offer potential for future research in this area.

Weaknesses:
- The related work section is inadequately presented, with only one paragraph dedicated to it, limiting context for the study.
- The theoretical basis for the observed two-stage processing is not well-explained, leaving questions about the conditions under which these phases emerge.
- The paper's focus on simpler tasks may lead to confirmation bias, and it lacks evaluations on more complex, real-world tasks.

### Suggestions for Improvement
We recommend that the authors improve the related work section by incorporating a broader range of literature to contextualize their findings, particularly studies that suggest two-phase processing in transformers. Additionally, providing a theoretical explanation for the distinct processing phases would strengthen the paper. To enhance clarity, we suggest clearer annotations and explanations for the plots, as well as including qualitative examples that intuitively describe how the model processes relational tasks. Finally, discussing potential empirical experiments to improve relational reasoning capabilities in vision models would add depth to the conclusions drawn.