ID: deaHiTb6Cu
Title: Fast Exact Leverage Score Sampling from Khatri-Rao Products with Applications to Tensor Decomposition
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 5, 7, 7, 5, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for efficient leverage score sampling from Khatri-Rao product matrices, which is crucial for applications in CP tensor decomposition using alternating least squares (ALS). The authors propose a novel data structure that preprocesses factor matrices to allow for sampling with a time complexity of \(O(NR^2 \log \max \{I_j, R\})\) per row. The theoretical foundation builds on prior work, particularly Theorem 3.1 from Malik et al. (2022), while introducing a binary-tree inversion sampling technique to enhance computational efficiency. The experimental results indicate that the proposed method outperforms existing leverage score sampling algorithms in terms of accuracy and efficiency.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and clearly presents its motivation and contributions.
- The novel data structure for leverage score sampling is a significant advancement.
- Experimental results demonstrate the method's effectiveness compared to previous algorithms, particularly in large-scale sparse tensor scenarios.

Weaknesses:
- Theoretical contributions, particularly Theorem 3.1, largely derive from existing literature, which may limit the perceived originality.
- Some sections, such as 3.1 and 3.2, lack clarity and could benefit from additional visual aids to summarize the sampling processes.
- The presentation of experimental results could be improved by including confidence intervals and clearer explanations of big-Oh complexities.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Sections 3.1 and 3.2 by adding a figure to summarize the sampling processes. Additionally, including confidence intervals in the experimental results would enhance the robustness of the findings. We also suggest revising the discussion of big-Oh complexities in Table 1 to provide clearer derivations and align them with Theorem 1.1. Finally, we encourage the authors to strengthen the motivation for their work by discussing additional applications beyond tensor decomposition.