ID: xDrKZOZEOc
Title: Fast T2T: Optimization Consistency Speeds Up Diffusion-Based Training-to-Testing Solving for Combinatorial Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 6, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Optimization Consistency Models (OptCM) as an innovative method for efficiently solving combinatorial optimization (CO) problems, specifically through supervised machine learning. The authors argue that OptCM addresses the computational intensity of traditional diffusion models by establishing direct mappings from noise levels to optimal solutions, facilitating rapid single-step solution generation. The contributions of this paper include significant reductions in computational overhead, consistent convergence of samples from different generative trajectories to optimal solutions, and enhanced solution exploration during inference. The authors emphasize the importance of generalization, particularly in out-of-distribution (OOD) settings, and the need for comprehensive evaluation against established heuristics across different graph types and densities.

### Strengths and Weaknesses
Strengths:  
- OptCM effectively reduces computational overhead, allowing for fast, single-step solution generation, which is advantageous for real-time applications.  
- The authors demonstrate significant improvements in runtime and solution quality compared to other diffusion-based methods.  
- The optimization consistency training protocol minimizes differences among samples from varying generative trajectories, ensuring robust solution generation.  
- The introduction of a consistency-based gradient search during testing enhances solution exploration and quality.  
- The paper effectively addresses the importance of generalization in machine learning for combinatorial optimization.

Weaknesses:  
- The advanced techniques in OptCM may be challenging to implement and require a deep understanding of the underlying principles.  
- The contribution is perceived as incremental, with concerns about the method's suitability for constrained combinatorial optimization problems.  
- The evaluation lacks comprehensiveness, particularly regarding performance on various graph types and the impact of training dataset size.  
- The model's performance is highly dependent on the quality and diversity of training problem instances, potentially limiting its effectiveness.  
- The paper lacks detailed comparisons with traditional, non-neural methods regarding performance and computational efficiency.  
- Generalization issues for supervised neural solvers, particularly in varying graph distributions, are not adequately addressed.  
- Scalability results for dense and sparse graphs are missing, and runtime comparisons do not include training times.  
- The authors' repeated requests for score reassessment may be perceived as diluting the review process.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational complexity of OptCM compared to traditional diffusion models and other state-of-the-art neural solvers. Additionally, the authors should clarify the specific optimizations implemented to achieve the reported speedup in solution generation. It is crucial to investigate how OptCM handles real-world graphs, particularly those that do not follow a specific degree distribution or density. The authors should also provide scalability results for larger problem sizes and include training times in runtime comparisons with heuristics and ILP solvers. Furthermore, we suggest that the authors improve the evaluation of their method by including results on the SATLIB dataset and exploring the performance of OptCM on graphs with varying densities and sizes. Specifically, the authors should assess the generalization capabilities of their model when trained on different values of p and utilize tools like NetworkX for generating diverse graph instances. Additionally, we recommend that the authors clarify how the CM operates in terms of generalization versus memorization and consider incorporating comparisons with other methods like KAMIS and CP-SAT for handling large, sparse graphs. Lastly, a more detailed discussion on the limitations of their approach and the specific scenarios where it excels would enhance the comprehensiveness of the study.