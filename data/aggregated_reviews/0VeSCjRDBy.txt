ID: 0VeSCjRDBy
Title: Adversarial Moment-Matching Distillation of Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 6, 7, 5, -1, -1
Original Confidences: 3, 3, 4, 2, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to knowledge distillation for large language models (LLMs) using an adversarial training method that incorporates both on and off-policy distillation. The authors motivate an RL-based formulation to minimize the imitation gap and introduce an adversarial training algorithm framed as a two-player minimax game. They provide a theoretical analysis showing that the proposed momentum-matching method offers a tighter bound on the imitation gap compared to traditional distribution-matching approaches. Experimental results demonstrate that the proposed method outperforms existing baselines in effectiveness across instruction-following and task-specific datasets.

### Strengths and Weaknesses
Strengths:
- The problem addressed is significant, and the method is novel and well-motivated.
- The paper is well-structured, presenting a clear narrative that facilitates understanding.
- Comprehensive experiments empirically validate the efficacy of the proposed method.

Weaknesses:
- The paper lacks a thorough comparison of the training costs of the proposed method relative to other approaches, limiting the assessment of its practical applicability.
- There is insufficient discussion regarding the computational complexity and memory overhead associated with the adversarial framework, particularly concerning the auxiliary networks for Q-value estimation.
- The experiments primarily focus on similar architectural frameworks, leaving the generalizability of the method across diverse models untested.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational and memory costs associated with their method, providing a detailed comparison with baseline methods. Additionally, we suggest conducting ablation studies to analyze the impact of on-policy and off-policy objectives on overall performance. Clarifying the parameterization of the Q-value function and detailing the training process for auxiliary models would enhance the paper's clarity. Finally, exploring the potential benefits of using different $\alpha$ values for on and off-policy optimization in Algorithm 1 could provide valuable insights.