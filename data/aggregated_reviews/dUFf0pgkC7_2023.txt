ID: dUFf0pgkC7
Title: HHD-Ethiopic: A Historical Handwritten Dataset for Ethiopic OCR with Baseline Models and Human-level Performance
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new OCR dataset for historical handwritten Ethiopic text recognition, named HHD-Ethiopic, which includes a training set and two test sets for evaluating model generalization. The dataset comprises text-line images paired with transcriptions from seven distinct books, ensuring diversity in cultural and religious content. The authors assess human-level performance and compare it with several state-of-the-art (SOTA) Transformer-based and CTC-based methods, providing a baseline for future research. They also propose evaluating existing OCR engines, particularly Tesseract, to validate their dataset. The authors acknowledge challenges with unbalanced class distribution of rare characters and have generated synthetic images to address this issue. They emphasize the importance of using appropriate evaluation metrics, particularly Normalized Edit Distance (NED), due to the extensive text sequences in their dataset.

### Strengths and Weaknesses
**Strengths:**
1. The paper is well-written and organized, clearly demonstrating its contributions.
2. It provides the first large dataset for recognizing historical handwritten Ethiopic text.
3. The dataset's diversity is notable, with images sourced from various cultural and religious contexts.
4. The authors take a proactive approach by generating synthetic data for underrepresented characters.
5. Detailed analyses of experimental results and human-level performance are included, along with the inclusion of additional evaluation metrics and SOTA methods.

**Weaknesses:**
1. The dataset lacks detailed statistical information, such as class distribution and image quality diversity.
2. The selection of models for experiments is inadequate, with some models not being widely used or validated.
3. The underrepresentation of rare characters in the dataset reduces its overall contribution.
4. Evaluating human-level performance is complicated due to varying participant counts.
5. The complexity of text-line accuracy metrics for longer sequences poses challenges in assessment.

### Suggestions for Improvement
1. We recommend that the authors add text line recognition accuracy results, as it is a commonly-used evaluation metric in text recognition.
2. The authors should provide a detailed description of the dataset's annotations and characteristics, including examples and statistical information on class distribution and image widths.
3. We suggest that the authors include additional baseline models originally proposed for Latin text recognition, such as CRNN, ASTER, SVTR, and ABINet.
4. The authors should consider generating synthetic datasets for training to improve model performance, as is common in Latin text recognition tasks.
5. We recommend using vector graphics or high-resolution images to enhance the dataset's quality.
6. The authors should clarify the differences between the IID and OOD test sets and provide more examples of the dataset's shortcomings to inspire future research.
7. We advise the authors to use additional evaluation metrics beyond CER, such as Normalized Edit Distance, to provide a more comprehensive assessment of model performance.
8. We recommend that the authors improve the clarity of their evaluation metrics, particularly in relation to the challenges posed by long text sequences.
9. We suggest that the authors consider removing TrOCR from the paper, as it may require more tuning for Ethiopic data, especially since they have already utilized other SOTA methods.
10. Finally, we encourage the authors to provide further details on the participant selection process to address concerns regarding the variability in human-level performance evaluations.