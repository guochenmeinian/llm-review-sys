ID: i4MutM2TZb
Title: Pre-trained Large Language Models Use Fourier Features to Compute Addition
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of how large language models (LLMs) perform addition, specifically focusing on the role of Fourier features. The authors demonstrate that different components of the model utilize distinct frequency ranges: the attention mechanism relies on high-frequency components, while the multi-layer perceptron (MLP) primarily uses low frequencies. The study shows that model embeddings and the pre-training process are crucial for learning these Fourier features, as models without pre-training struggle to induce them. Additionally, the authors explore token embeddings for numbers, detailing the construction and analysis of number embeddings using pre-trained language models. They clarify that the token embedding matrix, denoted as \( W_E \), corresponds to numbers in the range [0, 520], and discuss philosophical considerations regarding memorization versus computation in model performance, along with implications for arithmetic tasks.

### Strengths and Weaknesses
Strengths:
- The paper provides novel insights into the addition mechanism in LLMs, particularly regarding the role of Fourier components.
- It effectively highlights the importance of pre-training in enabling models to learn Fourier features.
- The authors provide a thorough explanation of the number embedding process and the significance of Fourier components, enhancing the reader's understanding of the model's behavior.
- The writing is clear, and the analyses are well-structured and visually presented, with a commitment to clarity and accuracy in addressing reviewer concerns.

Weaknesses:
- The study is limited to small number addition (less than 520) and does not explore larger numbers or multi-token representations, which restricts the generalizability of the findings.
- The experimental setup raises concerns, particularly regarding data leakage between training and test sets.
- Some figures are confusing, and the paper contains numerous typos, making it difficult to follow.
- Some statements, such as "primarily performed by the MLP modules alone," were initially misleading and required revision.
- The quantification of the superposition phenomenon remains unaddressed, which could limit the depth of analysis in the paper.

### Suggestions for Improvement
We recommend that the authors improve the experimental scope by including larger numbers and multi-token representations to enhance the generalizability of their findings. Additionally, addressing the potential data leakage in the training and test sets is crucial for validating the results. We suggest clarifying the role of positional embeddings in relation to Fourier features, as this could provide deeper insights into the model's behavior. Furthermore, we encourage the authors to refine the presentation of figures and correct typographical errors to enhance clarity. We also recommend that the authors improve the clarity of the discussion surrounding the superposition phenomenon by quantifying it specifically, as this could enhance the understanding of its impact on model performance. Lastly, we suggest incorporating the revised statement regarding the contributions of MLP and attention modules into the main body of the paper to accurately reflect the model's functionality and further exploring the implications of their findings on arithmetic tasks, particularly how leveraging Fourier features can enhance model performance.