ID: YHWXlESeS8
Title: Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new LLM benchmark called JEEBench, consisting of pre-engineering mathematics, physics, and chemistry problems derived from the IIT JEE-Advanced exam. The benchmark aims to evaluate LLMs' long-horizon reasoning and in-depth domain knowledge. The authors propose an innovative method for effective response selection using confidence-thresholding over self-consistency and identify common failure modes of GPT-4, including errors in algebraic manipulation and difficulties in translating abstract concepts into mathematical equations.

### Strengths and Weaknesses
Strengths:
- The benchmark effectively challenges current LLMs, including GPT-4 and PaLM-2, and is crucial for assessing and guiding LLM development.
- The paper provides a comprehensive evaluation of various models, covering aspects such as performance, calibration, and self-knowledge assessment.
- The qualitative analysis of error cases is extensive and insightful, with a clear taxonomy of errors.
- The writing is well-organized and clear.

Weaknesses:
- The omission of few-shot prompted evaluations limits the validation of the benchmark's utility.
- There is limited analysis of performance differences between models, which could provide valuable insights.
- Concerns exist regarding the dataset's potential inclusion in LLM training corpora, necessitating justification from the authors.

### Suggestions for Improvement
We recommend that the authors improve the validation of the benchmark by including few-shot prompted evaluations, as this would enhance the understanding of its challenge level. Additionally, we suggest providing a more detailed analysis of performance differences between models to elucidate trends. To address concerns about the dataset's training corpus inclusion, we recommend that the authors provide justification regarding the accessibility of the exam and answers used. Furthermore, we encourage the authors to explore the integration of calculation tools, such as Python or Wolfram Alpha, to assess LLM performance more comprehensively. Lastly, we advise enhancing the clarity of examples presented in the paper and addressing the copyrights of the dataset.