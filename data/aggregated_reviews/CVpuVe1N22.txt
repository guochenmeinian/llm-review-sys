ID: CVpuVe1N22
Title: Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 7, 7, 10, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Uncertainty of Thoughts (UoT) algorithm, which enhances large language models (LLMs) by enabling them to actively seek information through effective questioning. UoT integrates an uncertainty-aware simulation approach, uncertainty-based rewards motivated by information gain, and a reward propagation scheme for optimal question selection. Evaluations across multiple LLMs in scenarios such as medical diagnosis, troubleshooting, and the ‘20 Questions’ game demonstrate significant improvements in task completion rates and efficiency, highlighting UoT's effectiveness in reducing model uncertainty and enhancing information-seeking behavior. Additionally, the authors compare Direct Prompting (DP) and UoT methods using Llama 3 and GPT-4 across five datasets, with statistical significance tests indicating that UoT significantly outperforms DP (p < 0.05). The authors clarify their approach to uncertainty in LLMs, advocating for confidence-based sampling to enhance model performance, while addressing the complexity of assessing information gain in open-ended responses.

### Strengths and Weaknesses
Strengths:
1. The introduction of uncertainty-aware planning and reward propagation for question generation is a novel and significant contribution to improving LLMs’ performance in interactive environments.
2. The experiments are well-designed, covering diverse scenarios and multiple LLMs, providing robust evidence of UoT’s effectiveness.
3. The algorithm achieves substantial improvements in success rates and efficiency, effectively using entropy and information gain to measure and reduce uncertainty.
4. The availability of codes supports replication and future research.
5. The authors provide comprehensive statistical results demonstrating the superiority of UoT over DP.
6. The response to reviewer queries shows a willingness to clarify and improve the methodology, particularly regarding uncertainty and information gain.

Weaknesses:
1. The UoT framework's complexity, particularly its simulation and reward propagation components, may hinder implementation and integration into existing LLM systems without substantial computational resources. Additionally, the paper lacks an analysis and comparison of inference time between methods, which significantly affects user experience.
2. The evaluation primarily relies on success rates and conversation lengths, but additional qualitative analyses of generated questions and their impact on decision-making could provide deeper insights, particularly regarding user experience and the naturalness and relevance of questions.
3. The absence of significance test results in the main body of the paper initially detracted from the clarity of the findings.
4. The justification for using accumulated rewards over immediate rewards in the context of information gain could be more robust, as some reviewers expressed skepticism regarding its necessity.

### Suggestions for Improvement
We recommend that the authors improve the analysis and comparison of inference times for each method to address potential user experience issues. Additionally, incorporating qualitative analyses of the generated questions and their effects on decision-making would enhance the depth of evaluation. Including at least one human-based experiment would validate the approach in real-life scenarios, as the current evaluation using GPT-4 as the answerer may not reflect actual performance. We also recommend improving the clarity of the significance test results by including them in the main body of the paper. Furthermore, we suggest revisiting the discussion on accumulated rewards versus immediate rewards to address reviewer concerns, particularly regarding the relevance of intermediate gains in the context of information acquisition. Lastly, we encourage the authors to refine their terminology related to reward design to better align with established concepts in reinforcement learning and planning.