ID: mHtOyh5taj
Title: Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 7, 7, 7, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents Compare2Score, an LMM-based no-reference image quality assessment (IQA) framework that generates qualitative comparisons between images and translates these into continuous quality scores. The authors propose a soft comparison method that computes the likelihood of a test image being preferred over multiple anchor images, followed by MAP estimation for final scoring. Extensive experiments validate its state-of-the-art performance across various synthetic and realistic distortion datasets.

### Strengths and Weaknesses
Strengths:  
1. The motivation for using relative comparisons instead of absolute ratings is well-justified, facilitating the integration of multiple IQA datasets.  
2. The innovative "soft comparison" inference strategy effectively scores images for relative quality comparison.  
3. The model demonstrates state-of-the-art performance on benchmark IQA datasets under diverse testing conditions.  
4. The paper is well-written and easy to follow, with good reproducibility as code is provided.  

Weaknesses:  
1. The authors do not provide an in-depth analysis of the model's generalization capabilities on unseen distortions and datasets.  
2. Performance results on IDEFICS2, KADID-10k, and KonIQ-10k are missing from Tables 3 and 4.  
3. The impact of anchor image selection on model performance and potential biases requires further exploration.  
4. The title should be revised to reflect that only one large multimodal model is utilized.  
5. Cost analyses and ablation studies on the network structure are insufficient.  

### Suggestions for Improvement
We recommend that the authors improve the analysis of the model's generalization capabilities on unseen distortions and datasets. Additionally, please include the missing performance results for IDEFICS2, KADID-10k, and KonIQ-10k in the relevant tables. We suggest exploring different methods for anchor image selection and discussing how variations in standard deviation affect the pairing process and quality scores. Furthermore, we encourage the authors to provide more details on the running time computation, including the device and input images used. Lastly, we recommend revising the title to accurately reflect the use of a single model and enhancing the discussion on cost analyses and ablation studies.