ID: RuxBLfiEqI
Title: Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an innovative methodology for out-of-distribution (OOD) detection by applying perturbations to OOD samples, enabling the model to experience a more diverse set of OOD samples during training. The authors define these perturbations using an adversarial loss based on the uniform distribution loss, providing directional guidance for each instance. Results indicate that perturbed OOD samples enhance performance across key metrics such as False Positive Rate at 95% Recall (FPR95), Area Under the Receiver Operating Characteristic Curve (AUROC), and Area Under the Precision-Recall Curve (AUPR) compared to existing methodologies.

### Strengths and Weaknesses
Strengths:
- The novel approach of applying perturbations to broaden the sample diversity and target those near the decision boundary is intriguing.
- The adversariality concept against the uniform loss is a noteworthy property that warrants further exploration.
- A comprehensive ablation study on hyperparameters enhances the methodology's robustness.
- t-SNE visualizations effectively illustrate the proximity of perturbed OOD samples to in-class samples.

Weaknesses:
- The manuscript lacks empirical evidence demonstrating the superiority of adversarial perturbations over other types, necessitating systematic experimentation across various datasets.
- The rationale for simultaneously considering both perturbed and unperturbed OOD samples in the loss function is insufficiently explained, and a comparative analysis of using only one type would be beneficial.
- The ratio of in-distribution to OOD samples in training needs clarification, as it significantly impacts model performance.
- The manuscript does not adequately address how OOD samples are directed towards specific classes during the perturbation process.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation of their method by systematically comparing adversarial perturbations with other perturbation types across diverse datasets. Additionally, the authors should clarify the rationale for including both perturbed and unperturbed samples in the loss function and provide comparative results for each scenario. It is crucial to discuss the training ratio of in-distribution to OOD samples and its implications on model robustness. Lastly, the authors should elucidate how the gradient direction influences the classification of OOD samples during the perturbation process.