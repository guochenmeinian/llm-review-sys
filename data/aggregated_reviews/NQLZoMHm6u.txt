ID: NQLZoMHm6u
Title: NewTerm: Benchmarking Real-Time New Terms for Large Language Models with Annual Updates
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents NewTerms, a dataset and benchmark designed to evaluate the ability of large language models (LLMs) to understand new terms beyond their knowledge cutoffs. The authors propose a systematic and highly automated data curation pipeline that generates reliable questions, allowing for efficient benchmark generation across various tasks. They conduct benchmarking studies on popular LLMs, revealing significant insights into how these models learn new terms. While the authors acknowledge limitations in coverage, particularly regarding specialized terminology from non-English languages and niche domains, they assert that their pipeline can adapt to include a broader range of terms. Additionally, they address concerns about potential biases in term selection from online dictionaries and the benchmark's applicability to multilingual models.

### Strengths and Weaknesses
Strengths:
1. The systematic and rigorous approach to benchmark curation, including careful selection of new terms based on frequency and difficulty, is commendable.
2. The evaluation methodology is thorough, providing not only accuracy metrics but also a baseline for comparison, enhancing the relevance of the analysis.
3. The high degree of automation in benchmark generation is a notable strength, facilitating future research in LLM evaluation.
4. The automated construction pipeline is cost-effective and flexible, allowing for the generation of benchmarks with minimal human intervention.
5. The benchmark incorporates a diverse range of new terms, including professional terminology and dialects, which enhances its applicability.
6. The authors provide detailed analyses of LLM performance across different tasks, illustrating the strengths and weaknesses of various models.
7. The paper is well-organized and mostly clearly written.

Weaknesses:
1. The paper does not address how to improve LLMs' performance on new terms, leaving a significant question unanswered.
2. The use of unfiltered samples in Section 4.3 lacks justification, potentially introducing noise into the analysis.
3. Some sections contain minor confusions that detract from clarity, such as unclear relationships between perplexity and question difficulty.
4. The benchmark may not fully represent specialized terms from niche domains, limiting its usefulness for specific evaluations.
5. The reliance on online dictionaries for term selection could introduce biases, affecting the benchmark's representativeness.
6. The current focus on English terms may restrict its relevance for researchers working on multilingual NLP systems.

### Suggestions for Improvement
We recommend that the authors improve the discussion on how to enhance LLM performance on new terms, possibly by exploring retrieval-augmented generation (RAG) methods that incorporate up-to-date information. Additionally, we suggest providing justifications for the use of unfiltered samples in Section 4.3 to clarify their relevance. To enhance clarity, we encourage the authors to address specific confusions noted in the reviews, such as the relationship between perplexity and question difficulty, and the implications of LLMs learning new terms from different time periods. We also recommend improving the benchmark's coverage by incorporating more specialized terms from fields such as law and medicine and expanding the term selection process to include non-English languages and dialects to enhance multilingual applicability. To address potential biases, we suggest diversifying the data sources beyond online dictionaries. Furthermore, we encourage the authors to present more concrete examples of LLM performance variations across different tasks to provide deeper insights into their findings. Lastly, we recommend considering more frequent updates to the benchmark, such as monthly or quarterly, to keep pace with rapid developments in LLM research.