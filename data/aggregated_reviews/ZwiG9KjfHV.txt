ID: ZwiG9KjfHV
Title: OneBit: Towards Extremely Low-bit Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents OneBit, a framework for quantizing large language models (LLMs) to 1-bit weight matrices, introducing a novel parameter representation and an effective initialization method based on matrix decomposition. The proposed method maintains at least 81% of the original model's performance despite significant bit-width reduction. OneBit employs a new binary quantization design and modifies traditional quantization-aware training (QAT) by incorporating reconstruction error into the loss function. The experiments demonstrate superior results compared to existing low-bit quantization methods and address the feasibility of using quantized large models versus smaller models.

### Strengths and Weaknesses
Strengths:
- The paper proposes an aggressive compression method that is well-organized and easy to follow, exploring a meaningful research direction.
- The proposed method achieves good performance, maintaining at least 81% efficacy of the unquantized model.
- Experiments show that OneBit outperforms other methods in various tasks and addresses important questions in LLM research.

Weaknesses:
- The lack of a specialized CUDA kernel for optimizing binary operations complicates the evaluation of computational costs associated with the additional FP vectors.
- There are concerns regarding the absence of a specialized ablation study to assess the impact of the additional floating-point vectors on performance.
- The paper does not provide sufficient analysis of inference latency and throughput relative to accuracy, nor does it compare with other one-bit quantization methods like BitNet.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including comparisons with other one-bit quantization methods, such as BitNet, and provide empirical results for larger models like Llama-70b. Additionally, we suggest conducting an ablation study to clarify the impact of the floating-point vectors on performance and including assessments on generative tasks to enhance the versatility of the proposed method. Furthermore, addressing the clarity of figures in gray-scale and ensuring code availability would strengthen the paper.