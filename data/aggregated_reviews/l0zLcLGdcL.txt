ID: l0zLcLGdcL
Title: Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 3, 6, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Memory-Assisted Sub-Prototype Mining (MemSPM) method aimed at enhancing Universal Domain Adaptation (UniDA) by leveraging intra-class discrimination. The authors propose that MemSPM can effectively learn task-oriented features by mining sub-prototypes from the source data, thus addressing significant concept shifts. The method incorporates a reconstruction task for interpretability and is evaluated across four datasets under various domain adaptation settings.

### Strengths and Weaknesses
Strengths:
- The motivation to exploit intra-class structures is novel and intriguing within the UniDA community.
- The technical contributions are generally well-explained, and the paper is easy to follow.
- The proposed method shows substantial performance improvements over existing methods, particularly in the context of using a CLIP-pretrained backbone.

Weaknesses:
- The analysis supporting the effectiveness of exploiting intra-class structures is insufficient, with concerns about the added complexity of subclass learning and its benefits.
- The introduction of numerous hyper-parameters (e.g., $N$, $S$, $K$, $\lambda$, $\lambda_1$, $\lambda_2$, $\lambda_3$) lacks thorough investigation across different datasets.
- The paper does not adequately compare the proposed method against a simple baseline, nor does it provide sufficient ablation studies for the loss terms.
- Results using ResNet50 are deemed meaningless as the method was not tested on this backbone.
- The interpretability claims based on reconstruction are questionable, and the loss function $\mathcal{L}_{cdd}$ is not sufficiently detailed.
- Minor issues include typos and unclear mathematical expressions.

### Suggestions for Improvement
We recommend that the authors improve the analysis of subclass learning to justify its benefits for UniDA. A solid investigation into the impact of hyper-parameters across various datasets is essential. Additionally, we suggest including comparisons with a standard training baseline to better contextualize the proposed method's performance. The authors should conduct more comprehensive ablation studies to clarify the contributions of different loss terms and provide clearer explanations of the loss function $\mathcal{L}_{cdd}$. Addressing the interpretability claims with stronger evidence and ensuring clarity in mathematical expressions will enhance the paper's rigor. Finally, a thorough proofreading to correct typos and grammatical errors is necessary.