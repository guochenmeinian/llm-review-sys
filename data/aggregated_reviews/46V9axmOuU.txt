ID: 46V9axmOuU
Title: AP-Adapter: Improving Generalization of Automatic Prompts on Unseen Text-to-Image Diffusion Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents model-generalized automatic prompt optimization (MGAPO), a method designed to enhance the generalization of automatic prompts for unseen text-to-image models. The authors propose a two-stage approach called AP-Adapter, where the first stage involves generating keyword prompts using a large language model (LLM), and the second stage constructs an enhanced representation space through inter-model differences, adjusting prompt representations with domain prototypes. Experimental results indicate that AP-Adapter generates high-quality images on unseen diffusion models, surpassing existing methods in semantic consistency and aesthetic quality.

### Strengths and Weaknesses
Strengths:
- The introduction of the MGAPO task enhances the applicability of text-to-image model design in real-world scenarios.
- The paper clearly articulates related concepts and differences, particularly illustrated in Figure 1.
- The authors provide a new perspective on automatic prompt optimization at the feature level and demonstrate the effect of each loss component in Table 2.

Weaknesses:
- The core computation of domain prototypes, influenced by CLIP, lacks relevant ablation experiments.
- Human evaluation in Figure 4 introduces subjectivity, and the authors do not provide detailed explanations regarding this validation.
- The complexity of the proposed method, which includes additional models like a pre-trained LLM and CLIP, increases training and inference time without discussion of these implications.
- AP-Adapter relies on a large number of manually designed prompts for training, complicating data preparation.
- The performance improvement over baselines appears marginal, particularly when compared to manual prompts, raising questions about the fundamental limitations of automatic prompting.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in the methodology section, to enhance comprehension. Additionally, we suggest conducting ablation studies on the computation of domain prototypes to validate their influence. The authors should also provide a more detailed explanation of the human validation process and consider discussing the implications of the complexity introduced by additional models. Furthermore, we encourage the authors to explore ways to reduce reliance on manually designed prompts and to address the observed marginal performance gains compared to manual prompting. Lastly, clarifying the potential biases introduced by the consistent training and testing data from CIVITAI would strengthen the evaluation's fairness.