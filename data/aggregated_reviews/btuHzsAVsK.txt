ID: btuHzsAVsK
Title: Flow Snapshot Neurons in Action: Deep Neural Networks Generalize to Biological Motion Perception
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 6, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Motion Perceiver (MP), a novel AI model aimed at enhancing action recognition in biological motion perception (BMP) tasks. The authors introduce a comprehensive BMP benchmark dataset with 62,656 video stimuli across 24 conditions, demonstrating that MP significantly outperforms existing models by 29% in top-1 action recognition accuracy. The model utilizes patch-level optical flows and innovative neuron mechanisms, aligning its performance closely with human behavior in BMP tasks. The authors conduct additional experiments that validate the model's performance, particularly its surprising generalization from point light displays to RGB videos, supporting the hypothesis that the model architecture provides a strong mechanism for understanding motion. However, the connection to neuroscience is weak, and the authors should clarify the rationale for using a ViT as a feature extractor. Ablation studies are also conducted to identify critical components of the model.

### Strengths and Weaknesses
Strengths:
- The paper introduces a well-designed BMP benchmark dataset that challenges existing methods and provides a new standard for evaluating both human and AI performance.
- The methodology is rigorous, with detailed descriptions of the MP model architecture and experimental setup, showcasing strong evidence for the paperâ€™s claims.
- The model demonstrates strong generalization capabilities, particularly in transitioning from point light displays to RGB videos.
- The additional experiments enhance the paper's credibility and support the hypothesis regarding the model's robustness.
- The writing is clear, with effective use of figures and tables to illustrate key concepts and results.

Weaknesses:
- The paper lacks stronger baselines, as existing models are trained on a limited dataset, undermining fair comparison. The authors should consider fine-tuning from pretrained video models using self-supervised learning algorithms like VideoMAE.
- There is insufficient demonstration of how models trained directly on joint or sequential point videos perform, which is crucial for understanding the relative effectiveness of the new model.
- The performance of the model on higher-frequency point light displays is somewhat disappointing, as other methods slightly outperform it, raising concerns about its robustness in these scenarios.
- The expectation that other models would fail more dramatically on challenging stimuli was not met, indicating potential areas for improvement.
- The claim of human-like robustness is overstated, as the model does not consistently outperform others across all stimulus types, and the performance drop compared to humans is significant.

### Suggestions for Improvement
We recommend that the authors improve the comparison by including stronger baselines, such as models fine-tuned from pretrained video models using self-supervised learning algorithms like VideoMAE. Additionally, the authors should demonstrate how models trained directly on joint or sequential point videos perform to contextualize the new model's effectiveness. It is also essential to strengthen the connection to neuroscience by providing a rationale for the choice of ViT and exploring the significance of time resolution in action detection. Furthermore, we suggest that the authors enhance the model's performance on higher-frequency point light displays to improve its competitiveness against other methods. Finally, the authors should reconsider the claims regarding human-like robustness and provide a more nuanced discussion of the model's performance across different stimulus types.