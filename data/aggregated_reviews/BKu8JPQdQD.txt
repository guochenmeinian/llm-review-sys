ID: BKu8JPQdQD
Title: PUZZLES: A Benchmark for Neural Algorithmic Reasoning
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the PUZZLES benchmark, designed to evaluate the logical reasoning capabilities of reinforcement learning (RL) agents through a set of 40 diverse logic puzzles derived from Simon Tatham's Portable Puzzle Collection. The benchmark allows for adjustable puzzle sizes and difficulty levels, facilitating the assessment of agent generalization. The authors conducted baseline evaluations using various RL algorithms, demonstrating the challenges agents face in solving these puzzles and highlighting the benefits of action masking and discrete internal state observations. Additionally, the paper offers an interface for direct integration and evaluation of large language models (LLMs), with baseline evaluations conducted on the easiest puzzle settings for both GPT-4o Mini and Gemini 1.5 Flash. Results indicate that GPT-4o Mini solved slightly more puzzles than Gemini 1.5 Flash, albeit with a significantly longer evaluation time. A human expert evaluation showed a 100% success rate on all puzzles at the easiest difficulty level, while AI agents consistently solved only the untangle and cube puzzles.

### Strengths and Weaknesses
Strengths:
- PUZZLES offers a diverse set of puzzles, enabling thorough evaluations of algorithmic reasoning capabilities.
- The benchmark fills a significant gap in RL research by focusing on logical reasoning in single-agent environments.
- Adjustable difficulty levels enhance the ability to test generalization and adaptability of RL algorithms.
- Extensive evaluations of various RL algorithms provide valuable insights into their current capabilities and limitations.
- The integration of a user-friendly interface for LLM evaluation enhances accessibility.
- Comprehensive baseline evaluations provide clear comparative insights between different models.
- The human expert's performance establishes a strong benchmark for AI agents.

Weaknesses:
- The focus on RL may limit the benchmark's significance; incorporating interfaces for large language models (LLMs) could enhance its impact.
- The performance metrics primarily rely on average episode length, which may skew results; success rates should also be reported for better clarity.
- Limited exploration of advanced architectures like graph neural networks (GNNs) and the rationale for puzzle selection is lacking.
- The absence of human performance comparisons and a deeper analysis of algorithm failures restricts the benchmark's contextual understanding.
- The evaluation time for GPT-4o Mini is considerably longer than that for Gemini 1.5 Flash, raising concerns about efficiency.
- Missing results for four puzzles may limit the completeness of the findings.

### Suggestions for Improvement
We recommend that the authors improve the benchmark's significance by providing interfaces for large language models to evaluate their reasoning capabilities on the puzzles. Additionally, consider exploring alternative reward structures beyond sparse rewards to enhance learning efficiency. We suggest including comparisons to human performance to contextualize agent capabilities and conducting a detailed analysis of algorithm failures to inform future improvements. Furthermore, we encourage the authors to investigate the potential of advanced architectures like GNNs and to provide a clearer rationale for the selection of puzzles included in the benchmark. Lastly, we recommend reporting success rates alongside average episode lengths to offer a more comprehensive view of agent performance and improving the efficiency of the evaluation process for GPT-4o Mini to match the performance of Gemini 1.5 Flash. It is also crucial to include the missing results for the four puzzles in the final manuscript to ensure a comprehensive evaluation.