ID: UvX8QfhfUx
Title: Pgx: Hardware-Accelerated Parallel Game Simulators for Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 9, 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PGX, a GPU-based simulator for board games, developed in JAX, which utilizes auto-vectorization and Just-In-Time (JIT) compilation to significantly enhance the throughput of reinforcement learning (RL) environment simulations. PGX achieves speedups of 10x-100x compared to existing Python implementations like OpenSpiel and PettingZoo, enabling efficient simulations of games such as Go 19x19 and Chess. The authors demonstrate successful end-to-end training experiments using Gumbel AlphaZero across multiple board games, providing solid RL baselines that lower the barrier for researchers in this domain. Additionally, the authors acknowledge limitations in the API's extensibility and the current focus on standard board games, which may restrict its broader applicability. The manuscript has been updated to improve clarity in figures, enhance documentation, and address discrepancies in the motivation for environment choices.

### Strengths and Weaknesses
Strengths:
- **Affordability of Board Game RL**: PGX reduces the cost of conducting RL research in board games, simulating environments much faster than previous methods.
- **Successful End-to-End Pipeline**: The authors showcase effective training experiments, achieving rapid training times (e.g., a Go 9x9 agent in 2 hours using 8 A100 GPUs) at a low cost.
- The authors have made significant improvements to clarity and documentation based on reviewer feedback.
- The inclusion of diverse environments, including board games and MinAtari, showcases the library's versatility.
- The manuscript updates have enhanced readability and addressed previous concerns effectively.

Weaknesses:
- **Limited Game Variety**: The focus on standard board games restricts the environment's applicability, with exciting additions still pending implementation.
- The API's limitations in extensibility may hinder its broader adoption.
- The rationale for including certain environments, particularly single-player and MinAtari games, lacks clarity.
- Some figures, particularly Figure 3 and Figure 4, lack clarity and could benefit from better presentation and explanation of results.
- There is a need for more comprehensive benchmarking and the inclusion of baseline models for various environments.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their figures, particularly by enhancing Figure 3 to better illustrate throughput results and by providing a clearer rationale for the purpose of Figure 4. Additionally, we suggest that the authors improve the rationale for the choice of implemented environments, particularly regarding the inclusion of MinAtari games and single-player games like 2048. Including a detailed description of the PGX API in the main body of the paper would enhance usability, particularly by outlining the attributes in the state/observation. We also recommend conducting more extensive RL experiments and providing baseline models for all environments, including Go 19x19, Chess, and Shogi. Furthermore, addressing the time required for environment resets and discussing the scalability bottlenecks when transitioning from 1 GPU to 8 GPUs would provide a more comprehensive understanding of performance implications. Lastly, we encourage the authors to consider adding real-world environments to their benchmarks to illustrate the broader applicability of their design methodology.