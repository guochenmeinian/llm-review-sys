ID: AVd7DpiooC
Title: QKFormer: Hierarchical Spiking Transformer using Q-K Attention
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 5, 6, 8, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel spiking transformer, QKFormer, which features a spike-form Q-K attention mechanism with linear complexity, a hierarchical structure for multi-scale spiking representation, and a spiking patch embedding with a deformed shortcut to optimize spiking information transmission. The model achieves a top-1 accuracy of 85.65% on the ImageNet-1k dataset, marking a significant milestone as the first directly trained spiking neural network to exceed the 85% accuracy threshold.

### Strengths and Weaknesses
Strengths:
1. The introduction of a spike-form Q-K attention module with linear complexity significantly mitigates the computational complexity of spiking self-attention, showcasing originality and optimization in spiking neural network architectures.
2. The design of the spiking patch embedding with deformed shortcut (SPEDS) enhances spiking information transmission and integration, contributing positively to the QKFormer model's performance.
3. The QKFormer model outperforms state-of-the-art spiking neural networks on various datasets, particularly achieving over 85% accuracy on ImageNet-1K, indicating its potential impact on SNN research.

Weaknesses:
1. The assertion that SPEDS facilitates spiking information transmission lacks sufficient elaboration on the underlying mechanisms.
2. Clarification on why the low variance of Q-K attention negates the need for scaling operations would enhance the paper's rigor.
3. The writing quality requires improvement, including grammatical corrections and clearer expressions, such as revising "The input is" to "The inputs are" in Figure 1's caption and correcting "LF" to "IF" in line 316.

### Suggestions for Improvement
We recommend that the authors improve the elaboration on the mechanisms behind SPEDS to strengthen their claims. Additionally, clarifying the rationale for the low variance of Q-K attention would enhance the paper's credibility. To elevate the writing quality, we suggest a comprehensive review of the paper's language, grammar, and structure, including specific corrections noted in the weaknesses section. Furthermore, we encourage the authors to discuss the limitations in the main text and consider conducting additional experiments on neuromorphic datasets to demonstrate SPEDS's effectiveness.