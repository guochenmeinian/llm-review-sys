ID: A6JDQDv7Nt
Title: Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called Trajectory Aware Ensemble Exploration (TEEN) aimed at enhancing exploration in ensemble-based reinforcement learning by increasing the diversity among sub-policies. The diversity is quantified using KL divergence between trajectory distributions, and the method incorporates a variational lower bound into the policy gradient step with a regularizer term. To mitigate estimation bias in the Q function, the approach selects the minimum value from a random set of critics based on the average Q value across sub-policies. Theoretical analysis supports the design choices, demonstrating reductions in estimation bias and variance. Experimental validation on MuJoco and DeepMind Control Suite shows that TEEN effectively explores a wide range of states and outperforms existing DRL algorithms in continuous control.

### Strengths and Weaknesses
Strengths:
- The method improves exploration in ensemble-based RL by enhancing sub-policy diversity.
- Implementation is straightforward, involving a regularizer addition and a routine for sub-policy updates.
- Theoretical analysis justifies the target Q value selection.
- Experimental results validate the method's efficacy across various tasks.

Weaknesses:
- There is an inconsistency between Line 18 in Algorithm 1 and the text regarding sub-policy updates.
- Theorem 1 indicates reduced expected Q values, yet experiments show underestimation, raising questions about its implications.
- Appendix C is missing, and there are formatting issues, including a missing multiplier in Line 436.
- The paper frequently references Lemmas and equations that do not align with the main text.
- Grammar issues and unclear statements regarding the latent variable $z_k$ and the use of KNN in measuring Eq (8) need clarification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Algorithm 1 by ensuring consistency between Line 18 and the text regarding sub-policy updates. It would be beneficial to address the implications of Q value underestimation in the analysis. The authors should ensure that Appendix C is included and rectify the formatting issues, such as the missing multiplier in Line 436. Additionally, we suggest clarifying the relationship between Lemmas 2 and 3 with the main paper content and addressing the grammar issues noted. Finally, a discussion on the computational overhead of the ensemble and the performance on more complex tasks would enhance the paper's rigor.