ID: 81YIt63TTn
Title: Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Twin-Merging method for merging language models, addressing destructive interference in model merging techniques by maintaining a shared base model alongside separate task-specific knowledge structures. The authors propose a dynamic merging approach that adjusts merging weights at the input level using a router, which is crucial for adapting to data heterogeneity. The method aims to close the performance gap between conventional merging techniques and fine-tuned models while demonstrating superior performance across various tasks. Additionally, the authors address distribution shifts in data through dynamic merging, which adapts to test inputs, as evidenced by experiments on image domain shifts using Gaussian noise corruption. They clarify that their method does not require validation sets from each in-domain dataset, allowing for the use of accessible datasets like the Dolly dataset. The technique is designed for real-world LLM online serving, where models handle unpredictable data streams without gradient updates.

### Strengths and Weaknesses
Strengths:
- The paper shows remarkable performance improvements over existing baselines and demonstrates superior performance in merging tasks, even outperforming oracle models in some cases.
- The dynamic merging approach is innovative, and the use of SVD for compressing task vectors is beneficial for memory efficiency.
- The method achieves efficient storage and minimal time costs, making it practical for large language models.
- The experiments validate the hypotheses regarding task interference and shared/exclusive knowledge, providing clear results supporting claims about robustness against distribution shifts.

Weaknesses:
- There is a lack of technical details on the router module's architecture and training, which is critical for understanding the methodology.
- The validation set's construction is inadequately described, raising concerns about its impact on model performance.
- The method's scalability is questionable due to the computational burden of instance-dependent dynamic merging, particularly during inference.
- The paper introduces several new assumptions that may complicate comparisons with traditional task merging methods, and clarity in distinguishing their approach from previous methods could be improved.

### Suggestions for Improvement
We recommend that the authors improve the technical details regarding the router module's architecture and training process to enhance clarity. Additionally, a comprehensive description of the validation set construction should be included to address concerns about its influence on performance. It would be beneficial to evaluate the method on fully fine-tuned models for generative tasks to demonstrate its broader applicability. We also suggest addressing the scalability issues associated with the dynamic merging process to ensure practical usability in real-world applications. Furthermore, we recommend that the authors improve the clarity of their writing by elaborating on the differences between their method and previous merging techniques, and consider incorporating clearer illustrations to enhance understanding of the proposed approach. Lastly, further refinement of the assumptions made in the paper will strengthen its contributions to the field.