ID: DO9wPZOPjk
Title: BOLD: Boolean Logic Deep Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 9, 7, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 1, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel mathematical concept termed Boolean variation, which is utilized to define optimization algorithms for training deep neural networks (NNs) with Boolean weights and activations. Unlike contemporary binarized NNs that depend on floating-point latent weights during training, the authors propose a method that eliminates this reliance, thereby enhancing computational efficiency. The authors develop an optimization algorithm based on Boolean variation and provide a convergence analysis. They conduct extensive experiments across various domains, demonstrating that their method achieves performance comparable to full floating-point models while significantly reducing energy consumption, setting state-of-the-art (SOTA) benchmarks against binarized NNs. Additionally, the paper introduces a Boolean optimizer that utilizes an accumulator, which can alternatively be referred to as "momentum." The authors clarify that the "VGG-SMALL" architecture is a standard benchmark in the binarized neural network community and provide code references for the precision of the first layer in competing methodologies. They explain their approach to backpropagation through softmax and attention in BERT experiments and assert that regularization details are adequately referenced in the appendices, emphasizing the efficiency of their training epochs compared to baseline models.

### Strengths and Weaknesses
Strengths:
- The introduction of Boolean variation offers a more energy-efficient path for optimizing binary NNs compared to existing methods.
- The related work is adequately cited, and a theoretical analysis of the proposed optimization algorithm is presented.
- Comprehensive experiments across multiple deep learning applications validate the claims made.
- The methodology is clearly articulated, with detailed derivations, code samples, and discussions on training regularization techniques and energy estimation.
- The authors provide clear responses to reviewer questions, enhancing the understanding of their methodology.
- The paper is positioned within a relevant context in the binarized neural network community, citing standard architectures and methodologies.
- The authors demonstrate a commitment to addressing reviewer concerns and improving clarity in the camera-ready version.

Weaknesses:
- Clarity issues exist regarding the experimental methods and results, particularly concerning the logic gates used and the interpretation of certain results.
- The presentation of theoretical concepts is condensed, leading to potential confusion and leaving many questions unanswered.
- Some reviewers find the explanations in the rebuttal defensive and lacking in clarity, suggesting that the main paper does not adequately address certain concerns.
- There is confusion regarding the representation of energy efficiency on the V100, with reviewers suggesting that the paper implies capabilities that the hardware does not possess.
- The details on regularization techniques are perceived as insufficiently highlighted in the main text.

### Suggestions for Improvement
We recommend that the authors improve clarity in the experimental section by specifying the logic gates used for $\text{L}$ and whether different gate types were tested for performance differences. An ablation study on the parameter $\beta_t$ could provide insights into its impact on model performance. Additionally, we suggest investigating alternative stochastic rounding methods to ensure the expectation equals zero. The authors should also consider expanding the presentation of theoretical concepts to enhance understanding and address the numerous questions raised regarding notation and definitions throughout the paper. Furthermore, we recommend improving the clarity of the energy efficiency discussion related to the V100 by revising the caption of Figure 1 to specify that it represents a "hypothetical V100 equivalence with native 1-bit support." It would also be beneficial to move critical training regularization details from the appendix to the main paper to enhance visibility and to provide a reference for the definition mentioned in the related work section, ensuring that all commitments to clarifications in the camera-ready version are clearly listed for transparency.