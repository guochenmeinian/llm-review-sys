ID: Yhd0yzC8yD
Title: Linearly Decomposing and Recomposing Vision Transformers for Diverse-Scale Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to adapting Vision Transformer (ViT) models for diverse computational environments by linearly decomposing a large ViT model into basic components during training. These components can be flexibly recomposed to create smaller, pre-initialized models suitable for various deployment scenarios. The proposed method aims to address the challenges of fixed architectures in standard ViTs and offers an economical solution for generating models tailored to different computational resource constraints.

### Strengths and Weaknesses
Strengths:  
1. The paper introduces a unique decomposition-recomposition strategy that allows for flexible model generation without additional training, facilitating dynamic accuracy-efficiency trade-offs.  
2. The organization and logical structure of the paper enhance its clarity and ease of evaluation.  
3. Extensive experiments demonstrate the effectiveness of the method, showing that decomposed models can achieve competitive or superior performance compared to traditional model compression methods.  

Weaknesses:  
1. The training process for the decomposed modules lacks detailed explanation, particularly regarding how to ensure cumulative performance matches that of the original model.  
2. Insufficient clarity in the implementation details of the recomposition process and the empirical design, including the initialization methods and the arrangement of sub-module combinations.  
3. The literature review is superficial, failing to adequately differentiate the proposed solution from existing parameter-efficient paradigms.  
4. The writing quality requires significant improvement, with numerous typographical errors present throughout the manuscript.  

### Suggestions for Improvement
We recommend that the authors improve the description of the training process for the decomposed modules to clarify how cumulative performance can match that of the original model. Additionally, please provide a more detailed explanation of the recomposition process, including the initialization methods used for the 'with constraint' and 'without constraint' training approaches. We suggest including a figure to illustrate the different recomposed architectures for better understanding. Furthermore, a comprehensive literature review discussing parameter-efficient paradigms is essential to clearly position the proposed method within the existing body of work. Lastly, we urge the authors to enhance the overall writing quality and correct typographical errors throughout the manuscript.