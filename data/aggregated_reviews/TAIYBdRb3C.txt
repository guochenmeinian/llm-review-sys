ID: TAIYBdRb3C
Title: Curve Your Enthusiasm: Concurvity Regularization in Differentiable Generalized Additive Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 5, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on concurvity within Generalized Additive Models (GAMs), proposing a novel regularization scheme aimed at mitigating concurvity to enhance model interpretability. The authors introduce a differentiable regularizer that penalizes correlations between shape functions, demonstrating its application in both toy examples and real-world datasets. The findings suggest that while the regularizer can reduce concurvity, it may also lead to a trade-off with predictive accuracy. Additionally, the authors propose to enhance their submission by applying their method to more complex toy examples, particularly those from Kovács (2022), which would bridge the gap between simplistic examples and real-world datasets. They also discuss the visualization of feature interactions and the selection of the regularization parameter, $\lambda$, emphasizing the need for clearer graphical representations.

### Strengths and Weaknesses
Strengths:
- The paper effectively highlights the significance of concurvity and introduces a new regularization method that can be integrated into differentiable models.
- The authors have successfully replicated a complex toy example from Kovács (2022), demonstrating the effectiveness of their method.
- The organization and clarity of the paper are commendable, providing a solid foundation for understanding the problem and proposed solution.
- The inclusion of results from various regularization techniques highlights the advantages of concurvity regularization.
- Numerical experiments indicate the potential of the proposed methodology in reducing concurvity while maintaining predictive performance.
- The authors are open to incorporating additional visualizations and results based on reviewer feedback.

Weaknesses:
- The evaluation is limited, focusing on only a few datasets, which may not adequately demonstrate the robustness of the proposed method. More diverse datasets and comparisons with existing methods are necessary.
- The initial toy examples were criticized for being overly simplistic and unrealistic.
- The claims regarding improved interpretability and feature selection are not sufficiently substantiated with empirical evidence or comparisons to established feature selection techniques.
- The absence of pair plots in the main body of the paper limits the clarity of feature interactions.
- The current visualizations for selecting $\lambda$ could be improved for better understanding.
- The discussion on the trade-off between interpretability and accuracy lacks depth, and the practical implications of the regularization method remain unclear.

### Suggestions for Improvement
We recommend that the authors improve the breadth of their numerical experiments by including additional datasets and baselines to strengthen their claims. A comparative analysis with existing methods for handling concurvity, such as sparse regularization, should be included to contextualize the proposed approach. Additionally, we suggest that the authors provide clearer demonstrations of the interpretability benefits and feature selection capabilities of their method, possibly through visualizations or more detailed discussions. We also recommend improving the clarity of their results by including pair plots of both $Xᵢ$ vs $Xⱼ$ and $fᵢ(Xᵢ)$ vs $fⱼ(Xⱼ)$ to better illustrate the effects of regularization. Furthermore, incorporating vertical lines in the visualizations of $\lambda$ choices to clearly indicate the selected values for each dataset would enhance the interpretability of the results. Finally, addressing the trade-off between accuracy and interpretability in a more comprehensive manner would enhance the paper's contribution.