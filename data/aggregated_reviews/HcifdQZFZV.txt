ID: HcifdQZFZV
Title: Safe LoRA: The Silver Lining of Reducing Safety Risks when Finetuning Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Safe LoRA, a method designed to mitigate the harmful effects of fine-tuning on large language models (LLMs). The core concept involves projecting harmful gradient updates into a safety-aligned subspace, utilizing cosine similarity to determine the necessity of projection for each layer. The authors claim that this approach effectively preserves utility while enhancing safety.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a timely and critical issue regarding safety in LLM fine-tuning.
2. The proposed method is intuitive and appears to be an effective solution.
3. The use of cosine similarity for layer-specific projections is an interesting refinement that enhances practical performance.
4. The paper is well-written and accessible, likely appealing to a broad audience.

Weaknesses:
1. The baseline selection is limited; comparisons with additional methods such as Vaccine (Huang et al., 2024) should be included.
2. Important literature on harmful fine-tuning is missing from the discussion, which could enrich the context of the research.
3. The experimental design could be more comprehensive, particularly regarding the effects of harmful data ratios on defense performance.
4. The justification for the method's name, Safe LoRA, lacks clarity, as it does not explicitly connect the projection matrix to safety.

### Suggestions for Improvement
We recommend that the authors improve the baseline selection by including comparisons with Vaccine and other relevant literature on harmful fine-tuning. Additionally, we suggest that the authors provide a more thorough discussion of practical application scenarios for Safe LoRA, clarifying its intended use cases. To enhance the experimental rigor, we advise evaluating the method across more diverse datasets and scenarios. Finally, we encourage the authors to include more analysis and visualization to elucidate why projecting LoRA weights to a safety subspace yields effective results.