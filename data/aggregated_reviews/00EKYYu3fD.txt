ID: 00EKYYu3fD
Title: Complexity Matters: Rethinking the Latent Space for Generative Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to understanding and optimizing latent spaces in generative models, proposing a two-stage training paradigm called Decoupled Autoencoder (DAE). The first stage employs a weak decoder to aid the encoder in learning meaningful representations, followed by training a stronger decoder for generative tasks. The authors also introduce a distance metric to minimize the difference between latent and target distributions, demonstrating the effectiveness of DAE through experiments on models like VQGAN and DiT.

### Strengths and Weaknesses
Strengths:
1. The paper is well-structured, easy to follow, and presents a fresh perspective on latent codes.
2. The proposed method DAE is simple to implement and empirically validated, potentially influencing future generative modeling research.

Weaknesses:
1. The mathematical formulation, while sound, is excessive and could be condensed or moved to the appendix, as it mainly serves illustrative purposes.
2. The experimental results lack comprehensive evaluations against other relevant algorithms, and comparisons primarily focus on performance gains over vanilla models.
3. Important related works on latent distribution optimization are overlooked, which could enhance the discussion of the proposed method's contributions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the mathematical formulation by condensing it and relocating less critical parts to the appendix. Additionally, we suggest including more comprehensive evaluations of DAE against other relevant algorithms and explicitly highlighting the advantages of their approach over existing methods like AE-OT-GAN. Finally, addressing the overlooked related works in the discussion would strengthen the paper's contributions.