ID: 2cYxNWNzk3
Title: Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 5, 6, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel active defense framework for pre-trained language models (PLMs) against backdoor attacks by integrating a honeypot module that absorbs backdoor information, thereby protecting the main task module. The authors leverage the insight that low-level representations suffice for learning backdoor tasks but are inadequate for complex language tasks. They propose a training-time defense that maintains model integrity even when trained on tainted data, diverging from traditional two-stage backdoor removal efforts that rely on a clean dataset. The authors highlight a two-stage learning process in PLMs, where the initial focus is on task-related features before shifting to backdoor features. They propose using early stopping as a strategy to mitigate the learning of backdoor features, demonstrating its effectiveness in reducing the attack success rate (ASR) while minimally impacting original task performance.

### Strengths and Weaknesses
Strengths:
- The innovative use of a honeypot module to absorb backdoor information is a compelling and original approach.
- The authors provide a clear rationale for their key observation regarding low-level representations and backdoor tasks.
- The design of the GCE loss and a new weighted loss to train the honeypot and task classifier is well-conceived, focusing the task classifier on clean samples and the honeypot on poisoned samples.
- The authors provide a clear distinction between their approach and existing methods, emphasizing the unique aspects of their threat model.
- The experimental results indicate that early stopping can significantly reduce the ASR to around 10% while preserving task performance.
- The paper contributes to the understanding of the learning dynamics in PLMs concerning backdoor attacks.

Weaknesses:
- Some statements lack clear justification, such as the claim in Section 5.2 regarding better performance on large models, which would benefit from further analysis.
- The design of the weighted CE loss (Equations 3 and 4) is not empirically validated, particularly regarding the weight W(x) on clean versus poisoned samples.
- The threat model assumes a clean PLM fine-tuned with potentially poisoned data, which may not reflect practical scenarios where the PLM itself could contain backdoors.
- The proposed defense bears similarities to existing methods without empirical comparisons, making it difficult to assess its effectiveness relative to prior work.
- There are concerns regarding the robustness of the honeypot method against all-to-all attacks, as existing methods may still be improved for this context.
- The relationship between the proposed early-stopping strategy and existing literature, particularly the work by Zhu et al., lacks clarity, raising questions about originality and implementation details.
- The authors do not provide results for the early-stopping strategy applied in isolation, which could strengthen their claims.

### Suggestions for Improvement
We recommend that the authors improve the clarity and justification of their claims, particularly regarding the performance of the proposed defense on large models. Additionally, we suggest providing empirical validation for the weighted CE loss design and addressing the potential implications of the threat model. It would be beneficial to include comparisons with related defenses, such as CUBE and ABL, to substantiate the proposed method's advantages. We also recommend improving the clarity regarding the differences between their method and those proposed by Zhu et al., particularly in terms of implementation and outcomes. Furthermore, providing results for the early-stopping strategy used alone would enhance the robustness of their claims. Lastly, addressing the potential theoretical contradictions between the design assumptions and robustness against all-to-all attacks would strengthen the paper's argument.