ID: R8znYRjxj3
Title: Bayes-optimal learning of an extensive-width neural network from quadratically many samples
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 8, 8, 6, 4, -1, -1, -1, -1
Original Confidences: 2, 3, 2, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of a teacher-student setting with one-hidden layer neural networks using quadratic activations. It derives a closed-form expression for the optimal test error under a quadratic sample complexity, contributing to the understanding of Bayes-optimal estimators. The authors explore the performance of gradient descent (GD) and find that it can approach Bayes-optimal error in noiseless scenarios, while also discussing the implications of noise on GD's effectiveness.

### Strengths and Weaknesses
Strengths:
- The paper is accessible, clearly illustrating its context and contributions, making it understandable even for those with limited prior knowledge.
- The derivation of the minimum mean square error (MMSE) and the empirical comparison with GD provide valuable insights into the relationship between theoretical and practical performance.
- The connections made with existing literature, such as matrix denoising and ellipsoid fitting, validate the work's relevance.

Weaknesses:
- The exposition is somewhat awkward, particularly regarding the placement of the closed-form asymptotic limit of MMSE in Eqn (14) outside of the main theoretical results section.
- The analysis appears limited to the specific architecture studied, raising concerns about generalizability to more complex networks.
- The motivation for focusing on quadratic activations is under-explained, which may hinder broader applicability.

### Suggestions for Improvement
We recommend that the authors improve the organization of the paper by relocating the closed-form expression for MMSE to Section 3, ensuring it aligns with the main theoretical results. Additionally, we suggest addressing the generalizability of the analysis to more complex architectures, perhaps by discussing potential extensions beyond the one-hidden layer network. Furthermore, we encourage the authors to clarify the motivation for using quadratic activations and to elaborate on the implications of their findings for commonly used activation functions like ReLU and sigmoid.