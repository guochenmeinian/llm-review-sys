ID: niHkj9ixUZ
Title: Beyond Pretrained Features: Noisy Image Modeling Provides Adversarial Defense
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 3, 7, 5, 8, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel adversarial defense method called $De^3$, which utilizes Noisy Image Modeling (NIM) to enhance robustness against adversarial attacks. The authors propose adding Gaussian noise to adversarial samples and reconstructing the original images, demonstrating that NIM outperforms Masked Image Modeling (MIM) in terms of adversarial robustness while maintaining competitive performance on clean data. The method leverages a transformer-based encoder-decoder architecture and aims to improve feature learning through denoising as a self-supervised pretext task. The authors also highlight the differences between NIM and CIM, clarifying their unique contributions to the field, and compare NIM with MIM and simMIM to demonstrate its superior accuracy-robustness trade-off. They acknowledge the need for a performance comparison with denoising autoencoders (DAE) and provide experimental results to support their claims.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- The concept of using denoising to enhance adversarial defense is interesting and innovative.
- The method effectively utilizes the decoder for cleaning adversarial images without significant computational overhead.
- Experimental results indicate that NIM with $De^3$ achieves comparable performance to adversarial training and outperforms DAE in both clean accuracy and adversarial robustness.
- The authors effectively highlight the differences between NIM and CIM, clarifying their unique contributions to the field.

Weaknesses:
- The novelty is limited, as the approach is similar to CIM, which also focuses on denoising.
- Performance improvements are incremental, with limited applicability across datasets and tasks.
- The paper lacks extensive comparisons with other self-supervised methods and does not analyze the computational cost or hyperparameter sensitivity adequately.
- The datasets and backbones used for validation are scarce, and more diverse evaluations are needed.
- The authors do not conduct a sufficient number of experiments across various backbones and datasets, which is essential for validating the generalizability of their proposed method.
- The use of an outdated reference and the inappropriate citation of a blog to address reviewer concerns detracts from the professionalism of the discussion.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by clearly differentiating their approach from CIM and citing it appropriately. Additionally, conducting a more extensive comparison with well-known self-supervised methods like SimCLR, MOCO, and DINO would strengthen the paper. We suggest including more datasets and backbones in the evaluation to assess generalizability. Furthermore, a detailed analysis of the computational cost and hyperparameter sensitivity should be provided to enhance the robustness of the findings. Lastly, we recommend that the authors refrain from referencing external blogs in their discussions to maintain a professional tone and focus on the technical contributions of their work. Addressing the concerns regarding the "flooding out effect" and the denoising performance at high noise levels would also be beneficial.