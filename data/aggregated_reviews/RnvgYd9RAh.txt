ID: RnvgYd9RAh
Title: LACIE: Listener-Aware Finetuning for Calibration in Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 8, 6, 8, 4, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to address LLM miscalibration, specifically overconfidence in incorrect answers, through a listener-aware fine-tuning approach. The authors hypothesize that LLMs exhibit overconfidence due to a lack of knowledge and pragmatic grounding. Their fine-tuning method employs a preference function that rewards accurate expressions of confidence and penalizes inaccuracies. The authors fine-tune three LLMs of varying sizes on TriviaQA and compare their method against base models and other finetuning approaches. Results indicate significant improvements in calibration across all model sizes, with a noted increase in precision when evaluated by an LLM listener, although recall decreases. However, when assessed by human listeners, precision improves while recall remains consistent with baseline models. The authors also demonstrate out-of-distribution generalization and conduct a qualitative analysis of confidence expressions.

### Strengths and Weaknesses
Strengths:
- The authors effectively address the critical issue of LLM overconfidence, providing extensive motivation for their research.
- The pragmatic grounding approach to improve calibration is innovative and well-executed.
- The experimental setup is comprehensive, covering various model classes and sizes, and includes human evaluations and out-of-distribution generalization.
- Positive calibration results are observed across all models, with the model learning to abstain when uncertain.

Weaknesses:
- There is a notable decrease in recall and accuracy for models trained with this method, which is not adequately discussed in the abstract or introduction. This imbalance could be addressed to strengthen the paper.
- The analysis of the decline in accuracy primarily focuses on the percentage of abstains, lacking deeper exploration of the base model's performance on these cases.
- The paper would benefit from additional baselines, particularly non-pragmatic models, to better assess the robustness of the method.
- A table summarizing true/false acceptances/rejects from each experiment would enhance result interpretation.

### Suggestions for Improvement
We recommend that the authors improve the abstract and introduction by incorporating the observed decline in recall and accuracy, framing it as a trade-off that warrants further exploration. Additionally, we suggest conducting a more thorough analysis of the base model's performance on abstained cases, possibly by examining logits or sampling with temperature. Including a table in the appendix detailing true/false acceptances/rejects from experiments would aid in result interpretation. Finally, we encourage the authors to consider adding more diverse baselines, including non-pragmatic models, to strengthen the evaluation of their method's effectiveness.