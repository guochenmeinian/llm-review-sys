ID: q8SukwaEBy
Title: Learning from Active Human Involvement through Proxy Value Propagation
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 8, 7, 5, 5, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Proxy Value Propagation (PVP), a method for reinforcement learning that utilizes human intervention as a signal for quality instead of ground-truth rewards. The approach assigns positive values to state-action pairs influenced by human interventions and negative values to those that necessitate intervention. The authors claim that PVP can effectively combine labeled data from human demonstrations with unlabeled data from agent exploration using TD-learning. PVP is shown to learn self-driving policies more efficiently and with fewer crashes compared to naive reinforcement learning methods and other human-intervention techniques. The authors conduct extensive experiments across various driving simulators, including MiniGrid and CARLA, demonstrating the effectiveness of PVP through user studies and significant improvements over baseline methods.

### Strengths and Weaknesses
Strengths:
- The empirical performance of PVP is impressive, outperforming baselines in both human-intervention and non-human-intervention scenarios.
- The experiments are well-structured, involving real human users and diverse environments, which enhances the validity of the findings.
- The human-in-the-loop approach is relevant to practitioners, refining imitation learning and reinforcement learning in complex real-world applications.
- PVP is a conceptually straightforward algorithm, and the paper appears self-contained, facilitating reimplementation.
- The method is intuitive, with clear visualizations and supplementary materials that aid understanding.

Weaknesses:
- The treatment of human labelers' time delays is not adequately addressed, which could impact the effectiveness of the approach.
- The reliance on human intervention may lead to policy degradation if human actions are sub-optimal.
- The experimental design lacks details regarding participant demographics and instructions, which are crucial for interpreting user study results.
- The claim regarding compatibility with various human control devices seems trivial and may apply to many existing methods.
- The assertion that unlabeled state-action pairs are effectively integrated with human expert data lacks robust experimental support, raising concerns about potential equivalence to behavioral cloning under strong regularization.
- The comparison of results in Figure 2 between a basic random-exploration RL method and a human-in-the-loop method is not entirely fair; performance against state-of-the-art offline RL or supervised imitation learning should be included for a comprehensive evaluation.
- The clarity of how human demonstrations are utilized and the conditions under which human intervention occurs require further elaboration.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the user study by providing detailed information about participant demographics, background, and instructions. Additionally, addressing the time-delay of human feedback in the PVP framework is essential to enhance its robustness. The authors should also consider discussing the potential risks of human labeler errors more thoroughly and explore the implications of these errors on policy performance. Furthermore, we suggest that the authors clarify their training objectives by considering the use of rewards instead of Q-value targets, which may lead to more effective learning. We encourage the authors to improve the experimental justification for the integration of unlabeled data with human expert data to strengthen their claims about value propagation. Additionally, providing more detailed explanations of the warm-up procedure involving human demonstrations and clarifying the criteria for human intervention in dynamic environments would be beneficial. Addressing the concerns regarding the reliability of user study results in Table 3 is also essential; we encourage the authors to present more detailed experimental data and rationale for the observed standard deviation. Finally, optimizing the user study methodology to ensure participants can accurately assess algorithm performance after multiple experiments would enhance the validity of the results.