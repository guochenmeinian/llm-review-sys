ID: wIE991zhXH
Title: Bandits with Preference Feedback: A Stackelberg Game Perspective
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 7, 6, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to bandit optimization with preference feedback, focusing on continuous action spaces and kernelized reward functions. The authors propose an efficient algorithm, MAXMINLCB, which utilizes a game-theoretic Leader-Follower formulation and constructs preference-based confidence sets. The algorithm is shown to achieve a tighter regret bound compared to state-of-the-art methods, specifically improving by a factor of $T^{1/4}$.

### Strengths and Weaknesses
Strengths:
1. The technique for constructing the confidence set is innovative and results in tighter bounds.
2. The game-theoretic perspective allows for a better exploration-exploitation trade-off, as evidenced by experimental results.
3. The paper addresses a significant gap in the literature by considering nonlinear utility functions and provides a sample-efficient algorithm.

Weaknesses:
1. The practical applicability of the algorithm is questionable, particularly its scalability to higher dimensions, as it currently only demonstrates effectiveness in two dimensions.
2. The paper lacks a thorough comparison with related works, particularly Xu et al. (2024), which could provide a clearer understanding of the algorithm's performance.
3. The hyper-parameter selection process is not well-defined, raising concerns about the algorithm's implementation in practice.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by including comparisons with algorithms known to perform better than RUCB, such as RMED and Double Thompson Sampling. Additionally, we suggest providing a data-driven method for selecting hyper-parameters like $\gamma_t$, $L$, and $B$. To enhance the practical relevance, the authors should consider extending the work to contextual bandits and discuss the implications of their results in real-world applications. Finally, addressing the limitations of the current work in terms of social impact would strengthen the paper.