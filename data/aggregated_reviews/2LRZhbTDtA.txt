ID: 2LRZhbTDtA
Title: Not Just Object, But State: Compositional Incremental Learning without Forgetting
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel setting of incremental learning termed Compositional Incremental Learning (composition-IL), which focuses on recognizing not only new objects but also their states and compositions. The authors propose a prompting-based approach utilizing a pre-trained transformer network, introducing a three-way retrieval mechanism with tailored prompt pools for state, object, and composition-level representation learning. Additionally, the paper introduces a streamlined version of the original CompILer model, named Sim-CompILer, which reduces the number of hyperparameters by eliminating certain losses and prompts. The authors demonstrate that while CompILer achieves the highest performance, Sim-CompILer remains competitive. The experimental section evaluates the proposed methods against state-of-the-art techniques, highlighting the benefits of the three-way prompting strategy and the contributions of various loss functions, while also addressing implementation issues in baseline methods NCM and FeCAM.

### Strengths and Weaknesses
Strengths:
1. The paper is well-presented, with clear descriptions of the dataset and model.
2. The novelty of the experimental setting is commendable, as it addresses a gap in existing incremental learning approaches.
3. The use of multiple prompt pools related to different concepts/tasks enhances the methodology.
4. The authors effectively simplified the model in Sim-CompILer, making it more practical and easier to follow.
5. The paper includes extensive experiments and comparisons, establishing a solid benchmark for future work.
6. The responses to reviewer concerns demonstrate a commitment to improving the paper and clarifying key points.

Weaknesses:
1. The complexity of hyperparameters is a major concern, as multiple loss terms require careful tuning, complicating real-world application.
2. Statistical significance is questionable, as results from the final configuration are often close to simpler, ablated versions, suggesting potential noise in the findings.
3. The experimental section lacks comparisons with additional rehearsal baselines and does not sufficiently address the relationship with existing works in multi-label incremental learning.
4. There is a lack of systematic analysis regarding hyperparameters and model efficiency, particularly concerning the size of learned prompts.
5. The initial implementation errors in NCM and FeCAM led to unreasonable results, which required correction.
6. The paper currently lacks informative captions for figures, affecting readability.

### Suggestions for Improvement
We recommend that the authors simplify their approach by reducing the number of hyperparameters and loss terms, focusing on configurations that have a significant impact on results. Additionally, we urge the authors to conduct multiple runs for each experiment to ensure statistical significance and to include results for additional rehearsal baselines such as DER++ and ER-ACE. A more thorough discussion of the relationship between their proposed setting and existing benchmarks in multi-label incremental learning is necessary. Furthermore, we suggest providing a systematic analysis of hyperparameters $\lambda_1$ and $\lambda_2$ in Equation 9, as well as a detailed comparison of model efficiency with current prompt-based methods. We also recommend improving the discussion on the necessity of reducing hyperparameters and utilizing simpler models. Lastly, we encourage the authors to add proper informative captions for all figures in the paper to enhance clarity and understanding, and to address concerns regarding catastrophic forgetting in the attention-based mechanism to enhance the robustness of the proposed model.