ID: zV2GDsZb5a
Title: Neural Gaffer: Relighting Any Object via Diffusion
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 4, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Neural Gaffer, a method for single-image relighting using a diffusion model that conditions on a target environmental map. The authors fine-tune a pre-trained diffusion model on a synthetic relighting dataset, enabling the generation of high-quality relit images without explicit scene decomposition. The method shows promise in generalization and accuracy through evaluations on both synthetic and real-world imagery. Additionally, it can be applied to downstream tasks such as object insertion and relighting of neural radiance fields. The authors also assert that their approach can achieve full image generation with background synthesis, utilizing environment maps or diffusion-based inpainting when maps are unavailable. They clarify that quantitative evaluations focus solely on relit foregrounds, ensuring a fair comparison, and emphasize the user-friendliness of their method compared to DiLightNet, which requires more complex user inputs and processes.

### Strengths and Weaknesses
Strengths:
- Neural Gaffer effectively performs single-image relighting without requiring explicit scene decomposition, making it accessible for various applications.
- The method demonstrates effective relighting capabilities with high visual quality and accuracy, generating relit images under diverse environmental lighting conditions.
- The video results and qualitative evaluations are visually impressive, showcasing the method's capabilities.
- The authors provide thorough evaluations and comparisons with existing methods, including a user study that shows favorable results for their approach.
- The paper includes a clear explanation of the methodology and its advantages over DiLightNet.

Weaknesses:
- The method's reliance on a synthetic dataset raises questions about its performance in complex real-world scenarios, particularly with non-centered objects and intricate backgrounds.
- The necessity of environment maps for relighting remains a point of contention, with suggestions for exploring alternative approaches.
- The paper lacks comprehensive evaluations on datasets capturing real-world lighting complexities, which limits the assessment of its generalization capabilities.
- Comparisons with other lighting-aware compositing methods are missing, which could provide a clearer context for the method's performance.
- There are concerns regarding the reliability of metrics used for evaluation, as highlighted by references to recent literature.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of their method by including real-world datasets such as OpenIllumination or Stanford ORB to assess its performance in complex lighting scenarios. Additionally, the authors should explore how the method behaves with non-centered objects and in scenes with multiple objects. Conducting an ablation study to assess whether the task can be accomplished using only target image crops would also be beneficial. Providing more compelling examples involving faces or animals could enhance the demonstration of the method's generalizability. Furthermore, including comparisons with existing lighting-aware compositing methods and straightforward color adjustment techniques would help quantify the added value of the diffusion model approach. Lastly, addressing the runtime of the diffusion model, conducting a detailed analysis of failure cases, and considering user studies as a standard practice in relighting tasks would strengthen the paper's contributions and enhance the credibility of their results.