ID: G0yxFmP87g
Title: AmoebaLLM: Constructing Any-Shape Large Language Models for Efficient and Instant Deployment
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AmoebaLLM, a framework for fine-tuning and compressing pre-trained LLMs to deliver pruned subnets under various resource constraints. The framework comprises two stages: a knowledge-preserving subnet selection using dynamic programming (DP) to determine depth and width, and a one-for-all fine-tuning strategy that employs subnet-specific LoRA heads. The authors demonstrate the efficacy of their method through experiments on LLama 2 7B and Vicuna 7B v1.5, achieving state-of-the-art performance across multiple metrics.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant challenge in LLM deployment across resource-constrained devices, presenting a well-motivated and clearly written solution.
- The proposed method is novel and outperforms existing pruning techniques, providing valuable insights into the impact of model compression on various LLM metrics.
- The experimental setup is straightforward and easy to follow, contributing to the paper's clarity.

Weaknesses:
- Some technical details, such as the definitions of depth and width in decoder models, remain unclear, particularly regarding the classification of self-attention blocks and feedforward networks.
- The claim that full model fine-tuning suffers from gradient conflicts lacks strong empirical support, as the analysis in Section 2 is largely conjectural.
- The complexity of the DP algorithm is not adequately discussed, and the authors should clarify its computational demands and runtime.
- The evaluation of the optimal subnet selection process raises questions about its efficiency and the potential for non-optimal subnets to achieve better performance post-fine-tuning.

### Suggestions for Improvement
We recommend that the authors improve the clarity of technical definitions, particularly regarding depth and width in decoder models, and explicitly state how self-attention and feedforward layers are categorized. Additionally, we suggest providing stronger empirical evidence to support claims about gradient conflicts in full model fine-tuning, possibly by directly citing results from Section 5.3. It would also be beneficial to include a discussion of the DP algorithm's complexity and runtime in the main paper to address potential concerns. Lastly, consider providing comparisons of training efficiency across existing methods in the experimental section to strengthen the claims regarding efficiency.