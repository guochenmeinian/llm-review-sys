ID: 6tW1WEHIJe
Title: Is the Answer in the Text? Challenging ChatGPT with Evidence Retrieval from Instructive Text
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new dataset for question-answering that includes unanswerable and non-factoid questions, utilizing WikiHow articles as input. The dataset requires systems to identify evidence sentences or return no answer. The authors evaluate ChatGPT's performance in zero-shot and few-shot settings, revealing that ChatGPT struggles with no-evidence questions in the zero-shot setting, although performance improves in the few-shot setting at the expense of recall for with-evidence questions.

### Strengths and Weaknesses
Strengths:
- The problem setting involving unanswerable and non-factoid questions is intriguing.
- The dataset is well-designed and constructed, providing a valuable resource for future research.

Weaknesses:
- The findings regarding ChatGPT's performance may not be broadly applicable to the research community.
- The random baseline used for comparison is inadequate.
- The dataset is relatively small, comprising only 254 questions from 91 documents, with limited samples for three question types.
- The analysis is limited, lacking detailed statistics per question type and additional discussions or ablation studies.

### Suggestions for Improvement
We recommend that the authors improve the analysis by including statistics per question type and providing examples of failed cases. Additionally, the authors should consider adding actual outputs from ChatGPT in the appendix to enhance transparency. To strengthen the conclusions, we suggest comparing ChatGPT's performance against more robust baselines rather than just a random baseline. Finally, careful proofreading is necessary to clarify missing details and improve the overall presentation of the paper.