ID: ndoeHX1Acq
Title: One for All: Multi-Domain Joint Training for Point Cloud Based 3D Object Detection
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 5, 4, -1, -1, -1, -1
Original Confidences: 5, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents OneDet3D, a universal point cloud-based 3D object detector designed for multi-domain joint training, addressing the limitations of existing detectors that typically operate on single datasets. The authors propose techniques such as domain-aware partitioning to mitigate data-level interference and language-guided classification using CLIP embeddings to reduce category-level conflicts. The model demonstrates promising performance across various indoor and outdoor benchmarks.

### Strengths and Weaknesses
Strengths:
- The introduction of a universal model for 3D detection across multiple domains using a single set of parameters is innovative.
- The proposed techniques are simple yet effective, showcasing that multi-domain training can outperform single-domain training.

Weaknesses:
- Claims regarding the superiority of 3D sparse convolution over point-based feature extractors lack detailed elaboration.
- The domain router's method for obtaining domain labels is unclear, and the handling of varying dataset sizes and attributes is inadequately addressed.
- Several basic grammar issues and a lack of comparisons with existing fully sparse architectures weaken the manuscript.
- The experimental results, particularly for the nuScenes dataset, appear inconsistent with other methods.
- The manuscript lacks sufficient graphical illustrations and detailed explanations, particularly in Section 3.3, making it hard to follow.

### Suggestions for Improvement
We recommend that the authors improve the elaboration on the advantages of 3D sparse convolution over point-based feature extractors. Clarifying how domain labels are obtained and addressing the differences in dataset sizes and attributes would enhance the paper's rigor. The authors should correct grammatical errors and include comparisons with existing fully sparse architectures to strengthen their claims. Additionally, we suggest revising the experimental results for clarity and consistency, particularly for the nuScenes dataset. Enhancing Section 3.3 with more detailed explanations and graphical illustrations would improve readability and comprehension.