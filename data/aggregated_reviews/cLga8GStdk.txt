ID: cLga8GStdk
Title: LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low Resource and Extinct Languages
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 8, 10, 9, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LingOly, a benchmark of linguistic reasoning problems focusing on low-resource and extinct languages, derived from the UK Linguistics Olympiad. The benchmark consists of 1,133 questions designed to be solvable without prior knowledge of the languages being tested. The authors evaluate 11 state-of-the-art LLMs, introducing the metric Î”NC to assess performance relative to no-context scenarios, highlighting the challenges faced by LLMs in reasoning tasks.

### Strengths and Weaknesses
Strengths:
- The multilingual evaluation of LLMs is significant and addresses an important area in the field.
- The dataset is well-constructed, with permissions obtained from most problem authors, and includes a diverse range of tasks and well-designed evaluation metrics.
- The paper provides meaningful insights into the limitations of LLMs' reasoning abilities, particularly regarding low-resource languages.

Weaknesses:
- The focus on memorization in the introduction may detract from the core strengths of the work, and the authors do not adequately address data contamination in benchmark construction.
- Some evaluated LLMs may not be sufficiently multilingual for assessing low-resource languages, raising questions about their appropriateness for the benchmark.
- The evaluation lacks the use of Chain of Thought (CoT) prompting, which may weaken the reasoning assessment.

### Suggestions for Improvement
We recommend that the authors improve the introduction by clarifying the relevance of memorization in the context of their benchmark. Additionally, the authors should consider including a more diverse set of multilingual LLMs, such as Aya, to enhance evaluation accuracy. We suggest incorporating CoT in the evaluation process to better assess reasoning capabilities. Furthermore, including a detailed problem example in the main paper could enhance reader engagement and understanding. Lastly, we encourage the authors to explore alternative metrics, such as character n-gram based metrics or human-level performance baselines, to improve interpretability and provide a more comprehensive analysis of LLM performance.