ID: NDaaCaWS9N
Title: A scalable Bayesian continual learning framework for online and sequential decision making
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 6, 5
Original Confidences: 4, 3

Aggregated Review:
### Key Points
This paper presents a Bayesian continual learning model for classification that addresses concept shift and proposes a scalable approach that does not require storing samples. The authors build a separate probabilistic model for each data batch, utilizing Bayesian model averaging to update weights as new batches arrive. The method is novel, as it treats each batch as an independent model, which is significant given the challenges in continual learning. However, the empirical evaluation is limited to a small toy model, and the paper contains several typos and unclear sentences.

### Strengths and Weaknesses
Strengths:
- The model is well motivated and addresses an important problem in continual learning.
- The intuition behind the proposed model is clearly articulated.
- Limited numerical results demonstrate the method's robustness and adaptability to data shifts.

Weaknesses:
- Some model details, such as the justification for ensemble weight choices, are inadequately explained.
- The Bayesian update process for predictors lacks clarity, particularly in Section 2.3.
- The numerical example is overly simplistic, omitting the feature extraction stage, raising questions about performance in more complex scenarios.
- The numerical study is limited, lacking comparisons of oracles with different window sizes beyond the full history and a 5-batch window.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the Bayesian update process in Section 2.3 by providing more technical details. Additionally, the authors should justify the choice of ensemble weights more thoroughly. To enhance the empirical evaluation, we suggest including more complex scenarios that incorporate feature extraction and comparing oracles of varying window sizes to provide a more comprehensive assessment of the method's performance. Finally, we encourage the authors to proofread the paper to correct typos and improve the overall clarity of the writing.