ID: K1Uzj8tuwd
Title: Learning Mask-aware CLIP Representations for Zero-Shot Segmentation
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 4, 6, 5, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called Mask-aware Fine-tuning (MAFT) for leveraging pre-trained CLIP in zero-shot segmentation tasks. The authors propose an Image-Proposals CLIP Encoder (IP-CLIP Encoder) that processes multiple images and mask proposals simultaneously, incorporating mask-aware and self-distillation losses to enhance CLIP's sensitivity to different mask proposals. The experimental results indicate significant performance improvements over baseline methods, demonstrating the method's effectiveness in addressing challenges faced by frozen-CLIP-based segmentation.

### Strengths and Weaknesses
Strengths:
- The introduction of the IP-CLIP Encoder is a promising advancement, effectively addressing CLIP's insensitivity to varying mask proposals.
- The incorporation of mask-aware and self-distillation losses enhances the model's transferability while fine-tuning.
- The paper is well-written and presents clear experimental results that show substantial improvements over existing methods.

Weaknesses:
- The ability to handle multiple mask proposals is not unique to this method, as it has been previously addressed in methods like ZegFormer.
- The effectiveness of the mask-aware loss is limited by the quality of the mask proposals, which constrains the paper's innovation.
- The experimental setup is inadequate, focusing solely on zero-shot semantic segmentation without exploring open-vocabulary panoptic settings or utilizing larger datasets.
- The paper's presentation lacks clarity in certain areas, particularly regarding notation and the motivation behind specific design choices.

### Suggestions for Improvement
We recommend that the authors improve the experimental settings by extending the method to open-vocabulary panoptic settings and utilizing larger datasets for training. Additionally, conducting experiments with updated methods such as "Scaling Open-Vocabulary Image Segmentation with Image-Level Labels" (ECCV2022) would provide a more comprehensive evaluation. We also suggest simplifying and clarifying the notation used in the paper to enhance readability and understanding. Finally, addressing the specific questions raised regarding the mask-aware loss and the hyperparameter selection process would strengthen the paper's clarity and rigor.