ID: t2hEZadBBk
Title: Tailoring Self-Attention for Graph via Rooted Subtrees
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 6, 7, 5, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Subtree Attention (STA), a novel graph attention mechanism that addresses the limitations of existing local and global attention methods in Graph Neural Networks (GNNs). STA enables the root node to attend directly to its multi-hop neighbors, effectively gathering information from the entire rooted subtree within a single layer. The authors propose STAGNN, a graph neural network that integrates STA with a kernelized softmax algorithm, achieving linear time complexity and demonstrating superior performance on node classification tasks compared to existing GNNs and graph transformers.

### Strengths and Weaknesses
Strengths:
- The authors propose a novel multi-hop graph attention mechanism, STA, which effectively bridges local and global attention structures.
- The theoretical analysis demonstrates that STA can approximate global attention, addressing over-smoothing and over-squashing issues in message-passing neural networks.
- Empirical results indicate that STAGNN outperforms state-of-the-art GNNs and graph transformers on node classification tasks.

Weaknesses:
- The performance improvement of STAGNN over NAGphormer is marginal, with limited differences in scores across datasets, raising concerns about its practical advantage.
- The experimental evaluation is restricted to a small number of datasets, lacking larger datasets like OGB, which could better demonstrate the model's capabilities.
- There is insufficient analysis of STA's timing and space complexity, particularly in comparison to existing graph transformers, which may raise concerns about its efficiency.
- The paper lacks comprehensive baseline comparisons, omitting important models such as BernNet.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by including larger datasets, such as OGB, to better assess the model's performance in scenarios requiring long-range interactions. Additionally, we suggest providing a detailed analysis of STA's timing and space complexity to clarify its efficiency compared to existing methods. The authors should also consider including comparisons with more baseline models to strengthen the evaluation of STAGNN's performance. Furthermore, clarifying the discrepancies in equations and figures, as well as providing citations for kernelized attention methods, would enhance the paper's rigor.