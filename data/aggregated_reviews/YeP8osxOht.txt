ID: YeP8osxOht
Title: Bandit Social Learning under Myopic Behavior
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 7, 7, 3, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 2, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of social learning in a two-armed Bernoulli bandit scenario, where agents sequentially pull an arm based on a confidence-bound index related to the empirical mean, parametrized by $\eta$. The authors provide a tight characterization of the probability of learning failure, indicating that most agents will select the suboptimal arm depending on the reward gap and $\eta$. The results extend to Bayesian agents using truncated priors. The paper also introduces a bandit social learning (BSL) problem, analyzing how different myopic behaviors affect learning outcomes.

### Strengths and Weaknesses
Strengths:
- The motivation for studying the boundaries of greedy behavior in learning failure is solid, addressing a gap in existing literature.
- The techniques for proving lower bounds on probabilities of failure are novel compared to standard bandit literature.
- The paper is well-written and presents complex concepts clearly, with sound results and a good positioning within existing literature.

Weaknesses:
- The conceptual takeaways are not surprising, and the technical contribution is limited, primarily focusing on characterizing probabilities of failure as a function of $\eta$, whose value remains unclear.
- The analysis is restricted to a two-armed bandit model, raising questions about the implications for multi-armed scenarios.
- The interpretation of results is complicated by the dependency on $N_0$, the initial number of samples, which detracts from the clarity of the findings.
- The paper lacks experimental validation, which would enhance the practical relevance of the theoretical results.

### Suggestions for Improvement
We recommend that the authors improve the presentation and structure of the paper to enhance clarity, possibly by merging Theorems 3.1 and 3.9 and summarizing results in a table. Additionally, we suggest extending the analysis to multi-armed bandit scenarios to explore how the learning dynamics change with more arms. It would also be beneficial to connect the theoretical results to real-world applications and provide experimental simulations to validate the findings. Finally, we encourage the authors to clarify the significance of the assumptions, particularly regarding $N_0$, and consider the implications of heterogeneous agent behaviors in their model.