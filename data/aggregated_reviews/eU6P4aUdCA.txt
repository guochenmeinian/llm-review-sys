ID: eU6P4aUdCA
Title: Algorithmic Regularization in Tensor Optimization: Towards a Lifted Approach in Matrix Sensing
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an examination of gradient descent's role in inducing implicit regularization for tensor optimization within the lifted matrix sensing framework. The authors demonstrate that with sufficiently small initialization, gradient descent applied to the lifted problem yields rank-1 tensors and critical points with escape directions. Additionally, the authors propose a method for ensuring the conversion from spurious solutions to strict saddle points by appropriately choosing the parameter \( l \). They indicate that this choice, along with other constants, can determine the necessary number of iterations for tensors along the optimization trajectory to approximate rank-1. The findings underscore the importance of tensor parameterization for matrix sensing, combined with first-order methods, in achieving global optimality.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and provides useful insights, such as the detection of implicit bias with small initialization points.
- It establishes a novel implicit regularization of gradient descent for tensor optimization, particularly showing that gradient descent leads to approximately rank-one tensors until a certain time step.
- The technical tools introduced, like the v-eigenvalue definition, may benefit future research on implicit regularization in tensor problems.
- The paper includes detailed descriptions of algorithms and proofs in the appendix.
- The authors provide a practical approach to adjusting the stepsize \( \eta \), which can enhance convergence efficiency.
- Empirical evidence supports the sufficiency of \( l=3 \), which simplifies the optimization process.

Weaknesses:
- The practical applicability of the theory is not convincingly discussed, with concerns about the relevance of the quadratic activations used in experiments.
- Hyperparameters for optimization algorithms are not provided, raising doubts about the optimization efforts for the unlifted problem.
- The empirical evidence supporting the theory is insufficient, particularly regarding the tensor's rank during optimization.
- The paper is overloaded with technical details, making it challenging for a broader audience to grasp the main ideas.
- The paper may lack clarity in explaining the implications of the choice of \( l \) and its impact on the optimization trajectory.
- There could be insufficient discussion on the scenarios where \( \sigma_1(U) < 1 \) and its broader implications.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the practical applications of two-layer neural networks with quadratic activations and provide more comprehensive empirical evidence that demonstrates the viability of the lifted formulation in diverse scenarios. Additionally, the authors should clarify the hyperparameters used in their experiments and consider quantifying "low-sample" conditions while also exploring "high-sample" scenarios for completeness. We suggest that the authors improve the clarity of the explanation regarding the choice of \( l \) and its effects on the optimization process. Furthermore, expanding the discussion on the implications of having \( \sigma_1(U) < 1 \) would provide a more comprehensive understanding of its impact on the optimization strategy. Condensing the lengthy proofs in the appendix could enhance readability. Finally, addressing the limitations and gaps in the theory explicitly would strengthen the paper's contributions.