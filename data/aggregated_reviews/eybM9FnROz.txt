ID: eybM9FnROz
Title: Linear-Time Graph Neural Networks for Scalable Recommendations
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Linear-Time GNN (LTGNN) designed to efficiently handle large-scale user-item interaction graphs while maintaining strong expressive power. The authors propose implicit graph modeling and efficient variance-reduced neighbor sampling to achieve linear computational complexity. They conduct extensive experiments on three real-world datasets, demonstrating that LTGNN significantly reduces training time for GNN-based recommendation models without sacrificing performance.

### Strengths and Weaknesses
Strengths:
- The authors introduce relevant notations and definitions, detailing LTGNN's implicit modeling and variance-reduced neighbor sampling.
- Extensive experiments and ablation studies validate the model's effectiveness and scalability, showing substantial performance improvements over classical methods.
- The clarity and organization of the paper enhance readability and comprehension.

Weaknesses:
- The paper suffers from numerous grammatical and typographical errors, which detract from its presentation.
- The experimental baselines are outdated, with the latest baseline (DGCF) proposed in 2020, indicating a lack of engagement with recent advancements in the field.
- Certain experimental results are imprecise, with defaulted values and "OOM" entries in Table 3.
- The adaptability of LTGNN to evolving datasets and the exploration of different experimental settings are insufficiently addressed.

### Suggestions for Improvement
We recommend that the authors improve the overall presentation by correcting grammatical errors and typographical mistakes. Additionally, updating the experimental baselines to include more recent methods would enhance the relevance of the work. The authors should provide clearer explanations for the experimental results, particularly in Table 3, and consider including experiments on sparse graphs to assess LTGNN's performance under varying conditions. Furthermore, we suggest that the authors explore the adaptability of LTGNN to dynamically evolving datasets and report average performance metrics across multiple training runs to ensure stability. Lastly, including results from real industry scenarios would strengthen the paper's applicability.