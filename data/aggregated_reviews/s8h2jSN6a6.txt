ID: s8h2jSN6a6
Title: MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 8, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel benchmark dataset for Multi-Turn Multi-Image Dialog Understanding (MMDU), aiming to enhance research in Large Vision Language Models (LVLMs) by transitioning from single image-based Q&A to multi-turn, multi-image scenarios. The MMDU dataset features longer token lengths and a higher average number of images per sample compared to existing benchmarks. It includes a comprehensive evaluation of 15 models and introduces MMDU-45K for instruction tuning, demonstrating significant improvements in model performance.

### Strengths and Weaknesses
Strengths:
1. The MMDU dataset is constructed with human annotation and validation.
2. Comprehensive evaluation on 15 models showcases the dataset's robustness.
3. The innovative data collection pipeline enhances the dataset's quality.

Weaknesses:
1. The MMDU dataset consists of only 110 dialogues, despite having over 1600 Q&A pairs.
2. The manual annotation process lacks detail regarding reviewer selection and expertise.
3. The use of GPT-4o for quality assessment raises concerns about its effectiveness.
4. The term "dialog" in the title is misleading, as the dataset does not reflect natural dialog.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manual annotation process, detailing how reviewers are selected and their qualifications. Additionally, the authors should analyze the specific visual reasoning skills required to solve the dataset, as this is currently underexplored. Including a dataset comparison table would help highlight the merits of the MMDU benchmark. Furthermore, we suggest providing experimental results on long-context scenarios to support the dataset's claims. Lastly, the authors should consider revising the title to better reflect the dataset's nature, as the current terminology may mislead potential users.