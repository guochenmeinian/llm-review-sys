ID: AajIIYMm0d
Title: Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper analyzes the emergence and interaction of linguistic information in BERT-like models during training, proposing a new probing suite to extract task-specific subspaces. The authors observe that linguistic information develops over time, with performance improving sharply between 1k and 10k steps, and a more gradual increase thereafter. The paper also discusses the interesting phenomenon of improved probe compression rates in later training stages, although this does not correlate with out-of-distribution performance.

### Strengths and Weaknesses
Strengths:  
- The experiments are well-executed, providing interesting findings about the timecourse of linguistic information emergence.  
- The use of information-theoretic probes offers stronger theoretical support for previous conclusions regarding representation organization.

Weaknesses:  
- The framing of the paper is confusing, lacking clarity on its novelty and contribution compared to existing literature.  
- The analysis is often superficial, with a need for deeper exploration of certain findings.  
- The writing is not sufficiently reader-friendly, particularly in sections 3.2 and 3.3, and some figures are unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper's framing and explicitly articulate its unique contributions to the field. Additionally, consider conducting experiments across different model types and sizes to enhance the breadth of the analysis. We suggest focusing on deeper analyses of key findings while omitting more obvious outcomes. Finally, revising sections for readability and simplifying figures, such as Figure 6, would enhance comprehension.