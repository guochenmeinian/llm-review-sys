ID: OitmaxSAUu
Title: Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents mathematical evidence that traditional methods for interpreting transformers, such as analyzing attention patterns and feedforward weights, are unreliable. The authors demonstrate that a two-layer transformer can solve the bounded-depth Dyck-k language using only uniform attention in the second layer, challenging the assumption that interpretable attention patterns are necessary for task performance. Theorems 2 and 3 further explore the implications of this finding, showing that different pruning methods can lead to ambiguously interpretable layers. The authors validate their theoretical claims with experiments and propose a regularization term to enhance generalization to out-of-distribution examples. Additionally, the paper emphasizes that as model complexity increases, the set of non-interpretable solutions also expands. The authors argue that their work advances the understanding of attention patterns by modifying LayerNorm's position, demonstrating that modified architectures can still accurately learn the Dyck language while exhibiting uninterpretable attention patterns. They plan to extend their results to standard architectures and bounded input lengths in the camera-ready version.

### Strengths and Weaknesses
Strengths:
- The paper rigorously addresses the reliability of naive interpretability methods for transformers, providing valuable insights into the limitations of attention weights and feedforward layers.
- The choice of the bounded Dyck language as a case study is commendable, striking a balance between complexity and manageability.
- The authors provide a clear theoretical framework that connects simpler and more complex transformer models.
- Empirical results from 2-layer and 3-layer networks support their claims regarding attention patterns, and the inclusion of additional experiments, such as those on parity, enhances the robustness of their findings.

Weaknesses:
- Clarity issues arise from complex mathematical definitions and notation, particularly in Section 3.2, which could benefit from simplification.
- The paper's focus on bounded-depth Dyck language may limit its broader applicability, and the implications of the theoretical results for larger transformer architectures are not sufficiently explored.
- The empirical results lack robustness, as they rely on a limited number of random initializations and do not adequately address variations across multiple inputs.
- The paper does not quantify the proportion of interpretable versus non-interpretable solutions, raising questions about the generalizability of their claims.
- Some reviewers express dissatisfaction with the proof of the relationship between simpler and more complex models, indicating a need for clearer definitions and constructions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the mathematical definitions and notation to enhance readability, particularly in Section 3.2. Additionally, we suggest expanding the discussion on the implications of their findings for larger transformer architectures, as well as conducting more extensive empirical experiments across various Dyck languages to strengthen the robustness of their conclusions. It would also be beneficial to clarify the relationship between balanced attention and interpretability, as the current evidence may not convincingly support the claim that balanced attention patterns are inherently uninformative. Furthermore, we recommend that the authors provide a proof by construction that explicitly defines "more complex" models in the context of their work and include a discussion on the robustness of their experiments, particularly regarding the proportion of interpretable solutions. Finally, incorporating more diverse tasks, such as parity, would further strengthen their claims and provide additional empirical evidence.