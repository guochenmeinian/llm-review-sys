ID: 5Hdg5IK18B
Title: MG-Net: Learn to Customize QAOA with Circuit Depth Awareness
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 7, 6, 2, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a deep learning approach called Mixer Generator Network (MG-Net) aimed at enhancing the performance of the Quantum Approximate Optimization Algorithm (QAOA) by dynamically designing optimal mixer Hamiltonians under circuit depth constraints. The authors provide a theoretical analysis of parameter grouping and demonstrate that MG-Net improves the approximation ratio for QAOA problem instances, such as Ising models and weighted Max-Cut instances, up to 64 qubits.

### Strengths and Weaknesses
Strengths:  
- The main contribution, MG-Net, is a novel and innovative solution for customizing mixer Hamiltonians, supported by a theoretical analysis of QAOA convergence.  
- The performance evaluation is comprehensive, validating the proposed framework's effectiveness across various QAOA problem instances.  
- The paper is well-structured, with clear methodologies and supportive diagrams that enhance comprehension.  

Weaknesses:  
- The title's reference to "Quantum Hardware Constraints" is misleading, as it primarily discusses circuit depth without addressing other hardware factors like qubit connectivity or noise models.  
- The relevance of the work to the broader NeurIPS community is uncertain, as it specifically targets improvements in QAOA, which may not appeal to all quantum computing researchers.  
- The paper lacks an efficiency analysis, specifically the training and inference times for each method in Table 1.  
- The parameter grouping strategy does not guarantee a significant reduction in effective dimension, raising concerns about convergence claims.  
- The training details for the mixer generator with hardware information are inadequately presented.  
- Experimental results are shaky, with close performance to ma-QAOA and missing crucial information on the number of parameters for each method.

### Suggestions for Improvement
We recommend that the authors improve the title to accurately reflect the content regarding circuit depth and include discussions on other hardware constraints. Additionally, we suggest addressing the efficiency issue by incorporating training and inference times in Table 1. Clarifying the theoretical contributions in relation to existing literature on VQAs would strengthen the paper. The authors should also provide a comparison with the Grover-Mixer method and ensure that the training process can handle more complex gate sets. Finally, we encourage the authors to clarify the necessity of position embedding and the rationale behind the choice of $J_{ij}$ values.