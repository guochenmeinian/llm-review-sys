ID: mMnL0n7Cwy
Title: Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GraphPro, a benchmark designed to evaluate large language models (LLMs) in graph analysis, emphasizing the use of external APIs for scalability. The authors introduce the LLM4Graph datasets, which consist of crawled documents and auto-generated code, to enhance LLM performance on benchmark tasks. The study reveals that current LLMs struggle with structured data like graphs, highlighting the need for further research in this domain. The benchmark includes 512 problems across three categories: basic graph theory, graph statistical learning, and graph embedding.

### Strengths and Weaknesses
Strengths:
- The benchmark design aligns with human expert approaches by requiring programming solutions and leveraging external APIs.
- The LLM4Graph dataset provides valuable resources for improving LLM performance in graph analysis tasks.
- The study effectively highlights the limitations of LLMs in handling structured data, indicating a clear need for further advancements.

Weaknesses:
- The paper lacks a comparative analysis with recent graph-focused language models and benchmarks, which would contextualize the proposed approach.
- There is insufficient discussion on how diverse graph characteristics impact the performance of state-of-the-art graph LLM methods.
- The scalability claims require further clarification, particularly regarding the performance of LLMs with large graph inputs.

### Suggestions for Improvement
We recommend that the authors improve the contextualization of their approach by situating it within existing graph instruction-tuning frameworks and comparing it against the latest graph language models. Additionally, discussing the influence of diverse graph characteristics on LLM performance would provide deeper insights. It is crucial to rigorously assess the scalability of the proposed method, particularly how performance varies with graph size and structure. Including at least one non-LLM baseline, such as GNNs, would enhance the evaluation of LLM capabilities. Lastly, providing more examples of question types and clarifying the graph generation methods would improve the paper's clarity and comprehensiveness.