ID: us7p0VsOhl
Title: GLGR: Question-aware Global-to-Local Graph Reasoning for Multi-party Dialogue Reading Comprehension
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents "GLGR: Question-aware Global-to-Local Graph Reasoning for Multi-party Dialogue Reading Comprehension (MDRC)," which addresses multi-hop reasoning in global conversational structures and token-level semantic relations in utterances. The authors propose a Global-to-Local Graph Reasoning approach (GLGR) that integrates a Question-aware Interlocutor-Utterance Graph (QIUG) and a Local Semantic Role Graph (LSRG) using a two-stage encoder network. Experimental results demonstrate that GLGR achieves significant improvements over BERT and ELECTRA baselines, showcasing the effectiveness of the proposed reasoning framework.

### Strengths and Weaknesses
Strengths:
- The paper is well organized and effectively written.
- The two-stage reasoning framework is intuitive, connecting coarse-grained and fine-grained semantics.
- The integration of global and local reasoning methods yields substantial performance improvements.
- The approach demonstrates generalization across different pretraining schemes, which is promising.
- Detailed experiments and an ablation study validate the proposed method.

Weaknesses:
- The two-stage encoder network may be computationally expensive, limiting feasibility in large-scale applications.
- The method of comparison relies on outdated baselines, with insufficient task-related baseline comparisons.
- The graph construction heavily depends on external tools, which may affect stability.
- The performance decrease with longer context lengths raises concerns about practical applicability.
- The proposed technique includes many moving parts, which may impact its practical usability.

### Suggestions for Improvement
We recommend that the authors improve the method of comparison by incorporating more recent baselines, such as BiDEN, and providing additional task-related baseline comparisons. We also suggest conducting qualitative analyses of the outputs to enhance understanding. The authors should discuss the impact of the number of graph neural network layers on model performance and explore denoising methods to address complexities introduced by longer dialogues. Additionally, we encourage the authors to validate the effectiveness of Question-aware Reasoning through an ablation study and consider reversing the order of graph encoding to assess its potential impact on results.