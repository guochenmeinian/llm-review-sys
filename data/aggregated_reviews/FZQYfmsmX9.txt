ID: FZQYfmsmX9
Title: A Critical Evaluation of AI Feedback for Aligning Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 6, 6, -1, -1, -1
Original Confidences: 4, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation of Learning from AI Feedback (LAIF) in improving large language models (LLMs), specifically contrasting demonstrations generated by weaker LLMs with those from stronger LLMs. The authors find that the purported benefits of LAIF may be overstated, emphasizing the need for better construction of supervised fine-tuning (SFT) datasets using LLMs of comparable quality to those providing preference labels. The findings suggest that SFT may sometimes outperform the LAIF approach.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clearly outlines its motivations and contributions.
- It addresses a significant issue regarding the construction of experimental designs for LAIF, potentially opening new research avenues.
- Comprehensive experiments are conducted across various LLMs, providing valuable quantitative metrics and analyses.
- The authors effectively demonstrate the robustness of their observations through multiple model evaluations.

Weaknesses:
- Some motivations for the experimental setup appear incongruous, particularly regarding the role of human evaluators in LAIF.
- The distinction between LAIF and LHF is not consistently maintained, leading to potential confusion in the implications of the findings.
- Certain experiments lack sufficient diversity in base LLMs, and justifications for varying numbers of models are not provided.
- The analysis of the 10% rule in SFT performance is not adequately nuanced, and additional experiments could strengthen the conclusions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivations for their experimental setup, particularly regarding the role of human evaluators in LAIF. Additionally, we suggest conducting experiments where humans serve as evaluators to better assess the effectiveness of LAIF. The authors should also clarify the implications of their findings on LHF and ensure consistent terminology throughout the paper. Furthermore, we encourage the inclusion of more diverse base LLMs in experiments and provide justifications for any variations in model numbers used. Finally, we recommend re-evaluating the analysis surrounding the 10% rule in SFT performance to enhance its nuance and validity.