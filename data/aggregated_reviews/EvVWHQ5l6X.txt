ID: EvVWHQ5l6X
Title: One For All $\&$ All For One: Bypassing Hyperparameter Tuning with Model Averaging for Cross-Lingual Transfer
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to zero-shot cross-lingual transfer by proposing the "model averaging" technique, which cumulatively averages model checkpoints from different training runs. The authors demonstrate that this method leads to more robust performance in tasks such as NLI, NER, and QA compared to traditional methods like MAX SRC-DEV. Key findings include that cumulative averaging improves performance metrics and shows a positive correlation with oracle combinations of Max Dev and TRG DEV. The study also emphasizes the importance of evaluation protocols in cross-lingual transfer, highlighting the limitations of existing methods.

### Strengths and Weaknesses
Strengths:
- The paper introduces cumulative averaging as a viable alternative to traditional model selection methods, addressing significant limitations in zero-shot cross-lingual transfer.
- It conducts extensive experiments across multiple languages and tasks, providing a comprehensive evaluation of the proposed method.
- The literature review is thorough and contributes valuable insights to the community.

Weaknesses:
- The paper lacks a comparison with a simple prediction ensemble baseline, which could strengthen the findings.
- It does not adequately address standard baselines like "translate-train," which could enhance the robustness of the results.
- The writing is sometimes unclear, particularly regarding the distinction between "CA" and "Cumulative Averaging," leading to potential confusion.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing a more distinct explanation of "CA" and "Cumulative Averaging" in Table 1. Additionally, incorporating a simple prediction ensemble baseline, such as averaging predictions from individual snapshots, would provide a stronger comparative framework. We also suggest including standard baselines like "translate-train" to better contextualize the results. Finally, a clearer comparison with Schmidt et al. (2023) regarding the differences in methods and performance metrics would enhance the paper's rigor and transparency.