ID: I9DVeu8XKa
Title: CodeFusion: A Pre-trained Diffusion Model for Code Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CODEFUSION, the first diffusion model for code generation tasks, aiming to enhance diversity and maintain code validity. The authors employ a transformer architecture combined with diffusion techniques and utilize CPD tasks to prevent the generation of syntactically invalid programs. CODEFUSION is evaluated on NL2Code tasks for Bash, Python, and Excel CF rules, achieving performance comparable to mainstream auto-regressive LMs while demonstrating superior top-3/5 performance due to the inherent randomness of diffusion LMs.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach by leveraging diffusion LMs for code generation, marking a significant advancement in the field.
- Empirical evaluations on various code generation tasks demonstrate the effectiveness of CODEFUSION.
- The presentation meets the standards of an *ACL publication, with clear definitions and impressive experimental results.

Weaknesses:
- Comparisons with general-purpose models like text-davinci-003 and CodePTMs may be perceived as "unfair" due to their task-specific training.
- The abstract lacks clarity regarding the research motivation and objectives, particularly in articulating the importance of increasing code generation diversity.
- The paper does not provide sufficient evidence to support claims that diffusion models outperform conventional LLMs, nor does it analyze the diversity of generated code adequately.

### Suggestions for Improvement
We recommend that the authors improve the abstract to clearly convey the research motivation and objectives, specifying the importance of increasing diversity in code generation. Additionally, the authors should provide a detailed explanation of how the loss function design ensures diversity and consider including weight parameters for the second and third terms. It would be beneficial to compare CODEFUSION with state-of-the-art models such as StarCoder and CodeT5+, and to include results from popular benchmarks like HumanEval and APPS. Finally, we suggest adding more recent references related to code generation to ensure the paper is up-to-date.