ID: mmlQICRJMc
Title: AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an adapter technique that eliminates the need for finetuning on the sentence embedding objective for each domain-adapted language model, allowing the sentence embedding objective to be trained once for use across all models. The authors propose AdaSent, a method for domain-adapted sentence embeddings for few-shot classification, which combines Domain-Adaptive Pre-training (DAPT) and Sentence Embedding Pre-training (SEPT) to enhance classification accuracy while reducing training costs. Extensive experiments across multiple datasets support the effectiveness of both approaches.

### Strengths and Weaknesses
Strengths:
- The ability to train the sentence embedding adapter once and apply it to various domain-adapted LMs is a significant contribution.
- Comprehensive experiments provide strong evidence for the proposed methods.
- Clear writing and organization, with effective citations of previous work.

Weaknesses:
- Limited novelty, as the approaches are based on existing methods like SetFit and PEFT without substantial differentiation.
- Insufficient experimental evaluation, lacking detailed comparisons with state-of-the-art methods and statistical significance tests.
- Clarity and organization issues hinder understanding and reproducibility, particularly in the explanation of supervised contrastive learning.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contributions by clearly articulating how their approach differs from existing methods and why it is superior. Additionally, we suggest enhancing the experimental evaluation by including detailed comparisons with state-of-the-art methods, statistical significance tests, and ablation studies to support their claims. Finally, we encourage the authors to refine the clarity and organization of the paper, particularly in the sections discussing supervised contrastive learning, to facilitate better understanding and reproducibility.