ID: JTKd7zYROf
Title: Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 6, 7, 7, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a randomized sparse neural Galerkin (RSNG) algorithm aimed at solving time-dependent PDEs by utilizing parameterized functions as ansatz spaces with time-varying parameters. The authors propose a sequential-in-time learning approach, contrasting with global-in-time methods like PINNs, and introduce a novel remedy to address issues with existing methods by employing randomized sparse instantiations of the projection. The algorithm's effectiveness is demonstrated through benchmark examples, showing significant performance improvements over dense least-squares methods and traditional PINNs. Additionally, the authors assert that RSNG effectively addresses overfitting while providing significant speed improvements with minimal accuracy loss, acknowledging the importance of trade-offs between computational costs and sparsity parameters, as well as the need for further exploration of RSNG's applicability in parametric problems.

### Strengths and Weaknesses
Strengths:
- The premise of the paper is compelling, addressing the challenges of global-in-time methods like PINNs.
- Randomized sparse algorithms can enhance efficiency under certain conditions, as identified by the authors.
- Numerical experiments convincingly show that RSNG outperforms both dense least-squares methods and baseline PINNs.
- The authors have effectively addressed several reviewer concerns, enhancing the paper's quality.
- RSNG shows promise in ameliorating overfitting and improving computational efficiency in Neural Galerkin schemes.
- The authors plan to include additional experimental results and clarifications in the camera-ready version.

Weaknesses:
- Clarity issues exist, particularly in notation and explanations, necessitating a rewrite for better comprehension.
- The paper lacks theoretical justification for the proposed method, leaving readers uncertain about its applicability.
- Empirical evaluations are limited to well-studied benchmarks, with a need for broader testing against traditional numerical methods.
- The algorithm's scalability with spatial dimensions and its applicability to stiff problems remain unclear.
- Comparisons with existing literature are insufficiently detailed, and additional baselines are required for a comprehensive evaluation.
- Clarity issues remain regarding specific reviewer questions and theoretical foundations.
- The baseline comparisons with PINNs are perceived as weak, raising concerns about the competitiveness of RSNG against traditional numerical methods.
- The applicability of RSNG in critical problems, such as parametric PDEs, has not been adequately demonstrated.

### Suggestions for Improvement
We recommend that the authors improve clarity by elaborating on the notation and providing detailed explanations, particularly in the appendix. Specifically, they should clarify the roles of time and space variables in their equations and provide a rationale for the choice of parameters in their derivations. The authors should also include theoretical justifications for their method, discussing its expected performance and limitations. We suggest that the authors explicitly address the specific questions raised by reviewers, particularly regarding theoretical aspects. Additionally, we recommend that the authors provide stronger baseline comparisons with PINNs to enhance credibility. Expanding empirical evaluations to include comparisons with classical numerical methods and exploring the algorithm's performance in high-dimensional problems would strengthen the paper. It would also be beneficial to include experiments demonstrating RSNG's performance in parametric problems to showcase its broader applicability. Finally, we encourage the authors to elaborate on the limitations of their method in the camera-ready version, as discussed in the reviews.