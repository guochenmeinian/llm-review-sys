ID: fzlMza6dRZ
Title: GraphTrail: Translating GNN Predictions into Human-Interpretable Logical Rules
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 4, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel global explanation generation method for Graph Neural Networks (GNNs) that utilizes computation trees and Shapley values to derive logical formulas for class interpretations. The authors propose that by focusing on computation trees, they can reduce the complexity of Shapley value calculations from an exponential search space of subgraphs to a linear search space of trees. The method evaluates the influence of these trees on GNN predictions and generates logical rules, demonstrating its effectiveness across various GNN architectures and datasets. Additionally, the authors discuss the experimental design and interpretability of formulas generated by GLG, the only known technique for producing logical formulas, asserting that their approach surpasses GLG in both accuracy and interpretability.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant gap in the literature by employing Shapley values for global explanations of computation trees.
- The approach effectively simplifies the computationally intensive task of calculating Shapley values over subgraphs by focusing on computation trees.
- The authors have effectively addressed reviewer concerns and clarified key aspects of their methodology, particularly regarding the non-reproducibility of GLGExplainer.
- Experimental results provide some support for the authors' claims, showcasing ideal behavior in interpretable AI.

Weaknesses:
- The experimental results do not sufficiently demonstrate the superiority of this method over other explanation methods, as comparisons are limited to GLG.
- Some reviewers still perceive the experimental design as lacking robustness, with concerns about the ad hoc nature of comparisons.
- The generated explanations, while more interpretable than some global methods, remain complex and lengthy, making them difficult to read.
- The paper lacks adequate descriptions of the specific calculation methods for Shapley values and symbolic regression, relying instead on citations without sufficient detail. There is also a need for more concrete examples of global explainers that could enhance interpretability.

### Suggestions for Improvement
We recommend that the authors improve their experimental design by including comparisons with additional global explanation methods and instance-level explanations to better substantiate their claims. Furthermore, we suggest enhancing the clarity of the generated explanations to make them more accessible. Additionally, the authors should provide more detailed descriptions of the Shapley value calculation methods and symbolic regression techniques used in their approach. Addressing these weaknesses will strengthen the overall contribution of the paper. Lastly, we recommend that the authors provide clearer justifications for their comparison choices and include specific examples of existing global explainers that may produce more interpretable formulas, as this would further bolster their argument regarding the uniqueness of GLG.