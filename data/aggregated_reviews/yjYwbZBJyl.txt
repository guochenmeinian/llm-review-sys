ID: yjYwbZBJyl
Title: Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 8, 6, 6, 6, 7, -1, -1, -1
Original Confidences: 4, 2, 3, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a generalization of existing inconsistency results to non-interpolating models and various kernels, demonstrating that benign overfitting with moderate derivatives is impossible in fixed dimensions. The authors prove that interpolation with spiky-smooth kernels can be consistent, and such kernels can be induced by specific activation functions. The work also explores the generalization behavior of overfitting methods, emphasizing the significance of estimator smoothness over input dimension. Experimental results support the theoretical claims.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, providing novel insights into benign overfitting, particularly the importance of estimator smoothness.
- The authors improve previous inconsistency results for RKHS equivalent to Sobolev RKHS of smoothness \(s > d/2\) on the sphere \(\mathbb{S}^d\).
- The introduction of spiky-smooth kernels is an interesting and novel approach.

Weaknesses:
- The significance of the results is somewhat unclear, particularly regarding their applicability in modern high-dimensional settings.
- The results concerning spike-inducing kernels may appear hacky, as they suggest a dependency on dataset size that resembles adding a ridge parameter.
- There is a lack of explicit rate estimates for interpolation with spiky-smooth kernels, which could enhance understanding of optimal performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the significance of their findings and how they relate to existing methods, such as using a ridge parameter or optimal stopping. Additionally, we suggest including explicit rate estimates for spiky-smooth kernels to provide further insights into their design for optimal performance. It may also be beneficial to explore the implications of their results beyond the kernel regime, particularly in the feature learning context.