ID: ody3RBUuJS
Title: FedGCN: Convergence-Communication Tradeoffs in Federated Training of Graph Convolutional Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 6, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called FedGCN for federated training of graph convolutional networks (GCNs) aimed at semi-supervised node classification. The authors propose a method to communicate cross-client neighbor information only once before training, significantly reducing communication overhead and improving convergence speed. The framework allows for the selection of 0-, 1-, or 2-hop neighbor communication to balance overhead and model accuracy. Empirical results demonstrate FedGCN's effectiveness and minimal communication costs compared to previous methods. Additionally, the authors analyze federated learning (FL) with a focus on graph neural networks, addressing the use of real datasets with simulated data partitions. They clarify that cross-client edges are stored at both nodes, while personal node characteristics remain local, and acknowledge the confusion caused by the phrase "Keeping data where it is generated," planning to replace it with a clearer explanation. The authors also recognize the limitations of the Lipschitz property in real-world applications and express their intention to incorporate discussions on performance gains in cross-device settings and the implications of cross-client edges.

### Strengths and Weaknesses
Strengths:
1. The problem addressed is significant, as GNNs often deal with large-scale graphs that incur high computational and storage costs.
2. The narrative is well-presented, making the review of GCNs accessible to a broader audience.
3. The algorithm is simple and leverages established public encryption schemes, ensuring client data privacy.
4. The method shows superior performance over other federated algorithms across various benchmarks.
5. The authors demonstrate a willingness to address reviewer concerns and clarify complex concepts.
6. The paper includes empirical validation of qualitative observations regarding FedGCN's convergence.

Weaknesses:
1. The empirical results indicate only marginal gains compared to existing methods like FedSage+.
2. The motivation for the proposed method is unclear and appears contradictory regarding data partitioning and privacy.
3. The assumptions made in the analysis are overly strict and not well-justified, raising concerns about their applicability in real-world scenarios.
4. The experiments are conducted on small-scale datasets, which may not reflect the challenges posed by large graphs.
5. The assumptions made in the paper regarding FL are considered too strict for real-world applications, which raises concerns about the theoretical analysis presented.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation by addressing the contradictions in their claims about data partitioning and privacy. Additionally, we suggest providing a more robust justification for the assumptions made in the analysis, potentially through empirical validation. The authors should also consider conducting experiments on larger, real-world datasets to strengthen their claims about the framework's applicability. Furthermore, clarifying the technical details regarding the sufficiency of 1- and 2-hop feature aggregations and the implications of the encryption scheme on convergence would enhance the paper's rigor. Lastly, we recommend that the authors improve the clarity of their explanations, particularly regarding the storage of node features and the implications of cross-client edges, and further discuss the limitations of their theoretical assumptions in the updated version of the paper.