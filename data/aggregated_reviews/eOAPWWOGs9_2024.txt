ID: eOAPWWOGs9
Title: AutoPSV: Automated Process-Supervised Verifier
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 6, 7, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AutoCV, a novel method for creating process reward models (PRMs) that combines outcome and process supervision to generate process annotations from outcome-supervised verification models. The authors propose training on (S_{(1:t)}, y) pairs, where S_{(1:t)} represents the initial steps in a response and y is the correctness label. AutoCV demonstrates significant advantages in best-of-N sampling across five datasets in mathematics and commonsense reasoning, outperforming self-consistency baselines and requiring fewer tokens for annotations than existing methods.

### Strengths and Weaknesses
Strengths:
- The method provides a novel approach to obtain process annotations without manual supervision, derived from a solid theoretical foundation.
- It effectively extends beyond previous PRM work focused solely on mathematics, showing applicability in commonsense reasoning and natural language inference.

Weaknesses:
- The experimental results lack convincing strength, with minimal performance gains observed. 
- The clarity of section 4.2.1 is questionable, particularly regarding its focus on "process calculation hallucination detection," which may be more broadly applicable to detecting faulty steps.
- The absence of baselines in the main experiments raises concerns about the validity of the results.
- The setup does not clarify whether experiments are in-domain, necessitating out-of-domain evaluations for generalizability.
- Comparisons in Tables 8 and 9 may not fairly represent efficiency, as AutoCV's training costs could offset its token efficiency.
- The significance of results is diminished by the small performance improvements, raising questions about the utility of AutoCV compared to existing methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of section 4.2.1 to better articulate the specific advantages of their method in detecting faulty steps. Additionally, we suggest including baselines in the main experiments to strengthen the validity of the findings. Clarifying whether the experiments are in-domain and providing out-of-domain evaluations would enhance the generalizability of the results. We also advise a more comprehensive analysis of efficiency comparisons, possibly including the total GPU hours for training and inference. Lastly, we encourage the authors to address the minimal performance gains and explore additional contributions beyond efficiency to bolster the significance of their work.