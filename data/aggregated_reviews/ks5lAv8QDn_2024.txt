ID: ks5lAv8QDn
Title: DiffTextPure: Defending Large Language Models with Diffusion Purifiers
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 7, 7
Original Confidences: 3, 3, 3

Aggregated Review:
### Key Points
This paper presents DiffTextPure, a novel approach that purifies adversarial textual inputs using a diffusion-based method during a pre-processing phase. The authors propose this plug-and-play defense to enhance the robustness of large language models (LLMs) against jailbreaking attacks without modifying their core architecture. The methodology is well-defined, including both forward and reverse diffusion processes, and empirical results demonstrate significant improvements in robustness against various adversarial attacks.

### Strengths and Weaknesses
Strengths:
- The innovative application of diffusion models in the discrete domain for textual adversarial defense expands their applicability beyond the vision domain.
- DiffTextPure operates efficiently in a pre-processing step, avoiding the need for joint training with LLMs, which enhances its practicality for large-scale deployment.
- The authors provide theoretical guarantees for certified robustness, increasing the credibility of the method in practical applications.

Weaknesses:
- The computational overhead of discrete diffusion operations may limit scalability in high-throughput applications.
- DiffTextPure shows reduced effectiveness against expertise-based attacks, such as In-Context Attacks (ICA), which are common in real-world scenarios.
- The method is constrained by a token length limitation of fewer than 1024 tokens, which may hinder its effectiveness with longer text inputs.
- There is no analysis of the trade-off between performance and safety, raising concerns about potential performance degradation when changing input prompts.

### Suggestions for Improvement
We recommend that the authors improve the analysis of non-optimization-based attacks, particularly explaining why DiffTextPure performs poorly against ICA. This additional analysis would clarify the method's limitations and inspire potential improvements. Furthermore, we suggest enhancing the coherence between the explanation of diffusion models and the introduction of DiffTextPure to create a smoother flow and better understanding of the core concepts. Additionally, providing an example of ICA to illustrate the method's shortcomings and including results for Llama-3 in Table 1 would strengthen the paper.