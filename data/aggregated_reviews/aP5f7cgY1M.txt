ID: aP5f7cgY1M
Title: Rather a Nurse than a Physician - Contrastive Explanations under Investigation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analytic investigation into contrastive explanations in NLP models, focusing on their effectiveness compared to non-contrastive explanations. The authors analyze four English text classification datasets, fine-tuning three models (RoBERTa, GPT-2, and T5) and applying three post-hoc explainability methods (LRP, GradientxInput, GradNorm). Key contributions include the collection of human rationale annotations for 100 samples from the BIOS dataset, a cross-comparison of model-based rationales with human annotations, and insights into the alignment of human and model explanations across both settings.

### Strengths and Weaknesses
Strengths:
- Thorough analysis of contrastive explanations using multiple datasets and models.
- Valuable insights into the alignment of human and model explanations.
- Release of a new dataset with human annotations, promoting further research in contrastive XAI methods.

Weaknesses:
- Limited to three models and three post-hoc methods, which may not represent the full spectrum of NLP research.
- Lack of detailed analysis on the differences between contrastive and non-contrastive explanations.
- Absence of a clear definition for "plausible explanations," potentially leading to evaluation inconsistencies.
- Small size of the labeled subset (100 samples) raises questions about its adequacy.

### Suggestions for Improvement
We recommend that the authors improve the analysis by providing a detailed examination of the differences between contrastive and non-contrastive explanations, including how varying degrees of distinction between entities affect model rationales. Additionally, we suggest adding case studies that explore how model rationales change with different label predictions between settings. Clarifying the criteria for model and method selection, defining "plausible explanations," and analyzing the impact of factors like dataset size and model architecture on explanation quality would enhance the paper's rigor. Finally, we encourage the authors to elaborate on the rationale behind the small labeled subset and discuss plans for extending this work to other languages and tasks.