ID: JMbJeMTFos
Title: Improving word mover's distance by leveraging self-attention matrix
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an enhancement to the Word Mover's Distance (WMD) method for measuring sentence similarity by incorporating structural information from the self-attention matrix (SAM) of BERT, resulting in the proposed Word and Sentence Mover's Distance (WSMD). The authors validate their method on the PAWS dataset, demonstrating improved performance in paraphrase identification while maintaining similar results in semantic textual similarity tasks. The WSMD method optimizes both word vector distance and sentence structure distance, allowing for a more nuanced understanding of sentence semantics.

### Strengths and Weaknesses
Strengths:
- The proposed method effectively integrates sentence structure into the WMD framework, addressing limitations related to word order.
- The empirical validation on the PAWS dataset shows that WSMD outperforms baseline methods, particularly in paraphrase identification.
- The paper is generally well-written and presents a clear methodology.

Weaknesses:
- The writing can be difficult to understand, with specific figures (e.g., Figure 4 and Figure 6) lacking clarity.
- The performance improvement on the STS Benchmark dataset is limited, and the method's generalizability across different pre-trained language models is not fully explored.
- The evaluation datasets are relatively small, which may affect the robustness of the findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing, particularly in the presentation of figures, and consider moving detailed ablation studies to the Appendix to focus on their main contributions. Additionally, we suggest including comparisons with other semantic similarity methods, such as BERTScore, to strengthen the validation of their approach. Finally, exploring the applicability of their method across various pre-trained language models would enhance its generalizability and impact in the NLP field.