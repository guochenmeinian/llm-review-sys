ID: QeWibaTmnn
Title: Grasp as You Say: Language-guided Dexterous Grasp Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel robotic grasping task that utilizes natural language to generate grasping poses for dexterous hands, aligning with user intentions. The authors create a large-scale dataset, DexGYSNet, comprising 50,000 text-grasp pose pairs across 1,800 objects, and propose a two-stage framework, DexGYSGrasp, which combines a diffusion-based generative model and a regressive model to ensure diverse and high-quality grasp poses. The method is evaluated on intention consistency, grasp quality, and diversity, demonstrating its potential for real-world applications.

### Strengths and Weaknesses
Strengths:
1. The paper advances the field by addressing a significant problem in language-guided dexterous grasping and offers a comprehensive solution.
2. The dataset is a valuable resource, effectively utilizing the OakInk dataset and employing innovative annotation methods.
3. The two-stage framework enhances grasp quality and diversity, showcasing the necessity of progressive components.

Weaknesses:
1. The evaluation metrics, such as chamfer distance and contact distance, may not adequately reflect intention consistency due to the inherent imprecision of natural language.
2. The dataset's size and diversity are limited compared to existing datasets, which may affect performance on unseen categories.
3. The paper lacks detailed information on the training and test sets, particularly regarding the consistency of target poses and descriptions.
4. The method's reliance on a full point cloud and its static pose generation may hinder practical applications, necessitating an end-to-end pipeline.

### Suggestions for Improvement
We recommend that the authors improve the evaluation metrics by considering alternative measures that better reflect the generative nature of the task. Additionally, the authors should expand the dataset to include more diverse grasping poses to enhance generalization capabilities. Clarifying the training and test set consistency and providing more detailed experiments on unseen objects would strengthen the paper. Finally, we suggest exploring the use of a simulator, such as Isaac Sim, to validate grasp quality and success rates in real-world scenarios.