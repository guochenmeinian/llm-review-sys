ID: CoaB9M2BcF
Title: Causal Question Answering with Reinforcement Learning
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a reinforcement learning approach for causal question answering using the CauseNet knowledge graph. The authors introduce a new dataset tailored for causal questions and propose an Actor-Critic based RL agent that integrates supervised learning to navigate large action spaces and sparse rewards. The methodology is well-articulated, and the paper includes an ablation study that enhances the experimental depth.

### Strengths and Weaknesses
Strengths:
- The paper pioneers the use of reinforcement learning for causal question answering, demonstrating significant improvements over traditional methods like breadth-first search.
- The introduction of CauseNet provides a robust foundation for future research in causal reasoning.
- The methodology is clearly described, and the running example of pneumonia diagnosis effectively illustrates the paper's significance.

Weaknesses:
- The experimental section lacks comprehensive comparisons with existing models, particularly with GPT models, which could provide a clearer understanding of the RL agent's performance.
- The dataset is limited to binary causal questions, and the impact of defaulting to a NO answer is not adequately discussed, potentially leading to false negatives.
- The novelty of the approach is unclear beyond the use of CauseNet, with insufficient exploration of how it differs from existing models like DeepPath and MINERVA.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset's construction and provide a name for the new dataset. Additionally, including a comparison with GPT models, such as GPT-3.5 or GPT-4, would enhance the evaluation. The authors should also discuss the implications of defaulting to a NO answer and consider using more recent embeddings, such as BioBERT or ClinicalBERT, to better handle medical terminology. Furthermore, expanding the scope of the dataset to include more complex causal relationships and additional datasets for evaluation would strengthen the paper's contributions. Finally, we suggest incorporating human evaluation to solidify the empirical findings.