ID: hFVpqkRRH1
Title: Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 5, 7
Original Confidences: 5, 3

Aggregated Review:
### Key Points
This paper presents the Web2Code dataset and evaluation framework, designed to enhance multi-modal large language models (MLLMs) in understanding webpage screenshots and generating corresponding HTML code. It includes over 1.17 million instruction-response pairs, featuring webpage images, HTML code, and natural language question-answer pairs, primarily generated using GPT-3.5/4. The evaluation framework comprises two benchmarks: the Web Understanding Benchmark (WUB) and the Web Code Generation Benchmark (WCGB), which assess the performance of models trained on the Web2Code dataset, claiming superiority over previous datasets in web understanding and HTML generation tasks.

### Strengths and Weaknesses
Strengths:
1. The Web2Code dataset is notable for its large scale (1.18 million examples) and diversity, focusing on complex web understanding and HTML generation tasks.
2. The introduction of two evaluation benchmarks enhances the assessment of model performance in web understanding and HTML code generation.
3. Models trained on the Web2Code dataset demonstrate improvements in webpage-to-code translation and visual domain tasks, highlighting the dataset's effectiveness.
4. The dataset and benchmarks facilitate advancements in automated web development and UI prototype design, contributing to MLLM progress in coding.

Weaknesses:
1. Concerns exist regarding the quality and diversity of the synthesized data, as the authors do not adequately explain their quality control measures.
2. The reliance on a singular GPT-4V-based evaluation method in the WCGB benchmark is seen as subjective; additional objective metrics are recommended.
3. The evaluation experiments lack clarity on baseline settings and performance changes when incrementally adding training sets.
4. The authors' claims regarding existing datasets leading to performance degradation are unsupported by experimental results, limiting the evaluation of their dataset's effectiveness.

### Suggestions for Improvement
1. We recommend that the authors clarify whether webpage screenshots were input into GPT-4 for data synthesis, as this could be misinterpreted. Additionally, they should provide details on how they control the quality and diversity of the synthetic data.
2. We suggest incorporating more objective evaluation metrics, such as rule-based metrics or CLIP, to complement the GPT-4V-based evaluation in the WCGB benchmark.
3. We encourage the authors to report performance changes of models when incrementally adding training sets, particularly for LLaMA3, to provide a clearer understanding of dataset contributions.
4. We recommend that the authors support their claims regarding performance degradation with experimental results comparing WUB and WCGB trained on existing datasets like WebSight0.1 and 0.2.
5. We advise the authors to elaborate on privacy protection measures during data collection and processing to address potential legal and ethical risks.