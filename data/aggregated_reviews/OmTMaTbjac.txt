ID: OmTMaTbjac
Title: MAViL: Masked Audio-Video Learners
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 4, 4, 7, 8, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a self-supervised audiovisual representation learning framework, combining MAE with contrastive learning in both intra and inter modality settings. The authors propose a self-training schema where a student model aligns with the latent representations of unmasked input produced by a teacher model, enhancing the quality of learned representations without relying on external pre-trained models. The framework, named Masked Audio-Video Learners (MAViL), is pretrained on three pseudo tasks and evaluated on datasets like AudioSet and VGGSound, achieving state-of-the-art performance in audio-visual classification and retrieval.

### Strengths and Weaknesses
Strengths:
1. The work is well-motivated and generally easy to follow.
2. Comprehensive experiments and ablation studies support the design choices.
3. The integration of existing components into a cohesive framework is valuable.
4. The paper distinguishes itself from similar works that combine MAE with contrastive learning.
5. Promising self-training results provide valuable insights.

Weaknesses:
1. The effectiveness of the fusion layer is unclear, as it does not show meaningful gains over unimodal MAE.
2. The intra-modal loss shows marginal gains over inter-modal loss, which doubles computation costs.
3. The addition of joint AV-MAE does not significantly enhance baseline performance.
4. The paper lacks a thorough evaluation of video modalities and comparisons with state-of-the-art methods.

### Suggestions for Improvement
We recommend that the authors improve clarity on the effectiveness of the fusion layer and provide results without it to assess its impact. Additionally, the authors should include linear evaluation results to demonstrate the quality of learned representations. A more comprehensive comparison with prior works, particularly in video tasks, is necessary. We suggest evaluating the method on larger datasets like Kinetics400 and including results on UCF101 and HMDB51. Furthermore, the authors should clarify the rationale behind the choice of $\tau$ values for inter and intra losses and consider using different loss functions such as cross-entropy or cosine distance. Finally, addressing the impact of data augmentations and discussing limitations in the context of the proposed framework would enhance the paper's depth.