ID: KSjnVt9awC
Title: Revisiting the Knowledge Injection Frameworks
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the efficacy of current knowledge injection methods in large language models (LLMs), revealing that many perform worse than random knowledge or noise. The authors propose a simple remedy method that shows promise in improving performance. The study emphasizes the importance of refining knowledge before injection, suggesting that injecting smaller amounts of knowledge can yield better results than traditional methods.

### Strengths and Weaknesses
Strengths:
- The problem addressed is significant and relevant to the field of knowledge injection.
- The writing is clear and the experimental design is robust, supporting the authors' conclusions.
- The findings are meaningful, showing that unaligned knowledge can sometimes outperform aligned knowledge.

Weaknesses:
- There is a lack of clarity in illustrating the injection methods, particularly in distinguishing between text-based and embedding-based approaches.
- Some implementation details are vague, such as the differences between random and noise injection.
- The paper is difficult to follow in parts, particularly the introduction, and contains typographical errors.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the injection methods in section 4, explicitly distinguishing between text-based and embedding-based approaches. Additionally, we suggest providing more detailed implementation specifics for random and noise injection. To enhance readability, the authors should revise the introduction to simplify terminology and correct typographical errors throughout the paper. Finally, addressing the potential impact of hyperparameters and the number of epochs on results would strengthen the analysis.