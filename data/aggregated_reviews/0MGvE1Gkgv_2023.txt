ID: 0MGvE1Gkgv
Title: Katakomba: Tools and Benchmarks for Data-Driven NetHack
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 7, 4, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a set of tools for offline reinforcement learning (ORL) in the game NetHack, addressing limitations of current tools by proposing enhancements to increase data loader speed, provide standard ORL algorithm implementations with LSTM memory, and utilize RLiable tools for benchmarking metrics stored in Weights & Biases. The dataset is subdivided into variants based on human expectations of difficulty, allowing for multiple levels of task difficulty. The authors explore the complexities inherent in the game mechanics and suggest that offline RL methods can potentially outperform existing policies derived from suboptimal datasets, although current results do not yet support this claim. Evaluations reveal that common ORL algorithms perform poorly on these tasks, with the exception of REM, which fails to learn entirely. The paper discusses the significance of metrics such as the Optimality Gap and the normalization of scores, which are essential for evaluating agent performance.

### Strengths and Weaknesses
Strengths:  
- The paper identifies challenges in adopting the large-scale NetHack dataset for offline RL and makes significant engineering contributions, including faster data-loading pipelines and clean implementations of baseline offline RL methods.  
- NetHack serves as a challenging testbed for intelligent agents, and the proposed benchmark is expected to promote research in data-driven NetHack, providing practical value to the RL community.  
- The authors provide a clear explanation of the importance of offline RL in the context of NetHack, emphasizing the potential for agents to surpass rule-based and human players.  
- The inclusion of additional resources and clarifications in the updated version of the paper enhances the reader's understanding of the game mechanics and evaluation metrics.  
- The empirical evaluations support the claims of improved data loader speed and poor performance of baseline ORL algorithms.

Weaknesses:  
- The contribution appears modest, primarily enhancing existing datasets without substantial innovation.  
- The problem formulation lacks depth, particularly regarding game rules, roles, and transition tuples.  
- The paper lacks clarity on how normalized scores are computed and does not include standard deviations for benchmark results.  
- The claims regarding the performance of behavioral cloning (BC) versus offline RL (ORL) may be premature without extensive hyperparameter tuning and experimentation.  
- Limitations of the tooling itself are not thoroughly discussed, particularly regarding scalability and dataset gathering.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the normalized score computation and consider including standard deviations in benchmark results. Additionally, elaborating on the problem formulation by providing detailed explanations of the game mechanics, including rules, roles, and transition tuples, would enhance understanding. The authors should also discuss how their dataset differs from the AA dataset beyond categorization and provide more detailed analyses of claims made in Section 4. Furthermore, we suggest including experiments on computational scalability, exploring the scalability of training time with different hardware configurations, and addressing how the tooling can support gathering new datasets. Lastly, clarifying the installation requirements and providing a Google Colab example would facilitate user engagement.