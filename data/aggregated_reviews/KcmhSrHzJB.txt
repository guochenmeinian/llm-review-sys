ID: KcmhSrHzJB
Title: On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 6, 7, 3, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework that connects Deep Neural Networks (DNNs) with infinite tree-structured Probabilistic Graphical Models (PGMs). The authors demonstrate that DNN forward propagation corresponds to exact inference in the constructed PGM. They propose a Hamiltonian Monte Carlo (HMC) algorithm for model calibration and conduct experiments on synthetic data and the Covertype dataset to evaluate calibration error. The paper aims to provide insights into DNNs from a PGM perspective, although the practical utility of this connection remains uncertain.

### Strengths and Weaknesses
Strengths:
- The paper establishes a novel connection between DNNs and PGMs, revealing that DNN forward propagation aligns with exact inference in an infinite PGM.
- The proposed HMC-based learning algorithm shows promise in yielding well-calibrated models.
- The writing is generally clear, and the contributions and limitations are well articulated.

Weaknesses:
- The practical implications of transitioning from DNN function space to PGM space are not fully explored, with experiments lacking robust real-world validation.
- The presentation suffers from clarity issues, particularly in the theoretical exposition and experimental results, which could benefit from better organization and standard baselines.
- Some theoretical constructs are imprecisely presented, raising questions about their pedagogical utility.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical sections, particularly Theorem 1, by providing more precise definitions and addressing minor bugs and ambiguities. The experimental results section should be reorganized to clearly discuss research questions, metrics, baselines, and datasets before presenting results. Additionally, we suggest including comparisons with standard baselines and exploring the implications of using HMC in more detail. Finally, clarifying the definitions of "weight updating" and "sampling" in the algorithm pseudocode would enhance readability.