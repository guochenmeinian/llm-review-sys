ID: hsptWISmi6
Title: Post-hoc Utterance Refining Method by Entity Mining for Faithful Knowledge Grounded Conversations
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents REM, a refinement model that utilizes an encoder-decoder architecture with named entity recognition (NER) to enhance the faithfulness of language model responses to source knowledge. The encoder extracts named entities, while the decoder refines the original responses based on these entities and the source knowledge. REM aims to mitigate hallucinations in generated utterances by improving entity coverage and source faithfulness. The authors report results on various datasets, demonstrating the effectiveness of REM compared to several baselines.

### Strengths and Weaknesses
Strengths:
- REM significantly improves the entity coverage and source faithfulness of generated utterances.
- The ablation study confirms the importance of the NER sub-task for the model's performance.
- The paper provides a comprehensive evaluation using multiple metrics and datasets, and it addresses limitations transparently.

Weaknesses:
- The methodology for applying REM to larger language models (LLMs) is not clearly articulated.
- Key sections, including related work and implementation details, are relegated to the appendix, which detracts from the paper's accessibility.
- Human evaluation is limited to a few samples per dataset, which could be expanded for more robust findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the REM-LLM methodology by providing additional implementation details. It would also be beneficial to share the plug-and-play system along with the trained models. We suggest moving important sections from the appendix to the main text to enhance readability and comprehension. Additionally, expanding the human evaluation to include more samples would strengthen the findings. Finally, we encourage the authors to incorporate recent work on knowledge grounding to contextualize their contributions better.