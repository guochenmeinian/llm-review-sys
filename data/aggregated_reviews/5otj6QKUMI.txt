ID: 5otj6QKUMI
Title: Compression with Bayesian Implicit Neural Representations
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 6, 6, 8, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approximated correlation communication approach to compress Bayesian implicit neural representations (INRs) for both image and audio data. The authors propose two practical considerations: prior fitting and posterior refinement, achieving rate-distortion (R-D) performance comparable to state-of-the-art (SOTA) methods in image compression. The method employs variational Bayesian techniques and relative entropy coding, allowing for direct optimization of the R-D performance by minimizing the $\beta$-ELBO.

### Strengths and Weaknesses
Strengths:
- The novel use of correlation communication for compressing Bayesian INRs is impactful and extends beyond the compression community.
- The iterative algorithm for learning prior weight distributions and the progressive refinement process for the variational posterior contribute to improved performance.

Weaknesses:
- There is no existing algorithm that communicates correlation using a finite number of samples, which raises concerns about the accuracy of the posterior approximation. The expected code length of relative entropy coding does not emphasize the gap to the true posterior, potentially affecting performance.
- The advantage of the proposed method over MSCN is unclear, as it is only comparable on Kodak and slightly outperformed on CIFAR. The long encoding time due to progressive posterior refinement needs to be justified with evidence of faster runtime compared to MSCN.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation, specifically by clearly describing the notation for $\theta_p$, addressing typos, and ensuring all references are complete. Additionally, the authors should provide a detailed comparison of encoding times between their method and MSCN to substantiate claims of simplicity and efficiency. It would also be beneficial to report the converging speed of the models, such as PSNR-iteration curves, to better evaluate the trade-offs involved. Finally, we suggest including a discussion on the implications of using finite samples for posterior approximation and how this affects the R-D curve.