ID: RcqAmkDJfI
Title: Not All LLM Reasoners Are Created Equal
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 7, 6
Original Confidences: 5, 3, 4

Aggregated Review:
### Key Points
This paper presents a study on compositional GSM8K, introducing a 2-hop reasoning version called *Compositional GSM*, which consists of pairs (Q1, Q2) of modified problems from GSM8K. The final answer to Q1, referred to as X, is used as a variable in Q2. The authors measure the "reasoning gap," defined as the difference in accuracy between the original GSM8K and the compositional version, revealing that smaller models exhibit a significantly greater reasoning gap than larger models. The appendix provides comprehensive experimental details and discussions on reasoning with natural language versus code.

### Strengths and Weaknesses
Strengths:
* Clearly written, with a natural definition of the reasoning gap.
* Demonstrates a clear trend indicating that model size is crucial for "reasoning capacity," even on tasks considered easy for smaller models.
* Inspires further research into "composed questions" and sequential reasoning concepts.
* The intriguing finding that larger models have a smaller reasoning gap warrants further investigation.

Weaknesses:
* Minimal investigation into the reasons behind the reasoning gap, such as trends in incorrect responses.
* The claim that smaller models often answer the first question correctly but make subtle errors in the second should be supported by examples or a full experimental analysis.
* Lacks details necessary for reproducibility, including dataset construction, pretraining, and instruction tuning specifics.
* Insufficient discussion of related work.
* The checklist should be attached to the appendices.

### Suggestions for Improvement
We recommend that the authors improve the investigation into the reasoning gap by analyzing trends in incorrect responses. Additionally, justifying the claim regarding errors in the second question with examples or a full experimental analysis would strengthen the paper. It is essential to include detailed information on dataset construction, pretraining, and instruction tuning to enhance reproducibility. The authors should also consider discussing related work more thoroughly and attaching the checklist to the appendices.