ID: pBa70rGHlr
Title: Latent Space Translation via Semantic Alignment
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 8, 6, 5, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for translating learned representations between two pre-trained networks using simple transformations, enabling zero-shot stitching of encoders and decoders without additional training. The authors demonstrate the method across various architectures, modalities, and tasks, showing its applicability and effectiveness.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow.
2. The original setting of ad-hoc zero-shot stitching is significant, making the approach relevant for existing pre-trained models.
3. The method is simple yet yields convincing results, supported by extensive evaluations.

Weaknesses:
1. The exact experimental setup leading to Figure 2 is unclear, including the models used and their training details.
2. The non-monotonic trends in Figure 2 raise questions about the experimental design; averaging results across multiple experiments may provide clearer insights.
3. The assumption of linearity in the transformation between latent spaces lacks thorough analysis, particularly in the context of generative models, which may affect the method's efficacy.
4. The paper does not adequately compare the proposed method with other stitching techniques or provide a qualitative analysis against generative models like VAEs.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup, particularly regarding the models and training conditions used for Figure 2. Additionally, addressing the non-monotonic trends by averaging results across multiple experiments could enhance the robustness of the findings. We suggest including a qualitative comparison with generative models and other stitching methods to strengthen the justification for the proposed method. Finally, clarifying the construction of the anchor set and its implications for performance would improve the paper's overall clarity and impact.