ID: 4Un2TD9bNe
Title: Rethinking Memory and Communication Costs for Efficient Data Parallel Training of Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified framework to enhance ZeRO-based partitioning strategies, aiming to optimize the balance between memory and communication costs in training large language models (LLMs). The authors propose the Partial Redundancy Optimizer (PaRO), which includes PaRO Data Parallelism (PaRO-DP) for refining model state partitioning and PaRO Collective Communications (PaRO-CC) for optimizing communication topology. The strategies reportedly improve training speed significantly, achieving up to 266% over state-of-the-art data parallel strategies. However, the paper's novelty is questioned due to similarities with existing methods like MiCS and Alpa.

### Strengths and Weaknesses
Strengths:
- The paper systematically explores ZeRO-based partitioning strategies and expands the option space.
- PaRO-CC is introduced as an efficient collective communication method.
- Extensive experiments with numerous baselines enhance the credibility of the findings.
- The problem is well-formulated and easy to follow, considering various training scenarios.

Weaknesses:
- The title and claims may be overly ambitious, particularly regarding the focus on ZeRO-based strategies without integrating other methods.
- The efficiency claims for LLM training lack sufficient evidence, as the proposed methods are not necessarily coupled with LLMs.
- Missing comparisons with ZeRO-1 and other collective communication strategies limit the completeness of the experiments.
- Figures are complex and difficult to interpret.

### Suggestions for Improvement
We recommend that the authors improve the title to better reflect the scope of the paper, avoiding terms like "Rethinking" that imply broader contributions. Additionally, we suggest providing more evidence for the efficiency of the proposed methods in LLM training, possibly by including comparisons with ZeRO-1 and other relevant strategies. Clarifying the differences between PaRO-CC and existing methods, such as the Hierarchical Communication Strategy in MiCS, would strengthen the paper. Simplifying complex figures and ensuring all experimental configurations are reported would enhance clarity and completeness. Lastly, addressing the integration of PaRO with current 3D parallelism techniques is crucial for demonstrating its applicability.