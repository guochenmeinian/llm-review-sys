ID: Od6CHhPM7I
Title: Red Teaming Deep Neural Networks with Feature Synthesis Tools
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 7, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a systematic evaluation of interpretability tools in identifying humanly interpretable attributes in image classification, specifically focusing on trojan detection. The authors propose a synthetic benchmark involving a trojan identification task where a known trojan is injected into a model's training pipeline. They assess various interpretability tools on their ability to identify features leading to misclassification. The study reveals that robust feature-level adversaries are the most effective tools, while naive methods like saliency maps are inadequate. The authors also highlight limitations in current interpretability tools, particularly in identifying non-local backdoors.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant research question in interpretability: how to objectively quantify the quality of explanations.
- The trojan task serves as a clever proxy for measuring human interpretability.
- The inclusion of a human study to evaluate the utility of interpretability tools is commendable.
- The authors acknowledge the limitations of their work and the need for diverse techniques in interpretability benchmarks.

Weaknesses:
- The trojan tasks may be overly simplistic and not reflective of real-world interpretability challenges.
- The evidence against saliency maps lacks robustness, as the metric used may not accurately capture their effectiveness.
- The writing clarity is poor, making it difficult for readers to grasp the paper's main contributions and findings.
- The motivation for the study could be better articulated, and the introduction needs to reflect a stronger rationale for the research.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing, particularly in explaining the human study task and the overall objectives of the paper. A clearer introduction that emphasizes the practical implications of their work would strengthen the motivation. Additionally, we suggest including visualization results for feature attribution methods and providing more comprehensive data on attack success rates and clean accuracy of backdoored models. Finally, addressing the concerns regarding the ineffectiveness of saliency maps with a more appropriate metric could enhance the robustness of their claims.