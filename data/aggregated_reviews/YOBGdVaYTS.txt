ID: YOBGdVaYTS
Title: Initialization is Critical to Whether Transformers Fit Composite Functions by Reasoning or Memorizing
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates whether transformers learn primitive functions or merely memorize correlations when trained on a manually-designed toy compositional task involving composite numerical functions. The authors find that the learned mechanism exhibits a phase transition correlated with the parameter initialization scale: smaller scales lead to inferential solutions, while larger scales result in memorized solutions. The authors also propose that training accuracy alone cannot determine the type of solution learned by models, advocating for the use of testing accuracy or validation accuracy with out-of-distribution (o.o.d.) data. They illustrate that models with small initializations learn inferential solutions faster for low-complexity data, while larger initializations lead to symmetrical solutions across varying complexities. The paper conducts empirical analyses on information transmission, vector representation similarity, and model complexity, revealing a positive correlation between training data complexity and fitting difficulty. Additionally, the authors clarify their dataset construction process for the SCAN dataset, indicating that they derived smaller datasets from a fixed official dataset to analyze performance differences.

### Strengths and Weaknesses
Strengths:
- The investigation into whether transformer solutions are inferential or memorized is significant and relevant to the NeurIPS community.
- The paper presents a clear logical structure and conducts analyses from multiple perspectives, leading to a converging interpretation of the phase transition based on initialization scale.
- The additional experiments enhance the robustness of the study, particularly in demonstrating compositional generalization.
- The authors effectively address reviewer concerns, showing a willingness to clarify and improve their work.

Weaknesses:
- The experimental setting may be overly simplified, raising questions about the generalizability of conclusions to more complex tasks, as only empirical evidence is provided without theoretical guarantees.
- The relationship between this work and existing literature on "grokking" is not adequately discussed, which diminishes the novelty of the findings.
- There are ongoing concerns regarding the clarity and presentation of the paper, which may hinder reader comprehension.
- The rationale for certain methodological choices, such as the decision not to display inferential accuracy, could be better articulated.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their conclusions by conducting experiments on more complex compositional tasks, such as SCAN or COGS. Additionally, we suggest including discussions on how the findings could guide the training of real pre-trained language models, particularly in the context of initializing LoRA or Prompt-Tuning parameters. It would also be beneficial to clarify the relationship between this work and previous studies on "grokking" in the Related Work section. Furthermore, we advise the authors to enhance clarity by labeling figures more effectively and ensuring that abbreviations are defined upon first use. We recommend improving the clarity of the manuscript by revising the presentation of their findings, particularly in relation to the rationale behind their methodological choices. Consider moving the content of Section 6 to the appendix to streamline the main text and enhance reader understanding. Lastly, ensure that the new experimental results are clearly integrated into the revised version of the paper, as requested by reviewers.