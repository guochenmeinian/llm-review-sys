ID: XE5CNQwLZF
Title: Graph Pretraining and Prompt Learning for Recommendation
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GraphPL, a framework that combines graph pre-training and prompt learning for recommendation systems, addressing the challenge of evolving user preferences through a temporal prompt mechanism and a graph-structural prompt learning mechanism. The framework aims to enhance the adaptability of GNNs to dynamic user-item interactions and has been validated through extensive experiments, including a large-scale industrial deployment.

### Strengths and Weaknesses
Strengths:
- The integration of dynamic graph pre-training and prompt learning is a significant advancement in adapting GNNs to evolving user preferences.
- The framework effectively encodes temporal information and transfers pre-trained knowledge, enhancing adaptability to user behavior changes.
- Extensive experimentation and real-world deployment validate the framework's effectiveness, robustness, and efficiency.
- The lightweight and parameter-efficient nature of GraphPL facilitates seamless integration into existing GNN recommenders.

Weaknesses:
- The computational complexity introduced by the novel mechanisms is not adequately discussed, particularly regarding the trade-off between time cost and performance improvement.
- The paper lacks a comprehensive analysis of performance variations across different backbone graph models.
- There is insufficient discussion on the length of contextual information in the prompt learning paradigm.
- The relevant studies and comparisons with existing GNN-based methods are not comprehensive, missing important works in the field.

### Suggestions for Improvement
We recommend that the authors improve the title to reflect the dynamic scenario setting of the paper. Additionally, provide a detailed analysis of performance variations across different backbone graph models. It would be beneficial to include a notation table for key symbols and a pseudocode section outlining the algorithmic steps of the dynamic graph encoder. Furthermore, clarify the implementation of graph concatenation and sampling in Equation (8) and discuss the rationale behind the random and non-learnable gating weights in Equation (9). Lastly, ensure a more comprehensive review of relevant studies in the related works section to strengthen the paper's foundation.