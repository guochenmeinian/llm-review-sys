ID: ZfXRAqbBKX
Title: IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying and Reweighting Context-Aware Neurons
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 8, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework, IRCAN, designed to address knowledge conflicts in Large Language Models (LLMs) by enhancing the sensitivity of models to contextual knowledge. The authors propose a method that identifies and reweights neurons crucial for processing contextual cues, validated through experiments demonstrating significant improvements in completion and multi-choice tasks. However, the evaluation is limited to short contexts, raising concerns about scalability to longer contexts.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow.
2. The proposed method is novel and effectively addresses knowledge conflict issues.
3. Comprehensive experimental design shows significant improvements in task performance.
4. The framework is plug-and-play, requiring no additional training.

Weaknesses:
1. Evaluation is limited to short contexts, questioning scalability.
2. The assumption that contexts are often contradictory to parametric knowledge may not hold true.
3. Lack of comparisons with other steering methods and no exploration of the method's performance on RAG tasks.
4. The proposed method may increase inference time, and unexpected performance results between models require explanation.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of the IRCAN framework by testing its performance on additional knowledge-related datasets. Additionally, the authors should explore the scalability of their method to longer contexts and clarify the assumptions regarding context reliability. A comparison with fine-tuning as a baseline would provide valuable insights. Furthermore, we suggest including discussions on time complexity and addressing the unexpected performance discrepancies observed between different model versions. Lastly, a limitation section should be explicitly included to discuss the potential drawbacks and application scenarios of the proposed method.