ID: chxX4KuDDs
Title: SCOOT: SLO-Oriented Performance Tuning for LLM Inference Engines
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SCOOT, an automatic performance tuning system utilizing Bayesian optimization (BO) to optimize service level objectives (SLOs) in LLM inference engines. It addresses various optimization objectives and employs random forests to learn hidden constraints, enhancing tuning efficiency through parallel suggestions. The results indicate significant improvements in service metrics, particularly under heavy workloads, making it relevant to the growing demand for efficient LLM services.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written, logically coherent, and easy to understand.
2. It addresses a critical issue in LLM deployment, offering valuable insights for real-world applications.
3. The methodology is detailed, aiding reproducibility, and the system has been deployed in a production environment, demonstrating its feasibility.
4. Experimental results show substantial improvements over default configurations and other baseline methods.

Weaknesses:
1. The motivation lacks emphasis on unique gaps in existing tuning methods and the specific importance of the issues SCOOT addresses.
2. The generalizability of the approach is limited, focusing primarily on specific LLM engines and hardware configurations.
3. The paper does not sufficiently analyze the impact of individual parameters on service metrics or clarify which parameters were adjusted during tuning.
4. There is insufficient experimental evidence supporting the claim that SCOOT is universally applicable to various LLM inference engines.
5. The performance improvements over Vanilla BO are limited, and the paper does not adequately highlight SCOOT's advantages.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the rationale behind the selected configuration parameters and their interrelationships, as this is critical for the tuning process. Additionally, the authors should provide a more comprehensive analysis of parameter sensitivity and stability, particularly for key parameters like `max-num-seqs` and `max-num-batched-tokens`. It would be beneficial to include comparisons with state-of-the-art adaptive tuning methods to better contextualize SCOOT's performance. Furthermore, the authors should clarify the implications of using default parameters in real-world applications and consider discussing the challenges faced during the production deployment of SCOOT. Lastly, we suggest addressing the limitations of the random forest approach in scaling to complex parameter spaces.