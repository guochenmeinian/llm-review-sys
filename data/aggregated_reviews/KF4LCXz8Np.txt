ID: KF4LCXz8Np
Title: On the Generalization Error of Stochastic Mirror Descent for Quadratically-Bounded Losses: an Improved Analysis
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 7, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper analyzes the generalization bound for stochastic mirror descent (SMD) applied to linear models and a class of quadratically-bounded losses, considering both IID and Markov chain sampled data. The authors propose improvements over previous work by Telgarsky (2022), particularly in non-realizable settings, through a clever application of concentration inequalities. The results show enhanced accuracy by a logarithmic factor in realizable cases and a polynomial factor in non-realizable scenarios.

### Strengths and Weaknesses
Strengths:
1. The paper is well-organized and presents a clear improvement over prior work.
2. The approach, which analyzes the moment generating function of a martingale difference sequence, avoids previous coupling techniques and may have future applications.

Weaknesses:
1. The improvement in the realizable case is modest, limited to logarithmic factors, which may not significantly alter the understanding of SMD's generalization behavior.
2. The contribution over Telgarsky (2022) may not be sufficient to justify a new paper, as some results are similar, and the problem setting lacks novelty.

### Suggestions for Improvement
We recommend that the authors clarify how their contributions significantly differ from Telgarsky (2022) and elaborate on the practical implications of their improved bounds. Additionally, we suggest incorporating references to optimization error analyses of SMD, such as Dragomir et al. (2021) and Loizou et al. (2022), to provide context for readers familiar with those works. Lastly, addressing the measurability concerns raised in the proof of Lemma 4 would strengthen the paper's technical rigor.