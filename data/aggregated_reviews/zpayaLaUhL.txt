ID: zpayaLaUhL
Title: Absolute Position Embedding Learns Sinusoid-like Waves for Attention Based on Relative Position
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of attention mechanisms in pre-trained language models, focusing on the role of position embeddings. The authors conduct experiments to explore the relationship between attention and position embeddings, revealing that learnable absolute position embeddings exhibit sinusoid-like patterns. They find that attention heads can extract periodic components from these embeddings, and the self-attention mechanism adjusts the phase of these components, influencing attention direction. Additionally, the paper discusses the emergence of relative positional dependence in attention due to various factors, including word embeddings.

### Strengths and Weaknesses
Strengths:  
- The paper provides a thorough analysis of attention mechanisms, particularly regarding position embeddings.  
- Experimental results are clearly presented, supported by solid mathematical tools.  
- The theoretical interpretations and visualization techniques effectively convey findings.  

Weaknesses:  
- Findings may not be generalizable beyond the models and training objectives studied.  
- The relationships between sections are unclear, particularly in the introduction, and sections discussing nearby and adjacent tokens could be combined.  
- Experiments are limited to RoBERTa, with a small sample size, raising concerns about generalizability.  
- The contribution to the community and practical implications are not adequately articulated.  
- Some equations lack clarity, and there are discrepancies with existing studies that are not discussed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of relationships between different sections in the introduction and consider combining Sections 4 and 5. Additionally, the authors should enhance the prescriptiveness of equations, particularly in lines 244 and 329. To strengthen the paper, we suggest expanding the experiments to include other models and a larger dataset, such as GPT-2, and addressing the practical contributions of their findings. Furthermore, the authors should discuss discrepancies with existing studies and clarify the role of word embeddings in relative position-dependent attention patterns. Lastly, we encourage the authors to include missing references and improve the clarity of specific sections, such as the caption of Figure 1 and lines 223-227.