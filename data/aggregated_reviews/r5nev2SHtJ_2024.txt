ID: r5nev2SHtJ
Title: From Causal to Concept-Based Representation Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 6, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for recovering human-interpretable concepts from observations, proposing a concept-based representation learning method that relaxes causal notions in favor of a geometric interpretation of concepts. The authors establish a theoretical foundation for the identifiability of these concepts and validate their approach through experiments on synthetic data, multimodal CLIP models, and large language models (LLMs). Additionally, the paper emphasizes the limitations of existing methods that require extensive interventions, proposing that their approach can identify concepts with fewer interventions, which is particularly relevant given the challenges of applying current methods to real-world datasets. The authors include experiments with larger latent dimensions and discuss the implications of their findings in relation to existing literature on concept representation and learning.

### Strengths and Weaknesses
Strengths:  
- The authors introduce a novel perspective on concept-based learning, providing identifiability guarantees and addressing the limitations of causal representation learning (CRL).  
- The theoretical contributions are significant, demonstrating the advantages of concepts over interventions in representation learning and clearly distinguishing their work from existing methods.  
- The empirical experiments, including those with larger latent dimensions, add value to the theoretical claims and are well-supported by a thorough literature review that clarifies the paper's positioning and contributions.

Weaknesses:  
- The approach sacrifices causal semantics, which is problematic in contexts requiring deep causal understanding, such as root cause analysis.  
- The experimental section lacks coherence with the proposed theory, particularly regarding the relevance of experiments on CLIP and LLMs, and lacks depth in demonstrating the method's application to semi-synthetic datasets.  
- Some empirical findings are presented without sufficient theoretical backing, leading to questions about their relevance, and the connection between the proposed theory and practical applications remains unclear, particularly regarding the concept of heterogeneity in real-world data.  
- The assumptions made in the paper could limit the universal applicability of the theory, and the discussion around them is insufficient.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental section by ensuring that the experiments directly support the proposed theory. Specifically, including applications to semi-synthetic datasets, such as MNIST, would clarify the practical utility of their method. Additionally, we suggest that the authors provide a more rigorous theoretical explanation for the empirical findings related to CLIP and LLM experiments, and clarify the concept of heterogeneity in real-world data and how it can be verified. We also encourage the authors to elaborate on the assumptions made in the paper, discussing their implications and potential extensions to non-invertible functions. Finally, we recommend that the authors include a more detailed description of the contrastive learning method in the main text and make a minimal effort to contrast their approach with basic CRL methods to showcase potential advantages in fewer required domains.