ID: SqL94fLSM7
Title: Safety-Aware Fine-Tuning of Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 5, 5
Original Confidences: 3, 4, 4

Aggregated Review:
### Key Points
This paper presents a safety-aware fine-tuning (SAFT) method designed to detect and remove harmful data from training datasets for large language models (LLMs). The authors propose a scoring function that utilizes subspace information from harmful and benign samples, leading to a significant reduction in harmfulness across various LLMs. Experimental results indicate that models trained with this framework generate lower harmfulness scores without significantly compromising other metrics, such as helpfulness.

### Strengths and Weaknesses
Strengths:
- The proposed method effectively reduces harmfulness in LLMs, as demonstrated by experimental results across different dataset qualities.
- The paper includes detailed analyses and ablation studies, enhancing understanding of the method's efficacy.
- The writing is clear and accessible, making the paper easy to follow.

Weaknesses:
- The discussion on the disadvantages of manual filtering is insufficient, raising doubts about the method's motivation.
- There is a lack of visual aids to help readers comprehend the method.
- The intuition behind the spatial separation of harmful and benign samples is unclear, particularly regarding the interpretation of embeddings.
- A deeper analysis of results in Table 2 is missing, particularly concerning the SAFT sampling strategy and its comparison to other methods.
- The scalability of the approach to general tasks beyond harmful dataset classification is not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the disadvantages of manual filtering to clarify the motivation behind the proposed method. Additionally, including visual representations would enhance reader comprehension of the method. The authors should provide a clearer explanation of the spatial separation of harmful and benign samples and consider a more thorough analysis of the results presented in Table 2, particularly regarding the SAFT sampling strategy. Finally, we suggest that the authors address the scalability of their approach to general tasks outside the domain of harmful dataset classification, providing more analysis beyond the mention in the Broader Impact section.