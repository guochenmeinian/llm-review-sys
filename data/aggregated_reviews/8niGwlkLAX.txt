ID: 8niGwlkLAX
Title: Sparse Deep Learning for Time Series Data: Theory and Applications
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 7, 4, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 1, 2, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an expansion of sparse deep learning theory specifically for time series data, utilizing recurrent neural networks (RNNs), particularly long short-term memory (LSTM) networks. The authors propose a sparsity method aimed at improving uncertainty quantification and achieving state-of-the-art RNN compression. They provide a thorough theoretical foundation and demonstrate that their approach achieves competitive results compared to state-of-the-art methodologies, including conformal prediction. The empirical evaluation across three relevant tasks and four real-world datasets supports the theoretical claims of the method's validity and applicability. However, the novelty of the theoretical contributions is questioned, as they closely resemble prior works, and the evaluation of model compression lacks breadth.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and organized, with a clear narrative that includes related work, theoretical grounding, and empirical results.
- The authors exhibit a strong understanding of sparse deep learning theory, and the theoretical framework is well-developed and convincing.
- The integration of DNN sparsification with uncertainty quantification for time series data is a compelling direction.
- The empirical results indicate that sparse deep learning outperforms existing methods in uncertainty quantification and autoregressive order identification for time series data.

Weaknesses:
- The complexity of notation and numerous variables may hinder comprehension, particularly for newcomers to sparse deep learning theory.
- The rationale for selecting conformal prediction as the primary comparison is unclear, lacking sufficient context and justification.
- The literature review on conformal prediction appears limited, with insufficient references to relevant works.
- The theoretical results for RNNs and time series are nearly identical to previous works, raising doubts about their novelty.
- The prior annealing algorithm for model sparsification is identical to existing methods, and the contribution of constructing prediction intervals is considered minor.
- The evaluation of model compression lacks breadth, relying on a single dataset and baseline, which undermines the claim of state-of-the-art performance.
- The practical applicability of the proposed theory in real-world scenarios may be challenging compared to conformal prediction, which is noted for its straightforward implementation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of complex notations and provide more context for the theoretical concepts to enhance reader understanding. Additionally, the authors should justify the choice of conformal prediction as a comparison field and expand the literature review to include more relevant references. To enhance the novelty of their theoretical contributions, we suggest clearly distinguishing their results from prior works. We also recommend reformulating the term "compression" to "weight sparsification" or expanding the comparison to include low-rank approximation, quantization, and knowledge distillation methods. To strengthen the evaluation, please include results from additional datasets and compare against more recent methods, such as GraNet and other sparsification techniques. We advise incorporating uncertainty estimation methods like DropConnect and deep ensembles in the related work section. Furthermore, clarity could be enhanced by simplifying complex equations and notation, particularly in Theorem 3.8, and by considering the relocation or enhancement of Figure 1 to provide more informative content. Lastly, we recommend discussing the practical limitations of the proposed method, particularly regarding hyperparameter sensitivity and training time.