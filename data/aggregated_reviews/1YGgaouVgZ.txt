ID: 1YGgaouVgZ
Title: Wide Two-Layer Networks can Learn from Adversarial Perturbations
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 5, 5, 5, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical investigation of perturbation learning from a future hypothesis perspective, where classifiers are trained on adversarial examples with incorrect labels. The authors demonstrate that adversarial perturbations can be expressed as linear combinations of training samples, thus retaining information from the clean dataset. They also establish conditions under which predictions from classifiers trained on adversarial examples align with those trained on clean data. Experiments on a synthetic Gaussian dataset validate their findings regarding input and hidden dimensions.

### Strengths and Weaknesses
Strengths:  
* The paper enhances understanding of perturbation learning and the feature hypothesis, which are crucial in adversarial learning.  
* It offers a more general theory of perturbation learning compared to previous work, imposing fewer constraints on training data distribution and time.  

Weaknesses:  
* There is uncertainty regarding the suitability of the kernel regime for studying the feature hypothesis in adversarial training and explaining phenomena like adversarial example transferability and the robustness-accuracy trade-off.  
* The theory primarily focuses on perturbation learning, limiting its applicability to other phenomena.  
* The conditions in the main theorems lack interpretability and rigor, particularly the agreement condition, which raises questions about its validity in finite-width settings.  

### Suggestions for Improvement
We recommend that the authors improve the exploration of the feature subsets utilized by trained models to enhance the framework's applicability to various phenomena in adversarial training. Additionally, we suggest clarifying the interpretability of the conditions in the main theorems, especially regarding the agreement condition and its implications in finite-width scenarios. A more nuanced discussion on the limitations of their results, particularly concerning the kernel regime and its implications for feature learning, would also strengthen the paper.