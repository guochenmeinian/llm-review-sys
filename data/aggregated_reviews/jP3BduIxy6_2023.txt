ID: jP3BduIxy6
Title: Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for offline reinforcement learning (RL) and multi-agent reinforcement learning (MARL) using the game Honor of Kings (HoK). The authors introduce the Hokoff dataset, which aims to enhance existing offline RL methods by providing a diverse set of tasks that promote generalization, task complexity, and multi-task learning. The benchmark includes typical baselines for comparison and emphasizes HoK's complexity and suitability for algorithm development. The authors address various aspects such as generalization, task complexity, and the necessity of offline RL, while also discussing the limitations of existing benchmarks. The datasets are designed to facilitate research in both single-agent and multi-agent scenarios, providing a rich environment for testing RL algorithms.

### Strengths and Weaknesses
**Strengths:**
1. The introduction of a benchmark based on Honor of Kings is a critical contribution that can accelerate research in offline RL and MARL.
2. The comprehensive data, code, and documentation provided facilitate easy access for researchers, promoting further exploration and application.
3. The paper effectively highlights the unique challenges posed by HoK, such as its high-dimensional state-action space and partial observability.
4. The authors have made significant improvements in clarity, including enhancing figures and reorganizing sections for better flow.
5. The proposed datasets are designed to encourage robustness and generalization in offline RL algorithms.

**Weaknesses:**
1. The paper lacks a clear explanation of the connections and differences between single-agent and multi-agent tasks, which may confuse readers.
2. The benchmark is limited to a single game, which raises concerns about the diversity and generalizability of the findings for researchers seeking to test algorithms across multiple games.
3. Clarity issues exist regarding the definition of "tasks" and the availability of certain sub-tasks, which could mislead readers.
4. Some sections, such as the descriptions of generalization and the framework's effectiveness at sampling diverse datasets, were initially unclear and required further elaboration.

### Suggestions for Improvement
1. We recommend that the authors improve the discussion of the relationships and differences between single-agent offline reinforcement learning and multi-agent reinforcement learning, particularly in paragraphs 2 and 3.
2. The authors should provide further explanation of why task complexity, generalization, and multi-task learning are crucial in offline reinforcement learning to enhance reader understanding of the benchmark's significance.
3. We suggest that the authors clarify the descriptions of generalization and the reasons behind the framework's effectiveness in sampling diverse datasets.
4. The authors should clarify the terminology used in the paper, particularly the term "tasks," and ensure consistency throughout the document.
5. We recommend that the authors include a maintenance plan for the benchmark to ensure its continued relevance and usefulness over time.
6. Lastly, we encourage the authors to consider expanding their benchmark to include multiple games to enhance the diversity and applicability of their findings.