ID: MlADRQI0Wf
Title: Implicit Regularization of Decentralized Gradient Descent for Sparse Regression
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the convergence of the decentralized gradient descent (DGD) algorithm under the RIP condition with small initialization scale. The authors propose a truncated version, T-DGD, which offers a cheaper cost while maintaining comparable performance in specific scenarios. The manuscript also explores decentralized optimization for training linear sparse models, leveraging implicit regularization from the gradient descent process. The authors analyze DGD applied to a non-convex least squares formulation and validate the effectiveness of both DGD and T-DGD through numerical results.

### Strengths and Weaknesses
Strengths:  
- The theory is sound and nontrivial, proving convergence for DGD where previously only centralized GD results were available.  
- The manuscript is well-written and provides a novel perspective on decentralized optimization, with clear contributions reflected in Theorem 1.  
- The proposed T-DGD is an interesting application of theoretical observations, enhancing the theoretical claims.

Weaknesses:  
- The paper is somewhat technical, making it less accessible; higher-level ideas would improve understanding.  
- The approach's promotion of sparsity is unclear, lacking discussion and numerical exploration compared to L1 regularization methods.  
- Simulations are limited to diagonal linear networks, restricting generalizability, and the motivation for the chosen setting is not clearly articulated.  
- The contributions to the understanding of machine learning are not sufficiently distinct, as many concepts have been previously explored.

### Suggestions for Improvement
We recommend that the authors improve the accessibility of the paper by adding higher-level explanations alongside technical details. Additionally, we suggest including a comparison of the proposed methods with existing decentralized sparse solvers to clarify their contributions. It would also be beneficial to discuss how the approach promotes sparsity and to conduct numerical experiments in more diverse architectures to enhance the generalizability of the findings. Lastly, clarifying the motivation for studying the specific DGD setting would strengthen the paper's impact.