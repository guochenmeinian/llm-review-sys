ID: UVjuYBSbCN
Title: Toward a Well-Calibrated Discrimination via Survival Outcome-Aware Contrastive Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 6, 6, 4, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to deep learning for survival analysis, focusing on improving both discriminative and calibration performance through an auxiliary contrastive loss. The authors adapt the state-of-the-art contrastive learning method SCARF for tabular data by weighting negative pairs based on their time-to-event differences. Additionally, the paper includes a comparative evaluation of model calibration using alternative metrics, specifically Distributional Divergence for Calibration (DDC) and D-Calibration (D-CAL). The authors report significant calibration gains for ConSurv over benchmark models, particularly ranking-based deep survival models, as evidenced by p-values included in Table I. The methodology encompasses a detailed problem formulation, and results from multiple experiments demonstrate that the proposed model outperforms existing state-of-the-art methods across various real-world datasets.

### Strengths and Weaknesses
Strengths:
- The originality and clarity of the proposed adaptation to SCARF, emphasizing the importance of time-to-event differences in negative pairs.
- A solid mathematical foundation for the weighted contrastive loss and comprehensive experiments that validate the model's performance against significant baselines.
- The use of alternative calibration metrics (DDC and D-CAL) provides a comprehensive evaluation of model calibration.
- The reported significant calibration improvements for ConSurv across various datasets enhance the paper's contribution to the field.
- The paper is generally well-written, with clear descriptions of the methodology and results.

Weaknesses:
- The organization and presentation could be improved, particularly in the introduction where survival analysis terminology is introduced without adequate explanation.
- The clarity of experimental results is sometimes lacking, particularly in Table 1, where the meaning of standard deviations and bold/underlined values is not clearly defined.
- The statistical significance of performance improvements is not always evident, raising questions about the robustness of the reported results.
- The comparison with DRAFT is deemed inadequate; a more appropriate comparison would be with DATE, which is established as superior.
- The connection between contrastive learning and model calibration lacks theoretical backing, relying solely on experimental results.
- The paper contains sloppy mathematical notations and insufficient detail regarding modeling choices.

### Suggestions for Improvement
We recommend that the authors improve the organization of the introduction by explaining survival analysis terminology, such as censoring, at first mention. Additionally, we suggest enhancing the clarity of Table 1 by providing more detailed captions that explain the standard deviations and the significance of bold/underlined values. To address concerns about statistical significance, we encourage the authors to provide clearer explanations of how statistical tests were conducted and to ensure that normalization procedures do not lead to data leakage. Furthermore, we recommend improving the comparison by including DATE instead of DRAFT, as it offers a more equitable basis for evaluation. It is also essential to provide theoretical guarantees regarding the connection between contrastive learning and model calibration, as the current reliance on experimental results is insufficient. Lastly, we suggest addressing the mathematical notations and providing complete details on modeling choices to enhance clarity and rigor.