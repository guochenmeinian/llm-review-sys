ID: T0glCBw28a
Title: The ALCHEmist: Automated Labeling 500x CHEaper than LLM Data Annotators
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a labeling workflow called Alchemist that utilizes large language models (LLMs) to generate programs for labeling data, rather than relying on a teacher model for each data point. The authors demonstrate that this approach is effective and cost-efficient, employing an aggregation function to derive pseudo labels from multiple generated programs. They conduct ablation studies on multi-modality, supplementary information during program generation, and program diversity, showcasing the method's utility across various datasets.

### Strengths and Weaknesses
Strengths:
1. The use of LLMs for knowledge distillation in labeling datasets is timely and addresses the high costs associated with traditional methods.
2. Alchemist's innovative approach to generating weak labeling functions is a novel contribution, supported by the weak supervision literature.
3. The paper is well-organized, with a clear presentation of experiments that address key questions.
4. The flexibility and reusability of Alchemist's labeling programs enhance its practical implementation across diverse tasks.

Weaknesses:
1. The results indicate that performance consistency is not guaranteed across datasets, raising concerns about the effectiveness of Alchemist on new datasets without comparative analysis.
2. Generated programs often handle fixed tasks, limiting flexibility for more complex labeling scenarios that may still require specialist models.
3. The paper lacks a thorough exploration of the stability of generated programs, particularly regarding the impact of variables like temperature on quality and consistency.
4. The literature review could be improved by citing relevant works that discuss data annotation methodologies.

### Suggestions for Improvement
We recommend that the authors improve the clarity of performance expectations for new datasets by providing guidelines on assessing the applicability of Alchemist. Additionally, we suggest conducting comparisons with related works such as ScriptoriumWS and DataSculpt to highlight the novelty of their approach. The authors should also enhance their literature review by including significant prior works on data annotation. Furthermore, we encourage the authors to explore the stability of generated programs and consider implementing a data-driven feedback loop to ensure accuracy and compile success. Lastly, we advise softening claims regarding scaling to richer modalities until more challenging tasks are evaluated.