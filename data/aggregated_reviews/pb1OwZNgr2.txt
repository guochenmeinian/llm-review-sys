ID: pb1OwZNgr2
Title: Learning Generalizable Agents via Saliency-guided Features Decorrelation
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 7, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Stochastic Gradient Feature Decorrelation (SGFD), a novel approach aimed at enhancing the generalization capabilities of reinforcement learning (RL) agents across various environmental variations, including both task-relevant and task-irrelevant features. The authors propose a decorrelation of features through a resampling technique that minimizes the Frobenius norm of the cross-covariance matrix, utilizing Random Fourier features. The methodology focuses on effectively decorrelating the most variable features, guided by saliencies from an environment classification model. SGFD demonstrates significant improvements in generalization performance, particularly with task-relevant features.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and coherent, effectively explaining complex concepts.
- The authors provide a strong theoretical foundation, with well-motivated intuitions supporting their methodology.
- The saliency-guided optimization is an innovative approach, supported by a robust ablation study.
- SGFD shows noticeable enhancements in generalization performance, especially regarding task-relevant features.

Weaknesses:
- The methodology requires multiple environments for training the environment classifier, introducing manual supervision that may not be ideal.
- The full algorithm is challenging to comprehend without referring to Appendix A, indicating a need for more clarity in the main text.
- The introduction of the general objective—reweighting the batch sampled from the buffer—could be presented earlier for better reader understanding.
- The authors' claims of improvement over Soft Actor-Critic (SAC) are undermined by the use of similar updates as those in the Adaptive Meta-learner of Behavioral Similarities (AMBS), which should be explicitly acknowledged to avoid misunderstandings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the main text by providing a more straightforward explanation of the full algorithm without relying heavily on Appendix A. Additionally, introducing the general objective of reweighting the batch earlier in the paper would enhance reader comprehension. It would also be beneficial for the authors to explicitly acknowledge the similarities between their method and AMBS in the experimental section to ensure clarity and fairness. Furthermore, we suggest conducting additional experiments that analyze the mixture of changes in task-irrelevant and task-relevant features to strengthen their claims. Lastly, providing visualizations of training and test environments would help elucidate the practical applicability of the methodology.