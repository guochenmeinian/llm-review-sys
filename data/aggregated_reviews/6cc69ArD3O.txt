ID: 6cc69ArD3O
Title: Globally injective and bijective neural operators
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 4, 5, 8, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the injectivity and surjectivity of neural operators (NOs) in infinite-dimensional settings, extending known results from finite-dimensional domains. The authors provide conditions for the injectivity of infinite-dimensional ReLU networks, prove their universality, and extend findings to networks with non-linear kernels. Additionally, the paper offers a foundational theoretical framework focused on non-linear operators and mappings, primarily applied to solving inverse problems and differential equations. A worked example involving a non-linear integral operator is included, discussing the implications of injectivity and surjectivity in practical applications, particularly in the context of NOs and Fourier Neural Operators (FNOs). The authors argue that their work provides rigorous insights relevant for future applied research, especially in the context of NeurIPS.

### Strengths and Weaknesses
Strengths:  
- The paper offers significant theoretical results, addressing natural questions regarding injectivity and bijectivity in neural operators.  
- It provides a solid theoretical foundation and deep mathematical insights into non-linear operators and their applications, particularly in inverse problems.  
- The authors effectively highlight the importance of injectivity and surjectivity, and the writing is clear with well-structured proofs.  
- Practical implementation results in finite-rank cases enhance the theoretical findings.

Weaknesses:  
- The relevance of the work to the machine learning community is unclear, as the authors do not adequately motivate the importance of bijective neural operators.  
- The practical utility of the theoretical results remains ambiguous, particularly concerning the applicability of Proposition 5 to concrete architectures like FNOs.  
- The paper suffers from excessive abstract notation and lacks clarity in several definitions and concepts, making it difficult for readers to grasp the main contributions.  
- There is a noticeable absence of practical examples beyond the first, limiting the applicability of the theoretical results.  
- The weight matrices in FNOs are not necessarily square, raising doubts about the authors' claims regarding their injectivity and implications for training.  
- The authors do not sufficiently differentiate their work from existing literature, particularly Puthawala et al. (2022), raising questions about novelty.

### Suggestions for Improvement
We recommend that the authors improve the motivation for their work by clearly articulating why understanding bijective neural operators is significant for the machine learning community. Additionally, the authors should enhance the clarity of their presentation by explicitly linking their theoretical results to practical applications, particularly in inverse problems. It would be beneficial to rewrite the CRV to emphasize these connections and include worked-out numerical examples demonstrating how initial and trained FNOs satisfy the proposed conditions. We suggest reorganizing the paper to simplify abstract notation and provide intuitive explanations for key concepts, such as the directed spanning set in Definition 1. Furthermore, addressing the implications of finite-dimensional representations and aliasing errors on their findings, as highlighted in Bartolucci et al. (Arxiv:2305.19913v1), would strengthen the paper's contribution. Finally, a thorough comparison with existing literature, especially regarding the differences from Puthawala et al. (2022), would enhance the paper's novelty and relevance.