ID: UINHuKeWUa
Title: Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 3, 6, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 2, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper extends FlashAttention to support structured sparse attention, specifically QK-sparse and Hash-sparse attention, enabling further acceleration during training and evaluation. The authors propose a method that modifies FlashAttention to accommodate irregular block sparsity through dynamic schemes, achieving significant runtime gains and improved performance for long sequences.

### Strengths and Weaknesses
Strengths:
- The proposed method accelerates QK-sparse and Hash-sparse attention, potentially enhancing the efficiency of large language model training.
- Clear descriptions of the released kernels and comprehensive validation experiments are provided, demonstrating significant speed improvements over naive implementations.

Weaknesses:
- Experiments primarily compare against FlashAttention, a dense baseline, while sparse methods are naive implementations lacking performance optimization, limiting the demonstration of FlashAttention's benefits.
- The application of the proposed method is restricted to QK-sparse and Hash-sparse attention, which may not be broadly applicable.
- The novelty of the work is low, as it builds upon existing methods without substantial innovation.

### Suggestions for Improvement
We recommend that the authors improve the experimental comparisons by including efficient sparse acceleration libraries to better showcase the advantages of their method. Additionally, providing more details on the SCFA kernel design and addressing the reproducibility of the kernel would enhance clarity. We also suggest adding theoretical speedup metrics for the sparse attention methods and exploring the impact of block sizes on GPU occupancy and utilization. Lastly, conducting experiments with larger models and different tasks, such as text classification, would strengthen the paper's contributions.