ID: 9kFQEJSyCM
Title: RangePerception: Taming LiDAR Range View for Efficient and Accurate 3D Object Detection
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RangePerception, a range-view-based 3D object detection framework that addresses critical challenges such as spatial misalignment and vision corruption in range-view representation. The authors propose two novel algorithms: Range Aware Kernel (RAK) and Vision Restoration Module (VRM). RangePerception achieves superior averaged AP compared to previous state-of-the-art RV-based methods and demonstrates competitive performance against BEV-based methods like CenterPoint, while also being faster. The authors intend to release their code based on the OpenPCDet codebase, which is a significant contribution to the field.

### Strengths and Weaknesses
Strengths:
- The proposed RAK and VRM modules effectively tackle the issues of spatial misalignment and vision corruption, enhancing the performance of range-view-based detection.
- The paper provides a thorough analysis of existing challenges and presents well-grounded experimental validation for the proposed modules.
- RangePerception achieves state-of-the-art performance among range-view-based methods and shows promising results compared to BEV-based methods.

Weaknesses:
- The technical novelty is somewhat limited, as similar concepts have been previously explored in works like RangeDet and PolarNet.
- The experimental section lacks depth, with only two tables provided. More extensive ablation studies are needed, particularly for RAK's hyper-parameters and comparisons with alternative designs.
- The paper does not include results from other datasets, such as KITTI and nuScenes, which limits the generalizability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including additional ablation studies to provide deeper insights into the proposed modules, particularly focusing on various hyper-parameters for RAK. Furthermore, we suggest conducting experiments on other mainstream datasets like KITTI and nuScenes to validate the framework's performance across different scenarios. Additionally, clarifying the distinctions between their methods and existing approaches, such as Ring CNN in PolarNet and components in RangeDet, will enhance the transparency of their contributions.