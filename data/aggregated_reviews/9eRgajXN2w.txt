ID: 9eRgajXN2w
Title: BAT: Benchmark for Auto-bidding Task
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Benchmark for Auto-bidding Task (BAT), a comprehensive dataset designed to evaluate auto-bidding algorithms in online advertising. The dataset includes over 10 million records from first-price and VCG auctions, collected from more than 9,000 advertisers. It provides essential metrics such as auction outcomes and click-through rates, facilitating robust statistical analysis and machine learning model training. The authors also evaluate several baseline algorithms, including adaptive linear models and traffic-aware PID, on various optimization tasks.

### Strengths and Weaknesses
Strengths:  
- The dataset is extensive and mimics real-world scenarios, which is beneficial for the research community.  
- It encourages reproducibility and supports empirical evaluation in the rapidly growing field of automated bidding.  
- The inclusion of diverse auction types allows for the study of different bidding strategies.

Weaknesses:  
- The dataset's accessibility is unclear, as it has not been released, limiting the ability to assess its value fully.  
- The paper primarily emphasizes the dataset, making it more suitable for a dataset track rather than a research track.  
- The proposed algorithms do not explore the potential of reinforcement learning in real-time bidding, which is a missed opportunity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset's availability, specifying whether it will be publicly accessible in the future. Additionally, we suggest that the authors address grammatical issues and typos identified in the review, such as in Lines 295, 311, and 713. To enhance the visual representation of data, we recommend providing clearer explanations of features in Figure 2 and improving the legibility of x-axis labels in Figure 4. Finally, we encourage the authors to consider expanding the dataset to include additional metrics like user engagement and to explore the capabilities of reinforcement learning algorithms in their analysis.