ID: g3VOQpuqlF
Title: Adapting Pretrained Text-to-Text Models for Long Text Sequences
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for adapting pretrained text-to-text models to handle long text sequences by exploring three key aspects: model architecture, modeling objectives, and pretraining corpus. The authors propose replacing full attention with pooling-augmented blockwise attention and using a T5-style denoising objective with a mix of short and long spans. They also investigate the effectiveness of pretraining with randomly concatenated passages from a large open-domain corpus, achieving competitive results on long-text summarization and QA tasks.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a relevant topic in the NLP community and provides a comprehensive evaluation of various strategies applicable to different models.  
- It is clearly written and easy to follow, with strong empirical performance improvements over considered baselines.  
- The proposed model achieves state-of-the-art performance on long-sequence summarization tasks and demonstrates better performance on long documents compared to the base model.

Weaknesses:  
- The paper lacks a deep analysis of its significant contributions, with some findings appearing incremental or not sufficiently novel.  
- The conclusion that randomly concatenating passages yields better performance than natural long-form text is counterintuitive and requires further qualitative analysis.  
- Methodological issues, such as insufficient evaluation metrics and the need for more rigorous comparisons with existing approaches, may undermine the results.

### Suggestions for Improvement
We recommend that the authors improve the depth of analysis regarding the contributions of their work, particularly in distinguishing their findings from existing literature. Additionally, addressing the counterintuitive results related to the pretraining corpus through qualitative analysis or human evaluations would strengthen the paper. It would also be beneficial to consider baselines such as RoPE and Alibi relative positional embeddings to validate the performance improvements claimed. Finally, the authors should clarify the applicability of their techniques to decoder-only models, especially in multi-hop QA scenarios.