ID: tbRPPWDy76
Title: MEEP: Is this Engaging? Prompting Large Language Models for Dialogue Evaluation in Multilingual Settings
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for evaluating dialogue engagingness using large language models (LLMs) in a multilingual context. The authors define engagingness through six dimensions: Response Diversity, Interactional Quality, Sentiment, Interestingness, Othering, and Contextual Specificity. They introduce the Metric for Engagingness Evaluation using Prompting (MEEP) and demonstrate its effectiveness through multilingual datasets obtained via Microsoft Azure and Tencent services. The study reports improved correlation coefficients compared to previous methods.

### Strengths and Weaknesses
Strengths:
- The definition of engagingness with six dimensions offers linguistic interpretability.
- The MEEP technique achieves statistically significant improvements in correlation coefficients.
- The ablation study effectively disentangles the effects of different prompt design components.

Weaknesses:
- The evaluation is limited to turn-level engagingness and does not encompass dialogue-level assessments.
- The multilingual evaluation is restricted to English translations into Spanish and Chinese, raising concerns about reliability.
- The prompts lack organization and clarity, particularly in Tables 6 and 7, and the Chinese translation quality is poor.
- The novelty of the method is limited, primarily relying on prompt engineering, and the scope is confined to specific models like ChatGPT and GPT-3.5.

### Suggestions for Improvement
We recommend that the authors improve the theoretical discussion and provide more in-depth analyses to enhance understanding of the prompt design. Additionally, conducting more evaluation experiments categorized by language types and dialogue content would strengthen the paper. We suggest refining the prompt organization for clarity and addressing the translation quality issues in Chinese. Finally, expanding the evaluation to include open-source models like Pythia and Llama would enhance the generalizability of the findings.