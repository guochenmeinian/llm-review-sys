ID: ojgwuBVokp
Title: Random Entity Quantization for Parameter-Efficient Compositional  Knowledge Graph Representation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical analysis of random entity quantization for compositional knowledge graph embedding (KGE) methods, demonstrating that it can achieve state-of-the-art performance comparable to existing strategies. The authors justify this by showing that random quantization has high entropy at the code level, enhancing entity distinguishability. The study also critiques current quantization methods, suggesting that they may not perform as effectively as random quantization.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated, clearly organized, and presents interesting findings that could inspire future research in compositional KGE.
- The experimental results support the claims made, revealing that random quantization can yield results similar to complex methods.
- The authors conducted extensive experiments, which bolster the effectiveness of the proposed method.

Weaknesses:
- The rationale for why current quantization strategies based on PageRank or attribute similarity underperform compared to random quantization is insufficiently explained.
- The method lacks parameter efficiency, as indicated by the minor savings in Table 5.
- The focus on transductive link prediction undermines the claim of superiority for inductive link prediction, which is a key advantage of compositional KGE methods.

### Suggestions for Improvement
We recommend that the authors improve the explanation of why existing quantization strategies do not outperform random quantization and provide a general direction for enhancing entity quantization strategies. Additionally, we suggest including results on inductive link prediction to strengthen the argument for the proposed method's effectiveness. Lastly, clarifying the usage of the term "quantization" and marking the best performance in boldface in Tables 2, 3, and 4 would enhance clarity.