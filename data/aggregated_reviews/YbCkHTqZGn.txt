ID: YbCkHTqZGn
Title: Faithful Model Evaluation for Model-Based Metrics
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel significance test for model-based metrics, addressing the impact of model evaluation errors on confidence intervals and conclusions. The authors propose a variance estimation method coupled with a two-sample z-test for comparing NLP models. The study emphasizes the importance of this topic in the context of increasing reliance on machine learning evaluations.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a significant issue in the evaluation of machine learning models, proposing a solution that serves as a good alternative to traditional significance tests.  
- The mathematical expressions are clear and effective, and the experimental results support the proposed variance estimation.  
- The writing is clear, and the experiments are solid, contributing to the understanding of model-based metrics.

Weaknesses:  
- The approach relies on parameters estimated from in-domain data, which may not accurately reflect out-of-domain performance, potentially affecting variance estimation.  
- The decomposition of variance into $Var^M(C)+Var^M(T)$ is deemed unreasonable due to the correlation between evaluation results of models $C$ and $T$.  
- The study is limited to binary classification and tested only two datasets, lacking diversity in model-based metrics with varying levels of variance.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their approach by testing on a broader range of datasets and models, particularly those with varying levels of variance. Additionally, addressing the correlation issue in the variance decomposition would strengthen the validity of the proposed method. Finally, we suggest exploring the applicability of the significance test to commonly used metrics such as precision, recall, and F-score, as well as considering generalization beyond binary classification.