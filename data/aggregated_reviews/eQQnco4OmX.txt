ID: eQQnco4OmX
Title: Self-Calibrated Listwise Reranking with Large Language Models
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Self-Calibrated Listwise Reranking (SCaLR) framework, which leverages large language models (LLMs) for listwise reranking tasks. SCaLR addresses challenges associated with limited context windows and inefficiencies of traditional methods by introducing relevance-aware listwise reranking and a self-calibrated training process. The framework incorporates explicit relevance scores for global comparisons and calibrates list-view relevance scores using point-view scores, thereby reducing biases. Evaluations on the BEIR benchmark and TREC Deep Learning Tracks indicate that SCaLR outperforms existing methods.

### Strengths and Weaknesses
Strengths:
- Efficiently handles large candidate sets while maintaining global ranking quality.
- Introduces a novel self-calibration mechanism that effectively aligns point-view and list-view relevance.
- Demonstrates robustness against common reranking biases.

Weaknesses:
- Performance gains are marginal compared to simpler baseline models.
- High computational resource requirements for fine-tuning and inference.
- Certain methodological explanations, such as adaptive optimization, lack clarity.
- The methodology lacks theoretical justification for key parameters and sufficient analysis of computational efficiency.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of the set $C$ in the Preliminaries section to avoid confusion. Additionally, the authors should introduce the variable $L$ before defining $C'$ in formula (1). It would be beneficial to include state-of-the-art performance comparisons for TREC DL Tracks and BEIR to enhance the impact of the results. We suggest correcting the phrase "superior superior performance" to "show superior performance" in line 624.

To strengthen the paper, we recommend providing a direct comparison with simpler models to highlight the added value of SCaLR. Exploring scenarios where components like adaptive optimization and point-view relevance are removed could yield insights into their necessity. Furthermore, justifying the complexity of the method relative to performance gains and detailing the computational costs would enhance the argument. Lastly, we encourage the authors to clarify the fine-tuning process and consider releasing the training and inference codes for reproducibility.