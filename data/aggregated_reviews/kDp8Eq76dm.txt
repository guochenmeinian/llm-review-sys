ID: kDp8Eq76dm
Title: AlleNoise - large-scale text classification benchmark dataset with real-world label noise
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 5, 6, 5, -1, -1, -1
Original Confidences: 3, 4, 5, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AlleNoise, a large-scale benchmark dataset for text classification featuring over 500,000 examples across approximately 5,600 classes, with a realistic 15% label noise rate derived from user interactions on an e-commerce platform. The dataset includes both noisy labels and human-verified clean labels, facilitating a deeper understanding of noise impacts. The authors evaluate various established methods against this real-world noise, revealing their inadequacies and emphasizing the need for improved approaches.

### Strengths and Weaknesses
Strengths:
- The realistic nature of the label noise, derived from actual user data, represents a significant advancement over synthetic noise typically used in benchmarking.
- The scale of AlleNoise provides a rich dataset for training and testing models under diverse conditions.
- The provision of both noisy and verified clean labels allows for precise measurement of noise impact on model performance.
- The paper is well-organized, offering thorough descriptions of dataset creation, noise characteristics, and comprehensive benchmarking results.

Weaknesses:
- The benchmarked noise-robust learning methods are relatively outdated, lacking recent advancements.
- The dataset is sourced from a single e-commerce platform, which may introduce bias.
- Limited discussion on the patterns of wrong predictions and the implications of having multiple ground-truth labels for fine-grained classes.
- The paper could benefit from a more detailed exploration of the dataset's insights and the reasons behind the poor performance of existing methods.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of current methods by benchmarking more recent and robust solutions, as seen in the CIFAR-N and Clothing-1M leaderboards. Additionally, providing further insights and statistics from the dataset would enhance understanding of the challenges posed by real-world noise. We also suggest evaluating how data imbalance affects results and testing a wider range of models, including the latest advancements in NLP, to provide a more comprehensive evaluation of the dataset.