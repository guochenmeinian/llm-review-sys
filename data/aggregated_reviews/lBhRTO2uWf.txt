ID: lBhRTO2uWf
Title: Adversarial Learning for Feature Shift Detection and Correction
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an invariance-based approach to out-of-distribution generalization, focusing on feature shift localization and correction. The authors propose a method that filters input dimensions to minimize divergence between distributions \( p \) and \( q \), leveraging feature selection and an adversarial approach to correct features responsible for distribution shifts. The method includes two algorithms: DataFix-locate, which identifies shifted features using a random forest classifier, and DataFix-Correct, which imputes values for these features based on divergence measures. Additionally, the paper evaluates the proposed method for data imputation in downstream classification and regression tasks, incorporating new figures and clarifying the relationship between divergence measures and evaluation metrics. The authors acknowledge the complexity of feature shift correction and imputation, emphasizing the need for a broader understanding of these concepts in relation to downstream tasks.

### Strengths and Weaknesses
Strengths:
- The paper addresses a highly relevant problem with potential practical applications.
- The manuscript is clearly written and easy to follow.
- The proposed method outperforms existing baselines and is computationally efficient.
- The inclusion of downstream evaluation tasks enhances the manuscript's quality.
- New figures and clarifications improve the paper's readability and understanding.
- The results demonstrate that the proposed method, DataFix, often surpasses competing methods in various settings.

Weaknesses:
- The paper lacks contextualization within the out-of-distribution generalization literature, particularly regarding the standard covariate shift case and assumptions about conditional label distributions.
- Empirical assessments are limited to tabular data and a specific pair of distributions, raising questions about generalizability to new shifted distributions.
- The proposal does not consider representation learning techniques, which could provide alternative solutions to feature selection.
- The motivation behind the correction approach is unclear, particularly regarding the usefulness of the corrected data.
- The relationship between imputation and feature shift correction is not fully explored.
- The citations regarding alignment may not adequately cover the breadth of the topic.

### Suggestions for Improvement
We recommend that the authors improve the contextualization of their work within the out-of-distribution generalization literature, specifically addressing the assumptions related to conditional label distributions. Additionally, the authors should expand their empirical assessment beyond tabular data to include more complex structured data, such as images or text. Clarifying the motivation behind the correction approach and providing examples of how proposal values are constructed would enhance understanding. We also suggest improving the discussion on the relationship between evaluation metrics and imputation metrics, particularly in the context of datasets with available labels. Furthermore, the authors should elaborate on the subtleties of feature shift correction in relation to imputation. Finally, we suggest improving the paper's layout by moving essential details from the appendix to the main text to enhance readability and reporting classification and downstream tasks to further validate the proposed method's effectiveness.