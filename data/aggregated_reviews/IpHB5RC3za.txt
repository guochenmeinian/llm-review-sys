ID: IpHB5RC3za
Title: Real-time Stereo-based 3D Object Detection for Streaming Perception
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 5, 7, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a real-time stereo-based 3D object detection framework for streaming perception, named StreamDSGN. The authors propose three technical contributions: 1) a Feature-Flow-Fusion module that predicts future-frame features using a flow map to reduce misalignment, 2) a Motion Consistency Loss function for explicit supervision based on motion consistency between adjacent frames, and 3) a Large Kernel BEV Backbone for capturing long-range dependencies in low-framerate datasets. The effectiveness of the proposed method is validated through experiments on the KITTI dataset, demonstrating significant performance improvements over a baseline.

### Strengths and Weaknesses
Strengths:
1. The proposed algorithm addresses an important problem in real-world applications, particularly in autonomous driving, and the performance evaluation considers both accuracy and latency.
2. Experimental results are impressive, showing a 4.33% increase in 3D object detection streaming average precision compared to the baseline, with reproducible results supported by provided source code.
3. The presentation is clear, and the ablation study is well-structured, enhancing the understanding of the contributions.

Weaknesses:
1. The stereo detection setting is uncommon in autonomous driving, and the dataset is limited; extending the approach to multi-camera or camera+Lidar settings would be beneficial.
2. There is a lack of detailed computational latency analysis for each algorithm module, making it difficult to assess the trade-off between performance and latency.
3. The novelty of the proposed method is not significant, as the largest performance gains appear to stem from the fusion of past frame features rather than the new modules.
4. The test setup may not provide a sufficiently distinct testing set, as interleaved training/testing frames could lead to a smaller domain gap.

### Suggestions for Improvement
We recommend that the authors improve the generalization capability of the proposed algorithm by testing it in multi-camera or camera+Lidar settings. Additionally, a detailed computational latency analysis for each module should be included to better illustrate the performance-latency trade-offs. We suggest elaborating on the intuition behind the Motion Consistency Loss, particularly regarding the necessity of velocity and acceleration losses. Furthermore, we encourage the authors to compare their method against more advanced trajectory prediction baselines to assess its robustness. Lastly, clarifying the test setup and ensuring that it aligns with standard protocols for the KITTI dataset would strengthen the evaluation.