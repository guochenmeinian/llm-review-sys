ID: orSVYeobMr
Title: RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an adaptive gradient update approach aimed at enhancing the robustness of language models (LMs) through a fine-tuning technique called Robustifying LMs via Adversarial perturbation with Selective Training (RoAST). The authors propose a selective update strategy that resembles "gradient dropout," effectively increasing model robustness by masking gradients based on their variance. The experimental results demonstrate strong performance across multiple tasks, indicating the potential for broader applicability beyond specific NLP tasks.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and motivated, making it easy to follow.
- The proposed RoAST method introduces a novel approach to multi-perspective robustness in LMs.
- Comprehensive experiments and thorough analysis support the claims, showing state-of-the-art results.

Weaknesses:
- Limited experimentation across diverse tasks and datasets; reliance on BERT-based models restricts generalizability.
- Some explanations and analyses lack depth, particularly regarding the importance of the importance score and the integration of parameters.
- The derivation of key concepts is unclear, and the novelty of the selective training method is questioned.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the derivations, particularly regarding the assumptions made in the analysis. A more detailed exploration of why the importance score may not significantly impact performance would strengthen the paper. Additionally, we suggest incorporating more challenging benchmarks and exploring non-BERT-based models, such as decoder-only structures, to enhance the generalizability of the findings. Finally, we encourage the authors to provide further experimental results, such as those from FreeLB++, to better contextualize their approach within existing literature.