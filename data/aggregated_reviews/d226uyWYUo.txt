ID: d226uyWYUo
Title: Knowledge Graph Completion by Intermediate Variables Regularization
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a general model for regularizing various tensor-decomposition-based knowledge graph completion models, including real/complex CP decomposition and Tucker decomposition. The authors express these models as a block-term decomposition of a dense tensor, establishing rank-based conditions for expressing symmetry, antisymmetry, and inverse relations. They propose a regularization strategy based on the norms of factor matrices and intermediate terms from tensor contractions, evaluated on three datasets and six models, demonstrating accuracy improvements.

### Strengths and Weaknesses
Strengths:
- The rank-based justification for learning logical rules from knowledge graph models is intriguing, offering a linear algebraic perspective.
- The hyperparameter tuning is well-conceived, as it significantly enhances model performance.
- The regularization strategy shows competitive accuracy improvements on established datasets, indicating that many state-of-the-art models can benefit from additional regularization.

Weaknesses:
- The utility of Theorem 1 is unclear, as simpler proofs exist for symmetry properties in individual models, raising questions about its practical application.
- The proposed regularizer adds numerous terms to the loss function, and while it theoretically improves accuracy, the computational cost and runtime implications remain unaddressed.
- The rationale for dividing models into P parts lacks depth, leaving readers uncertain about its theoretical or practical significance.

### Suggestions for Improvement
We recommend that the authors clarify the utility of Theorem 1 by providing examples where it yields insights not obtainable through simpler proofs. Additionally, we suggest including empirical data on the runtime impact of the proposed regularization terms to assess practical implications. A more thorough discussion on the choice of P and its effects on model performance would enhance the paper's clarity. Finally, addressing the computational efficiency of the proposed methods and their applicability to various tensor decomposition models would strengthen the overall contribution.