ID: FUnEkOkodU
Title: Token-Scaled Logit Distillation for Ternary Weight Generative Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 6, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to enhance Quantization-Aware Training (QAT) for generative language models through Token-Scaled Logit Distillation (TSLD). The authors propose modifications that utilize knowledge distillation while addressing cumulative quantization errors introduced by masked attention. The method aims to prevent overfitting by adjusting logit distillation based on token confidence, thereby improving performance in language modeling and reasoning tasks. Additionally, the authors introduce a QAT-KD methodology applied to the OPT-125m PTB fine-tuned model, utilizing two approaches: Logit+GT and TSLD. They provide an anonymous link to share model weights and guidelines for performance verification, indicating that the evaluation results align with those from the official huggingface environment. The QAT-KD is tailored for task-specific fine-tuned generative language models (GLMs), and the authors plan to implement the INT2 datatype in the future to optimize model size.

### Strengths and Weaknesses
Strengths:
- The TSLD method is innovative and effectively addresses the overfitting problem in QAT.
- The paper provides a thorough analysis of cumulative quantization errors and presents empirical results demonstrating improved generation quality and reasoning ability across various model sizes.
- The authors provide clear guidelines for model weight examination and performance verification, enhancing reproducibility.
- The shared model weights and logs facilitate a thorough evaluation of the QAT training process.

Weaknesses:
- The paper lacks clarity in organization, making it difficult to follow the logical flow between sections.
- The evaluation is limited to a small number of datasets and tasks, which may not sufficiently support the claims made regarding the method's effectiveness.
- The motivation behind the proposed dynamic weighting method is not adequately explained, and the evidence provided is insufficient to justify its validity.
- The performance of the model may not be satisfactory if evaluated on tasks not specifically fine-tuned.
- The current model weights are not encoded in the INT2 type, which may limit efficiency until this is addressed.

### Suggestions for Improvement
We recommend that the authors improve the organization of the paper to enhance clarity and logical flow. Additionally, consider expanding the evaluation to include more datasets and tasks, such as Q&A tasks and SuperGlue tests, to provide a more comprehensive assessment of the proposed method. It would also be beneficial to clarify the motivation behind the dynamic weighting method and provide more robust evidence to support its claims. Furthermore, addressing the technical concerns regarding time complexity and the accuracy of notations in the mathematical formulations would strengthen the paper. We also suggest that the authors improve the model's performance on non-fine-tuned tasks to broaden its applicability and expedite the implementation of the INT2 datatype to enhance model size efficiency.