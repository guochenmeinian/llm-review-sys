ID: 0WLMVDdvDF
Title: No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 7, 7, 7, 7, -1, -1, -1
Original Confidences: 1, 3, 4, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on online learning in adversarial Markov Decision Processes (MDPs) where loss and transition functions can change adversarially. The authors propose algorithms that achieve regret guarantees of $O(\sqrt{T} + C^P)$, where $C^P$ quantifies the degree of adversarial transitions. Notably, the first algorithm requires knowledge of $C^P$, while subsequent refinements eliminate this requirement through a black-box reduction approach. The algorithms also adapt to varying degrees of adversarial behavior in loss functions, achieving regret bounds of $O(U + \sqrt{U C^L} + C^P)$, with $U$ being a gap-dependent coefficient.

### Strengths and Weaknesses
Strengths:
- The paper introduces novel algorithms that address adversarial MDPs with changing transitions, achieving significant regret bounds.
- The elimination of the requirement to know $C^P$ through a black-box reduction is a notable contribution.
- The techniques and results presented are innovative and may have broader implications for future research in reinforcement learning.

Weaknesses:
- The organization of the paper may hinder comprehension; the main algorithm should be presented more clearly in the main text.
- Technical details are often unclear, and the writing could benefit from improved readability, particularly in the proofs.
- The justification for the specific type of regret studied is lacking, especially regarding its relevance in mild-corruption settings.

### Suggestions for Improvement
We recommend that the authors improve the organization of the paper by presenting the main algorithm in the main text for clarity. Additionally, focusing on how the proposed algorithms differ from existing methods and the challenges they address would enhance understanding. We also suggest providing a dedicated conclusion section to summarize key findings and contributions. To improve readability, consider simplifying the presentation of technical details and proofs. Finally, we encourage the authors to justify the choice of the benchmark policy and address the implications of the defined regret in the context of adversarial dynamics.