ID: 6c2s6HddQ4
Title: The Locality and Symmetry of Positional Encodings
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a detailed analysis of positional encodings in transformer-based language models, specifically BERT. The authors investigate two core properties, locality and symmetry, and demonstrate that models exhibiting these properties can achieve improved inductive bias and performance in both pretraining and non-pretraining settings. They introduce probing tasks to assess the sensitivity of models to semantic role swapping, revealing weaknesses associated with the symmetry property.

### Strengths and Weaknesses
Strengths:
- The work provides a comprehensive investigation of positional encodings, with findings that may inspire future research.
- The analysis includes extensive evaluations across various NLU tasks, showing that initializing BERT with symmetric/localized positional encodings can yield significant improvements.
- The proposed probing tasks are valuable for further exploration of word swap effects.

Weaknesses:
- The study is limited to BERT-style models, neglecting decoder-only and encoder-decoder architectures, which restricts the generalizability of the findings.
- The symmetry equations presented do not align with the original definitions, raising concerns about their validity in measuring the attention matrix's symmetry.
- The metrics employed, particularly the Identical Word Probing method, may not accurately reflect real-world language scenarios.

### Suggestions for Improvement
We recommend that the authors expand their analysis to include decoder-only and encoder-decoder models to enhance the generalizability of their findings. Additionally, we suggest exploring alternative methods for measuring positional encodings beyond the Identical Word Probing approach. Clarification on the calculation of the symmetry metric is needed, particularly regarding the involvement of the current position index. Furthermore, we encourage the authors to investigate whether better-designed positional encodings could lead to faster convergence in training experiments. Lastly, addressing the incomplete sentences and typos throughout the paper will improve clarity and presentation.