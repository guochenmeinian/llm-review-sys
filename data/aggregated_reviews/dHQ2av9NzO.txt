ID: dHQ2av9NzO
Title: On the Convergence of Black-Box Variational Inference
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 7, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents convergence results for black-box variational inference (BBVI) using ordinary stochastic gradient descent (SGD) and proximal SGD under specific assumptions. It focuses on the reparameterization gradient setting with the location-scale variational family, particularly mean-field and Cholesky parameterization, and a symmetric base distribution. The authors demonstrate that nonlinear scale parameterizations are suboptimal, a claim supported by empirical tests on synthetic and realistic problems, indicating that proximal methods converge faster. The work builds on previous literature, notably Domke (2020), and establishes that the energy term of the evidence lower bound (ELBO) is smooth under certain conditions, while nonlinear diagonal conditioners can break convexity, affecting convergence rates.

### Strengths and Weaknesses
Strengths:
- The paper advances previous convergence results by reducing assumptions required in earlier works and empirically validates the superiority of linear scale parameterization.
- The presentation is mostly clear, with helpful highlighting of assumptions and extensive details on the experimental setup, enhancing reproducibility.
- The theoretical contributions provide practical insights into the trade-offs between linear and nonlinear diagonal conditioners.

Weaknesses:
- The paper inadequately addresses its limitations, particularly the focus on reparameterization gradient BBVI, which is not mentioned in the abstract. The title may mislead readers into thinking it resolves a broader issue.
- The experimental evaluation primarily employs the Adam optimizer, despite theoretical results concerning standard and proximal SGD, and does not explore the ProxAdam&nonlinear combination.
- The results are technical, with potential errors in Theorem 3 and unclear variable definitions, suggesting a need for a comprehensive table of variable names and their meanings.

### Suggestions for Improvement
We recommend that the authors improve the limitations section by explicitly stating all assumptions and limitations, including the focus on reparameterization gradient BBVI. The title should be revised to reflect the specific scope of the work more accurately. Additionally, we suggest including results for the ProxAdam&nonlinear combination to provide a more comprehensive evaluation of the methods. Clarifying the definitions and roles of variables in Theorem 3, as well as providing a table for easy reference, would enhance the paper's clarity. Lastly, we encourage the authors to consider the implications of using Adam in their experiments versus standard SGD and to address this in the revised manuscript.