ID: srJvUWZu6L
Title: ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for continual test-time adaptation (CTTA) by utilizing low-rank and high-rank adapters to capture domain-agnostic and domain-specific knowledge, respectively. The authors propose a Homeostatic Knowledge Allotment (HKA) strategy to balance the contributions of these adapters. Experiments conducted on benchmarks such as ImagenetC, CIFAR10C, CIFAR100C, and Cityscapes-to-ACDC demonstrate the effectiveness of the proposed approach, achieving state-of-the-art results.

### Strengths and Weaknesses
Strengths:
1. The paper effectively distinguishes between domain-specific and domain-agnostic knowledge using two separate adapters.
2. The HKA strategy for updating adapter contributions based on prediction uncertainty is innovative.
3. An ablation study validates the importance of different components.
4. The experimental results for zero-shot generalization in the TTA setting are novel.

Weaknesses:
1. There is a lack of theoretical justification for why the low-rank adapter is associated with domain-agnostic knowledge and the high-rank adapter with domain-specific knowledge.
2. The backbone architecture used is inconsistent with prior works, and performance metrics for certain datasets and architectures are missing.
3. The requirement to retrain models with added adapters using source data limits the applicability of off-the-shelf pre-trained models.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the roles of the low-rank and high-rank adapters in learning domain-specific and domain-agnostic features. Additionally, please provide performance results for the proposed approach on Imagenet-to-ImagenetC with ResNet50 and CIFAR100-to-CIFAR100C with ResNeXt-29. Clarifying the necessity of retraining with source data would enhance the paper's applicability. We also suggest revising the references to ensure they are from conference or journal versions and correcting minor typographical errors throughout the manuscript. Lastly, we encourage the authors to explore the implications of reversing the conditions for updating scale factors in Equation 4 and to provide more detailed explanations regarding the integration of different ViDAs into the pre-trained model.