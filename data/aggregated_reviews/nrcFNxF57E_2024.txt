ID: nrcFNxF57E
Title: Partial Gromov Wasserstein Metric
Conference: NeurIPS
Year: 2024
Number of Reviews: 22
Original Ratings: 7, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Partial Gromov-Wasserstein (PGW) distance, an unbalanced version of the Gromov-Wasserstein distance that incorporates a total variance (TV) penalty. The authors establish its metric properties and propose two algorithms, Frank-Wolfe and a line search method, to compute PGW. Additionally, the paper discusses the relationship between the solutions of PGW and the Modified Partial Gromov-Wasserstein (MPGW), particularly in the context of critical points and hyper-parameter settings. The authors assert that while specific solutions may coincide, the overall solution sets of PGW and MPGW are distinct. The experiments demonstrate PGW's effectiveness in various tasks, including shape matching and retrieval, particularly in handling outlier data.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a significant gap in unbalanced Gromov-Wasserstein problems by introducing a TV-relaxed formulation.  
- The theoretical proofs of the metric properties are solid, and the paper includes diverse experimental applications that validate the proposed methods.  
- The organization and clarity of the paper are commendable, with rigorous theoretical developments and comprehensive experiments.  
- The authors provide clear explanations regarding the relationship between PGW and MPGW solutions, effectively addressing reviewer concerns.  
- The clarification of the minimum value of \( M \) is well-articulated, enhancing understanding of the underlying mathematical framework.  
- The acknowledgment of PGW's metric properties and the suggestion for further experimental validation is a positive aspect.

Weaknesses:  
- The literature review lacks references to similar works that formulate partial Wasserstein metrics with TV constraints.  
- The proof of the metric properties is inadequately presented in the main text, which is a critical aspect of the work.  
- There is insufficient comparison with the KL version regarding robustness against outliers, and scalability discussions are limited.  
- The hyperparameter selection process, particularly the justification for the $\lambda$ value in PGW and MPGW methods, is unclear.  
- The responses do not fully address the practical implications of PGW's utility in real-world applications, as highlighted by reviewers.  
- There is some ambiguity regarding the implications of "another solution" in the context of PGW, which could lead to confusion among readers.  
- The authors' assertion that the solution sets are different may not fully account for practical scenarios where hyper-parameter tuning is involved.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including relevant works on partial Wasserstein metrics with TV constraints. The presentation of the proof for the metric properties should be enhanced in the main text to emphasize its importance. Further comparisons with the KL version regarding outlier robustness and a detailed discussion on scalability for large datasets would be beneficial. Additionally, we suggest clarifying the hyperparameter selection process, particularly the rationale behind choosing the $\lambda$ value for both PGW and MPGW methods. Including a parameter sensitivity analysis in the experimental section would also strengthen the paper. Furthermore, we recommend that the authors improve the practical demonstration of PGW's utility in real-world datasets, as this would strengthen their claims regarding its advantages. Clarifying the implications of "another solution" in the context of PGW would enhance reader comprehension. Finally, we suggest that the authors consider discussing how solutions of PGW might relate to those of MPGW when tuning over hyper-parameters, as this is a common practice in model evaluation.