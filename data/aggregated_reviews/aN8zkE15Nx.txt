ID: aN8zkE15Nx
Title: An Investigation of LLMsâ€™ Inefficacy in Understanding Converse Relations
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark called ConvRe for evaluating Large Language Models' (LLMs) understanding of converse relations, where the order of arguments is reversed (e.g., "x has part y" vs. "y has part x"). The benchmark includes two categories: Re2Text and Text2Re, assessing models' abilities to match relations with textual descriptions. It comprises 17 relations and 1240 triples, evaluated across three LLM families (GPT, CLAUDE, and FLAN-T5) under various settings, including zero-shot and few-shot learning. The authors find that performance declines with larger models, indicating a tendency towards shortcut learning.

### Strengths and Weaknesses
Strengths:  
- The introduction of the ConvRe benchmark provides a valuable tool for assessing LLMs' comprehension of structured semantics in formal languages.  
- The evaluation protocol is comprehensive, exploring various prompting methods and few-shot examples, revealing significant insights into LLMs' reasoning capabilities.  
- The paper is well-written and presents carefully designed experiments that convincingly support its claims.

Weaknesses:  
- The results on ChatGPT and GPT-4 are not reproducible, limiting the benchmark's applicability.  
- The scope is narrow, focusing solely on converse relations, which may not capture broader issues of spurious correlations in LLMs.  
- Some figures contain redundant information, and certain details, such as the input context's influence on model decisions, lack clarity.

### Suggestions for Improvement
We recommend that the authors improve the reproducibility of results by providing more detailed specifications of the models and data used, particularly for ChatGPT and GPT-4. Additionally, condensing the paper could enhance clarity, addressing redundancies in Figures 2, 3, and 4, and considering tables for presenting data in Figures 5 and 6. To broaden the study's impact, we suggest exploring the implications of shortcut learning beyond converse relations, potentially integrating discussions on "shortcuts," "superficial correlations," and "spurious correlations." Finally, clarifying the influence of input context on model performance would strengthen the paper's arguments.