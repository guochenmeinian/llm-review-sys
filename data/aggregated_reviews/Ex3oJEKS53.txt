ID: Ex3oJEKS53
Title: Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 5, 5, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extension of the Kronecker-Factored Approximate Curvature (K-FAC) method for training generic neural networks, particularly focusing on transformers and graph neural networks. The authors propose two variants, K-FAC-expand and K-FAC-reduce, which leverage weight-sharing mechanisms and demonstrate exactness for deep linear networks. Experimental results indicate that these methods achieve validation metrics comparable to well-tuned first-order baselines with fewer steps and reduced wall-clock time.

### Strengths and Weaknesses
Strengths:
1. The paper effectively extends K-FAC to transformers and graph neural networks, addressing limitations in previous works.
2. The proposed methods are mathematically grounded, with proofs of exactness in deep linear settings.
3. The methods achieve competitive validation metrics with fewer steps compared to first-order methods, suggesting practical applicability.
4. Comprehensive supplementary materials, including mathematical derivations and additional experiments, are provided.

Weaknesses:
1. The evaluation of transformers is limited to ImageNet; a focus on language tasks would be more appropriate.
2. While exactness is proven for linear settings, error bounds for non-linear scenarios would enhance the analysis.
3. Time and space complexity details for both K-FAC-expand and K-FAC-reduce are lacking.
4. Limited comparisons with other efficient optimization methods reduce the contextual understanding of the proposed methods.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including language tasks to better assess the performance of their optimizers. Additionally, providing error bounds for non-linear settings would strengthen the theoretical contributions. It is also advisable to include time and space complexity analyses for both K-FAC variants. Furthermore, we suggest comparing the proposed methods with a broader range of efficient optimization techniques to contextualize their contributions more effectively. Lastly, enhancing the clarity of the presentation, particularly in the theoretical sections, would benefit readers unfamiliar with the notation.