ID: CrADAX7h23
Title: DAGER: Exact Gradient Inversion for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 6, 7, 7, -1
Original Confidences: 4, 3, 2, -1

Aggregated Review:
### Key Points
This paper presents a method to recover user training data from gradients in federated learning, leveraging the existence of a linear network component. The authors propose that the gradient of this component can be expressed as a linear combination of its input, enabling the recovery of input token embeddings from the first layer of a transformer. By extending this approach to the second layer, the entire training sequence can be reconstructed. The authors empirically demonstrate significant improvements over the state of the art. Additionally, the paper introduces the DAGER algorithm, which utilizes low-rank decomposition of self-attention layer gradients to filter incorrect embedding vectors and reconstruct input text.

### Strengths and Weaknesses
Strengths:
- The idea is straightforward and effective, with impressive numerical improvements over existing methods.
- Solid experiments and mathematical proof support the findings, particularly the innovative application of low-rank decomposition on self-attention.
- The paper is well-written, with clear diagrams and figures that enhance understanding.

Weaknesses:
- The theoretical development is weak; technical assumptions need clearer articulation, such as requirements on the loss function and differentiability.
- The justification for the condition b<d is insufficient, as the authors' argument regarding reasonable input lengths and batch sizes is unconvincing.
- The conceptual rationale behind the effectiveness of Algorithm 1 is unclear, necessitating a more thorough explanation.
- Notation is heavy and difficult to follow; a notation table would aid comprehension.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework by clearly stating the technical assumptions, including any requirements on the loss function and differentiability. Additionally, the authors should provide a more robust justification for the condition b<d, addressing the potential discrepancy between embedding space size and token count. A deeper analysis of why Algorithm 1 performs well is warranted, particularly regarding the linear independence of embedded tokens. To enhance clarity, we suggest adding a notation table or input before the algorithm. Furthermore, discussing the implications of using quantized models and the influence of LoRA for fine-tuning would be beneficial. Lastly, including reconstructed text for comparison would strengthen the paper's contributions.