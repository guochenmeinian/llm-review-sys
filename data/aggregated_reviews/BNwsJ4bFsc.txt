ID: BNwsJ4bFsc
Title: VisoGender:  A dataset for benchmarking gender bias in image-text pronoun resolution
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 8, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the VisoGender benchmark, designed to assess occupation-related gender biases in vision-language models (VLMs). The benchmark includes scenes with WinoGender-like captions and evaluates bias through gender resolution accuracies and the ratios of male and female professionals retrieved from gender-neutral queries. The authors demonstrate that several state-of-the-art VLMs struggle with gender resolution in complex scenes, while captioning models tend to be more accurate and less biased than CLIP-style models. The dataset includes images paired with referential clauses, and bias is assessed through two tasks: pronoun resolution and gender retrieval. The authors evaluate various CLIP-based models and BLIP, revealing pronounced gender biases, particularly in images featuring individuals of opposite genders. Notably, biases in BLIP align with the representation of genders in the US. The authors acknowledge the limitations of their dataset, which reflects a Western-centric, cisgendered view of binary gender, and emphasize the importance of addressing biases in VLMs.

### Strengths and Weaknesses
Strengths:
- The authors effectively ground the biases of VLMs in real-world harms, motivating the benchmark.
- VisoGender is claimed to be the first benchmark for evaluating gender biases in visual-linguistic coreference resolution, making it relevant to the bias evaluation community.
- The authors' clear conception of bias as occupation-related gender bias is commendable.
- The finding that resolution and retrieval bias do not align across models is intriguing and contributes to understanding bias in VLMs.
- The dataset construction is methodologically sound, and the evaluation methods are well-designed.
- The thoughtful discussion of limitations and the inclusion of a comprehensive datasheet in supplementary materials enhance the paper's rigor.

Weaknesses:
- The context and positionality of the authors are not sufficiently clear, raising questions about the benchmark's Western-centric focus.
- The approach of combining WinoGender schema with scenes appears incremental; alternative curation choices are not discussed.
- The benchmark's reliance on inferred gender labels is problematic, as it risks misgendering individuals and conflating "groundtruth gender" with pronouns.
- The external validity of the curated scenes and captions is questionable, particularly regarding their applicability to real-world scenarios.
- The presentation of results is somewhat convoluted, making it challenging to follow.
- The distinction between CLIP and captioning models is unclear, leading to confusion regarding model categorization.
- Many results appear subtle, raising concerns about their robustness without error bars or multiple runs.
- The BLIP-2 results in Table 3 lack clarity and do not align with the narrative presented.
- The methodology, particularly the labeling of individuals' gender presentation, remains fundamentally problematic and raises ethical concerns.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their context and positionality, particularly regarding the potential Western-centric nature of their benchmark. The authors should explore and articulate the implications of filtering out occupations with insufficient image data, as these may represent significant biases in existing VLMs. Additionally, we suggest that the authors conduct a more structured description of the image selection and labeling processes, and include error analysis to identify instances where all models fail. 

The authors should also clarify the decision-making process for subject versus object in images and comment on the interpretability of resolution accuracy gaps when overall accuracy is low. It is crucial to remove references to "groundtruth" labels and reframe results in terms of comparisons to typical human pronoun resolution. Furthermore, we recommend that the authors address ethical concerns regarding consent and copyright, particularly in relation to the images used in their benchmark. 

To enhance the clarity of the results section, we suggest that the authors streamline the presentation and enhance the narrative. Specifically, they should elevate the distinction between accuracy and bias, reduce the space occupied by figures and tables, clearly label and reference subfigures, restructure subsections to organize results by task and type, refine definitions for model categorization, include estimates of robustness through error analysis, and clarify the BLIP-2 results in Table 3 to ensure consistency with the overall narrative. Lastly, we encourage the authors to address the ethical implications of inferring gender from images more thoroughly, ensuring that their approach is sensitive to the complexities of gender identity.