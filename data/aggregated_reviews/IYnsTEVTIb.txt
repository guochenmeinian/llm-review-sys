ID: IYnsTEVTIb
Title: Det-CGD: Compressed Gradient Descent with Matrix Stepsizes for Non-Convex Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 5, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 2, 2, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of optimization problems under a non-convex and smooth setting, utilizing a PSD matrix for smoothness and matrix stepsizes instead of scalars, reminiscent of Newton's method. The authors propose two update methods: $DS^k \nabla f(x_k)$ and $T^k D \nabla f(x_k)$, with the latter being computationally simpler. The paper extends these formulations to a layerwise structure applicable in neural network training and introduces a federated algorithm that employs matrix stepsizes. Experimental results favor the proposed algorithm over DCGD.

### Strengths and Weaknesses
Strengths:
- The integration of distributed optimization with matrix smoothness and stepsizes is a compelling topic, with explicit construction of the stepsize matrix leading to optimal convergence behavior.
- The paper is the first to provide theoretical insights in the federated setting, a crucial area in optimization.
- The compatibility analysis with compressor/sketches enhances its relevance to federated learning.

Weaknesses:
- The complexity of computing the $DS^k \nabla f(x_k)$ update is not quantified, raising concerns about its practicality compared to Newton's method.
- The utility of sketching matrices $S^k$ and $T^k$ appears limited to layerwise or blockwise structures, necessitating further elaboration for broader optimization frameworks.
- The paper lacks a thorough examination of existing literature on sketching methods, particularly in relation to second-order methods and interior point methods.
- The convergence results primarily focus on the $D$-norm rather than the Euclidean norm, which is noted as a limitation.

### Suggestions for Improvement
We recommend that the authors improve the quantification of the computational complexity for the $DS^k \nabla f(x_k)$ case to justify its use over Newton's method. Additionally, please elaborate on how to design sketching matrices for general optimization tasks beyond layerwise structures. A more comprehensive review of existing literature on sketching in second-order methods should be included. Furthermore, we suggest conducting additional experiments that compare convergence against total communication overhead and clarifying the treatment of the error level as $\epsilon^2$. Lastly, addressing the lack of experiments specifically in the neural network setting with block-diagonal smoothness would strengthen the paper.