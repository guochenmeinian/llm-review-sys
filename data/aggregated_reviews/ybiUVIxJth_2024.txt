ID: ybiUVIxJth
Title: Policy Aggregation
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 6, 6, 7, -1, -1
Original Confidences: 3, 3, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on aggregating preferences of multiple agents within a reinforcement learning framework, modeled as a multi-objective MDP with $n$ different reward functions. The authors propose using the state-action occupancy measure instead of directly aggregating agents' policies or reward functions. They explore social choice theory concepts, such as the Borda count and approval voting, and address issues related to affine transformations of reward functions. The paper also investigates efficient algorithms for approximate fairness and analyzes the complexity of existing voting rules.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and easy to read.  
- It addresses a well-motivated problem in policy aggregation and proposes novel algorithms for achieving fairness.  
- The theoretical analysis of voting rule complexities is significant and opens avenues for future research.  
- The algorithms are validated through experiments.  

Weaknesses:  
- The contributions are limited, focusing only on the full information case.  
- The justification regarding affine transformations of reward functions is seen as weak; normalization could make reward functions directly comparable.  
- The empirical results may not add significant value, as the focus should be on algorithm design and theoretical results.  
- A common metric for fairness in the experimental section would enhance comparisons.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper's title to better reflect its focus on reward aggregation rather than policy aggregation. Additionally, we suggest that the authors consider normalizing reward functions to address concerns about affine transformations. To enhance the empirical section, we recommend using a common metric to quantify fairness across different algorithms. Finally, if consensus among reviewers supports it, we advise dropping the empirical results section to allow for more detailed exposition of the proofs.