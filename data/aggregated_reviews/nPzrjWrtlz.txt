ID: nPzrjWrtlz
Title: The Truth, The Whole Truth, and Nothing but the Truth:  A New Benchmark Dataset for Hebrew Text Credibility Assessment
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HeTrue, a new dataset for evaluating the credibility of statements made by Israeli public figures and politicians, consisting of 1021 statements annotated by professional journalists. The authors conduct experiments to establish a baseline and compare text-only methods with those incorporating additional data such as metadata and context. They develop several credibility assessment models, including a feature-based model and state-of-the-art transformer-based models. Results indicate that the best performance occurs when models integrate both statement and context, establishing HeTrue as a challenging benchmark for future model training and evaluation.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and professionally written, with a new dataset collected and annotated by experts.
- The experiments are well-designed, applying state-of-the-art approaches and demonstrating the dataset's suitability as a benchmark.
- The dataset quality is high, providing valuable metadata and context for credibility assessment.

Weaknesses:
- There is limited discussion on how the findings compare with other works, particularly regarding model performance across different datasets and languages.
- The paper lacks clarity on the data collection process from The Whistle and how bias was mitigated during annotation.
- Some sections are overly verbose, and the related work section could be more focused on similar datasets for underrepresented languages.

### Suggestions for Improvement
We recommend that the authors improve the discussion on how their findings relate to existing literature, particularly regarding model performance comparisons. Clarifying the data collection process and addressing potential biases in annotation is essential. Additionally, we suggest condensing verbose sections and moving critical discussions from appendices to the main body. Including simpler baselines for context and evidence integration, such as concatenating with a <SEP> token, would strengthen the justification for the proposed models. Lastly, providing examples of statements, evidence, and context in the appendix would enhance clarity.