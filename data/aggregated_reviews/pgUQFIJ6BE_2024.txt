ID: pgUQFIJ6BE
Title: Near-Optimal Distributed Minimax Optimization under the Second-Order Similarity
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 5, 7, 6, -1, -1
Original Confidences: 4, 3, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents SVOGS, an improved algorithm for distributed minimax optimization utilizing client mini-batch sampling and gradient variance reduction. The authors provide theoretical rates on communication complexity and gradient computations, demonstrating that SVOGS achieves the corresponding lower bounds. The study focuses on distributed min-max optimization under the assumption of second-order data similarity, proposing lower complexity bounds for (strongly)-convex-(strongly)-concave functions with Lipschitz gradient. The algorithm is noted for being optimal in communications and nearly optimal in local gradient calls.

### Strengths and Weaknesses
Strengths:
1. The presentation is clear, with well-defined problem statements and assumptions.
2. The theoretical results are comprehensive and well-supported by simulation experiments.
3. The algorithm effectively closes the complexity gap for distributed min-max optimization problems with second-order similarity.

Weaknesses:
1. The experimental section is limited, testing the proposed method on only one small dataset, which raises concerns about its practical applicability.
2. The implementation details, including parameter tuning (e.g., $b, p, \alpha, \gamma$ in Algorithm 1), are insufficiently addressed, suggesting that the method may require more fine-tuning than prior approaches.
3. The novelty of the algorithm is primarily in node sampling, which may not present significant challenges compared to existing methods.
4. The rationality of Assumption 2 and the complexity of local gradient calls require further discussion and clarification.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by testing SVOGS on a broader range of datasets to enhance generalizability. Additionally, the authors should provide detailed implementation guidelines, including parameter tuning, to substantiate the practical value of their method. Clarifying the unique characteristics of the algorithm and discussing the implications of Assumption 2 in greater detail will strengthen the theoretical contributions. Finally, addressing the gap in local gradient calls complexity and the distinction between optimistic gradient and extra-gradient methods will enhance the clarity and depth of the paper.