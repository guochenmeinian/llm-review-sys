ID: 8jB6sGqvgQ
Title: Efficient Adversarial Training in LLMs with Continuous Attacks
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to adversarial training for large language models (LLMs) by utilizing continuous attacks in the embedding space, which significantly enhances computational efficiency compared to traditional discrete methods. The authors propose a fast adversarial training algorithm (AdvUL) that employs two losses: one for strengthening the model against continuous embedding attacks and another for maintaining model utility. Additionally, they introduce C-AdvIPO, an adversarial variant that does not depend on utility data for robust alignment. Empirical evaluations across various models demonstrate improvements in robustness against discrete attacks.

### Strengths and Weaknesses
Strengths:
- The proposed method is easy to understand, implement, and generally applicable.
- It dramatically improves the efficiency of adversarial training for LLMs without significantly compromising model utility.
- The writing is clear, with well-structured formulas that effectively convey the proposed methods.

Weaknesses:
- The experimental evaluation lacks comprehensiveness; comparisons should include more models and diverse safe answers, as well as stronger attack methods beyond GCG, AutoDAN, and PAIR.
- The introduction of additional hyper-parameters complicates practical application for users.
- Some details, such as the description of the proposed method's pipeline and hardware settings, are insufficiently clear, which could hinder reader understanding.

### Suggestions for Improvement
We recommend that the authors improve the comprehensiveness of their experiments by including comparisons across more models and testing multiple safe answers to enhance the generalizability of their findings. Additionally, incorporating evaluations against more advanced attack methods would strengthen the paper's contributions. We also suggest clarifying the method's pipeline and providing explicit descriptions of experimental settings, including hardware specifications, to aid reader comprehension. Finally, addressing the introduction of hyper-parameters and their implications for practical application would be beneficial.