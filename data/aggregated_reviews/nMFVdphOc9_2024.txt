ID: nMFVdphOc9
Title: Rule Based Learning with Dynamic (Graph) Neural Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 4, 3, 3, 3, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for integrating expert knowledge into neural network architectures through rule-based layers, specifically introducing RuleGNNs as an application. The authors evaluate RuleGNNs against state-of-the-art (SOTA) methods, demonstrating competitive performance through empirical studies on synthetic and real-world datasets. Theoretical discussions clarify the assumptions underlying the proposed method, while the experimental results cover a range of alternative methods.

### Strengths and Weaknesses
Strengths:
- The concept of dynamic rule-based layers in neural networks, particularly for graph neural network learning, is innovative and extends existing methods.
- Theoretical discussions and assumptions are clearly articulated.
- The experimental evaluation includes a sufficient variety of alternative methods.

Weaknesses:
- The paper does not adequately address the challenges of generating high-quality rules from expert knowledge, focusing instead on their application.
- There is a lack of clarity regarding the nature of the rules in RuleGNNs and their influence on learning model parameters.
- The discussion of experimental results is insufficient, particularly regarding the superior performance of WL-Kernel on certain datasets.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the challenges of building quality rules and the feasibility of this step in the proposed method. Additionally, clarity around the structure and influence of rules in RuleGNNs should be enhanced. The authors should provide a more thorough analysis of the experimental results, particularly regarding the characteristics of datasets where RuleGNNs may outperform alternatives like WL-Kernel. Furthermore, including ablation studies or hyper-parameter sensitivity analyses would help elucidate how different rules or learning algorithms impact performance. Lastly, we suggest revising the presentation for clarity, particularly in the notation and definitions used throughout the paper.