ID: t3Xj7YD7fL
Title: An NLP Benchmark Dataset for Predicting the Completeness of ESG Reports
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 5, 5, 3, 4, -1
Original Confidences: 4, 4, 4, 4, -1

Aggregated Review:
### Key Points
This paper presents a dataset of ESG reports from Chinese-listed companies, comprising 14,468 reports and over 8,000 manually labeled sentences for topic and quality classification. The authors propose two classification tasks: topic classification and quality classification, aimed at evaluating the completeness of ESG reports. The research demonstrates promising results, achieving approximately 85.66% accuracy in evidence page detection. However, the authors' assumption that completeness is the primary criterion for assessing report quality is questioned, as other factors like accuracy and transparency are also significant.

### Strengths and Weaknesses
Strengths:
- The paper introduces a large and comprehensive dataset, enhancing the field of ESG report analysis.
- High accuracy in evidence page detection indicates reliable identification of relevant ESG information.
- The use of a standard ESG tree for topic annotation is commendable.

Weaknesses:
- The dataset's focus on Chinese-listed companies limits the generalizability of findings and may introduce language and cultural biases.
- The assumption that completeness is the most crucial criterion for report quality is questionable, as other factors are also important.
- Manual annotation, while ensuring accuracy, raises scalability concerns and potential human error.
- Lack of evaluation of Chinese pretrained models in experiments, despite the dataset being in Chinese.
- Some experimental details are unclear, such as the methods used for obtaining results in Table 2.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by considering a more diverse dataset. Additionally, the authors should clarify the relationship between predicting topic and quality labels and evaluating completeness, as this gap may hinder understanding of the proposed tasks' significance. We suggest including evaluations of Chinese pretrained models in the experiments to better align with the dataset's language. Furthermore, the authors should provide more insights into numerical results, including explanations for model performance and misclassifications. Lastly, enhancing the clarity of the methodology and addressing the labeling process's quality will strengthen the dataset's practical impact.