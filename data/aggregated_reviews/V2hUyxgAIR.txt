ID: V2hUyxgAIR
Title: Casting hybrid digital-analog training into hierarchical energy-based learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 8, 7
Original Confidences: 4, 3

Aggregated Review:
### Key Points
This paper presents algorithms for inference and weight updates in deep networks that integrate alternating blocks of feedforward layers and energy-based models (EBMs). The authors address the challenges of combining these components, as feedforward layers are straightforward while EBMs require internal optimization, such as Equilibrium Propagation (EP). The authors suggest that decomposing the model into independent EB blocks can reduce computation time, a point that should be emphasized in the introduction and abstract.

### Strengths and Weaknesses
Strengths:  
- The methods are clearly articulated, and the results are promising.  
- The paper identifies a significant gap in the maturity of EBM implementations, advocating for hybrid approaches.  

Weaknesses:  
- The motivation for combining feedforward and energy-based components is not fully convincing, particularly regarding computational benefits.  
- Several minor errors and ambiguities exist, including references to equations and indexing issues.

### Suggestions for Improvement
We recommend that the authors improve the motivation for their approach by clearly articulating the computational advantages of combining feedforward and energy-based components in the introduction and abstract. Additionally, please clarify why EP is incompatible with common non-stationary operations like activation functions and explore potential solutions. Correct the reference to Eq. 2 in the text and ensure that the indexing of F and w on line 93 is accurate. In Figure 2, consider rescaling the y-axis or plotting on a log scale to better illustrate layer performance differences. Finally, explicitly confirm whether the advantages over purely EBM methods stem from nonlocal operations like batch normalization.