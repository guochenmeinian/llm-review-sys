ID: NMMnxhQm01
Title: The Distributional Hypothesis Does Not Fully Explain the Benefits of Masked Language Model Pretraining
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the distributional property of pre-trained language models, focusing on how masked language model (MLM) pretraining impacts downstream fine-tuning from the perspective of the distributional hypothesis. The authors analyze both synthetic and real-world datasets to assess whether semantic equivalence enhances sample efficiency and generalization capability. They conclude that while semantic equivalence contributes to improved sample efficiency, it does not fully account for the models' generalization capabilities, particularly in the presence of semantic distribution shifts.

### Strengths and Weaknesses
Strengths:
- The paper adopts a novel perspective by analyzing MLM pretraining through the lens of the Distributional Hypothesis.
- The synthetic experiment is well-designed, clearly demonstrating the influence of distributional properties on sample efficiency.
- The paper is well-written and easy to follow, providing valuable insights into the interplay between semantic relationships and model pretraining.

Weaknesses:
- The details of the synthetic data and real tasks are unclear, complicating the interpretation of analyses and figures.
- The real-data experiment yields less clear results, lacking thoroughness in exploring different methods for measuring semantic distance.
- The scope is limited, focusing primarily on simple pattern matching tasks and omitting other potentially relevant relationships, such as hypernyms or causal relationships.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the synthetic data and real tasks to enhance the interpretability of analyses and figures. Additionally, we suggest conducting a more thorough exploration of the real-data experiments, possibly by trying different ways of measuring semantic distance from the pre-trained model. Furthermore, addressing the limitations of the single-token context in section 6.3 by comparing contextualized embeddings could provide deeper insights. Lastly, please specify the correlation measure used in section 6.4 to strengthen the methodological rigor.