ID: HkMCCFrYkT
Title: HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents HDR-GS, a novel framework for high dynamic range (HDR) novel view synthesis that utilizes a Dual Dynamic Range (DDR) Gaussian point cloud model. The authors propose a method that significantly improves upon existing HDR NVS techniques, particularly those based on Neural Radiance Fields (NeRF), by achieving over 1000Ã— faster inference speeds and reducing training time to 6.3% of that required by HDR-NeRF. The framework incorporates a global tone-mapping operation and parallel rasterization processes for efficient rendering of HDR and low dynamic range (LDR) images.

### Strengths and Weaknesses
Strengths:
- HDR-GS demonstrates superior performance, achieving 126 fps compared to the slow inference speeds of previous NeRF-based methods.
- The methodology is well-organized, with comprehensive experiments validating the framework's effectiveness.
- The writing style is clear, aiding reader comprehension of the Gaussian point cloud model and its attributes.

Weaknesses:
- The paper lacks a detailed discussion of the innovations and theoretical foundations behind the model design, which could enhance its persuasiveness.
- The necessity of parallel rendering for loss calculation is not sufficiently justified, and the mathematical explanations in Section 3.2 may not represent a primary innovation.
- The method requires specific data acquisition conditions that may be challenging in practice, such as capturing images with varying exposure settings.

### Suggestions for Improvement
We recommend that the authors improve the manuscript by providing a more thorough analysis of the performance differences between HDR-GS and HDR-NeRF, particularly regarding the substantial speed improvements attributed to 3D Gaussian Splatting. Additionally, the authors should elaborate on their choice to model HDR color first using the initialized DDR Gaussian point cloud, clarifying its impact on HDR and LDR results. Condensing or omitting Section 3.2 could streamline the paper, and justifying the necessity of parallel rendering in their approach would strengthen the argument. Finally, including more details about the experimental setup in Section 4.1, such as the specific perceptual network used for LPIPS, would enhance clarity.