ID: OyTIV57Prb
Title: CAPP-130: A Corpus of Chinese Application Privacy Policy Summarization and Interpretation
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 8, 3, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CAPP-130, an annotated Chinese privacy policy corpus comprising 130 prominent apps, and TCSI-pp, a method for summarizing such policies based on the annotated corpus. The corpus is annotated by legal experts, aimed at enhancing the understanding and summarization of privacy policies. The authors demonstrate that TCSI-pp outperforms large language models in generating comprehensible summaries of privacy policies. The authors propose various applications for the corpus, including text classification and automatic summarization. The manuscript has undergone revisions to address feedback regarding annotation bias, evaluation metrics, and the inclusion of additional related work.

### Strengths and Weaknesses
Strengths:
- The selection and collection process for the policies is robust, with annotations cross-validated by legal experts, achieving a high Cohen's kappa value of 0.907.
- The creation of the CAPP-130 corpus fills a significant gap in privacy policy research, particularly in non-English contexts.
- The dataset's labor-intensive annotation process enhances its value, especially compared to outdated datasets like OPP-115.
- The authors have made substantial revisions in response to feedback, enhancing the clarity and depth of the manuscript.
- The summarization framework is straightforward, utilizing a variety of extraction and generative transformation frameworks.
- The incorporation of diverse evaluation metrics, including Micro-F1, Macro-F1, BERTScore, BARTScore, and Carburacy, strengthens the evaluation framework.

Weaknesses:
- The paper lacks detailed information on the annotation process and data splitting among annotators, raising concerns about the reliability of the inter-annotator agreement and the potential for annotation bias.
- The manuscript lacks detailed documentation on the dataset's availability and maintenance, which could hinder replication efforts.
- The absence of the appendix limits the assessment of experimental results and methodology.
- Some experimental results and methodological details are inadequately addressed, particularly regarding data splitting and the annotation process.
- The evaluation metrics for the summarization are not comprehensive, lacking comparisons with human-generated summaries and robust semantic metrics.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the annotation process by detailing how data was split among annotators and whether overlapping data was used. Additionally, we suggest that the authors provide more detailed explanations of the annotation process and data splitting methodologies to enhance transparency. Including Sentence-BERT in the experiments could enhance results. The authors should also provide the missing appendix to allow for a thorough evaluation of the methodology and results. Furthermore, we suggest conducting user studies to quantitatively assess the readability of the generated summaries and comparing TCSI-pp's outputs with human expert summaries to better understand its effectiveness. Lastly, addressing potential ethical concerns regarding information loss and misinterpretation in automated summarization is crucial, and it would be beneficial to explore the limitations of the proposed method more thoroughly, particularly regarding its applicability beyond Chinese privacy policies. Finally, we encourage the authors to consider including more examples of the corpus's applications in the main text, rather than relegating them to an appendix.