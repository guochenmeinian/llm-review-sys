ID: r8M9SfYMDi
Title: Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called dataset decomposition (DD) aimed at enhancing the pre-training of large language models (LLMs). The authors propose organizing training datasets into buckets with sequences of uniform length from individual documents, allowing for variable sequence length training. This approach mitigates inefficiencies associated with traditional fixed-length token sequences and cross-document attention, leading to improved training times and model performance. The authors also introduce a Grow-P2 curriculum that enhances training efficiency and stability.

### Strengths and Weaknesses
Strengths:
1. The proposed Grow-P2 curriculum is beneficial for practitioners pretraining large language models.
2. The method is straightforward and effective, with extensive experimental analyses supporting its claims.
3. The results demonstrate promising improvements in task performance and reduced training time.

Weaknesses:
1. The paper primarily presents empirical results without sufficient theoretical novelty, as the proposed techniques largely extend existing strategies like bucketing and curriculum learning.
2. The writing quality requires improvement, particularly in clarifying concepts such as the "penalty" associated with document lengths and the pacing mechanism for the cyclic curriculum.
3. The exploration of training curriculum is limited to sequence lengths, neglecting other dimensions of curriculum design.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the pacing mechanism for the cyclic curriculum by providing pseudo-code or mathematical formalism. It is essential to explain how sampling odds change when transitioning from easy to hard examples. Additionally, we suggest enhancing the discussion on the implications of learning rate schedules in relation to the cyclic curriculum, particularly addressing concerns about potential implicit biases due to learning rate decay. Finally, a more detailed analysis of why DD performs better in long-context scenarios should be included to strengthen the paper's contributions.