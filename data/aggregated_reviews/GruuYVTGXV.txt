ID: GruuYVTGXV
Title: Dual Critic Reinforcement Learning under Partial Observability
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 3, 5, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a dual-critic architecture for reinforcement learning in partially observable Markov decision processes (POMDPs). The authors propose a method that integrates a standard critic, which utilizes historical information, and an unbiased asymmetric critic that incorporates both history and state information. The dual value function, a convex combination of these two critics, aims to reduce variance and improve learning efficiency. The effectiveness of this approach is validated through experiments in various environments, including MiniGrid and Box2D. Additionally, the paper discusses the dynamics of multi-agent reinforcement learning (MARL), particularly focusing on the implications of partial observability in POMDPs. The authors propose that the centralized critic, represented as $V(h, s)$, ensures an unbiased policy that converges to the optimal solution. However, they acknowledge that in POMDPs, the state often does not fully encompass history, leading to potential bias in the policy and hindering convergence. The authors clarify their dynamic weighting mechanism in DCRL, which aims to reduce variance while maintaining the advantages of state information.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and easy to follow, with a clear motivation for the dual-critic approach.
- The theoretical foundation is sound, demonstrating that the dual critic reduces variance while maintaining unbiasedness.
- Experimental results show significant improvements over baseline methods, achieving faster convergence and higher returns in complex scenarios.
- The authors provide a clear theoretical foundation for their claims, referencing Theorem 4.2 and supporting their arguments with empirical results.
- They demonstrate a thoughtful approach to balancing the critics in DCRL, emphasizing the dynamic adjustment of weights based on the advantage $\delta(h, a)$.

Weaknesses:
- The originality of the proposed method is somewhat limited, as it builds on existing techniques without sufficiently distinguishing its contributions.
- The choice of when to clip to $\beta = 0$ lacks clarity, and the rationale behind this heuristic is not well-explained.
- There is insufficient discussion on the implications of variance reduction and the conditions under which the dual critic performs optimally, particularly regarding the weighting mechanism between the critics.
- The paper lacks clarity in some descriptions, particularly regarding the implications of the dynamic weighting mechanism and the role of $\beta$.
- There is a potential challenge in accurately identifying training stages, which may lead to suboptimal solutions if not managed properly.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the clipping of $\beta$ and provide a more detailed explanation of its significance. Additionally, a discussion on the potential numerical instabilities introduced by the jump-discontinuity in the dual advantage and critic should be included. We suggest that the authors explore comparisons with commonly used POMDP methods, such as Dreamer and Believer, and include stronger baselines that leverage state information while addressing the potential increase in training time associated with pre-training representation models. Finally, a more thorough analysis of when the proposed method is most beneficial would enhance the paper's contributions.