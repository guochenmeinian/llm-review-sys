ID: WPxa6OcIdg
Title: Estimating Epistemic and Aleatoric Uncertainty with a Single Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 5, 6, 6, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents HyperDDPM, a method that applies hypernetworks to estimate both aleatoric and epistemic uncertainty in diffusion models. The authors propose a framework that combines diffusion networks to sample predictions from a single set of model weights and a Bayesian Hyper Network to generate sets of weights for the main model. They validate their approach on synthetic data and real-world applications, including medical imaging and weather forecasting, demonstrating its ability to disentangle uncertainty components.

### Strengths and Weaknesses
Strengths:
1. The paper is clear and well-written, making it easy to read.
2. It addresses an important topic in uncertainty quantification for generative models.
3. The proposed method shows promising empirical results on both synthetic and real-world datasets.
4. The authors provide a codebase for transparency and reproducibility.

Weaknesses:
1. The paper lacks a Contributions section that clearly states the novel claims and results.
2. Insufficient evidence is provided to demonstrate the ability to disentangle epistemic and aleatoric uncertainty.
3. The theoretical justifications do not align well with the experimental approach, containing questionable claims.
4. There is a lack of comprehensive runtime comparisons and error bars in the experimental results.
5. The technical novelty is limited, primarily applying existing methods in a new context without highlighting this in the abstract.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by including a dedicated Contributions section. Additionally, the experimental section should provide more robust evidence for disentangling uncertainty components, perhaps by independently varying noise levels and dataset sizes. We suggest that the authors clarify the theoretical justifications and ensure they align with the experimental results. Furthermore, including a summary of runtime comparisons and error bars would enhance the empirical evaluation. Lastly, we encourage the authors to consider incorporating more contemporary baseline methods beyond MC Dropout to strengthen their comparisons.