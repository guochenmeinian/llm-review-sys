ID: 8aDG51pxFc
Title: No Change, No Gain: Empowering Graph Neural Networks with Expected Model Change Maximization for Active Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 7, 7, 7, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel active learning method for graph neural networks (GNNs) that extends the classic expected model change maximization (EMCM) principle to graph-structured data. The authors propose a new Bayesian interpretation of the SGC model, unifying the training process of GNNs and addressing challenges in applying EMCM to graphs. The method includes approximation techniques for efficient computation and reveals a theoretical connection to expected error minimization. Comprehensive experiments demonstrate the method's efficiency and effectiveness.

### Strengths and Weaknesses
Strengths:
1. Originality: The paper is the first to adapt the EMCM method for graph-structured data, providing a new Bayesian perspective on GNN training.
2. Quality: The work is well-motivated with a strong theoretical foundation, including an elegant closed-form solution for approximated expected model change and convincing experimental results.
3. Clarity: The paper is easy to follow, with clear notations and concise statements, supported by illustrative case studies.
4. Significance: The contributions connect the active learning and GNN communities, offering a performance-based active learning method that addresses existing literature gaps.

Weaknesses:
1. The background on the EMCM method could be elaborated, particularly the intuition behind Eq.(7).
2. Efficiency comparisons are limited to three small-scale datasets; testing on larger datasets like the ogbn dataset is recommended.
3. The conclusion is brief; more discussion on future work is needed.
4. Minor typographical errors need correction, such as "embedding s" to "embeddings."
5. Additional discussions on the deep connections between Bayesian interpretations and the proposed method are necessary.

### Suggestions for Improvement
We recommend that the authors improve the background knowledge on the EMCM method, particularly by providing more context and intuition behind Eq.(7). Testing the proposed method on larger datasets, such as the ogbn dataset, would enhance the efficiency comparison. The conclusion should be expanded to include more insights on future work. Additionally, please correct typographical errors and address the need for deeper discussions on the connections between the Bayesian interpretations of GNNs and the proposed active learning method. An ablation study on the approximations made in the model would also be beneficial, particularly regarding the impact of retained eigenvalues/vectors on performance.