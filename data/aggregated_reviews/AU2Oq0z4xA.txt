ID: AU2Oq0z4xA
Title: IMU2CLIP: Language-grounded Motion Sensor Translation with Multimodal Contrastive Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a pre-training approach to align three modalities of data: IMU (Inertial Measurement Unit), video, and text. The authors utilize a customized architecture combining CNN and RNN layers for IMU encoding, while video and text inputs are encoded using the pre-trained CLIP model. The unified model is evaluated on two tasks: retrieving IMU from text and mapping IMU to actions, demonstrating improved performance through the integration of video and text modalities. 

### Strengths and Weaknesses
Strengths:
- The alignment of IMU with other modalities is a relatively unexplored area, and the proposed approach achieves strong results on key benchmarks.
- The method offers a low-resource solution for media search, leveraging wearable technology effectively.

Weaknesses:
- The novelty of the pre-training method is questioned, as it appears to be an extension of existing contrastive learning frameworks without addressing specific challenges.
- There is ambiguity regarding the definition of pre-training and the datasets used, as well as the role of CLIP in extracting video embeddings.

### Suggestions for Improvement
We recommend that the authors clarify the definition of pre-training and specify the datasets utilized for this purpose. Additionally, we suggest addressing the concerns regarding the novelty of the approach by explicitly detailing the unique challenges that the proposed method overcomes compared to existing frameworks. Furthermore, it would be beneficial to include comparisons with prior motion datasets to substantiate the effectiveness of the IMU2CLIP representation across established benchmarks.