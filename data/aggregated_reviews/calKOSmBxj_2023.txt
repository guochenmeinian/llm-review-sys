ID: calKOSmBxj
Title: HoK3v3: an Environment for Generalization in Heterogeneous Multi-agent Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multi-agent reinforcement learning benchmark, HoK3v3, based on the popular MOBA game Honor of Kings, extending the previous 1v1 environment to a 3v3 format. The authors emphasize the environment's accessibility and ease of use for researchers, providing a step-by-step tutorial for implementation. The proposed environment includes diverse character types and team compositions, enhancing the challenges of training agents. Experiments demonstrate that algorithms like PPO, MAPPO, and a communication-based PPO variant can effectively train against a baseline AI, highlighting the difficulties of generalizing strategies across varying opponent lineups. The authors also address various concerns regarding the limitations of existing RL methods, the clarity of hierarchical action spaces, and the need for additional baseline algorithms. The environment is designed to be accessible, requiring only a single GPU and a substantial number of CPU cores, and aims to facilitate hierarchical research in MARL.

### Strengths and Weaknesses
Strengths:
- The 3v3 environment significantly expands the previous 1v1 setup, introducing complex team composition challenges.
- The paper introduces the first publicly available MOBA environment for the RL community.
- Comprehensive documentation and tutorials enhance accessibility for researchers.
- The GPU benchmarking indicates that the training process is accessible to a broad range of researchers, minimizing infrastructure barriers.
- The experiments effectively illustrate the generalization challenges faced by agents in diverse team compositions.

Weaknesses:
- The claim regarding the limitations of existing RL methods is overextended.
- The paper lacks a discussion on the ethical implications of deploying RL agents in human games, particularly regarding their potential to replace human players.
- Clarity regarding the hierarchical action space and the necessity of sub-tasks requires improvement.
- Comparisons with existing environments, such as Dota 2 and StarCraft, are limited, particularly concerning vectorization capability and speed.
- Limited baseline algorithms tested may not represent the full spectrum of MARL performance.

### Suggestions for Improvement
We recommend that the authors enhance the discussion on the significance of sub-task results, particularly how they contribute to positive transfer to the main task. Additionally, the authors should improve the clarity of the hierarchical action space by providing more detailed explanations and illustrations in the main text. It would be beneficial to enhance the discussion on the scientific hypotheses that can be explored through the defined sub-tasks, including concrete examples of their value in MARL research. Including a wider range of baseline algorithms in the experiments would provide a more comprehensive evaluation of the environment's performance. Furthermore, the authors should address the societal implications of using RL agents in multiplayer games, considering the potential effects on player skills and accessibility. Lastly, improving documentation for Linux users and ensuring that all pretrained models and necessary documentation are readily available for review would enhance usability and facilitate reproducibility.