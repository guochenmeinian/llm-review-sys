ID: DN4aStIEXS
Title: Toward the Evaluation of Large Language Models Considering Score Variance across Instruction Templates
Conference: EMNLP/2024/Workshop/BlackBoxNLP
Year: 2024
Number of Reviews: 2
Original Ratings: -1, -1
Original Confidences: 4, 5

Aggregated Review:
### Key Points
This paper investigates the robustness of evaluation in large language models (LLMs) by examining their sensitivity to prompt perturbations. The authors aim to enhance model evaluation by considering performance across multiple instruction templates, proposing a new dataset and a metric called the Sharpe score to account for variability across prompts. However, the paper includes extensive discussions on fine-tuning setups and model specifics that may detract from the core research focus.

### Strengths and Weaknesses
Strengths:
- The research question regarding LLMs' sensitivity to prompt variation in natural language understanding (NLU) is significant and underexplored.
- The paper contributes valuable insights into the use of plain versus instructed models in zero-shot NLU.
- It systematically compares decoding methods and advocates for constrained decoding in zero-shot evaluations.
- A scalable metric that penalizes high variance across templates is proposed.

Weaknesses:
- The paper lacks effective experimental design to address the critical research question regarding the fragility of traditional accuracy-based metrics.
- The clarity of Fig. 1 is insufficient, and it is unclear whether the generation of multiple prompt templates is manual or automated.
- The choice of Japanese as a complementary language raises questions about its necessity and the convenience of existing datasets.

### Suggestions for Improvement
We recommend that the authors improve the structure of the paper by focusing on the exhaustiveness of the prompt perturbation experiments and validating the new metrics based on $\mu$ and $\sigma^2$. Additionally, enhancing the clarity of Fig. 1 and providing more substantial changes in evaluation templates would be beneficial. The authors should also consider generating multiple templates automatically, potentially using models like GPT-4, to strengthen their methodology.