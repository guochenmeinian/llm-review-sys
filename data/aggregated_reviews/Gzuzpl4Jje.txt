ID: Gzuzpl4Jje
Title: Continual Learning for Multilingual Neural Machine Translation via Dual Importance-based Model Division
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to address catastrophic forgetting in multilingual neural machine translation (MNMT) by proposing a dual importance-based parameter division approach. The authors aim to effectively learn new languages while maintaining translation performance on existing languages. Experimental results indicate competitive BLEU scores for both new and original languages across various settings, demonstrating the method's effectiveness.

### Strengths and Weaknesses
Strengths:
- The paper provides a practical solution to catastrophic forgetting, supported by thorough experiments and analysis.
- The incremental model tuning strategy does not require original training data, enhancing its applicability.
- The writing is clear and the methodology is easy to follow.

Weaknesses:
- The method necessitates complex training pipelines, including an additional phase for model pruning.
- Experimental results show significant performance degradation on original language pairs, raising concerns about the model's reliability.
- The rationale for selecting specific pre-trained models is unclear, and further analysis on parameter importance is needed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their rationale for model selection and provide additional experiments involving unseen language pairs. It would be beneficial to conduct an ablation study comparing dual-importance with single-step pruning. Additionally, we suggest including results from a trainable neural metric (e.g., COMET) to validate the BLEU score findings. Finally, addressing the performance degradation on original language pairs with detailed explanations would enhance the paper's robustness.