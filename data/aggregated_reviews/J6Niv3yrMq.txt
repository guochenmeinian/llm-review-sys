ID: J6Niv3yrMq
Title: Glance and Focus: Memory Prompting for Multi-Event Video Question Answering
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 3, 7, 6, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Glance-Focus model for video question answering (VideoQA), which mimics human reasoning by first glancing at a video to generate dynamic event memories and then focusing on relevant moments for answering questions. The model supports both unsupervised and supervised learning methods for memory generation, utilizing bipartite matching for supervised cases. It employs a multi-level cross-attention mechanism to enhance the interaction between questions and video frames. The authors designed two loss functions for supervised learning ($L_{L1}$ and $L_{cls}^{sup}$) and adapted them for unsupervised learning by replacing them with $L_{iou}$, $L_{cls}^{uns}$, and $L_{cert}$. The model is evaluated on three benchmarks—STAR, AGQA, and EgoTaskQA—achieving state-of-the-art performance, although claims of SOTA performance on ActivityNet-QA are questioned. The authors assert their model's robustness against overfitting and emphasize the effectiveness of their glance module and focus strategy through extensive ablation studies.

### Strengths and Weaknesses
Strengths:
1. The model's approach to simulating human-like reasoning through glancing and focusing is innovative and has significant potential for addressing multi-event VideoQA challenges.
2. The flexibility of the implementation allows for processing both annotated and unannotated videos, which is a practical advantage.
3. The use of information maximization for unsupervised event generation is a pioneering effort in VideoQA.
4. The authors provide a clear and structured presentation of their loss functions and experimental results.
5. The method demonstrates promising performance across multiple benchmarks, indicating potential for cross-dataset learning.
6. The rebuttal addresses reviewer concerns effectively, leading to increased scores from several reviewers.

Weaknesses:
1. The writing quality needs improvement, with long and complex sentences that hinder clarity.
2. The assumptions regarding the number of events (C) in videos require further clarification, particularly whether C is consistent across videos.
3. The paper lacks detailed analyses on the relationships between unsupervised learning and key event memory generation.
4. Claims of SOTA performance on ActivityNet-QA are questioned, with suggestions that more recent models outperform the authors' method.
5. Lack of clarity regarding specific experimental settings and comparisons with other models, particularly concerning the JustAsk baseline and feature usage.
6. The evaluation may be biased due to training on specific datasets, raising concerns about overfitting to dataset statistics.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing to enhance readability. Additionally, we suggest conducting a thorough ablation study on the impact of different choices for C on performance. It would be beneficial to elaborate on the reasoning behind the distribution of category labels in unsupervised event generation. We also encourage the authors to improve the precision of their claims regarding SOTA performance, particularly on ActivityNet-QA, by providing a more calibrated comparison with recent models like FrozenBiLM. Furthermore, including detailed information on the specific features, model configurations, and task protocols used in their experiments would enhance transparency. Lastly, addressing the potential societal impacts of the proposed method and exploring the scalability of the Glance-Focus model beyond the current datasets would strengthen the paper.