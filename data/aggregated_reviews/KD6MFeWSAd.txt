ID: KD6MFeWSAd
Title: The probability flow ODE is provably fast
Conference: NeurIPS
Year: 2023
Number of Reviews: 25
Original Ratings: 5, 4, 6, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents polynomial-time convergence guarantees for probability flow ODEs in score-based generative models, achieving improved dimension dependence compared to previous works on DDPM. The authors propose a modified process that interleaves deterministic integration with stochastic correction steps, inspired by existing stochastic frameworks. Additionally, the paper explores the linear growth of error in total variation (TV) distance within diffusion models, introducing a novel theoretical framework that incorporates a Wasserstein-to-TV regularization effect. The authors provide proofs demonstrating that error terms scale linearly with the total time of the process and address instability issues in high dimensions by adjusting density calculations in their code.

### Strengths and Weaknesses
Strengths:
1. The paper is well-structured and clearly written, making complex concepts accessible.
2. The theoretical results are solid and contribute new insights into the capabilities of ODE-based methods compared to SDEs.
3. The introduction of a novel theoretical framework enhances understanding of error dynamics in diffusion models.
4. The authors effectively clarify the significance of their contributions and address reviewer concerns regarding theoretical aspects.

Weaknesses:
1. The work lacks empirical validation, with only a simple numerical experiment presented in the appendix, raising questions about the practical applicability of the proposed methods.
2. The inclusion of an additional stochastic corrector step introduces extra computational costs without sufficient justification.
3. Comparisons with existing sampling methods and a more thorough exploration of the proposed algorithms' performance on real-world datasets are missing.
4. The presentation of toy experiments has been criticized as not representative enough of the paper's contributions.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including a brief description of numerical experiments in the main text, rather than relegating them to the appendix. It is essential to clarify the conclusions drawn from the numerical experiments, particularly regarding the convergence of the method and the interpretation of terms like "approximately." Additionally, we suggest conducting experiments on real-world datasets using pre-trained diffusion models to substantiate the theoretical claims. Consideration should also be given to removing less representative toy experiments, as the primary evaluation should focus on the mathematical interest of the results and proof techniques. Finally, incorporating high-dimensional synthetic simulations could bolster the practical relevance of the theoretical findings, drawing inspiration from existing literature on high-dimensional sampling.