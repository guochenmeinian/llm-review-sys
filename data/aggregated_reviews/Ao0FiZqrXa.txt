ID: Ao0FiZqrXa
Title: Simple and Fast Distillation of Diffusion Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 8, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a fast-training accelerate sampling algorithm based on the distillation paradigm, which performs trajectory matching across all time points (t=80-0.006). The approach avoids the overhead of bi-level optimization using the `detach()` operation in PyTorch and enhances performance by adjusting hyperparameters such as the loss function ($L_1$), step-condition, and analytical first step. The authors propose a simplified fast distillation method that reduces redundant time steps in training, achieving good results in a short time compared to existing methods.

### Strengths and Weaknesses
Strengths:
1. The algorithm's results are impressive, outperforming all known distillation-based accelerated sampling algorithms.
2. The experiments are thorough and well-analyzed, particularly the guidance scale analysis on Stable Diffusion.
3. The paper is exceptionally well-written, providing clarity and inspiration, especially with the introduction of the SFD algorithm and innovative training techniques.

Weaknesses:
1. While the algorithm reduces training costs, it may lower the maximum performance threshold, raising questions about potential performance if training duration is extended.
2. Some techniques lack innovation and appear as technical explorations rather than novel contributions.
3. Certain figures are difficult to understand and require better explanations.
4. The selection of hyperparameters, such as t_min, seems arbitrary, and the performance of other training methods like CTM under the same conditions is not reported.

### Suggestions for Improvement
We recommend that the authors improve the clarity of figures, particularly fig.6, by providing more detailed explanations. Additionally, conducting more ablation experiments or clarifying the criteria for selecting hyperparameters would enhance the paper. Including preliminary experiments to explore whether SFD can accelerate DiT would be beneficial. Finally, we suggest that the authors discuss the implications of the forward diffusion process on SFD's performance and consider evaluating the method's effectiveness across different noise schedules and resultant trajectories.