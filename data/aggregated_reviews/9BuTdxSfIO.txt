ID: 9BuTdxSfIO
Title: kNN-CM: A Non-parametric Inference-Phase Adaptation of Parametric Text Classifiers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a kNN-CN method aimed at enhancing text classification performance by integrating k-Nearest Neighbors (KNN) with language models during the inference phase. The authors demonstrate improved performance across various tasks, including 8 SuperGLUE tasks, 3 adversarial natural language inference datasets, 11 question-answering datasets, and 2 sentiment classification datasets. The proposed method can serve as a plug-in for RoBERTa, facilitating out-of-domain classification, domain adaptation, and low-resource classification.

### Strengths and Weaknesses
Strengths:
- The integration of a nonparametric KNN method into a classification model shows potential for improving test-phase adaptation performance.
- The experiments are extensive, systematic, and provide persuasive results across a diverse range of datasets.
- The paper is well-written and presents its findings clearly.

Weaknesses:
- The technical novelty is limited, as KNN methods have been extensively studied in related fields, leading to concerns about the incremental nature of the work.
- There is a lack of comparison with state-of-the-art (SOTA) baselines specifically designed for out-of-domain and low-resource scenarios.
- The experiments primarily focus on a single baseline, which may not adequately represent the contributions of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing insights into the unique aspects of the kNN-CN approach to address concerns about its contribution. Additionally, it is crucial to include comparisons with relevant SOTA baselines in out-of-domain and low-resource contexts. We suggest that the authors consider incorporating more pragmatic baselines from the semi-parametric literature to strengthen their experimental results. Furthermore, the discussion of the Î» hyperparameter should be expanded, including its values in the results table for each experiment. Lastly, we encourage the authors to clarify whether the applicability of kNN-CN extends beyond masked language models to autoregressive models and to elaborate on potential extensions to conversation modeling and continual learning, or to remove these references if further details cannot be provided.