ID: YSMLVffl5u
Title: CELLE-2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image Transformer
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CellBERT-E, a transformer-based model designed to generate protein localization images from amino acid sequences and nucleus images. The model utilizes a VQGAN tokenizer for images and a pretrained protein language model for amino acid sequences, employing mask modeling for pretraining. Experiments indicate that CellBERT-E performs reasonably well in protein localization prediction and can generate amino acid motifs from image inputs. The model's non-autoregressive generation approach enhances efficiency compared to the previous CELL-E model.

### Strengths and Weaknesses
Strengths:
1. The application of a transformer-based generative model to protein localization prediction is innovative.
2. The non-autoregressive generation method improves efficiency over prior models.
3. The model's capability to generate short amino acid sequences from nucleus images is a noteworthy exploration.

Weaknesses:
1. The absence of baseline models limits the comparative analysis; the authors only evaluate different settings of CellBERT-E without including other relevant baselines like CELL-E.
2. Current experimental settings do not sufficiently validate the impact of hyperparameters on model performance.
3. The evaluation metrics employed may not adequately capture the nuances of protein localization prediction, as they primarily focus on general image/sequence generation metrics.

### Suggestions for Improvement
We recommend that the authors improve their comparative analysis by including baseline models, particularly CELL-E, to provide a clearer context for CellBERT-E's performance. Additionally, further validation of hyperparameter choices is necessary; specifically, the authors should clarify the rationale behind the architecture settings in Tables 1 and 2. We suggest exploring domain-specific evaluation metrics for protein localization prediction to complement the current metrics. Furthermore, the authors should consider data augmentation strategies to mitigate potential overfitting due to limited training data. Lastly, we encourage the authors to investigate the effects of random sampling on cosine similarity in in-filling tasks to establish a more robust evaluation baseline.