ID: OcaifDZKkA
Title: Active Learning for Natural Language Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a systematic study of Active Learning (AL) for Natural Language Generation (NLG), exploring a diverse set of tasks and selection strategies. The authors analyze the performance of existing AL strategies, revealing inconsistencies in their effectiveness compared to random selection. The study categorizes AL strategies into representativeness and informativeness and applies them to four primary tasks: paraphrase generation, style transfer (formality), summarization, and question generation, utilizing the Flan-T5 pre-trained model.

### Strengths and Weaknesses
Strengths:
- The paper provides a novel and systematic exploration of AL for NLG, laying a foundation for future research.
- It offers valuable insights into the performance of AL strategies, highlighting their inconsistencies and the need for new methods.
- The comparative analysis of strategies and the discussion on the complexities of human language contribute significantly to the field.

Weaknesses:
- The findings are based on a single base model and specific hyperparameters, which may limit the generalizability of the results.
- The study does not fully address real-world dataset challenges, such as variations in dataset quality and temporal drifts in data distributions.
- Hyper-parameter tuning is not feasible in the context of this large AL study, potentially affecting the applicability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by incorporating multiple base models and varying hyperparameters in their experiments. Additionally, we suggest that the authors address the inherent gaps between their AL experiments and real-world applications by considering factors such as dataset quality and temporal shifts in data distributions. Finally, we encourage the authors to devise novel algorithms for utilizing unlabeled data to enhance model performance after fine-tuning.