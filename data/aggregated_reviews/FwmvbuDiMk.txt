ID: FwmvbuDiMk
Title: Data Quality in Imitation Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 2, 6, 6, 3, 7, 5, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 2, 4, 4, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of data quality in imitation learning (IL), proposing metrics based on action divergence and transition diversity to evaluate datasets. The authors argue that improving dataset quality is crucial for mitigating distribution shift issues, which are prevalent in IL. They explore the relationship between these metrics and the performance of learned policies, emphasizing the importance of action consistency in expert demonstrations.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with clear descriptions and a comprehensive related work section.
- The proposed metrics for data quality are interesting and provide a novel perspective on dataset evaluation in IL.
- Empirical evidence supports the significance of the proposed metrics, particularly regarding the effects of system noise on training effectiveness.

Weaknesses:
- The contributions lack novelty, as some theoretical results have been previously established in the literature.
- Key properties of the proposed metrics may require running the learned policy, which diminishes their practical applicability.
- The paper suffers from repetition and lacks a limitations section, which could provide a more balanced view of the work's scope and applicability.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contributions by clearly distinguishing their work from existing literature, particularly regarding Theorem 4.1 and transition diversity. Additionally, we suggest reducing redundancy throughout the paper and expanding the limitations section to address the practical challenges of measuring data quality. Including more diverse experiments and clarifying the implications of their findings on dataset curation would enhance the paper's impact. Finally, we encourage the authors to provide practical insights on how their metrics can guide data collection in IL.