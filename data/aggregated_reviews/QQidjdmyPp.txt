ID: QQidjdmyPp
Title: Fractal Landscapes in Policy Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 4, 9, 7, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the dynamics of policy gradient training in reinforcement learning, arguing that the policy gradient often does not exist due to the lack of regularity of the training objective. The authors introduce the "maximum Lyapunov exponent" (MLE), which captures the sensitivity of trajectories to policy parameters, and derive a threshold beyond which sufficient regularity cannot be guaranteed. They estimate the Hölder coefficient of the training objective from sample trajectories and validate through numerical simulations that this coefficient correlates with the stability of policy gradient training. Additionally, the paper explores the policy gradient landscape in continuous state-action MDPs, asserting that it can be non-smooth, particularly when the MDP's maximum Lyapunov exponent exceeds \( -\log \gamma \). The authors clarify that maximal Lyapunov exponents indicate the smoothness of the landscape around specific points and emphasize that policy gradient methods struggle in fractal landscapes due to the absence of gradients. They also address concerns regarding potential 'false negatives' in their proposed tests and acknowledge the need for further exploration of how the fractal landscape impacts reward performance.

### Strengths and Weaknesses
Strengths:  
- The paper addresses important and underexplored problems in deep reinforcement learning and policy optimization landscapes for continuous state-action MDPs.  
- The chaos-theory-inspired approach and the approximation of the Hölder exponent from samples are relevant contributions.  
- The empirical results demonstrate a clear correlation between the estimated Hölder coefficient and training stability, supported by worked examples and simulations.  
- The authors effectively clarify misconceptions regarding the maximum Lyapunov coefficient and its relevance to algorithm performance.  
- The presentation is comprehensive, with well-explained theoretical results and empirical validation, along with a thorough literature review.

Weaknesses:  
- The main theoretical result suggests that a small enough Lyapunov exponent guarantees smoothness of the policy gradient objective but does not prove the converse, leaving ambiguity regarding large MLE values.  
- There is a disconnect between the stated results and the claim of non-smoothness, particularly regarding the relationship between the Hölder exponent and the non-smoothness of \( J \).  
- The proofs contain potential flaws that hinder clarity and confidence in the results, with specific inequalities and logical connections being questioned.  
- The exposition on the fractal properties of the optimization landscape is insufficient, relegated to the appendix without clear connections to the main results.  
- The paper lacks a thorough demonstration of how the landscape influences reward performance, which could strengthen its impact.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions, particularly for the Maximum Lyapunov Exponent and the divergence terms in Section 3.3. Additionally, we suggest providing more robust justification for the claim regarding Lipschitz continuity and its implications for policy optimization. It would be beneficial to clarify the assumptions made in estimating the Hölder exponent and to include more experimental details, such as the specific algorithms used and the handling of random seeds. We also recommend improving the clarity of the connection between the Hölder exponent and the non-smoothness of the objective function, possibly by providing a relevant theorem that links the maximum Hölder coefficient to non-smoothness. Revisiting the proofs to address the highlighted inequalities and logical gaps would ensure that the arguments are coherent and verifiable. Furthermore, integrating the discussion of fractal theory into the main text rather than the appendix, explicitly connecting it to the paper's results, would enhance the manuscript. Finally, incorporating a discussion on how the reward performance is influenced by the fractal landscape would underscore the significance of identifying such landscapes and enhance the paper's relevance to practitioners in reinforcement learning.