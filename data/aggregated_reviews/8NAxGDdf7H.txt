ID: 8NAxGDdf7H
Title: Few-Shot Class-Incremental Learning via Training-Free Prototype Calibration
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a strategy called Training-frEE calibratioN (TEEN) for Few-Shot Class-Incremental Learning (FSCIL), aimed at enhancing the discriminability of new classes by fusing new prototypes with weighted base prototypes. The authors observe that novel classes are often misclassified as base classes and propose a calibration strategy to mitigate this issue. TEEN shows remarkable performance improvements over baseline methods in various datasets, including miniImageNet, CUB, and CIFAR.

### Strengths and Weaknesses
Strengths:  
- The paper effectively addresses the challenging FSCIL scenario, providing a simple yet effective calibration strategy that enhances performance.
- The methodology is well-motivated, with clear experimental details that facilitate reproducibility.
- The authors demonstrate a commitment to exploring underrepresented aspects of few-shot learning, focusing on novel class performance metrics.

Weaknesses:  
- The paper introduces hyperparameters that require exhaustive searching for each dataset, raising concerns about the method's robustness across different datasets and incremental procedures.
- There is a lack of cross-dataset experiments to validate the proposed method's effectiveness in transferring knowledge.
- The novelty of the approach is questioned, as the concept of connecting base and novel classes has been previously explored in the literature.
- The paper lacks a comprehensive comparison with state-of-the-art methods in few-shot learning scenarios, limiting its contribution to the field.

### Suggestions for Improvement
We recommend that the authors improve the robustness of the method by conducting more extensive ablation studies on the hyperparameters across various datasets. Additionally, we suggest providing a more detailed justification for the design of the calibration method, including empirical visualizations or theoretical analyses to support the choice of linear interpolation. Furthermore, we encourage the authors to include comparisons with state-of-the-art methods in few-shot learning to better contextualize their contributions and enhance the comprehensiveness of their findings.