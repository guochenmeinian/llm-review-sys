ID: aIyNLWXuDO
Title: Transformers Can Do Arithmetic with the Right Embeddings
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 6, 7, -1, -1, 6, 4, 9
Original Confidences: 4, 3, 4, -1, -1, 4, 4, 4

Aggregated Review:
### Key Points
This paper presents a novel encoding scheme, termed “abacus embedding,” aimed at addressing the limitations of transformers in representing positional information, which is vital for arithmetic tasks. The authors propose an ad-hoc positional embedding that encodes the location of each digit relative to the start of the number, enhancing the transformer's ability to align digits. The effectiveness of this method is evaluated through addition, multiplication, and sorting tasks, particularly focusing on out-of-distribution (OOD) test cases. The paper also explores the integration of recurrent blocks to improve performance further.

### Strengths and Weaknesses
Strengths:
- The problem addressed is well-motivated and relevant, with the proposed solutions yielding impressive performance.
- The experiments are well-designed and comprehensive, confirming the conjectures through extensive testing.
- The originality of the abacus embeddings and the clarity of presentation contribute to the paper's technical soundness.

Weaknesses:
- The fixed hyperparameter k for abacus embeddings limits the flexibility and generalizability of the encoding scheme.
- The paper lacks a discussion on the choice of k and its impact on performance, particularly regarding higher values.
- The effectiveness of the method for tasks beyond addition, such as multiplication and sorting, remains inadequately explored, raising questions about its broader applicability.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the hyperparameter k, including how its value is determined and the potential benefits of varying it. Additionally, it would be beneficial to investigate the possibility of implementing a unified model capable of learning multiple tasks simultaneously, rather than relying on ad-hoc models. The authors should also delve deeper into the observed performance discrepancies between addition and multiplication, particularly regarding OOD generalization. Furthermore, we suggest that the authors clarify the limitations of abacus embeddings in relation to other arithmetic operations and consider comparing their approach with recent proposals like CoPE to evaluate its relative effectiveness. Lastly, enhancing the clarity of the writing, especially in sections discussing recurrent architectures, would improve the overall presentation.