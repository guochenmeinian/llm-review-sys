ID: UIIi9hBNW8
Title: "You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the capabilities of GPT-3, GPT-4, and ChatGPT in generating Abstract Meaning Representation (AMR) structures. The investigation focuses on both zero-shot and few-shot scenarios, utilizing a comprehensive evaluation framework that includes high-level and low-level criteria. The main contribution is a qualitative analysis revealing that while these language models can identify some core aspects of semantic structure, they exhibit significant limitations in producing fully accurate parses. The authors also explore whether the models have learned from previous AMR data, finding no significant differences in performance.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, structured, and easy to follow.
- It introduces a comprehensive evaluation framework for assessing AMR parses generated by LLMs.
- The analysis provides valuable insights into the linguistic capabilities and limitations of the models.

Weaknesses:
- The small test set of only 30 sentences raises concerns about potential biases in the results.
- There is a lack of quantitative analysis to evaluate the accuracy of the language models.
- The evaluation of generated AMR graphs lacks clarity, and the criteria used for assessment are confusing.

### Suggestions for Improvement
We recommend that the authors improve the quantitative analysis by incorporating established AMR evaluation metrics such as SEMBLEU and Smatch to provide a clearer assessment of the models' performance. Additionally, we suggest that the authors include a more detailed categorization of the errors produced by the models to enhance the understanding of their limitations. Clarifying the evaluation criteria with full examples would also help in making the analysis more comprehensible. Finally, expanding the dataset and refining the prompt design could yield more robust insights into the capabilities of the LLMs.