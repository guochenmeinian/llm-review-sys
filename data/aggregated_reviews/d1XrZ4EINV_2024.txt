ID: d1XrZ4EINV
Title: LeDex: Training LLMs to Better Self-Debug and Explain Code
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 7, 5, 8, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for enhancing the self-debugging capabilities of language models through a combination of supervised fine-tuning (SFT) and reinforcement learning (RL). The authors propose a pipeline that samples incorrect code outputs from smaller models, utilizes larger models like GPT-3.5 for generating explanations and refinements, and incorporates a novel reward mechanism that emphasizes both code refinement and explainability. Empirical results indicate improvements in model performance across various benchmark datasets. Additionally, the authors compare their method with teacher models, specifically CodeLlama-34B and GPT-3.5-Turbo, demonstrating that their SFT/RL models achieve competitive performance. The paper addresses concerns regarding generalization, reward design, and the evaluation of models during testing, emphasizing the importance of comprehensive training data to avoid over-specialization.

### Strengths and Weaknesses
Strengths:
- The proposed pipeline demonstrates strong empirical gains in self-debugging capabilities.
- The work addresses a significant issue regarding limited coding improvements from self-refinement in smaller language models.
- The reward setup that integrates explainability with code refinement is innovative and shows promising results.
- The additional experimental results are interesting and promising, particularly the argument that using CodeBLEU stabilizes training.
- The authors effectively address reviewer concerns, leading to improved clarity in their responses.

Weaknesses:
- The paper lacks conceptual novelty, primarily relying on known techniques and not sufficiently distinguishing its contributions from existing literature.
- The approach's reliance on larger models for generating refinements complicates the assessment of the smaller models' contributions.
- There is insufficient comparison with related works and teacher models, making it difficult to position the paper within the existing body of research.
- The results indicate that the explain + refine approach does not consistently outperform the refine-only approach, raising questions about its effectiveness.
- The paper is perceived as low on conceptual novelty and does not position itself well within the existing literature.

### Suggestions for Improvement
We recommend that the authors improve the conceptual novelty of the paper by clearly articulating how their approach differs from existing works, particularly those cited in the related literature. Additionally, we suggest that the authors provide a more thorough comparison with teacher models and related works to clarify their contributions. It would be beneficial to evaluate the models on competitive programming datasets like APPS and CodeContests to assess performance on more challenging tasks. Furthermore, we encourage the authors to explore the implications of using larger models for refinement and consider discussing the potential downsides of over-specialization in their training methodology. Lastly, we recommend addressing the effectiveness of the explain + refine approach more rigorously, particularly in relation to its performance compared to the refine-only approach, and incorporating results and discussions from identified gaps in their next version to strengthen the overall impact of the paper.