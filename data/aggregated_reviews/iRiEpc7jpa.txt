ID: iRiEpc7jpa
Title: Low-Rank Learning by Design: the Role of Network Architecture and Activation Linearity in Gradient Rank Collapse
Conference: NeurIPS
Year: 2023
Number of Reviews: 5
Original Ratings: 6, 3, 5, 3, -1
Original Confidences: 3, 4, 2, 3, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive analysis of gradient rank in deep neural networks (DNNs), focusing on how architectural choices and data structures influence gradient rank bounds. The authors propose a theoretical framework for training fully-connected, recurrent, and convolutional networks, demonstrating the impact of design choices such as activation function linearity and bottleneck layers. Additionally, the study addresses the phenomenon of "Neural Collapse," where linear classifiers converge to specific geometrical structures during late-stage training, emphasizing the role of geometric constraints in learning.

### Strengths and Weaknesses
Strengths:  
The paper provides a novel theoretical analysis of gradient rank across various neural network architectures, offering valuable insights for designing new modules and activation functions. The empirical validation of the proposed methods is also detailed and contributes to the understanding of learning dynamics in DNNs.

Weaknesses:  
The presentation suffers from significant clarity issues, with abrupt transitions between sections and insufficient discussion accompanying formulae. The flow of the paper is hindered by poorly structured arguments, and critical definitions, such as BPTT, are missing. Additionally, the analysis lacks exploration of normalization and regularization techniques, and the experiments validating bottleneck layers do not include convolutional networks, limiting the robustness of the findings.

### Suggestions for Improvement
We recommend that the authors improve the overall clarity and flow of the paper by providing self-contained introductions for each section and ensuring that formulae are accompanied by thorough discussions. Additionally, addressing formatting issues and ensuring consistent references throughout the document is essential. We suggest including experiments on convolutional networks to strengthen the validation of bottleneck layers and expanding the analysis to include normalization and regularization techniques. Finally, clarifying the definitions of terms such as BPTT and ensuring that all stated contributions are present in the main text will enhance the paper's impact.