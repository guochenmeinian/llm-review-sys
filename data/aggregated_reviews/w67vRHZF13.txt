ID: w67vRHZF13
Title: Unified Generative and Discriminative Training for Multi-modal Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel learning paradigm for multi-modal large language models (MLLMs) that utilizes interleaved image-text corpora. The authors propose a structure-induced training strategy to impose semantic relationships between input samples and the MLLMâ€™s hidden state. They apply a dynamic time warping framework to compute semantic similarity between image-text sequences and utilize a discriminative loss on the similarity matrices derived from raw inputs and hidden states. The framework effectively leverages multiple vision and language encoders, demonstrating strong performance across various multimodal comprehension benchmarks, including the DEMON benchmark and retrieval-augmented generation tasks.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow.
2. It introduces a novel method that successfully integrates generative and discriminative training paradigms, addressing inherent weaknesses in existing Vision-Language Models (VLMs).

Weaknesses:
1. The impact of including interleaved image-text pairs on MLLM learning is not discussed, particularly regarding performance on visual-language benchmarks and retrieval tasks.
2. There is limited discussion on potential limitations and generalization to other vision-language tasks not covered in the experiments.
3. The performance improvement of the proposed method, Sugar, is not significant compared to some baselines, raising concerns about its effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the impact of interleaved image-text pairs on MLLM performance, particularly in relation to basic visual-language benchmarks and retrieval tasks. Additionally, we suggest providing more detailed ablation studies to clarify the contributions of each component, such as the dynamic sequence alignment and the GAK. It would also be beneficial to address how well the proposed method generalizes to other vision-language tasks and to clarify the evaluation protocol used in the experiments.