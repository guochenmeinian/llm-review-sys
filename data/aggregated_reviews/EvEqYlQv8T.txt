ID: EvEqYlQv8T
Title: Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for automating the updating of datasets for evaluating large language models (LLMs), addressing benchmark leakage, difficulty control, and evaluation stability. The authors propose two strategies: the mimicking strategy, which generates samples similar to the original data, and the extending strategy, which broadens samples using Bloom's taxonomy. Extensive experiments on MMLU and BIG-Bench demonstrate the effectiveness of these strategies in maintaining benchmark stability and providing fine-grained performance analysis.

### Strengths and Weaknesses
Strengths:
- The proposed dataset update strategies (mimicking and extending) are novel and address critical challenges in LLM evaluation.
- The paper demonstrates the effectiveness of these strategies in mitigating benchmark leakage and ensuring stable evaluation results.
- The strategies are practical and can be implemented using existing LLM technologies.
- The motivation for the research is clear and relevant to current issues in LLM evaluation.

Weaknesses:
- The paper focuses on a limited set of benchmarks (BIG-Bench and MMLU), which may weaken the generalizability of its claims.
- There is a lack of detailed comparison with existing methods for dataset updates, and the selected LLMs for generating new samples may introduce biases.
- The evaluation relies on human feedback, which could affect the accuracy and clarity of generated samples; more robust evidence is needed to support the evaluation process.
- The limitations section is incomplete, and the discussion of potential biases in generated datasets is insufficient.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by evaluating their strategies on a wider range of benchmarks. Additionally, a detailed comparison with existing dataset update methods should be included to clarify the advantages of the proposed strategies. The authors should address the potential biases introduced by the selected LLMs and provide a more thorough discussion of the human evaluation process, including evidence to support its validity. Furthermore, we suggest adding a comprehensive limitations section that discusses the implications of performance fluctuations observed in experiments and the generalizability of the proposed methods to larger datasets.