ID: mfWGm5FeQw
Title: LUSTER: Link Prediction Utilizing Shared-Latent Space Representation in Multi-Layer Networks
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LUSTER, a novel link prediction algorithm for multi-layer networks that utilizes shared-latent spaces to enhance inter-layer information sharing. The framework integrates intra-layer and shared-latent space representations through four modules: Representation Extractor, Latent Space Learner (with adversarial training), Complementary Enhancer, and Link Predictor. Extensive experiments validate LUSTER's effectiveness, achieving significant improvements in AUC compared to state-of-the-art methods.

### Strengths and Weaknesses
Strengths:
1. The innovative use of shared-latent space with adversarial training effectively captures inter-layer coupling, enhancing link prediction accuracy.
2. Comprehensive experiments demonstrate consistent and significant performance improvements across various datasets.
3. The paper is well-structured and clearly written, with effective use of figures and visualizations to present ideas and results.

Weaknesses:
1. The paper lacks a detailed analysis of the computational complexity of the LUSTER framework, particularly regarding time and space efficiency.
2. Scalability to large-scale networks is not thoroughly discussed.
3. The manuscript does not adequately address notable existing multilayer link prediction methods, limiting the breadth of baseline comparisons.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational complexity of LUSTER, specifically addressing how it scales with the number of layers and nodes in the network. Additionally, we suggest incorporating a broader range of recent baseline methods in the comparisons to strengthen the evaluation. Clarifying the motivation for the shared-latent space and enhancing the explanation of the "h consecutive links" concept would also benefit the manuscript. Finally, we advise exploring alternative architectures, such as GAT or state-of-the-art GNN methods, for the representation extractor to potentially improve performance.