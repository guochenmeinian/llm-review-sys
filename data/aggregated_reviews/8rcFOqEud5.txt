ID: 8rcFOqEud5
Title: ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ReST-MCTS*, a novel framework for self-training large language models (LLMs) that integrates process reward guidance with Monte Carlo Tree Search (MCTS). The method aims to generate high-quality reasoning traces and per-step values for training policy and reward models, thereby eliminating the need for manual annotations. Experimental results demonstrate that ReST-MCTS* outperforms existing self-training methods in accuracy across various reasoning tasks, including benchmarks like SciBench and MATH. Additionally, the authors discuss the significance of their work and the design of reward mechanisms, expressing a willingness to engage further with reviewers to address any outstanding concerns regarding their findings.

### Strengths and Weaknesses
Strengths:
- The integration of MCTS with process reward models represents a significant advancement in self-training for LLMs, allowing for automatic generation of high-quality reasoning traces.
- The paper provides robust theoretical foundations and clear definitions for key concepts, enhancing the methodology's effectiveness.
- Comprehensive experimental validation shows substantial improvements in performance on multiple benchmarks.
- The authors demonstrate a proactive approach by inviting further discussion and feedback, indicating their commitment to refining their work based on reviewer input.

Weaknesses:
- The paper lacks sufficient discussion on the value function, particularly in distinguishing between the expected total reward and the evaluation of the current state.
- There is a need for more evidence to validate the reasonableness of the intermediate values used in the reasoning process.
- Scalability concerns arise regarding the method's applicability to larger datasets or more complex reasoning tasks, which may require additional strategies for effective implementation.
- The absence of comparison with AlphaLLM undermines the perceived novelty of the work.
- The review lacks specific critiques or detailed feedback on the paper's content, which may limit the authors' ability to make targeted improvements.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the value function, particularly clarifying the differences between the expected total reward and the evaluation of the current state. Additionally, providing more evidence to support the validity of the intermediate values would strengthen the paper's arguments. To address scalability concerns, we suggest including additional strategies or evidence demonstrating the method's effectiveness on larger datasets or complex reasoning tasks. Lastly, we urge the authors to include a comparison with AlphaLLM to better contextualize their contributions and highlight the novelty of their approach. Furthermore, we recommend that the authors enhance the clarity of their reward design issue and provide more detailed explanations of the significance of their work to better address potential reviewer concerns.