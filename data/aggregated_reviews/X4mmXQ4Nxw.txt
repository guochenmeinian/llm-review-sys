ID: X4mmXQ4Nxw
Title: Performance-optimized deep neural networks are evolving into worse models of inferotemporal visual cortex
Conference: NeurIPS
Year: 2023
Number of Reviews: 28
Original Ratings: 6, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the decoupling of improved performance in deep neural networks (DNNs) on ImageNet from their ability to predict neural responses in the macaque inferior temporal (IT) cortex. The authors conduct experiments involving the shifting of images across the visual field while maintaining monkey fixation to assess the significance of spatially distributed features. They compare various architectures from PyTorch's timm library, including CNNs and ViT-like models, and find that aligning model gradients with human data enhances DNN predictions for IT neurons. The authors propose that the technique of "neural harmonization" can improve DNN predictive accuracy in primate IT regions. However, they acknowledge that the results may not generalize across all datasets, particularly those that are out-of-distribution (OOD) from ImageNet. The supplementary materials indicate that their stimuli align better with ImageNet, ruling out dataset misalignment as an explanation for their findings. Additionally, the authors discuss the challenges of interpreting results from the Brain-Score datasets, which are OOD, and express intent to include additional analyses in the Appendix.

### Strengths and Weaknesses
Strengths:
- The paper addresses a timely and relevant issue in computational neuroscience, demonstrating that ImageNet performance is no longer strictly correlated with brain predictive performance.
- The use of a novel dataset that has not been previously analyzed in this context enhances the contribution to the field.
- The introduction of the "neural harmonizer" technique effectively aligns human and DNN responses, providing a foundation for applications in prosthetics and reducing reliance on animal experimentation.
- The authors demonstrate that neural harmonization can improve DNN predictions on specific datasets, suggesting a potential avenue for future research.

Weaknesses:
- The paper lacks clarity on the presentation of images to DNNs, particularly regarding the image shifting process.
- There is a lack of clarity regarding the generalizability of the harmonization results across different datasets, particularly those that are OOD.
- The classification of models into categories such as CNN/Transformer/Robust/Self-supervised is confusing and could benefit from a clearer, mutually exclusive division by architecture or training type.
- Sparse details on the training of the neural harmonizer raise questions about the methodology, including the meaning of the gradient operator and whether second-order gradient optimization is employed.
- The authors' hesitance to include results from the Brain-Score datasets raises concerns about the completeness of their analysis and the potential over-extrapolation of their conclusions.
- The feature importance figures lack clarity on the methods used for plotting and quantifying feature relevance, and the claims made regarding model attention to background features are not objectively measured.
- The paper's framing suggests a novel claim about DNN performance that has been previously reported, which could be better contextualized.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the image presentation process to better explain how images were shifted during DNN fitting. Additionally, we suggest a more coherent classification of models, potentially by architecture or training type, to avoid confusion. The authors should provide more detailed information on the training of the neural harmonizer, including clarifications on the gradient operator and optimization methods used. We also encourage the authors to quantify feature importance across a larger dataset to substantiate their claims and to explicitly state the limitations of neural harmonization in relation to OOD datasets. It is essential to acknowledge that the method may only work on certain datasets due to distribution differences. Furthermore, we suggest that the authors include results from the Brain-Score datasets in their final submission to provide a more comprehensive analysis and address potential reader concerns about the applicability of their findings. Lastly, revising the wording in the abstract and relevant sections to ensure that the scope of their conclusions is accurately conveyed, particularly regarding the performance of harmonized DNNs across different datasets, would strengthen the paper's overall impact.