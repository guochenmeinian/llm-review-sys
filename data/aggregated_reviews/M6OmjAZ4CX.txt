ID: M6OmjAZ4CX
Title: Language Models can Solve Computer Tasks
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach, Recursively Criticizes and Improves (RCI), for guiding large language models (LLMs) to execute computer tasks using natural language commands. The RCI method outperforms existing techniques and establishes a new state-of-the-art on the MiniWoB++ benchmark when combined with the InstructGPT-3+RLHF LLM. The authors demonstrate that RCI enhances LLMs' reasoning abilities on natural language reasoning tasks and performs better when integrated with chain of thought (CoT) prompting.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and clear, effectively presenting its main ideas and contributions.
2. RCI is an original method that addresses the grounding problem of LLMs, leveraging self-criticism and self-improvement to complete computer and reasoning tasks.
3. The method shows promising results across a diverse range of web-based tasks and reasoning challenges.
4. RCI achieves competitive performance with fewer data requirements compared to previous approaches.

Weaknesses:
1. A detailed comparison with existing methods that utilize environmental feedback or model self-reflection, such as self-debugging and reflexion, is lacking.
2. The impact of RCI iteration steps on performance is unclear, and there is no explicit criterion for terminating RCI, which may not be feasible for new tasks in real-world applications.
3. The evaluation does not include a sufficient variety of models to demonstrate RCI's effectiveness across different architectures.
4. Limitations regarding the model's inability to handle tasks with complex UI manipulations and the context width constraints are not adequately discussed.

### Suggestions for Improvement
We recommend that the authors improve the comparison of RCI with recent approaches like ReACT and InnerMonologue to clarify its novelty and advantages. Additionally, conducting tests on a broader range of models, including zero-shot and few-shot settings, would strengthen the evidence for RCI's generalizability. The authors should also clarify the criteria for terminating RCI iterations and address the computational costs associated with multiple attempts. Finally, a more thorough discussion of the limitations, particularly regarding generalization to real-world tasks and the context window, would enhance the paper's rigor.