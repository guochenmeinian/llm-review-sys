ID: pT8DIhsJCw
Title: Parameter-efficient Tuning of Large-scale Multimodal Foundation Model
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 5, 5, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 1, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents AURORA, a parameter-efficient adaptation technique for multimodal models that utilizes mode approximation to enhance knowledge transfer and improve modality alignment. The authors propose a lightweight prompt tuning method employing CP decomposition to adapt features, alongside innovative techniques such as Informative Context Enhancement and Gated Query Transformation. These methods aim to facilitate better fusion of modalities and have shown promising results in experiments, outperforming existing methods while using fewer parameters. The authors also intend to rectify terminology by replacing "prompts" with "parameter-efficient transfer learning method" for accuracy and plan to enhance the writing and provide additional details on important modules and training techniques, including parameter tuning tricks.

### Strengths and Weaknesses
Strengths:
1. The innovative use of mode approximation for efficient adaptation demonstrates significant potential in reducing parameter usage while maintaining performance.
2. The introduction of Informative Context Enhancement and Gated Query Transformation provides novel approaches to modality alignment, which could be applicable in various contexts.
3. The authors have effectively addressed major concerns raised by reviewers, resulting in improved feedback and scores.
4. Commitment to making complete code and training details publicly available demonstrates transparency and support for the community.
5. The paper includes extensive evaluations and thorough ablation studies that justify the design choices made.

Weaknesses:
1. The terminology surrounding 'soft prompts' is misleading; the method aligns more closely with parameter-efficient transfer learning, necessitating clearer technical language.
2. The implementation details of Gated Query Transformation are insufficiently explained, hindering replication and understanding.
3. The reliance on manual tuning of the rank hyperparameter poses scalability challenges, and the authors do not sufficiently address how to optimize this selection.
4. The current version lacks sufficient detail on important modules and training methodologies.

### Suggestions for Improvement
We recommend that the authors improve the clarity of technical terminology, particularly by replacing "prompt learning" with "multimodal parameter-efficient transfer learning based on mode approximations" to avoid confusion. Additionally, we suggest supplementing the paper with more detailed explanations of the Gated Query Transformation's implementation to enhance clarity and replicability. Addressing the manual tuning of the rank hyperparameter with a more comprehensive discussion on optimal selection methods would also strengthen the paper. Furthermore, enhancing the writing quality and including broader details on training techniques, such as parameter tuning tricks, would provide valuable insights and improve the overall quality of the work.