ID: 3fd776zKmo
Title: Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 7, 6, 6, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the over-smoothing problem in Transformers, revealing that self-attention layers minimize a functional that leads to token uniformity. The authors propose a novel regularizer, NeuTRENO, which penalizes the norm of the difference between output and input tokens to preserve token fidelity. The experimental results demonstrate that NeuTRENO outperforms baseline Transformers in mitigating over-smoothing across various tasks, including visual and language-related applications. The paper distinguishes itself by providing a theoretical analysis from a gradient flow perspective, which has not been extensively addressed in prior research.

### Strengths and Weaknesses
Strengths:
- The theoretical investigation is rigorous and provides a unique perspective on the over-smoothing issue by examining self-attention as a gradient descent step.
- The proposed solution, NeuTRENO, is shown to be effective in both theoretical and empirical evaluations, enhancing performance in vision and language tasks.

Weaknesses:
- The theoretical analysis primarily applies to Transformers with single-head self-attention, necessitating further evaluation of its efficacy in multi-head configurations.
- The paper lacks a comprehensive set of comparative experiments to assess the influence of varying initial values of the parameter $\widetilde{\lambda}$ on model performance.
- Some experimental results may not robustly support the authors' claims, potentially leading to misleading interpretations.
- The discussion on the broader implications of alleviating over-smoothing is insufficient, particularly regarding its impact on out-of-distribution settings and transfer learning capabilities.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework to encompass multi-head self-attention mechanisms, ensuring broader applicability. Additionally, the authors should conduct a more extensive set of comparative experiments to analyze the effects of different initial values of $\widetilde{\lambda}$. It would also be beneficial to enhance the discussion surrounding the practical implications of alleviating over-smoothing, particularly in relation to out-of-distribution scenarios and transfer learning. Lastly, we suggest refining the presentation of mathematical concepts and ensuring all symbols and terms are clearly defined to enhance clarity and comprehension.