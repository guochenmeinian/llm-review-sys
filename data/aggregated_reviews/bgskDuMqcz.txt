ID: bgskDuMqcz
Title: Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for improving domain adaptation using in-context learning (ICL) by retrieving semantically similar sentences from a source domain to augment target unlabeled examples. The authors propose different strategies for encoder-only and decoder-only models under both inference-only and fine-tuning setups, demonstrating improved performance in Named Entity Recognition (NER) and sentiment analysis tasks. The methodology involves combining retrieved sentences with the target input and utilizing basic loss functions for label prediction and language modeling.

### Strengths and Weaknesses
Strengths:
- The paper conducts rigorous experiments across various language models and datasets, showing that the proposed methods can enhance performance compared to conventional ICL approaches.
- The use of ICL is well-motivated, and the experimental results indicate improved performance on key NLP tasks.

Weaknesses:
- The novelty of the methodology is questionable, as it relies on established loss functions and lacks significant differentiation from existing approaches.
- Concerns arise regarding the suitability of large language models (LLMs) for domain adaptation, particularly when used solely for inference without explicit training on the source distribution.
- The experiments are incomplete, lacking comparisons with other unsupervised domain adaptation methods and existing research results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their contributions by explicitly distinguishing their work from existing literature on LM-based unsupervised domain adaptation. Additionally, consider addressing the potential performance drop when the source and target domains are significantly different. It would be beneficial to explore the extension of the framework to multiple source and target domains. Furthermore, we suggest providing practical guidelines on model selection and ICL techniques based on the specific characteristics of the language models used. Lastly, we encourage the authors to refine the writing for clarity and coherence.