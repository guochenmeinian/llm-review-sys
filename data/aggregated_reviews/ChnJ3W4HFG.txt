ID: ChnJ3W4HFG
Title: Enhancing Robustness of Last Layer Two-Stage Fair Model Corrections
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a kNN-based label noise correction strategy aimed at enhancing the performance of two-stage last-layer retraining methods for group robustness under moderate label noise. The authors demonstrate that the performance of existing methods, RAD and SELF, declines significantly in the presence of label noise, while their kNN method achieves improved worst-group accuracy across several benchmark datasets. The work highlights the vulnerability of state-of-the-art last-layer retraining methods to label noise and introduces kNN-RAD and kNN-SELF, which integrate kNN label spreading with existing methods.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a critical issue in group robustness by mitigating the effects of class label noise without requiring group annotations.
2. The proposed kNN method is computationally efficient and can be easily incorporated into existing training pipelines.
3. The evaluation is thorough, covering multiple datasets and comparing against state-of-the-art methods, with results presented clearly.

Weaknesses:
1. The authors should discuss baseline label noise in datasets like Waterbirds, which may already exhibit non-zero label noise, potentially affecting the robustness of RAD and SELF.
2. Clarification is needed regarding the assumptions in Section 2 about the pretraining dataset for the neural network, as the current explanation is ambiguous.
3. The focus on the misclassification version of the SELF algorithm raises questions, as recent work suggests that early-stop disagreement SELF performs better; justification for this choice is necessary.
4. The paper lacks a rigorous theoretical analysis of the kNN label spreading method and its performance under various conditions.

### Suggestions for Improvement
We recommend that the authors improve the discussion on baseline label noise in the datasets used, particularly addressing the implications for RAD and SELF. Additionally, the authors should clarify the assumptions regarding the pretraining dataset in Section 2, specifying whether it refers to ImageNet or the downstream dataset. We suggest that the authors consider incorporating the early-stop disagreement SELF method, as it may enhance performance and address variance issues noted in their approach. Furthermore, a more comprehensive theoretical analysis of the kNN label spreading method, including convergence proofs and performance bounds, would strengthen the paper. Lastly, exploring other label propagation techniques and comparing them with kNN could provide valuable context for their choice of method.