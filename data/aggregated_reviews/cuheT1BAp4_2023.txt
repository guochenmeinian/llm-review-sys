ID: cuheT1BAp4
Title: Object Reprojection Error (ORE): Camera pose benchmarks from lightweight tracking annotations
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 7, 6, 6, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel evaluation protocol called Object Reprojection Metric (ORE) for assessing the accuracy of estimated camera trajectories without requiring precise 3D ground truth data. By leveraging lightweight object tracklet annotations, ORE evaluates camera trajectories through reprojection of object tracklets into 2D. The authors validate ORE against established metrics and introduce the EgoStatic dataset, a large-scale benchmark for evaluating camera trajectories in diverse environments. The benchmark is designed to identify optimal settings for existing methods, such as COLMAP and MonoDepth2, while addressing challenges related to fine-tuning on ego-centric datasets. The authors acknowledge the need for further clarification on the performance of COLMAP relative to other methods and the implications of fine-tuning on model generalization.

### Strengths and Weaknesses
Strengths:
- The simplicity and practicality of ORE make it a valuable tool for evaluating camera trajectories in scenarios lacking 3D ground truth data.
- The EgoStatic dataset is extensive, providing ample data for training and evaluating deep learning models.
- The paper includes thorough comparisons with established metrics, enhancing the credibility of ORE.
- The writing is clear and accessible, making the paper easy to read.
- Comprehensive experiments establish the correlation of ORE with standard metrics.
- The authors acknowledge and plan to clarify ambiguities regarding the performance of COLMAP compared to DROID-SLAM.
- The study includes ablation analyses that demonstrate the correlation between the number of tracklets and accuracy.

Weaknesses:
- The paper lacks clarity regarding whether all static objects in the EgoStatic benchmark are included for evaluation, and an ablation study on the number of objects could provide insights.
- There is confusion regarding the role of COLMAP in depth estimation, particularly in relation to DROID-SLAM.
- Table 2 is confusing due to differing units for the metrics presented, which may mislead comparisons.
- The assertion that fine-tuning did not improve EgoStatic requires further investigation to understand potential distribution differences.
- The relationship between COLMAP's performance and depth estimation accuracy needs clarification, as it appears contradictory in some statements.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the inclusion of static objects in the EgoStatic benchmark and consider conducting an ablation study on the number of objects used. Additionally, we suggest revising Table 2 to clearly indicate the differing units for each metric and to enhance the understanding of the results. Further discussion on the comparison to geometric reprojection error would be beneficial, particularly in addressing the implications of selecting small static objects. We also encourage the authors to provide a more detailed explanation of the relationship between COLMAP and DROID-SLAM's depth estimation accuracy to resolve existing confusion. Lastly, we recommend that the authors investigate the impact of fine-tuning on performance in ego-centric datasets similar to Ego4D to provide a more comprehensive understanding of the results and consider incorporating additional datasets for evaluation to substantiate the claimed advantages of their methods.