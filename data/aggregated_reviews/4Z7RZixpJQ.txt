ID: 4Z7RZixpJQ
Title: ProSST: Protein Language Modeling with Quantized Structure and Disentangled Attention
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ProSST, a method for training protein language models that integrates both sequence and structural information. The authors utilize a geometric vector perceptron trained with a structural denoising objective to create a structure encoder, followed by k-means clustering of local residue neighborhoods to generate a codebook of structural tokens. A masked language model is trained using disentangled attention, conditioned on structural token inputs and corrupted sequences. The authors claim that ProSST outperforms state-of-the-art methods in various tasks under both zero-shot and supervised learning settings.

### Strengths and Weaknesses
Strengths:
- The approach effectively incorporates explicit structural information into protein language models, enhancing performance.
- ProSST expands the codebook size of structural tokenizers and demonstrates the benefits of disentangled attention for mutation effect prediction and improved pre-training perplexity.
- The paper is well-structured, with clear presentation and comprehensive experiments validating the proposed methods.

Weaknesses:
- The rationale for the two-step structural quantization process is unclear, especially compared to simpler methods like SaProt.
- The novelty of the approach is somewhat limited, as it is not the first hybrid model for protein language representation.
- The experimental setup raises concerns, particularly regarding the small validation sets and the lack of error or significance analysis in the results.

### Suggestions for Improvement
We recommend that the authors clarify the reasoning behind the two-step structural quantization approach and compare it with methods like VQ-VAE used in FoldSeek. Additionally, the authors should investigate the computational cost of tokenizing structures and consider using smaller neighborhood sizes to alleviate computational burdens. We suggest providing a more thorough analysis of the disentangled attention mechanism and its impact on performance, as well as ensuring that the experimental section includes sufficient details for reproducibility. Finally, we encourage the authors to address the limitations regarding the reliance on structural data and explore potential "sequence-only" modes for the model.