ID: AfnJBOXfAU
Title: COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates fairness in personalized explanation generation for recommendations, highlighting how biases in user-written training data can lead to unfair treatment based on protected attributes. The authors propose a framework named COFFEE, which aims to achieve measure-specific counterfactual fairness by disentangling user attribute representations and employing policy learning to impose fairness constraints. Extensive experiments demonstrate the effectiveness of COFFEE in reducing quality disparities across user attributes.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important issue of counterfactual fairness in personalized text generation, particularly in explanation systems.
- The technical contributions, including disentangled representations and policy learning, are novel and promising.
- The mathematics and experimental results are generally sound and well-organized.

Weaknesses:
- The application of the proposed framework lacks robustness, with concerns about the justification of quality metrics and the assumptions in the counterfactual inference module.
- The generality of COFFEE appears limited, with insufficient exploration of alternative quality measures and the impact of user attributes on representations.
- The framing of the problem may overstate the generalizability of the approach beyond the specific context of explanation generation.

### Suggestions for Improvement
We recommend that the authors improve the justification of quality metrics as proxies for human preferences and provide more evidence that differential quality is inherently problematic. Additionally, we suggest clarifying the assumptions underlying the counterfactual inference module, particularly regarding the disentanglement of sensitive attributes from user representations. The authors should also consider conducting ablation studies and exploring the impact of multiple attributes on the framework's effectiveness. Finally, enhancing the problem formulation and situating the work within the broader literature on algorithmic fairness would strengthen the contribution.