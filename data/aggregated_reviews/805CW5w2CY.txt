ID: 805CW5w2CY
Title: A Simple Solution for Offline Imitation from Observations and Examples with Possibly Incomplete Trajectories
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 4, 6, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Trajectory-Aware Imitation Learning from Observations (TAILO), a method for offline imitation learning that addresses the challenge of learning from incomplete trajectories. TAILO distinguishes expert demonstrations from sub-optimal data to recover the task reward function and relabels sub-optimal transitions to enhance training data for policy learning through weighted behavior cloning. The authors also propose a novel two-step training paradigm for positive-unlabeled (PU) learning, combining Eq. 4 and binary classification through Eq. 5, demonstrating its effectiveness in environments like halfcheetah-mismatch, where traditional methods fail. They clarify their unique contributions to the reward-weighted regression (RWR) framework, emphasizing differences in reward label assumptions, theoretical derivation, and design choices. The empirical results demonstrate that TAILO outperforms several baselines, particularly in scenarios with incomplete trajectories, and the authors argue that their contributions to offline Learning from Observations (LfO) are significant, analyzing shortcomings of prior state-of-the-art methods and proposing a new, effective solution.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant problem with potential real-world applications.
- Empirical performance is promising, showing robustness across various data conditions.
- The method is conceptually simple and well-motivated.
- The authors provide a clear explanation of their novel two-step PU learning approach and its empirical success.
- They effectively differentiate their work from prior RWR methods, highlighting unique aspects of their contributions.
- The response to reviewer concerns is comprehensive and addresses key issues raised.

Weaknesses:
- The presentation contains inaccuracies and requires careful revision, particularly regarding foundational definitions and claims about existing methods.
- The contribution appears relatively minor, as many techniques used are standard in the reinforcement learning literature.
- The theoretical foundation is lacking, and the absence of ablation studies makes it difficult to assess the importance of individual components.
- Some reviewers maintain that the novelty of the work is not sufficiently strong, impacting their overall assessment.
- The theoretical contribution is perceived as lacking depth by certain reviewers.

### Suggestions for Improvement
We recommend that the authors improve the accuracy of their statements regarding offline imitation learning and clarify the definitions used in the paper. It is crucial to address the inaccuracies in the abstract and other sections, such as the description of DICE methods. We suggest conducting ablation studies to evaluate the effectiveness of the Positive-Unlabeled learning method and various weighting strategies in behavior cloning. Additionally, providing a more robust theoretical foundation for the proposed methods and clarifying the derivation of the objectives in Equations 4, 5, and 6 would strengthen the paper. Further empirical validation across diverse scenarios could enhance the robustness of their claims. Addressing the concerns regarding the theoretical depth may also help in gaining broader acceptance from reviewers. Finally, we encourage the authors to explore real-world applications of their method to enhance its relevance.