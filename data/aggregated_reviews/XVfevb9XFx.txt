ID: XVfevb9XFx
Title: Optimistic Critic Reconstruction and Constrained Fine-Tuning for General Offline-to-Online RL
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 6, 6, 6, -1, -1
Original Confidences: 4, 4, 1, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents a general offline-to-online (O2O) reinforcement learning method that integrates with any offline RL algorithm. It tackles evaluation and improvement mismatches between offline datasets and online environments through three main strategies: (1) re-evaluating the offline critic optimistically; (2) calibrating the critic with the offline actor; and (3) performing constrained online fine-tuning. The authors validate their approach through extensive theoretical analysis and empirical experiments, demonstrating stable performance improvements across various simulated tasks.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow.
2. The re-evaluation and calibration procedures are crucial for enhancing offline-to-online RL training, supported by both empirical evidence and theoretical guarantees.
3. The proposed method effectively balances specificity and flexibility, addressing two distinct mismatches with policy re-evaluation and value alignment techniques.
4. The experimental evaluation is thorough and strongly supports the claims, showing superiority over multiple state-of-the-art methods.

Weaknesses:
1. Several minor presentation issues detract from the visual quality of the paper, such as messy equations and similar curve colors in figures.
2. The third component of the method lacks clarity regarding its necessity and potential generalizability across different online methods.
3. The paper does not include a Limitation section, and the time complexity of the method is not analyzed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by addressing minor issues, such as adjusting the spacing in equations and enhancing figure color differentiation. Additionally, please clarify which offline pre-training methods were used for each result and provide ablations comparing the same online RL algorithm with different offline pre-training methods. Including results on the Adroit binary task and full comparisons for the AntMaze tasks in the main text would strengthen the paper. Finally, we suggest adding an analysis of the method's time complexity and explaining the necessity of L_retain in the value alignment objective.