ID: WMzQIP70O0
Title: BiVLC: Extending Vision-Language Compositionality Evaluation with Text-to-Image Retrieval
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 8, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new image-text retrieval benchmark called BiVLC, which evaluates both image-to-text and text-to-image retrieval tasks. The benchmark is structured similarly to Winoground, utilizing real images and captions from SugarCrepe, along with synthetic negative images generated by a text-to-image model (SDXL-DPO). The authors find that BiVLC is more challenging than SugarCrepe but easier than Winoground, with current discriminative VLMs like CLIP and NegCLIP underperforming compared to human benchmarks. Additionally, the authors evaluate a proposed training method and dataset for vision-language models (VLMs), specifically focusing on compositionality. They acknowledge the limitations of the Winoground dataset, which is deemed unsuitable for evaluating VLM compositionality due to its small size and lack of diverse instances. The authors plan to enhance their dataset and improve their methodology based on reviewer feedback, including the incorporation of additional references and a deeper analysis of the impact of synthetic images on model performance.

### Strengths and Weaknesses
Strengths:
- The dataset and benchmark are of high quality, with effective paired evaluation protocols that mitigate language biases found in previous benchmarks.
- The collection pipeline is efficient and well-documented, and the dataset is accessible on HuggingFace.
- The authors provide a thorough rebuttal addressing reviewer concerns and demonstrate strong experimental results with generative VLMs.
- The intention to revise the text for better coverage of related work indicates a commitment to improving the paper's academic rigor.

Weaknesses:
- Some low-quality images remain in the dataset, which could affect the benchmark's reliability.
- The novelty of the contribution is questioned, as it primarily extends existing datasets and methods without sufficient differentiation.
- The evaluation on Winoground shows only modest improvements over baseline models, raising questions about the effectiveness of training with synthetic images.
- The current dataset lacks diversity, and the authors recognize the need for additional filtering and expansion.
- The writing quality requires improvement, with several statements needing revision for clarity and accuracy.

### Suggestions for Improvement
We recommend that the authors improve the dataset by discarding or regenerating low-quality samples, as quality is paramount for a testing benchmark. Additionally, we suggest including references to other benchmarks supporting bidirectional retrieval, such as EqBen and CoLA, to strengthen the paper's context. The authors should also consider conducting a small-scale human evaluation to compare BiVLC with existing benchmarks. Furthermore, we encourage the authors to explore the use of generative VLMs like VQAScore and VisualGPTScore, which have shown better performance in related tasks. We also recommend that the authors improve the coverage of related work as promised in the rebuttal, specifically by including references to previous works that utilize synthetic images for training and evaluation. Additionally, we suggest conducting a deeper analysis of the impact of synthetic images on model performance and consider extending the dataset to include other languages and categories to enhance its diversity. Finally, we advise careful proofreading to address clarity issues and typos throughout the paper, as well as clarifying the limitations of their training methodology and exploring the potential of generative multimodal models in future work.