ID: ZupoMzMNrO
Title: Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Learning-to-Cache (L2C), a method designed to accelerate inference in diffusion transformers by caching redundant computations across timesteps. By transforming the non-differentiable layer selection problem into a differentiable optimization problem through feature interpolation, the authors propose a routing matrix $\beta$ that indicates whether features from previous timesteps can be reused. Experimental results demonstrate that L2C can eliminate up to 93.68% of computations in specific steps, achieving better performance than existing methods like DDIM and DPM-Solver.

### Strengths and Weaknesses
Strengths:  
1. The transformation of the layer selection problem into a differentiable optimization problem is a clever approach that underpins the optimization of the routing matrix $\beta$.  
2. The exploration of feature caching mechanisms for inference acceleration in DiT models is meaningful, with experimental results validating the method's effectiveness.  
3. The paper is well-organized, clearly written, and presents comprehensive figures and tables that enhance understanding.  

Weaknesses:  
1. The scalability of L2C is a concern, as it requires training for different DiT models and diffusion schedulers, limiting its applicability.  
2. The paper does not include experiments on text-to-image models based on the DiT architecture, raising questions about potential limitations in training on large-scale datasets.  
3. Specific training costs are not reported, and it is unclear if L2C increases memory usage due to the overhead of the routing matrix $\beta$ and feature caching.  
4. The contribution appears incremental, primarily introducing a learnable router, and could benefit from deeper innovations and insights.

### Suggestions for Improvement
We recommend that the authors improve the scalability of L2C by addressing the training requirements for different models and diffusion schedulers. Additionally, conducting experiments on text-to-image models based on the DiT architecture would provide a more comprehensive evaluation of the method's applicability. It is also essential to report specific training costs and clarify whether the routing matrix $\beta$ introduces additional memory overhead. Lastly, we suggest exploring the integration of L2C with other acceleration techniques, such as step distillation, to enhance inference speed further.