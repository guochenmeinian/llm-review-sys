ID: gUEBXGV8JM
Title: Alias-Free Mamba Neural Operator
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel neural operator called MambaNO (Mamba Neural Operator) for solving Partial Differential Equations (PDEs). The key contributions include: 
1. A new integral form termed "mamba integration" with O(N) computational complexity that captures global function information.
2. An alias-free architecture that integrates mamba integration with convolution to capture both global and local function features.
3. Theoretical analysis demonstrating that MambaNO is a representation-equivalent neural operator (ReNO) capable of approximating continuous operators for a broad class of PDEs.
4. Extensive empirical evaluation showcasing state-of-the-art performance across various PDE benchmarks.

### Strengths and Weaknesses
Strengths:
1. Novelty: The introduction of the Mamba architecture for neural operators is innovative, combining global and local features effectively.
2. Theoretical foundation: Solid theoretical analysis, including proofs of representation equivalence and approximation capabilities, enhances understanding of the method's efficacy.
3. Comprehensive experiments: Thorough evaluations across diverse PDE types and comparisons with multiple state-of-the-art baselines strengthen claims of generalization ability.
4. Performance improvements: Significant accuracy and efficiency enhancements across different PDE types are noteworthy.
5. Alias-free framework: Addressing aliasing issues potentially improves model stability and generalization.
6. Efficiency: The O(N) complexity of mamba integration is advantageous for high-dimensional problems.

Weaknesses:
1. Hyperparameter sensitivity: The paper lacks a comprehensive analysis of the model's sensitivity to hyperparameters, such as layer count and state space dimensionality.
2. Limitations in multi-scale phenomena: The paper does not thoroughly analyze MambaNO's effectiveness in handling PDEs with significantly different scales of behavior.
3. Boundary condition handling: There is insufficient discussion on how various boundary conditions are managed within the MambaNO architecture.
4. Lack of statistical significance: Benchmark comparisons lack statistical significance, making results less convincing.
5. Transparency issues: Missing details on hyperparameters and training configurations for baseline models hinder reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the analysis of hyperparameter sensitivity, particularly regarding the number of layers and state space dimensionality. Additionally, a deeper exploration of MambaNO's capabilities in addressing multi-scale phenomena is necessary. We suggest providing a detailed discussion on boundary condition handling within the architecture. Furthermore, we encourage the authors to include statistical significance in benchmark comparisons to enhance the credibility of their results. Lastly, we recommend that the authors clarify the alias-free concept in the main body and improve the explanations of experimental settings in the figures.