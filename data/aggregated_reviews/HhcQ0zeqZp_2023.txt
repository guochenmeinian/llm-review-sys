ID: HhcQ0zeqZp
Title: Benchmarking Large Language Models on CMExam - A comprehensive Chinese Medical Exam Dataset
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 7, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CMExam, a comprehensive Chinese medical QA dataset derived from the Chinese National Medical Licensing Examination, featuring over 60,000 instances annotated across five criteria: disease groups, clinical departments, medical disciplines, areas of competency, and question difficulty levels. The authors benchmark 15 large language models (LLMs), including both general and medical domain models, providing extensive quantitative and qualitative analyses. The dataset aims to fill gaps in evaluation resources for Chinese LLMs and introduces an innovative strategy called GPT-Assisted Annotation. The authors also enhance the dataset's informational depth by cross-referencing exam questions with solutions from various online medical examination preparation platforms. They acknowledge the importance of distinguishing their dataset from existing ones, such as MLEC-QA, and have made efforts to highlight these differences.

### Strengths and Weaknesses
Strengths:  
- The dataset's extensive annotation process, utilizing both GPT-4 and medical experts, offers valuable insights into various medical proficiencies of different models.  
- CMExam serves as a benchmark for evaluating large-scale models, conducting thorough quantitative and qualitative assessments.  
- The paper introduces significant novel annotation layers, enhancing the dataset's utility for evaluating LLMs.  
- A thorough benchmarking effort involving multiple models is conducted, showcasing the dataset's robustness.  
- The authors have effectively addressed reviewer concerns, enhancing the clarity and depth of the paper.  
- Supplementary experiments on few-shot learning and chain-of-thought reasoning provide a holistic evaluation of model performance.  
- The results presented offer interesting comparisons among various language models, contributing to a comprehensive overview.

Weaknesses:  
- The novelty of the dataset may be undermined by existing datasets like MLEC-QA; a clearer distinction is needed.  
- The discussion of the "GPT-Assisted Annotation" strategy is insufficiently detailed, requiring elaboration on its implementation and effectiveness.  
- The paper lacks comprehensive details on the data annotation process and human performance evaluation, which are critical for contextualizing results.  
- Missing comparisons with relevant models such as MedAlpaca and generative models like BART and T5 limit the evaluation's comprehensiveness.  
- The related work section lacks discussion of several pertinent studies, which could provide necessary context.  
- Annotations are limited to the test set, which may restrict the dataset's utility for broader applications.  
- The paper does not initially include code for reproducibility, which is crucial for validating results.  
- Some reviewers noted the need for clearer explanations and links to online medical QA platforms within the main text.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset's contributions by including a table that highlights the differences between CMExam and MLEC-QA. Additionally, a more detailed description of the "GPT-Assisted Annotation" strategy should be provided, including the number of annotation rounds, inter-annotator agreement, and the prompts used. We suggest enhancing the methodology section by detailing the data annotation process, including the qualifications of human annotators and the guidelines followed. Furthermore, the authors should include a comparison with the Medical Domain LLM MedAlpaca and generative models like BART and T5 to provide a more comprehensive evaluation. We also recommend ensuring the reproducibility of their results by providing accessible code and clarifying the dataset's annotations, including the ICD-11 codes. Lastly, we encourage the authors to evaluate the factuality of generated responses in the open-ended solution explanations, as this is crucial in the medical domain, and to explicitly state in the main text that annotations are only present in the test set. Additionally, please include footnotes with links to online sources for questions in the "Dataset Characteristics" section for better understanding, and consider extending annotations to the training and validation sets in future work to enhance the dataset's overall utility.