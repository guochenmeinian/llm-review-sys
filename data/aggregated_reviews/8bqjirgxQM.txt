ID: 8bqjirgxQM
Title: Understanding Social Reasoning in Language Models with Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 9, 8, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the BigToM dataset, generated by GPT-4 from causal templates to represent theory-of-mind (ToM) scenarios as causal graphs, facilitating interventions, controls, and probes. The authors evaluate five LLMs: text-davinci-003, gpt-3.5-turbo, gpt-4-0314, claude-v1.3, and llama-65b-q5, finding that GPT-4 approaches human capabilities in social reasoning, while others lag. The dataset comprises 200 templates across 25 control-condition combinations, yielding 5000 evaluations. The authors propose a scalable framework for generating evaluations, addressing challenges in previous methodologies and establishing a benchmark for social reasoning. Additionally, they introduce a method for creating a "clean" evaluation set that addresses inferential abilities in language models, arguing that their circular evaluation methodology is a virtue for testing reasoning. They generate a new dataset, BigToM-Claude, using Claude-2 to validate their approach and demonstrate consistent performance across different models. The authors provide a detailed analysis of their dataset's diversity and limitations, acknowledging potential biases inherent in synthetic datasets.

### Strengths and Weaknesses
Strengths:
- The evaluation of LLMs' social reasoning is well-motivated.
- The dataset is large, diverse, and methodologically thorough, rated highly by human participants.
- The paper is exceptionally clear, with comprehensive documentation and a detailed discussion of prior works.
- The method adds value to the literature by solving a concrete problem of evaluation set creation.
- The authors have conducted additional experiments and analyses to address reviewer concerns, enhancing the robustness of their findings.

Weaknesses:
- The evaluation methodology may appear circular, as the models being tested also generate the test items, raising concerns about validity.
- The sample size for data quality evaluation is small, raising concerns about reliability and generalizability.
- There is a lack of detailed resource usage information and training processes in the original submission.
- Some misspellings remain in the text, detracting from the overall quality.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the circular evaluation methodology by considering alternative validation studies or explicitly addressing it as a limitation. Additionally, we suggest that the authors improve the clarity regarding the dataset size and its implications for generalizability. The authors should specify the limitations of the proposed dataset and its potential biases in a dedicated section to ensure transparency. To enhance data quality evaluation, we recommend increasing the sample size further and providing a more detailed explanation of the "Expert" dataset, including the rationale for its composition. Lastly, we encourage the authors to clarify the performance metrics used and ensure consistency in the presentation of prompt templates to eliminate ambiguity, while also providing more detailed information about resource usage and the training process to enhance the paper's comprehensiveness.