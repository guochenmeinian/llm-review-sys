ID: UTwuDTpdNO
Title: Meta Stackelberg Game: Robust Federated Learning against Adaptive and Mixed Poisoning Attacks
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 7, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 1, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Meta Stackelberg Game (meta-SG) framework aimed at enhancing the security of Federated Learning (FL) systems against adaptive and mixed poisoning attacks. The authors propose a Bayesian Stackelberg Markov game (BSMG) combined with a meta-learning approach to create a robust defense mechanism. The method is theoretically proven to converge efficiently and is empirically validated to perform well against various adversarial attacks, including model poisoning and backdoor attacks. Additionally, the paper introduces a federated learning framework that incorporates generative modeling and the inverting gradient (IG) method to infer clients' data distribution. The authors note that their approach can function effectively even with slight deviations from the true distribution, as evidenced by results related to RL-based attacks and defenses. They also address privacy concerns regarding the use of public datasets in FL, highlighting that such practices are common and can enhance robustness while maintaining privacy guarantees.

### Strengths and Weaknesses
Strengths:
- The introduction of the meta-SG framework offers an innovative perspective on defending against adaptive and mixed attacks in FL.
- A solid theoretical foundation is provided, demonstrating that the proposed algorithm converges to a first-order Îµ-equilibrium.
- Extensive experiments show significant improvements in defense against various attack types compared to existing methods.
- The meta-learning component enhances the adaptability of the defense mechanism in uncertain environments.
- The integration of generative modeling and IG methods demonstrates a novel approach to inferring data distribution in FL.
- The framework's resilience to slight deviations from true distributions is a significant advantage.
- The discussion on the use of public datasets provides a well-rounded perspective on privacy in FL, supported by relevant literature.

Weaknesses:
- The approach appears computationally intensive, requiring substantial resources for pre-training and adaptation, which may limit practical application.
- The scope of tested attacks may not encompass all real-world adversarial strategies, raising concerns about the generalizability of results.
- The scalability of the proposed method to larger and more diverse FL environments remains uncertain, particularly given its computational demands.
- There is a slight violation of privacy due to the need for ground truth data from clients, which could undermine the core motivation of FL.
- The paper does not explore the implementation of more advanced generative models or IG methods, which could enhance its applicability to complex datasets.
- There is uncertainty regarding the acceptability of minor privacy leakage in FL, which may affect the overall evaluation of the framework.

### Suggestions for Improvement
We recommend that the authors improve the empirical evidence regarding the method's run-time overhead to better assess its practicality. Additionally, clarifying the specific attacks used during pre-training and their relation to real FL environments would enhance the study's transparency. It would be beneficial to explore the scalability of the meta-SG framework in larger FL setups and address how it handles adaptive attack types not seen during pre-training. We also suggest incorporating more complex datasets beyond MNIST and CIFAR to validate the model's effectiveness in diverse scenarios. Lastly, we recommend improving the discussion on the implications of slight privacy leakage in federated learning to clarify its acceptability and addressing the privacy concerns more thoroughly in the conclusion to provide a more balanced view of the framework's limitations.