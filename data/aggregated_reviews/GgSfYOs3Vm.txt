ID: GgSfYOs3Vm
Title: Can Small Language Models be Good Reasoners in Recommender Systems?
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Step-by-step knowledge distillation framework for recommendation (SLIM), which enables sequential recommenders to utilize the reasoning capabilities of Large Language Models (LLMs) in a cost-effective manner. The authors propose to distill a smaller LLM (LLaMA 7B) from a larger teacher model (gpt-3.5-turbo) using chain-of-thought (CoT) prompting based on user behavior sequences. The generated rationales serve as labels for training the smaller model, enhancing its performance in both ID-based and ID-agnostic recommendation scenarios.

### Strengths and Weaknesses
Strengths:
- The distillation approach effectively reduces inference costs associated with LLMs in real-world recommendations.
- The proposed CoT prompting and teacher-student model training strategy is logical and well-articulated.
- Experimental results demonstrate notable performance improvements, and the paper is well-written and accessible.

Weaknesses:
- The title "small language model" is misleading, as LLaMA-7B is not small compared to other models.
- The distillation process lacks technical insights and follows a traditional student-teacher architecture without significant innovation.
- The experiments are limited to the Amazon Review dataset, and the baseline methods compared are insufficient, lacking stronger LLM-based comparisons.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the surprising results in Tables 1 and 2, particularly regarding why the gpt-3.5-turbo version (SLIM-) does not outperform the small LLM version (SLIM). A detailed case study on this phenomenon would be beneficial. Additionally, the authors should carefully analyze the generalization ability of the distilled small LLM across different recommendation domains, conducting experiments to assess whether a model trained on one dataset can effectively enhance performance on another. 

Furthermore, we suggest revising the title to something like "Small Large Language Model" to avoid confusion. The authors should also engage with recent works that demonstrate the capabilities of few-shot LLMs in outperforming traditional recommendation systems, as mentioned in lines 85-89. Lastly, expanding the experimental dataset and comparing against stronger baseline methods would enhance the robustness of the findings.