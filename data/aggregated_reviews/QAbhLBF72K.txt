ID: QAbhLBF72K
Title: What makes unlearning hard and what to do about it
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 6, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the challenges of machine unlearning, focusing on the properties of forget sets, specifically their entanglement with retain sets and memorization scores. The authors propose a framework called RUM, which categorizes forget sets based on memorization scores and applies different unlearning algorithms to each subset. Additionally, the authors introduce a novel metric, ToW, designed to holistically capture "difficulty" in unlearning tasks, facilitating efficient analyses. They argue that trends observed in ToW align with those from example-level disagreements, suggesting robustness in their findings. The authors clarify that the most meaningful comparison for MIA is between L1-sparse vanilla and L1-sparse RUM, asserting statistical significance in their results. They also discuss the importance of data ordering in unlearning difficulty and propose future work to explore this aspect further.

### Strengths and Weaknesses
Strengths:  
- The paper addresses significant questions regarding the characteristics of forget sets that complicate machine unlearning, providing valuable insights.  
- The proposed RUM framework is well-motivated and demonstrates improved performance through empirical results.  
- The writing is clear and the paper is well-structured, making it accessible to readers.  
- The paper effectively addresses concerns about efficiency and provides a robust analysis of unlearning difficulty using the ToW metric.  
- Statistical significance of results is well-supported, enhancing the credibility of findings.  
- The authors demonstrate a willingness to engage with reviewer feedback, indicating a commitment to improving the paper.

Weaknesses:  
- The efficiency of the proposed methods is not thoroughly discussed, raising concerns about computational costs, especially in large models.  
- The reliance on the ToW metric for evaluation is problematic, as it may not accurately reflect the effectiveness of unlearning algorithms.  
- The metric's reliance on average accuracy may obscure differences in predictive distributions between models, raising concerns about its validity.  
- The experiments are limited to CIFAR datasets and ResNet models, which may restrict the generalizability of the findings.  
- The focus on classification tasks limits the generalizability of findings to other domains, such as generative models.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the efficiency of the RUM framework, particularly regarding the computational costs associated with the memorization score calculations and the refinement step. Additionally, we suggest that the authors consider incorporating MIA into their evaluation of unlearning difficulty, as its omission raises questions about potential biases in their results. It would also be beneficial to extend the experimental evaluations to include a broader range of datasets and model architectures to enhance the generalizability of their findings. We recommend that the authors improve the clarity of the ToW metric by providing a thorough discussion of the differences between average accuracy and example-level disagreements, potentially including results in the appendix. Furthermore, we suggest that the authors consider overhauling the metric to address its inherent flaws, ensuring it accurately reflects model performance. Lastly, expanding the scope of experiments to include diverse datasets beyond CIFAR would enhance the comprehensiveness of the study.