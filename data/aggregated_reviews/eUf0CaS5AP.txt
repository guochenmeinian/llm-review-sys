ID: eUf0CaS5AP
Title: XAGen: 3D Expressive Human Avatars Generation
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 5, 7, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 5, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a generative model for 3D expressive human avatars, enhancing expressive control by employing tri-plane representations for hands and faces, supervised with adversarial loss. The authors demonstrate state-of-the-art performance across various datasets, enabling animation of hand and jaw movements, and showcase two practical applications of the model. The method builds upon existing frameworks, integrating techniques for generating bodies, faces, and hands separately to improve overall quality.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a state-of-the-art generative model for 3D clothed human avatars with expressive control.
2. It is well-written and easy to follow, with extensive and convincing evaluations.
3. The method demonstrates significant improvements in generating high-quality avatars, particularly in facial and hand articulation.
4. The evaluation includes rigorous comparisons with existing methods, showcasing clear performance advantages.

Weaknesses:
1. The technical novelty is limited, as the approach largely combines existing methods without introducing fundamentally new concepts.
2. The quality of facial and hand articulation remains inconsistent, with a lack of comprehensive comparisons against other baselines.
3. Some architectural choices lack clear justification, and the presentation quality could be improved.
4. Limitations regarding the handling of loose clothing and the quality of generated geometry are not adequately addressed in the main text.

### Suggestions for Improvement
We recommend that the authors improve the novelty aspect by clearly articulating the unique contributions of their approach compared to existing methods. Additionally, please provide more comprehensive evaluations against a broader range of baselines to substantiate claims of improvement in facial and hand articulation. We suggest enhancing the presentation quality by clarifying architectural choices and including visual comparisons across multiple datasets. Lastly, we encourage the authors to explicitly address limitations related to the quality of generated geometry and the handling of loose clothing in the main text.