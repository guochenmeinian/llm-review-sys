ID: tzxP9Rx0LV
Title: Knowledge Distillation Performs Partial Variance Reduction
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 6, 6, 7, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of knowledge distillation (KD) from an optimization perspective, revealing that KD acts as a stochastic variance reduction mechanism, contingent on the teacher model's characteristics. The authors provide a theoretical framework for understanding the distillation process, including the convergence guarantees of the KD algorithm and a closed-form solution for the optimal distillation weight in linear models. Empirical experiments validate the theoretical findings, although the core results do not directly apply to deep networks.

### Strengths and Weaknesses
Strengths:
- The paper offers a novel and clear interpretation of the KD objective, providing valuable insights into its mechanics and potential algorithmic implications.
- The theoretical analysis is straightforward and supports the determination of the distillation weight $\lambda$, which, while not fully practical, inspires further exploration.
- Empirical results corroborate theoretical claims, enhancing the paper's credibility.

Weaknesses:
- The paper lacks a direct comparison of theorems 1 and 2 with convergence guarantees from vanilla SGD, limiting insights into the advantages of KD.
- The applicability of the core proposition to deep networks is uncertain, with limited empirical evidence supporting its approximation.
- The analysis does not adequately address why a more capable teacher may not always lead to better student performance, contradicting some prior observations in the field.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the relationship between teacher quality and student performance, particularly addressing the contradictions with prior empirical findings. Additionally, we suggest providing a direct comparison of the convergence guarantees from the proposed method against those from vanilla SGD to clarify the advantages of KD. It would also be beneficial to include more empirical evidence regarding the applicability of the results to deep networks and to clarify the connections between variance reduction and improved training outcomes. Lastly, consider refining the notation for student model parameters to align with common conventions in the literature.