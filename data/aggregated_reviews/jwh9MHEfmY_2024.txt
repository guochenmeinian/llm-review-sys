ID: jwh9MHEfmY
Title: Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 5, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to address the overoptimization issue in reward modeling within reinforcement learning from human feedback (RLHF). The authors propose regularizing hidden states in reward models by utilizing an additional output head that predicts the text of winning trajectories in comparison data. This approach aims to maintain strong representations from pretraining and adapt them to post-training data, enhancing generalization. The paper also introduces a generalizable reward model (GRM) that modifies the standard reward-learning objective by incorporating an auxiliary task with a separate language modeling head.

### Strengths and Weaknesses
Strengths:  
- The paper effectively tackles the significant issue of overoptimization in RLHF, supported by a strong motivation and comprehensive experimental evaluation across various baselines, settings, and datasets.  
- The idea of adding an auxiliary loss is simple yet elegant, with a clear presentation of implementation methods (DPO and SFT).  
- The experiments are thorough, demonstrating consistent improvements from GRM, particularly in low-data scenarios.  

Weaknesses:  
- The proposed method lacks novelty, as combining DPO and SFT loss in reward modeling has been previously explored.  
- There is insufficient experimental evidence supporting the claim that regularizing feature distortion during fine-tuning is beneficial for reward modeling.  
- Some results, such as label smoothing, are missing from key figures, and the experimental section lacks alignment results after RL.  
- The reliance on the reward model in the PPO algorithm raises concerns about the solidity of the experiments, particularly with the inclusion of synthetic data.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their approach by exploring additional auxiliary tasks beyond DPO and SFT. It would be beneficial to provide experimental evidence supporting the motivation for regularizing feature distortion in reward modeling. Additionally, we suggest including the missing results for label smoothing in the figures and expanding the experimental section to include alignment results after RL. Finally, discussing the impact of training the reward models for only 2 epochs and the potential for overfitting would enhance the paper's rigor.