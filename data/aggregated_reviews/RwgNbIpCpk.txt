ID: RwgNbIpCpk
Title: Reparameterized Multi-Resolution Convolutions for Long Sequence Modelling
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 7, 6, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MRConv, a novel approach to parameterizing global convolutional kernels for long-sequence modeling. MRConv employs a multi-resolution strategy, aggregating outputs of sub-kernels with batch normalization and linear rescaling. The authors propose several innovative techniques, including linear reparameterization and weighted combinations of multi-resolution kernels, which demonstrate significant performance improvements across various benchmarks, including Long Range Arena and ImageNet-1K. Extensive empirical evaluations showcase the robustness and versatility of MRConv, achieving near state-of-the-art performance while maintaining computational efficiency. The paper includes detailed ablation studies and comparisons with existing methods, although some reviewers express concerns regarding the clarity and comprehensiveness of these comparisons.

### Strengths and Weaknesses
Strengths:
1. The proposed method addresses a significant challenge in long-sequence modeling, achieving state-of-the-art results on multiple benchmarks.
2. The empirical evaluations are comprehensive, showcasing the robustness and versatility of MRConv across different tasks and modalities.
3. The paper provides essential details in the ablation studies, particularly regarding runtime comparisons and normalized parameter counts, enhancing clarity.
4. The authors demonstrate significant performance improvements through innovative techniques, such as the weighted summation of kernels and multi-resolution Fourier parameterization.
5. The inclusion of runtime plots and FLOPs comparisons adds valuable insights into computational efficiency.

Weaknesses:
1. The paper lacks empirical evaluations on generative tasks, such as language modeling, which are critical for assessing the method's capabilities.
2. There is insufficient theoretical analysis regarding the expressivity of multi-resolution convolution methods compared to state-space models.
3. The additional memory and time costs during training are not adequately discussed, raising concerns about the method's scalability.
4. Some reviewers find the FLOPs comparison misleading, as it lacks comprehensive metrics across different architectures, which may obscure the true computational demands of MRConv.
5. There is a need for more extensive comparisons with recent methods, particularly in the context of NLP, where MRConv's performance relative to attention-based architectures is questioned.
6. The organization of responses to reviewer concerns lacks clarity, making it difficult for reviewers to follow the authors' rebuttals.
7. The paper does not provide a final practical version of MRConv, leaving readers unclear on the most effective implementation.
8. Some statements in the paper lack proper references, which could undermine their validity.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including results on language modeling and generative tasks, as well as conducting experiments in controlled environments to better understand the layer's properties. Additionally, we suggest providing a theoretical analysis of the expressivity of MRConv compared to other models. 

We also advise the authors to include a complexity analysis and throughput evaluations to clarify the memory and time costs associated with training. Furthermore, we recommend improving the clarity of the FLOPs comparison by including additional metrics that reflect computational efficiency, such as the number of parameters and training times. 

It would be beneficial to incorporate more comprehensive comparisons with recent methods like CHELA and GLA to strengthen the paper's claims regarding novelty and performance. We encourage the authors to provide clearer explanations of the convergence plots and the reasons behind the observed training and testing accuracies. Lastly, we recommend presenting a final practical version of MRConv that consolidates the findings from the various kernel parameterization strategies, along with a summary of their performance metrics, and to ensure that the limitations of MRConv are explicitly stated in the final version to enhance transparency. Additionally, we encourage the authors to support their claims with appropriate references and ensure consistent notation throughout the paper to enhance clarity and rigor.