ID: U6fp6IUBdr
Title: Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 5, 7, 7, 5, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a generalization of ProxConnect, introducing ProxConnect++ which incorporates forward-backward quantizers and includes some binarization techniques as special cases. The authors propose BNN++ to demonstrate the effectiveness of ProxConnect++, showing competitive performance on image classification benchmarks with CNNs and vision transformers. The paper also emphasizes the practical implications of the proposed framework, achieving significant memory reduction with a modest accuracy drop.

### Strengths and Weaknesses
Strengths:  
- The paper effectively generalizes ProxConnect and provides a unified framework, ProxConnect++, for designing new forward-backward quantizers.  
- Extensive experiments validate the advantages of BNN++ across various datasets and architectures, demonstrating its practical applicability.  
- The clarity and accessibility of the writing enhance the understanding of the theoretical framework and empirical results.  

Weaknesses:  
- The novelty of the proposed framework is limited, as it primarily extends existing work without introducing new forward and backward quantizers that outperform prior methods.  
- The experimental comparisons are insufficient, lacking engagement with several state-of-the-art binarization techniques and classic works in the field.  
- The number of experimental runs is too low to ensure statistical significance, particularly in critical tables.  

### Suggestions for Improvement
We recommend that the authors improve the novelty of their framework by designing new forward and backward quantizers that demonstrate superior performance compared to existing methods. Additionally, we suggest including comparisons with a broader range of state-of-the-art binarization techniques to enhance the comprehensiveness of the paper. To address statistical concerns, increasing the number of runs in experiments, particularly for Tables 3 and 4, is essential. Furthermore, we advise clarifying the content of Table 2 and considering the inclusion of motivation figures earlier in the paper to strengthen the narrative. Lastly, we recommend revising line 136 from “proved” to “proven” for grammatical accuracy.