ID: Nk2vfZa4lX
Title: Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a qualitative study exploring the perceptions of systematic review experts regarding the utility and risks of large language models (LLMs) in generating medical evidence reviews. The authors conducted interviews with domain experts to characterize the potential applications of LLMs, while also addressing concerns about inaccuracies and accountability. The study outlines evaluation criteria for biomedical LLMs, contributing novel insights into expert perspectives on LLMs for drafting systematic reviews.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and methodologically sound, featuring insightful interviews that provide valuable perspectives on the use of LLMs in medical systematic reviews.
- It identifies 11 general concepts characterizing model-generated summaries, enhancing the understanding of LLM applications.
- The study has the potential to improve the reliability of LLM-generated evidence summaries.

Weaknesses:
- The paper lacks transparency regarding the interview data, as transcripts and additional resources were not released, which could have demonstrated methodological rigor.
- The comparative analysis of the three LLMs (Galactica, BioMedLM, and ChatGPT) is insufficient, as they were evaluated uniformly despite notable differences.
- The prompting techniques employed were basic, failing to leverage the full potential of LLMs, and the sample size presented to experts was limited, impacting the representativeness of findings.

### Suggestions for Improvement
We recommend that the authors release the interview data and its codification to enhance methodological transparency and support future research. Additionally, we suggest conducting a more comprehensive comparative analysis of the LLMs to provide insights into their relative performance in generating systematic reviews. Improving the prompting techniques by using carefully curated prompts could yield better results. Finally, we encourage the authors to contextualize their qualitative approach within the social context of medical research practice, emphasizing ecological validity while addressing the limitations of LLMs.