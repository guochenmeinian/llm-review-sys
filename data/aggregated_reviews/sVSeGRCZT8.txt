ID: sVSeGRCZT8
Title: Three Stream Based Multi-level Event Contrastive Learning for Text-Video Event Extraction
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel three-stream multimodal event extraction approach that incorporates text, visual appearance features, and video motion features through optical flows. The authors propose three components: 1) utilizing optical features, 2) multi-level event contrastive learning to align embedding spaces, and 3) dual querying text that maps tokens to visual appearance features (VAF) or optical flow features (OFF). The approach demonstrates significant improvements over unimodal and multimodal baselines, with most enhancements attributed to components 2 and 3. The experimental results are robust, supported by ablation studies, and the paper is well-written, ensuring reproducibility.

### Strengths and Weaknesses
Strengths:
1. The proposed approach is logical and intuitively utilizes flow-based features for event extraction.
2. The experimental results are solid, clearly demonstrating the impact of each component.
3. The paper includes insightful ablation studies and thorough literature surveys, advancing the state of the art.

Weaknesses:
1. The novelty of using optical flow features is questionable, as they are commonly employed in vision tasks.
2. Section 3.1 lacks clarity in defining the multimodal event extraction task, primarily focusing on text-based events.
3. The claim that previous research disregards motion representation in videos is inaccurate, as video transformers do capture motion.

### Suggestions for Improvement
We recommend that the authors improve the explanation in Section 3.1 to provide a clearer definition of multimodal event extraction. Additionally, addressing the novelty of optical flow features more convincingly would strengthen the paper. We suggest exploring the possibility of using saliency features instead of optical flow features to suppress background noise. Furthermore, we encourage the authors to consider experimenting with different frame sampling strategies, such as importance sampling, and to clarify the rationale behind sampling every 16 frames. Lastly, we recommend addressing the concerns regarding the data requirements for contrastive learning, potentially by integrating ideas from score-based or diffusion models to enhance the framework.