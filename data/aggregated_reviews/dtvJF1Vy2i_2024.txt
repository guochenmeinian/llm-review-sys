ID: dtvJF1Vy2i
Title: What matters when building vision-language models?
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive investigation into critical design choices for vision-language models (VLMs), including unimodal backbones, connectors, visual tokens, and data utilization strategies. The authors conduct extensive experiments, leading to significant findings that contribute to the VLM research community. They ultimately train a foundational VLM with 8 billion parameters that achieves state-of-the-art performance.

### Strengths and Weaknesses
Strengths:  
The writing is clear and easy to follow, with thoroughly conducted experiments that provide valuable insights into VLM design. The focus on timely and important topics enhances the paper's contribution to the AI research community. The availability of trained models and public code further strengthens the work.

Weaknesses:  
The choice to use cross-attention training for the ablation study, despite opting for a fully autoregressive training approach, raises questions about the validity of finding 1. Additionally, the evaluation primarily compares open-source models, lacking a discussion of state-of-the-art commercial models, which would provide a more comprehensive performance assessment. The model's in-context learning ability is also evaluated too narrowly, and some findings appear to contradict existing literature without sufficient discussion.

### Suggestions for Improvement
We recommend that the authors improve the clarity around the use of cross-attention in the ablation study and address the discrepancies in their findings. Including comparisons with state-of-the-art commercial models and more open-source models, as listed on platforms like [MMBench](https://mmbench.opencompass.org.cn/leaderboard), would enhance the evaluation. Additionally, investigating the model's performance with an increased number of demonstrations could provide deeper insights into its capabilities. We also suggest that the authors elaborate on how the new dataset YYYYYY differs from existing instruction tuning datasets and ensure that the superior performance is not solely attributed to the dataset itself. Lastly, a more thorough discussion of the findings in the context of existing works would strengthen the paper's contributions.