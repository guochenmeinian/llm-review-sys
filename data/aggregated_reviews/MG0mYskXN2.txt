ID: MG0mYskXN2
Title: Should Under-parameterized Student Networks Copy or Average Teacher Weights?
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 4, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical study on the optimal configuration for an under-parameterized student network to approximate a teacher network with more neurons. Specifically, it investigates whether the student should copy or average the weights of the teacher, providing a closed-form solution for the case of a single student neuron. The authors analyze the optimization landscape and derive critical points for the student network, focusing on the constrained optimization problem that serves as a relaxation of the original approximation problem.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and presents interesting theoretical results regarding the weight copying and averaging strategies.
- The constrained optimization approach is innovative and provides a compelling framework for understanding the student-teacher relationship.
- The analysis of critical points and the empirical verification of results contribute to the paper's merit.

Weaknesses:
- The strict conditions under which the theoretical results hold limit their practical significance, raising questions about their applicability.
- The intuition behind the theoretical results is not well articulated, making it difficult to grasp their importance.
- The presentation can be confusing due to context switching between different network configurations, and some figures are hard to interpret.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the intuition behind their theoretical results to enhance practical understanding. Additionally, addressing the limitations of the constrained optimization problem in relation to the original problem would strengthen the paper. We suggest including a discussion on how to characterize which teacher neurons to copy in the multi-neuron case, and providing clearer explanations and labels in figures to aid comprehension. Lastly, we encourage the authors to maintain consistency in terminology, particularly regarding activation functions, and to consider adding theorem statements for key results to improve the overall presentation.