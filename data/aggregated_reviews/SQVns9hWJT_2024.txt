ID: SQVns9hWJT
Title: TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 7, 5, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TextCtrl, a diffusion-based method for scene text editing (STE) that addresses limitations in existing GAN-based and diffusion-based approaches. TextCtrl utilizes prior guidance control through text style disentanglement and text glyph structure representation to enhance style consistency and rendering accuracy. A Glyph-adaptive Mutual Self-attention mechanism is introduced to improve visual quality, and the authors create the ScenePair dataset for fair evaluation. Experimental results indicate that TextCtrl significantly outperforms prior methods in style fidelity and text accuracy.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written, with clear presentation.
2. The proposed method effectively targets structural integrity and stylistic consistency, achieving high style fidelity and recognition accuracy.
3. The introduction of the ScenePair dataset contributes valuable resources to the STE community.
4. Experimental results demonstrate substantial performance improvements over existing methods.

Weaknesses:
1. The inclusion of several hyperparameters in TextCtrl raises concerns about user experience in setting them for different datasets.
2. Some figures, such as Figure 3 and Figure 6, lack clarity and could benefit from enhancements, such as highlighting pretrained and trainable parts or reducing text complexity.
3. The paper lacks a comprehensive ablation study to evaluate the contributions of individual tasks in text style processing.
4. The method is limited to text editing and does not encompass text generation capabilities, which restricts its applicability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of hyperparameter settings for user guidance across different datasets. Additionally, we suggest enhancing the clarity of figures, particularly Figure 3 and Figure 6, by highlighting key components and simplifying visual complexity. We also encourage the authors to include a comprehensive ablation study to systematically evaluate the contributions of each task in text style processing. Finally, addressing the limitations regarding the method's scope to include text generation capabilities would enhance its applicability.