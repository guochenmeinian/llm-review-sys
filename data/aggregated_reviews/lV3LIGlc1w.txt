ID: lV3LIGlc1w
Title: Not All Out-of-Distribution Data Are Harmful to Open-Set Active Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an active learning approach for open-set learning, proposing a sampling scheme called Progressive Active Learning (PAL) to intentionally include out-of-distribution (OOD) data to enhance the OOD detector. The authors argue that this method addresses the training imbalance between the ID classifier and OOD detector. Extensive experiments demonstrate the effectiveness of PAL across various open-set active learning scenarios.

### Strengths and Weaknesses
Strengths:
1. The problem addressed is significant and relevant to the machine learning community.
2. The proposal is straightforward and suggests that OOD samples can aid open-set active learning, supporting the claim that "Not All Out-of-Distribution Data Are Harmful."
3. The empirical performance is notable, with significant improvements observed in the experiments.

Weaknesses:
1. The overall framework appears trivial, being a traditional active learning method adapted for an OOD detector and ID learner.
2. The sampling criterion design lacks novelty and plausibility; the uncertainty weight is merely the prediction score from prior work, which does not effectively differentiate the informativeness of different data points.
3. The manuscript lacks theoretical analysis and a discussion of related works in open-set semi-supervised learning.
4. The framework's flow and retraining process descriptions are unclear, and important baselines are missing from the comparisons.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis to support their claims and include discussions on related works such as OAT, ODNL, and Open-sampling. Clarifying the flow of the framework and the retraining process is essential, particularly how f^c is trained and the final loss used. Additionally, the authors should consider using alternative metrics for obtaining s^{ID} and include important baselines in their comparisons. A computational cost analysis of the PAL method, especially regarding the meta-weight learning process, would also enhance the manuscript's practicality.