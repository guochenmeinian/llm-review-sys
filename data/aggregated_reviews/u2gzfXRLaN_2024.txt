ID: u2gzfXRLaN
Title: Transformation-Invariant Learning and Theoretical Guarantees for OOD Generalization
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 4, 4, 6, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical framework for learning under distribution shifts characterized by a family of transformations $\mathscr{T}$. The authors aim to train a classifier that minimizes test error across all possible transformations applied to a distribution $D$. The study explores both known and unknown hypothesis classes $\mathcal{H}$, deriving sample complexity bounds based on the VC dimension of the composite class $\mathcal{H} \circ \mathscr{T}$. The results include generalization error bounds for various scenarios, including worst-case risks and regret minimization. Additionally, the authors introduce a novel approach to multi-distribution learning by providing transformations \( T_i \) in advance, allowing the learner to generate \( k \) transformed samples from a single sample of distribution \( D \). They clarify that the reduced sample complexity arises from this additional structure, contrasting it with traditional multi-distribution learning, which requires \( k \) samples to access each distribution \( D_i \).

### Strengths and Weaknesses
Strengths:
- The authors introduce a clean theoretical framework that captures various distribution shift scenarios, appealing to the theory audience at NeurIPS.
- The model provides mathematically rigorous results for sample complexity in terms of the VC dimension of the composite class.
- The paper discusses practical applications of the framework, including settings relevant to modern neural network architectures.
- The paper effectively clarifies the relationship to multi-distribution learning, enhancing the reader's understanding.
- The simplicity of the new model allows it to cover previously studied settings, making it easier for readers to position the work relative to related literature.

Weaknesses:
- Many results beyond the main upper bound feel incomplete; for instance, the lower bound based on VC dimension is weaker than typically expected.
- The use of data augmentation in the unknown hypothesis setting raises concerns about the computational cost, as the model assumes the ERM oracle can handle potentially infinite augmented samples.
- The theoretical contributions largely rely on standard VC theory, lacking clear technical novelty.
- Some reviewers express concerns regarding the novelty of insights provided, suggesting that the results may primarily stem from existing findings rather than introducing significant new contributions.

### Suggestions for Improvement
We recommend that the authors improve the characterization of lower bounds to provide a stronger combinatorial understanding of the model. Additionally, addressing the computational implications of data augmentation in the unknown hypothesis scenario would enhance the robustness of the framework. Clarifying the novelty of the theoretical results and their practical implications in the empirical evaluations would also strengthen the paper's contribution. Furthermore, we suggest that the authors improve the discussion on the unique insights their approach offers beyond the specific algorithms for previously studied settings. Lastly, further clarification on the implications of the reduced sample complexity in practical applications could enhance the paper's impact.