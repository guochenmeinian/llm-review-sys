ID: SQicD307Oh
Title: State-free Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 7, 4, -1, 7
Original Confidences: 4, 2, -1, 4

Aggregated Review:
### Key Points
This paper presents a black-box approach to transform any no-regret algorithm into a state-free algorithm, addressing a significant issue in reinforcement learning (RL) theory. The authors focus on establishing theoretical regret bounds for this approach, which is particularly relevant for stochastic or adversarial MDPs. The proposed algorithm is adaptive, removing the dependency on the size of the state space, and its performance scales with the set of reachable states.

### Strengths and Weaknesses
Strengths:  
- The introduction and related work are well-written, providing clarity on the current challenges in RL theory.  
- The theoretical results are significant and the algorithmic solution is elegant, applicable to any basic RL algorithm with regret guarantees.  
- The results hold for both stochastic and adversarial settings, and can extend to removing the dependency on the horizon H.

Weaknesses:  
- The algorithm description is difficult to follow due to undefined symbols and ambiguous definitions, making verification of results challenging.  
- There is a lack of toy experiments to illustrate the behavior of state-free versus non-state-free algorithms under incorrect or insufficient state knowledge.  
- The paper could benefit from a clearer comparison of final bounds in the stochastic setting and explicit bounds for simple doubling trick strategies.  
- The role of epsilon is unclear, as it appears to be set to 0 without affecting the outcome.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the algorithm description by defining all symbols and providing unambiguous definitions. Additionally, including toy experiments would enhance understanding of the algorithm's performance under varying conditions. We suggest providing a clean comparison of the final bounds in the stochastic setting and making explicit the bounds for simple doubling trick strategies. Finally, clarifying the role of epsilon in the algorithm would strengthen the paper.