ID: gMjIUZBKH8
Title: AdaVAE: Bayesian Structural Adaptation for Variational Autoencoders
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a structural adaptation algorithm for Variational Autoencoders (VAEs), termed AdaVAE, which aims to optimize the network structure by adapting both depth and width through a Bayesian framework. The authors utilize a Beta Process for network depth inference and a Bernoulli process for neuron pruning in hidden layers. The experimental evaluation highlights the framework's adaptation capabilities, convergence behavior, and overfitting prevention. The authors also discuss integration with various VAE backbones and variants.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant yet under-explored aspect of VAE architecture, focusing on structural adaptation.
- The methodology is technically sound and presented clearly, making it accessible to readers.
- The proposed approach shows promise in preventing overfitting and improving performance compared to existing regularization methods.

Weaknesses:
- The authors do not sufficiently differentiate their work from existing methods that utilize Beta-Bernoulli processes, leading to a lack of depth in the comparative discussion.
- The experimental evaluation is limited to simple network structures, lacking tests on more complex architectures.
- Key details regarding metrics, prediction processes, and computational complexity are inadequately addressed, which could hinder understanding and practical application.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related works, particularly focusing on the differences and advantages of their approach compared to existing methods like [4]. Additionally, the authors should expand their experimental evaluation to include more complex network structures and provide a clearer analysis of the chosen metrics and their implications. Addressing the stability of the training process and the computational complexity of the approach is essential. Finally, including a training algorithm and clarifying the prediction process would enhance the paper's clarity and accessibility.