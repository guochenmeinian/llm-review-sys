ID: rgtrYVC9n4
Title: Discovering Sparsity Allocation for  Layer-wise Pruning of Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 32
Original Ratings: 4, 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DSA, an automated framework for determining layer-wise sparsity in large language models (LLMs). The authors propose using an evolutionary algorithm to discover optimal sparsity allocation functions, which enhances pruning techniques and improves model performance across various tasks. Extensive experiments indicate that DSA outperforms existing methods like SparseGPT and Wanda on multiple benchmarks. The paper also introduces a novel allocation function applied to ConvNeXt, demonstrating superior performance at higher sparsity levels compared to existing methods, with significant gains in accuracy across models such as LLaMA-1, LLaMA-2, and LLaMA-3, showing improvements ranging from 1.24% to 7.68%. A comprehensive comparison of DSA against ECoFLaP reveals consistent advantages across multiple models and datasets.

### Strengths and Weaknesses
Strengths:  
1. The paper addresses a significant issue in LLM efficiency by introducing a novel automated framework for sparsity allocation.  
2. Extensive empirical results demonstrate that DSA consistently improves performance across various tasks and datasets.  
3. The proposed DSA method exhibits strong generalizability and effectiveness across different models and sparsity levels.  
4. The authors provide detailed experimental results that substantiate their claims of improved performance.  
5. The allocation function's architecture-agnostic nature enhances its applicability to various model structures.  

Weaknesses:  
1. The novelty of the approach is limited, as it combines existing techniques like AutoML and evolutionary algorithms, which are well-explored.  
2. Some existing works, such as "ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models," address similar problems, potentially diminishing the perceived novelty.  
3. The methodology may require significant computational resources, which could limit practical applications.  
4. Results on certain benchmarks, while improved, may not be groundbreaking enough for acceptance in top-tier conferences.  
5. Some reviewers express confusion regarding the overall design and implementation pipeline of the method.  
6. There are lingering questions about the practical transferability of the allocation function to models with differing architectures.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology, particularly in Section 4, by providing insights into the design of the allocation function and the rationale behind the preprocessing, reduction, transformation, and post-processing steps. Additionally, we suggest demonstrating results on higher sparsity levels, such as 60% and 70%, to strengthen the findings. It would also be beneficial to explore the performance of DSA in structured sparsity settings, as this could yield more meaningful acceleration. Furthermore, we recommend improving the clarity of the method's design and implementation pipeline to address reviewer confusion. Finally, we suggest providing more detailed explanations or examples of how the allocation function can be effectively transferred to models with different structures, such as LLaMA2-70B or the OPT model family, to enhance understanding and confidence in the method's applicability across various architectures.