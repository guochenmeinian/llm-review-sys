ID: ZZoW4Z3le4
Title: DiGRAF: Diffeomorphic Graph-Adaptive Activation Function
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 5, 6, 5, 5, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents DIGRAF, a Diffeomorphic Graph-Adaptive Activation Function designed for graph data in Graph Neural Networks (GNNs). The authors argue that the activation function should be node-specific rather than uniform, leveraging Continuous Piecewise-Affine Based transformations. Extensive experiments demonstrate that DIGRAF outperforms various baseline activation functions across multiple datasets and tasks, showcasing its adaptability and effectiveness.

### Strengths and Weaknesses
Strengths:
1. DIGRAF is a novel activation function that exhibits desirable properties such as differentiability, zero-centering, and permutation equivariance, with solid theoretical guarantees.
2. The experimental analysis is comprehensive, effectively supporting the claims with robust data across node classification, graph classification, and regression tasks.
3. The paper addresses critical questions regarding the proposed method's effectiveness, providing clear evidence and well-designed figures that enhance understanding.

Weaknesses:
1. The primary contribution appears to be the introduction of GNN_act, but the discussion on its effectiveness is relatively brief; more detailed theoretical justifications are needed.
2. There is a lack of ablation studies to clarify how each property and design contributes to performance gains, leaving questions about the necessity of certain components.
3. The paper does not adequately analyze the performance and gradient optimization aspects, nor does it clarify how GNN_act boosts performance or its specific contributions.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding GNN_act by providing more detailed theoretical justifications for its effectiveness. Additionally, conducting more ablation studies would help elucidate the contributions of various properties to performance gains. We also suggest including a detailed analysis of performance and gradient optimization aspects to strengthen the paper's claims. Furthermore, clarifying the influence of DIGRAF's properties on convergence and performance would enhance the understanding of its advantages. Lastly, addressing the hyper-parameter effects and providing source code would improve transparency and reproducibility.