ID: JzQ7QClAdF
Title: Opening the Vocabulary of Egocentric Actions
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 4, 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for open vocabulary object understanding in egocentric videos, focusing on action recognition. The authors propose using CLIP features to generalize to unseen objects through a combination of object mixing and learned prompts, tested on datasets such as EPIC-Kitchens-100, Assembly 101, and Something-Else. The results indicate that the proposed method outperforms existing approaches across all datasets for both seen and unseen nouns. The paper also introduces object agnostic pretraining and active object prompting to enhance recognition capabilities.

### Strengths and Weaknesses
Strengths:
- The method effectively expands action recognition beyond a closed vocabulary, utilizing the CLIP aligned image-text space.
- The thoughtful setup of open vocabulary benchmarks and the comprehensive experiments enhance the validity of the proposed approach.
- The ablation studies demonstrate the effectiveness of most proposed components.

Weaknesses:
- The focus on open vocabulary for objects, excluding verbs, limits the baseline's usefulness for future research.
- Some design choices lack empirical motivation, such as the use of pre and post context prompts and the decision to keep the CLIP text encoder frozen while fine-tuning object embeddings.
- Comparisons with relevant prior works are insufficient, and the impact of certain components, like L_in, remains unclear.

### Suggestions for Improvement
We recommend that the authors improve Figure 2 to clearly illustrate examples of anchor, positive, mixed, and negative objects. Additionally, the authors should provide values for k1 and k2, clarify their selection process, and investigate the sensitivity of the model to these parameters. Line 210 should be revised for clarity regarding the fine-tuning of object embeddings. Furthermore, the authors should explore the values of alpha and lambda, detailing how they were chosen and the model's sensitivity to these hyperparameters. Lastly, we suggest including comparisons with additional open vocabulary object recognition methods and addressing the potential impact of class balancing versus object mixing in the ablation studies.