ID: pFTBsdZ1UM
Title: Indicative Summarization of Long Discussions
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a prompt-based zero-shot method for generating indicative summaries of long discussions by clustering argument sentences into subtopics, creating concise summaries for each cluster, and assigning media frames to these summaries. The authors evaluate their approach using 19 LLMs, demonstrating that GPT3.5 outperforms other open-source models. The study includes a user evaluation of the clustering and labeling process.

### Strengths and Weaknesses
Strengths:
- The paper conducts comprehensive experiments with a variety of LLMs, allowing for a robust comparison of state-of-the-art models.
- The annotation interface for ranking cluster quality is user-friendly and practical.
- Detailed explanations of preprocessing steps and clustering methods enhance reader understanding.

Weaknesses:
- The definition of indicative summarization lacks clarity, particularly in distinguishing it from extreme summarization.
- The evaluation of end-to-end task performance is insufficient; comparisons with summarization methods are missing.
- The design choices, particularly regarding sentence clustering and frame assignment, are not well-justified, and the technical novelty is unclear.
- The user study's baselines could be improved, and the corpus-based filtering's importance requires further exploration through an ablation study.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the indicative summarization definition and explicitly differentiate it from extreme summarization. Additionally, the authors should include comparisons with other summarization methods in their evaluations to better substantiate their claims. A discussion on the design choices, including the rationale for clustering before frame assignment, is necessary, along with an ablation study to assess the impact of corpus-based filtering. Finally, enhancing the user study by including more participants and alternative baselines would provide a more comprehensive evaluation of the proposed method.