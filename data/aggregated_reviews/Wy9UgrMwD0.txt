ID: Wy9UgrMwD0
Title: No Representation, No Trust: Connecting Representation, Collapse, and Trust Issues in PPO
Conference: NeurIPS
Year: 2024
Number of Reviews: 26
Original Ratings: 6, 7, 6, 6, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates feature collapse in the Proximal Policy Optimization (PPO) algorithm, revealing that increased sample reuse leads to deterioration in feature rank and plasticity. The authors establish connections among feature rank/norm, plasticity loss, trust region violation, and learning performance, asserting that their work is the first to highlight these relationships in on-policy algorithms. They propose several techniques to mitigate feature collapse, including feature regularization, network sharing, and adjustments to Adam hyperparameters, although these solutions do not fully resolve the problem. Additionally, the authors introduce Proximal Feature Optimization (PFO) to address learning challenges under non-stationarity, demonstrating its effectiveness in mitigating issues related to feature representation.

### Strengths and Weaknesses
Strengths:
- This is the first work to explore feature collapse in on-policy algorithms, providing novel insights into representation dynamics and their implications for performance.
- The experimental design is robust, utilizing appropriate benchmarks like Atari and MuJoCo, with extensive and well-conducted experiments that employ multiple metrics, including feature rank, feature norm, plasticity loss, and dead neuron counts.
- The connection between feature collapse and trust region violations is intriguing and distinguishes this work from previous studies, contributing valuable insights to the field.

Weaknesses:
- The demonstration that training for more epochs leads to feature collapse is atypical for PPO, as it is uncommon to run PPO for 6 or 8 epochs.
- The explanation of feature collapse in critic networks is insufficient and requires further elaboration.
- While the proposed PFO method mitigates feature rank, plasticity loss, and excess ratios, it does not demonstrate clear improvements in learning performance, such as episode return, which undermines its significance.
- Key claims regarding the capabilities of on-policy methods and the effects of plasticity loss are contested by existing literature, which may weaken the authors' arguments.

### Suggestions for Improvement
We recommend that the authors improve the explanation of feature collapse in critic networks in Section 3.1. Additionally, consider providing a clearer solution to mitigate feature collapse in PPO, as shown in Figure 6. We suggest investigating the impact of replacing the clipping operation with a KL penalty, as proposed in the original PPO paper, and exploring the application of regularization in weight space using an exponentially weighted moving average (EWMA) update. Furthermore, it would be beneficial to analyze Phasic Policy Gradient (PPG) to see if similar patterns of feature collapse emerge. We also recommend improving the organization of empirical results and conclusions to enhance clarity and impact. More discussion is needed to differentiate PFO from existing works like DR3 and RD, emphasizing its unique contributions. Additionally, we advise the authors to acknowledge relevant prior work, particularly studies by Dohare et al. and Abbas et al., to provide a comprehensive context for their findings and strengthen the overall narrative of the paper. Finally, we encourage the authors to clarify the relationship between representation collapse and trust region collapse, ensuring that mitigation strategies are discussed throughout the training process.