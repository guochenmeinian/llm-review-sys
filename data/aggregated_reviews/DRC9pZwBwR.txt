ID: DRC9pZwBwR
Title: Recursive Introspection: Teaching Language Model Agents How to Self-Improve
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RISE: Recursive IntroSpEction, a novel approach for fine-tuning Large Language Models (LLMs) to enhance their self-improvement capabilities. The authors propose treating the fine-tuning process as a multi-turn Markov decision process (MDP), allowing LLMs to introspect, reason, and correct mistakes over multiple iterations. The RISE algorithm employs on-policy rollouts and reward-weighted regression (RWR) objectives, demonstrating significant performance improvements on math reasoning tasks with models like Llama2 and Mistral.

### Strengths and Weaknesses
Strengths:
- The proposed RISE framework is novel and general, potentially applicable to a wide range of tasks.
- The iterative fine-tuning and on-policy rollouts provide a robust mechanism for models to learn from their mistakes.
- The experiments are comprehensive, featuring diverse benchmarks and thorough ablation studies.

Weaknesses:
- The current learning method primarily resembles a distillation process; exploring direct reinforcement learning over trajectories could enhance the approach.
- Experiments are mainly focused on math benchmarks; including additional task types could strengthen the findings.
- While the authors provide reasons for improved outputs through an improving trajectory, concrete examples would enhance the argument.
- The similarity of RISE to in-context reinforcement learning warrants further discussion.

### Suggestions for Improvement
We recommend that the authors improve the exploration of direct reinforcement learning over trajectories to enhance the learning method. Additionally, incorporating experiments on a broader range of tasks, such as natural language understanding and code generation, would strengthen the paper. Providing concrete examples to support the hypotheses regarding improving trajectories would also be beneficial. Finally, we suggest including a discussion on the relationship between RISE and in-context reinforcement learning to clarify its unique contributions.