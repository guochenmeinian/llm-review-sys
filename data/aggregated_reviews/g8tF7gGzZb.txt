ID: g8tF7gGzZb
Title: Computational Pathology at Health System Scale â€“ Self-Supervised Foundation Models from Billions of Images
Conference: AAAI
Year: 2024
Number of Reviews: 4
Original Ratings: 7, 8, 6, 7
Original Confidences: 4, 4, 3, 5

Aggregated Review:
### Key Points
This paper presents a comprehensive study on self-supervised learning (SSL) in computational pathology, focusing on the pre-training and downstream performance evaluation of visual foundation models using a massive dataset of over 3 billion images. The authors propose three models based on Visual Transformer Architecture combined with DINO and MAE algorithms, demonstrating superior performance on clinically relevant tasks compared to models pre-trained on general image data. The findings indicate a significant advancement in computational pathology research.

### Strengths and Weaknesses
Strengths:
- The study addresses a critical gap in applying SSL algorithms in computational pathology.
- The compilation of the largest academic pathology dataset to date is a significant contribution.
- The comparison of pre-training methods provides valuable insights for developing performant models.
- The authors' intention to open-source their model is a valuable contribution to the field.

Weaknesses:
- The paper lacks a detailed discussion on the limitations and challenges of SSL algorithms in clinical workflows.
- There is insufficient emphasis on the potential impact of findings on real-world clinical applications.
- Ethical considerations and potential biases associated with large-scale datasets are not adequately discussed.
- Some experimental results and figures could be clearer, and there are inconsistencies in model reporting.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their figures by reducing the number of results shown across epochs, as this complicates interpretation. Instead, consider using tables or bar charts for selected models. Additionally, we suggest that the authors clarify the rationale behind not displaying all models in Figure 2 and include the main results from Supplementary Figure 1 in the main paper. The authors should also ensure that the baseline model architecture matches that of the experimental models to isolate the effect of pre-training data. Furthermore, we encourage the authors to discuss the poor performance observed in Task 6 and explore the reasons behind the performance of ViT-large with MAE. Lastly, we recommend providing more demographic details about the patient samples to enhance understanding and consider including more SSL-based baselines to strengthen their claims of superiority.