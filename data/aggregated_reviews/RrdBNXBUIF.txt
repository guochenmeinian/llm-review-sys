ID: RrdBNXBUIF
Title: Cluster-aware Semi-supervised Learning: Relational Knowledge Distillation Provably Learns Clustering
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 5, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of relational knowledge distillation (RKD) in a semi-supervised learning context, diverging from previous knowledge distillation theories that primarily emphasize feature matching. The authors introduce the concept of low clustering error to quantify discrepancies between predicted and actual clusterings, demonstrating that RKD can achieve low clustering error. They also establish a label efficiency theorem within a cluster-aware semi-supervised learning framework and unify data augmentation consistency regularization with RKD, highlighting the local focus of consistency regularization versus the global perspective of RKD.

### Strengths and Weaknesses
Strengths:  
- Originality: The focus on relational aspects rather than mere feature matching is a significant and interesting direction. The exploration of clustering awareness in semi-supervised learning is also novel.  
- Quality: The paper is well-written, with rigorous definitions, theorems, and proofs.  
- Clarity: The introduction effectively outlines the main contributions, and the technical content is presented in a structured manner.  
- Significance: The paper establishes a valuable connection between RKD and spectral clustering, providing theoretical insights into various aspects.

Weaknesses:  
- The intuition behind Assumption 4.2 is unclear, and the toy example provided is overly specialized, failing to clarify the complex definitions. More elaboration and a non-trivial example would enhance understanding.  
- The necessity of considering limited unlabeled samples is questionable; additional motivation in Section 4.2 could improve the presentation.  
- The connections between results, particularly regarding the DAC and RKD, are weak and may lead to misconceptions about their relationship.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Assumption 4.2 by providing more intuitions and a non-trivial example to illustrate its relevance. Additionally, we suggest adding more motivation for the limited unlabeled samples setting in Section 4.2 to enhance the paper's presentation. Furthermore, we advise strengthening the connections between the results, particularly clarifying how the DAC and RKD relate to avoid potential misconceptions in the introduction. Finally, we encourage the authors to conduct more extensive empirical studies on larger datasets to substantiate the effectiveness of the proposed method.