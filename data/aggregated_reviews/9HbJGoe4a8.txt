ID: 9HbJGoe4a8
Title: Sound of Story: Multi-modal Storytelling with Audio
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Sound of Story (SoS), a large multi-modal story dataset focusing on audio, comprising 27,354 stories derived from CMD and LSMDC datasets. The authors propose two tasks: cross-modality retrieval (audio <-> video and audio <-> text) and audio generation. The dataset's audio components are decoupled from speech to prevent language information leakage, which is intended to enhance retrieval tasks. Baseline results indicate reasonable performance improvements over random baselines.

### Strengths and Weaknesses
Strengths:  
- The unique focus on speech-decoupled audio contributes to the multi-modal storytelling field.  
- The dataset is substantial and will be publicly available, potentially benefiting the community.  
- The demo website showcases promising baseline model performance.

Weaknesses:  
- The genre of audio clips is unclear, and a high-level categorization would aid in understanding model capabilities for error analysis.  
- The rationale for removing speech from audio is not well justified, raising questions about the dataset's motivation.  
- Technical details regarding similarity score computations and the retrieval section's experimental results are insufficiently explained.  
- The proposed tasks appear to lack a strong connection to the SoS dataset, as similar tasks could be performed with other video datasets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the audio clip genres by providing a high-level categorization. Additionally, the authors should better articulate the significance of the speech-decoupling process and its implications for multimodal retrieval. It would be beneficial to include details on score computation in Equations (1) and (2) and clarify the aggregation process for similarity computations. We also suggest incorporating results from pretrained multimodal audio-visual (wav2CLIP) or audio-text encoders (CLAP) in the retrieval experiments. Lastly, the authors should clarify the relationship between the models $SOS_{image}$ and $SOS_{image+text}$ in the audio generation setup.