ID: nN8TnHB5nw
Title: Memory Efficient Optimizers with 4-bit States
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 7, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for quantizing optimizer states to 4-bit to reduce memory consumption during training, specifically focusing on the 4-bit AdamW optimizer. The authors analyze first and second momentums, addressing the zero-point problem in second-order momentum quantization. They provide empirical results demonstrating that the quantization time overhead is relatively low for LLaMA-7B due to a large gradient accumulation step and significant communication overhead. Extensive experiments are conducted across various tasks, including vision and language models, showcasing the robustness and efficiency of the proposed method. The authors highlight that the fused implementation of 4-bit AdamW consistently outperforms the 32-bit version in terms of speed, particularly due to reduced memory footprint and faster optimizer operations.

### Strengths and Weaknesses
Strengths:
- The analysis of outlier patterns in momentums is thorough, providing valuable insights.
- The paper includes extensive empirical data on the performance of 4-bit AdamW across different models, showcasing its efficiency.
- The analysis of time overhead and memory usage is well-structured, providing clear insights into the benefits of quantization.

Weaknesses:
- The evaluation is limited to fine-tuning LLaMA-7B on Alpaca, lacking assessments on larger models like LLaMA-33B or LLaMA-65B.
- The accuracy of the 4-bit optimizer is lower than that of fp16 or int8 versions, raising concerns about stability across various settings.
- The quantization time overhead for certain configurations is not explicitly measured, which may limit the understanding of its impact in those cases.
- The comparison with Dettmers et al.'s 8-bit optimizers could be expanded to clarify the advantages of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including experiments on larger models such as LLaMA-33B and LLaMA-65B to better demonstrate the method's scalability. Additionally, it would be beneficial to assess the performance on common sense reasoning tasks and MMLU to validate the optimizer's effectiveness in preserving instruction-following abilities. We also suggest enhancing the clarity of figures by increasing font sizes for better readability. Furthermore, we recommend improving the clarity of the quantization time overhead measurements for all configurations, ensuring that all relevant data is presented. Lastly, we suggest including a more detailed comparison with Dettmers et al.'s 8-bit optimizers to better highlight the advantages of their approach.