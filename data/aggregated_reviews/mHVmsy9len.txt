ID: mHVmsy9len
Title: Bounds for the smallest eigenvalue of the NTK for arbitrary spherical data of arbitrary dimension
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 6, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 3, 5, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the smallest eigenvalue of the Neural Tangent Kernel (NTK) for shallow and deep connected ReLU networks, utilizing the hemisphere transform. The authors propose bounds that are distribution-independent, only requiring that data lies on the unit sphere and is not too collinear. The results extend previous work by relaxing assumptions on input data dimensions and distribution, making them applicable to a broader range of machine learning problems.

### Strengths and Weaknesses
Strengths:
- The paper drops the input data dimension requirement from previous studies, enhancing flexibility.
- It introduces innovative techniques, such as the hemisphere transform and harmonic analysis, to analyze the NTK.
- The results are relevant for understanding neural network dynamics and are applicable to both shallow and deep networks.

Weaknesses:
- The requirement for data to be $\delta$-separated is not as general as claimed, lacking sufficient justification.
- The paper is limited to ReLU activation functions, which may restrict its applicability.
- There is a need for clearer comparisons to previous work, particularly regarding Corollary 2 and the bounds derived from it.

### Suggestions for Improvement
We recommend that the authors improve the justification for the $\delta$-separation requirement and clarify its implications. Additionally, the authors should consider including empirical studies to support their theoretical claims. We suggest restructuring the paper to reduce redundancy between the introduction and Section 2, and to enhance discussions on how the bounds influence deep neural network performance. Furthermore, addressing the limitations related to the ReLU activation function and exploring generalizations would strengthen the paper's contributions.