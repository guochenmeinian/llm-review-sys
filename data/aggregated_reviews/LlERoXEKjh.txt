ID: LlERoXEKjh
Title: Training shallow ReLU networks on noisy data using hinge loss: when do we overfit and is it benign?
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 7, 8, 6, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of overfitting in shallow ReLU networks trained with hinge loss on noisy data, specifically focusing on label flipping noise. The authors analyze a setting where data points are defined by a direction $\mathbf{v}$ for clean labels, with noise orthogonal to $\mathbf{v}$. They identify conditions leading to benign overfitting, harmful overfitting, and “non-overfitting,” based on the signal strength and the trajectory of gradient descent. The work contributes to understanding benign overfitting in nonlinear networks, a topic that has seen limited exploration compared to linear models.

### Strengths and Weaknesses
Strengths:
- The paper addresses an intriguing question regarding benign overfitting, contributing significantly to the understanding of shallow fully-connected ReLU networks under hinge loss.
- The results provide novel insights into the relationship between signal strength and types of overfitting, with a detailed analysis of gradient descent dynamics.
- The proof techniques and intuitions are interesting and enhance comprehension of the network's behavior.

Weaknesses:
- There is a discrepancy between Assumption 1 in the main text and the appendix, particularly regarding the inclusion of the parameter $\rho$. The authors should clarify the relationship between these assumptions.
- The paper's setting is quite specific, and the implications of not including bias terms in the network are not fully explored. The authors should discuss how adding biases might affect the results.
- The writing contains several minor errors and lacks clarity in some areas, which detracts from the overall presentation.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the relationship between the two versions of Assumption 1 to clarify their implications. Additionally, the authors should address how the inclusion of bias terms might alter the results and consider exploring Gaussian noise distributions to assess the robustness of their findings. We suggest that the authors provide an upper bound for Theorem 3.6, as this could enhance the understanding of overfitting behavior under varying margins. Furthermore, we encourage the authors to refine the writing for clarity and to correct identified typos and ambiguities. Lastly, emphasizing the significance of using hinge loss compared to logistic loss in the introduction could strengthen the paper's impact.