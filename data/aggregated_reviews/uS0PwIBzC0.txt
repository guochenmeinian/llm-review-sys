ID: uS0PwIBzC0
Title: SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 6, 3, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a parameter-efficient fine-tuning (PEFT) method named Singular Value Fine-Tuning (SVFT), which utilizes Singular Value Decomposition (SVD) of pretrained weights to enhance fine-tuning efficiency. The authors propose a learned residual $\Delta W = UMV^\top$, where $M$ is a sparse trainable matrix, leading to fine-tuned weights $\hat{W} = U\Sigma V^\top + UMV^{\top}$. Four variants for sparsity are introduced: Plain, Banded, Random, and Top-k. The method claims to recover up to 96% of full fine-tuning performance while training only 0.006 to 0.25% of the parameters. The authors provide theoretical insights into the method's properties regarding structure, expressivity, and rank of the fine-tuning residuals. Additionally, the paper includes a comparative analysis of SVFT variants, specifically $SVFT^R$, $SVFT^T$, and $SVFT^P$, highlighting the implications of parameter constraints and rank reductions in fine-tuning performance. Significant performance gains are noted when the total number of learnable parameters exceeds that of Plain, particularly starting at d=2. The empirical findings suggest that Singular Vectors can enhance fine-tuning, warranting further theoretical exploration.

### Strengths and Weaknesses
Strengths:  
1. The approach of using SVD for parameter-efficient fine-tuning is innovative and interesting.  
2. The increase in trainable parameters is lower than that of existing methods like LoRA/DoRA when adapting more layers.  
3. The paper offers theoretical insights into the method's properties.  
4. SVFT demonstrates better performance than VeRA, suggesting its efficacy in reducing trainable parameters without the dimensionality restrictions present in VeRA.  
5. The extensive empirical results across various tasks demonstrate the effectiveness of SVFT variants compared to existing methods.  
6. The authors provide a clear rationale for their approach, emphasizing the learnability of additional off-diagonal elements.  
7. The observation regarding performance loss due to truncating singular vectors is informative.  

Weaknesses:  
1. **Missing citation and comparison with prior work**: The method is similar to previous work [1], which has not been cited, necessitating a comparison for completeness.  
2. **Clarity and coherence of writing**: The paper lacks citations for baseline methods in Table 3, does not clarify baseline methods in the Appendix, and inconsistently mentions adapted weight types across benchmarks.  
3. **Increased memory usage during training**: While SVFT reduces parameter count, it may lead to higher memory usage, which needs to be reported.  
4. **Inconsistencies in evaluations**: The choice of adapted weight types varies across models, complicating performance attribution and necessitating consistent comparisons with LoRA and DoRA.  
5. **Confusing results on vision tasks**: The performance of SVFT on different benchmarks is unclear, and comparisons with LoRA and DoRA need to be more explicit.  
6. The exploration of optimal sparsity patterns is deemed insufficient, lacking a clear methodology for task-specific designs.  
7. The significance of the proposed SVFT$^T$ variant is not convincingly established, raising questions about its value relative to other variants.  
8. The motivation and contribution of the SVFT paradigm are perceived as unclear due to its prior discussion in the literature.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of citations and comparisons with prior work, particularly with [1]. Additionally, the authors should enhance the coherence of writing by consistently mentioning baseline methods and adapted weight types throughout the paper. It is crucial to report actual memory usage during training for all methods, particularly when adapting multiple weight types. To address inconsistencies in evaluations, we suggest that the authors perform comparisons with LoRA and DoRA using the same adapted weight types. Furthermore, we encourage the authors to explore and articulate a more comprehensive approach to designing appropriate sparse patterns tailored to downstream tasks. Lastly, the authors should clarify the motivation and contribution of the SVFT paradigm, particularly in distinguishing it from previously discussed methods, and provide a clearer justification for the exploration of SVFT$^T$ and its relevance within the broader context of their findings.