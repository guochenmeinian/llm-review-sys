ID: o91in9tDEs
Title: Distributional Policy Evaluation: a Maximum Entropy approach to Representation Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 2, 6, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 1, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel Maximum Entropy framework for policy evaluation in distributional reinforcement learning (RL), introducing two algorithms: Distributional Maximum Entropy Policy Evaluation (D-Max-Ent PE) and D-Max-Ent Progressive Factorization. The authors derive a generalization error bound and propose a progressive state space factorization to refine the representation while balancing bias and variance. Experiments are conducted to demonstrate the effectiveness of the proposed methods.

### Strengths and Weaknesses
Strengths:
- The introduction of Maximum entropy density estimates into distributional RL is a novel contribution.
- The proposed progressive factorization appears to enhance the representation of RL.
- The writing is generally clear and the theoretical results are well-structured.

Weaknesses:
- The motivation for considering maximum entropy estimates in representation learning is unclear, leaving the benefits of the proposed methods ambiguous.
- The paper lacks rigorous discussion of the RL context, particularly regarding the Bellman operator and contraction mapping, which raises questions about the technical soundness of the approach.
- Empirical evaluations are limited and do not sufficiently demonstrate the effectiveness of the methods in real-world distributional RL environments.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the representation issue in distributional RL, explicitly defining what this issue entails. Additionally, we suggest that the authors incorporate a discussion on the contraction properties of the Bellman operator under the maximum entropy density estimate to strengthen the technical foundation of their claims. To enhance the empirical validation of their approach, we encourage the authors to conduct experiments in more complex environments, such as Atari games, to better illustrate the practical benefits of their proposed algorithms. Finally, we advise adding legends and clearer explanations for the figures to improve the presentation of experimental results.