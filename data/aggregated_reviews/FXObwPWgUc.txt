ID: FXObwPWgUc
Title: Leveraging GPT-4 for Automatic Translation Post-Editing
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of using large language models (LLMs), specifically GPT-3.5 and GPT-4, for automatic translation post-editing across multiple language pairs. The authors demonstrate the efficacy of GPT-4 in achieving state-of-the-art performance on various translation quality metrics. They introduce several measurements for evaluating post-editing efficacy, including Translation Edit Rate (TER), Edit Efficacy over Error Spans (E3S), and Edit Realization Rate (ERR). The authors also discuss the advantages of LLMs for post-editing, such as enhanced multilingual understanding and potential for cultural customization.

### Strengths and Weaknesses
Strengths:
- The authors showcase significant quality improvements in post-editing over Neural Machine Translation (NMT) outputs across multiple languages.
- The investigation into the chain of thoughts for machine translation quality is insightful, particularly regarding the fidelity of proposed edits.
- The paper includes a thorough literature review, demonstrating a strong understanding of the field.

Weaknesses:
- The paper lacks a clear justification for the necessity of NMT models when using powerful LLMs for post-editing.
- There is no ablation study explaining the choice of prompt design, which could enhance performance.
- Reliance on an external model via API raises reproducibility concerns, with insufficient details on the underlying model's changes.
- The evaluation of post-editing experiments is incomplete, lacking certain language pairs.
- The title may mislead as it implies a focus solely on GPT-4, while GPT-3.5 is also evaluated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the use case for NMT models in conjunction with LLMs. Including GPT-4 translations and their post-editing scores would provide valuable insights. Additionally, conducting an ablation study to justify the prompt design choices would strengthen the paper. To enhance reproducibility, the authors should provide more details on the API's underlying model and address potential data pollution concerns. Expanding the evaluation to include all relevant language pairs and considering open-source LLMs for comparison could further enrich the study.