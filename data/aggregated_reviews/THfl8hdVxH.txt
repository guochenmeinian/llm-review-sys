ID: THfl8hdVxH
Title: White-Box Transformers via Sparse Rate Reduction
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 8, 6, 7, 5, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel optimization target called sparse rate reduction, extending the previous rate reduction framework. The authors propose a transformer-like architecture named CRATE, designed to learn mathematically interpretable representations through sparse rate reduction. By unrolling the iterative optimization process into neural layers, CRATE achieves performance comparable to Vision Transformers (ViT) while allowing for the transformation of data into nearly-orthogonal axis-aligned subspaces, enhancing the interpretability of learned representations. The work provides insights into the connections between score functions, self-attention, and rate reduction, particularly under idealized token distributions. The authors emphasize the significance of their white-box model, derived from first principles, which offers insights into the redundancy of QKV matrices in ViT.

### Strengths and Weaknesses
Strengths:  
- The manuscript is well-written, with proper citations and insightful discussions.  
- The idea of extending rate reduction to sparse rate reduction is novel and represents a significant contribution to the field.  
- The derivation of CRATE from first principles provides a mathematically interpretable framework and promotes learning useful representations, which is a valuable goal in deep learning.  
- The identification of redundancy in ViT's QKV matrices presents practical implications for parameter efficiency.  
- Empirical results indicate that the proposed architecture maintains performance levels similar to ViT.

Weaknesses:  
- While results on ImageNet are promising, the proposed architecture does not achieve state-of-the-art (SOTA) performance under fair comparisons.  
- Some sentences in the manuscript are overly concise and may confuse general readers.  
- The explanations in certain sections, such as L143 and footnote 4, lack clarity regarding mathematical roles and separation.  
- The motivation for introducing the sparse coding term in equation 1 needs further clarification, and the authors should discuss the design principles of optimization metrics more thoroughly.  
- The derivations in Section 2 are dense, making them difficult to follow, and the relationship between the architecture and objectives could be better articulated.  
- The comparison of sparsity metrics between CRATE and ViT may lack clarity, as different layers are evaluated.  
- Some reviewers expressed skepticism regarding the practical implications of the theoretical insights presented.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manuscript by revising overly concise sentences to enhance readability for general readers. Specifically, please elaborate on the explanations in L143 and footnote 4, as well as footnote 6, to clarify the mathematical roles involved. Additionally, we suggest that the authors clarify the motivation for the sparse coding term in equation 1 and discuss whether the L0 norm is the best implementation for sparse coding.

Furthermore, we encourage the authors to comment on the connection between the optimization metric and specific tasks, particularly in relation to object detection and image reconstruction. To strengthen the paper, consider including additional experimental results, such as comparisons with larger models or results from Imagenet-21k, and exploring the implications of the sparse rate reduction theory on transformer design. 

We also recommend that the authors improve the clarity of their comparisons by measuring sparsity at the output of the activation functions in both CRATE and ViT, as this would provide a more relevant metric. Additionally, please address the potential confusion regarding the necessity versus sufficiency of sparse rate reduction in their hypotheses to ensure a clear understanding of their contributions. Finally, we encourage the authors to further explore the implications of their findings on the interpretability and efficiency of deep learning architectures in future work.