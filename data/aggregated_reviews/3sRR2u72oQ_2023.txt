ID: 3sRR2u72oQ
Title: INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 3, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the INSPECT dataset, a large multimodal dataset comprising CT images, radiology reports, and electronic health record (EHR) data from over 20,000 pulmonary embolism (PE) patients. The authors propose a benchmark involving eight diagnostic and prognostic tasks, demonstrating that integrating medical imaging with structured EHRs enhances diagnostic performance for PE. The authors clarify key differences between INSPECT and the RSNA PE dataset, emphasizing that INSPECT includes a larger number of studies, multiple cases per patient, and diverse data modalities. The paper highlights the dataset's uniqueness and size, claiming it to be the largest of its kind, and addresses the importance of integrating imaging and EHR data for improved model performance, proposing that INSPECT encourages the development of advanced multimodal fusion techniques.

### Strengths and Weaknesses
Strengths:
- The dataset is the largest multimodal collection integrating EHR and imaging data.
- The authors provide a well-justified and documented curation and cleaning process.
- The benchmarking results are interesting and could stimulate further research.
- The authors provide a comprehensive comparison between INSPECT and RSNA datasets, highlighting significant differences in data structure and content.
- The inclusion of a sample dataset and a timeline for full dataset release demonstrates responsiveness to reviewer concerns regarding data availability.
- The authors acknowledge the need for improved metrics and have added AUPRC and ECE to their benchmarks, enhancing the evaluation framework.

Weaknesses:
- Data access issues exist, as a representative part of the dataset was not available online at the time of review.
- The complete dataset is not available for reviewers, raising concerns about the paper's validity and reproducibility.
- The dataset lacks a DOI, raising concerns about long-term accessibility and reproducibility.
- The requirement for personal information to access the dataset breaches the anonymous review protocol, potentially compromising reviewer confidentiality.
- The choice of metrics used for evaluation is not well justified, and there are inconsistencies in the metrics applied across tasks.
- Some labels were generated using an ML algorithm, which may introduce uncertainty; expert labels for the test set would enhance credibility.
- The novelty of the INSPECT dataset is questioned, as the differences from RSNA may not be substantial enough to justify its uniqueness.

### Suggestions for Improvement
We recommend that the authors improve data access by ensuring a representative subset is available to reviewers at the time of submission, and that the complete dataset is accessible prior to the final decision. Additionally, consider revising the data access protocol to eliminate the need for personal information, thereby adhering to anonymous review standards. The authors should clarify how the INSPECT dataset differs from the RSNA PE dataset, emphasizing its unique contributions. To enhance reproducibility, we suggest that the authors obtain a DOI for the dataset and adopt a more flexible licensing approach, such as CC0 or CC-BY. Furthermore, we encourage the authors to provide a detailed rationale for their choice of metrics and to ensure consistency in their application across tasks. Lastly, including expert labels for the test set and addressing the ambiguity in label generation would strengthen the dataset's reliability and perceived novelty.