ID: Bukc7HhE3Y
Title: Query in Your Tongue: Reinforce Large Language Models with Retrievers for Cross-lingual Search Generative Experience
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework that utilizes reinforcement learning (RL) for Cross-Lingual Information Retrieval (CLIR), moving away from post-retrieval translation solutions. The authors propose an end-to-end retrieval and responder model trained with synthetic queries through contrastive learning, which is a significant contribution. The methodology involves generating multilingual positive and negative queries using a large language model (LLM) to enhance retrieval performance, evaluated on two CLIR tasks. The results indicate that the proposed method outperforms several baselines, including prominent LLMs.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and easy to follow.
- The novel approach of using RL and synthetic queries for CLIR is intriguing and has potential for low-resource languages.
- Extensive experiments validate the proposed solution, showing improvements over existing models.

Weaknesses:
- The limited number of datasets used for experiments raises concerns about the validity of the findings.
- The choice of baselines and the separation of experiments for retrieval and responder are confusing.
- The performance gains may be attributed to fine-tuning on synthetic datasets, leading to unfair comparisons with baselines.
- The literature review lacks focus on key influences relevant to the work, and the ablation study does not adequately assess the impact of individual components.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by incorporating additional datasets to validate their findings more robustly. Clarifying the rationale behind the choice of baselines and discussing the potential benefits of comparing with recent multilingual dense retrievers would enhance the paper's context. Additionally, we suggest conducting experiments to analyze the impact of the dynamic clipping range $\epsilon_l$ on performance and expanding the literature review to include discussions on data augmentation and RL-guided generation. Finally, addressing the concerns regarding the quality of generated queries and the implications of using both positive and negative prompts in training would strengthen the paper.