ID: Oc1SIKxwdV
Title: Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 4, 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GRACE, a method for continual model editing that utilizes an external key-value memory to modify the output of a pre-trained model's layer. The approach allows for efficient editing by storing input representations as keys and learning corresponding values through gradient descent. The authors propose a mechanism to adjust the deferral radius for each key, enabling generalization across similar edits. Additionally, the paper investigates the inference time of a QA T5 model after extensive edits, specifically measuring the impact of 3,000 edits made to the model's 4th layer. The results indicate that while the unedited model maintained a stable inference time of approximately 0.022 seconds, the edited model's time increased to 0.078 seconds. The evaluation shows promising results across various benchmarks, demonstrating GRACE's potential for handling numerous sequential edits while acknowledging the need for improved scalability and efficiency.

### Strengths and Weaknesses
Strengths:
- The method addresses a highly relevant problem in model editing, particularly for large pre-trained models.
- GRACE's simplicity and efficiency make it a practical solution for real-world applications.
- The paper is well-structured and clearly written, with a thorough analysis of the method's performance and limitations.
- The authors provide a detailed experimental setup and results that clarify the impact of edits on inference time, contributing significantly to the understanding of model editing and its implications for inference efficiency.

Weaknesses:
- The choice of editing layer and the value of the deferral radius \(\epsilon\) significantly impact the system's behavior, yet the selection process relies on heuristic methods, which may not be robust.
- The empirical comparisons primarily focus on NLP tasks, limiting the generalizability of the findings.
- GRACE may primarily memorize edits rather than generalize them effectively, raising concerns about its practical applicability.
- The increase in inference time by 3.5 times after 3,000 edits raises concerns about the practical applicability of the method.
- The scalability of the approach remains a significant limitation, as the slowdown affects all model uses without prior knowledge of edits.

### Suggestions for Improvement
We recommend that the authors improve the robustness of the layer selection process by exploring more systematic approaches beyond heuristic search. Additionally, we suggest conducting a more focused analysis of edit generalization beyond simple rephrases to enhance the practical utility of model editing in real-world applications. Expanding the empirical evaluation to include a wider range of tasks, particularly in vision, would enhance the paper's completeness. It would also be beneficial to clarify how practitioners can choose appropriate values for \(\epsilon\) and the editing layer, as this is crucial for effective deployment. Furthermore, we recommend that the authors improve the scalability of their method to address the inference time concerns raised by reviewers, particularly in terms of inference time with increasing edits, to provide valuable insights into its practical use.