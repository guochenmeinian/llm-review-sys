ID: yxjWAJzUyV
Title: REBEL: Reinforcement Learning via Regressing Relative Rewards
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 5, 7, 8, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the REBEL algorithm, which simplifies policy optimization in reinforcement learning by transforming it into a regression problem based on relative rewards. The authors conduct a thorough theoretical analysis, demonstrating that Natural Policy Gradient (NPG) can be viewed as a special case of REBEL. Comprehensive experiments in language modeling and image generation validate the effectiveness of REBEL, showing competitive or superior performance compared to existing methods like PPO and DPO.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a significant topic by proposing a simplified approach to policy optimization in RLHF.
2. The theoretical analysis of REBEL is detailed and insightful, establishing connections to existing RL methods.
3. The presentation is clear, with a logical flow and good readability.
4. The empirical results are comprehensive and well-presented, demonstrating the algorithm's effectiveness.

Weaknesses:
1. The claim regarding REBEL's ability to handle intransitive preferences is not sufficiently supported in the main content, particularly concerning reward score accuracy.
2. The experimental validation is limited, with insufficient baseline comparisons and lack of exploration of the relationship between regressor performance and dataset quality.
3. There are inconsistencies regarding the necessity of critic-based variance reduction in stochastic MDPs, which contradicts the paper's critique of PPO's complexity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the claim about REBEL's handling of intransitive preferences by addressing the degradation of reward score accuracy in the main content. Additionally, we suggest including a separate "Limitations" section to summarize the constraints of the proposed method. To enhance experimental validation, the authors should consider a wider range of benchmarks and provide more detailed comparisons with baseline algorithms, including hyperparameter settings. Furthermore, we encourage the authors to explore the relationship between dataset quality and regressor performance, and to provide experimental results or theoretical insights regarding the choice of base distribution \(\mu\). Lastly, addressing the inconsistencies related to stochastic MDPs with preliminary results or theoretical insights would strengthen the paper.