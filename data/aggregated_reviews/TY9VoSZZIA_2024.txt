ID: TY9VoSZZIA
Title: A two-scale Complexity Measure for Deep Learning Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 6, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new capacity measure for statistical models, termed two-scale effective dimension (2sED), which is used to derive an upper bound on the generalization error under specific model assumptions. The authors demonstrate how to lower bound the 2sED for Markovian models, including feed-forward neural networks. The measure correlates with the training loss of neural networks, providing insights into model complexity.

### Strengths and Weaknesses
Strengths:
- The authors address a fundamental problem in theoretical machine learning and introduce a novel approach.
- The mathematical results are solid and well-presented, balancing precision and applicability to deep learning architectures.
- The complexity measure is intuitive and shows predictive utility regarding model performance.

Weaknesses:
- The significance of the results is unclear, particularly regarding the empirical Fisher matrix, which yields the same generalization bound for all models fitting the training set.
- Some assumptions, such as the Lipschitz condition on the Fisher matrix, appear difficult to verify for practical deep neural networks.
- Technical details are lacking, including the derivation of 2sED and explicit examples of Markovian models.
- Empirical results are limited to correlations with training loss, neglecting validation/test loss.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the significance of their results, particularly how the generalization bound can facilitate model selection. Additionally, the authors should provide explicit examples of deep neural networks that satisfy the assumptions made, particularly the Lipschitz condition on the Fisher matrix. We also suggest including a more thorough discussion of previous work on neural network expressivity to contextualize their contributions better. Furthermore, the authors should enhance the empirical analysis by correlating 2sED with validation/test loss. Lastly, we encourage the authors to address computational bottlenecks related to estimating eigenvalues of the Fisher information matrix and consider alternative methods for box-counting dimensions.