ID: 6Wm4202Wvc
Title: Label Privacy in Split Learning for Large Models with Parameter-Efficient Training
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on label leakage in split learning, particularly in the context of fine-tuning large models using parameter-efficient training (PEFT). The authors propose two privacy-preserving methods for gradients and activations during split learning, evaluated on various large models, including Llama2-7B, using LoRA and full model fine-tuning. The study highlights significant privacy concerns associated with fine-tuning large language models (LLMs) and demonstrates that the proposed methods effectively reduce label leakage while maintaining utility. Additionally, the authors address reviewers' concerns by acknowledging the need for a more rigorous and formal probabilistic bound, such as the definition of differential privacy, and propose to examine the privacy bounds and proofs of existing multiparty computation (MPC) methods that are similar to their approach.

### Strengths and Weaknesses
Strengths:
1. The paper addresses the urgent need for privacy-preserving split learning in large models and fine-tuning with LoRA.
2. The writing is generally clear, with a logical flow of ideas.
3. The proposed methods are evaluated across different pre-trained large models, aligning with current real-world applications of LLMs.
4. The authors have effectively addressed most concerns raised by the reviewers, demonstrating thoughtful engagement with the feedback provided.

Weaknesses:
1. The paper lacks a comprehensive comparison with existing literature on label leakage defenses and trivial solutions like differential privacy, which are easy to implement.
2. The proposed methods do not adequately address modern use cases such as text and image generation, where label protection is critical.
3. The theoretical guarantee lacks rigor and formality, particularly regarding probabilistic bounds.
4. There is a need for a more comprehensive examination of existing MPC methods' privacy bounds.
5. Minor writing issues exist, including the need for clearer introductions to API fine-tuning and the reorganization of certain paragraphs for better clarity.

### Suggestions for Improvement
We recommend that the authors improve the comparison of their methods with existing literature on label leakage defenses and include discussions on trivial solutions like differential privacy. Additionally, the authors should explore the applicability of their methods to modern use cases, such as text and image generation, where label privacy is crucial. We suggest improving the theoretical guarantee by providing a more rigorous and formal probabilistic bound, specifically the definition of differential privacy. Furthermore, we encourage the authors to check the privacy bounds and proofs of existing MPC methods similar to their proposed approach to enhance the soundness of their work. Finally, we suggest shortening the introduction regarding API fine-tuning and reorganizing specific paragraphs for clarity, while also clarifying the definition of LoRA before using the acronym and focusing on split learning without introducing unnecessary concepts like vertical federated learning.