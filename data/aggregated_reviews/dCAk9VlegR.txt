ID: dCAk9VlegR
Title: This Looks Like Those: Illuminating Prototypical Concepts Using Multiple Visualizations
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel interpretable prototype-based classifier, ProtoConcepts, which enhances the interpretability of prototypical networks by utilizing multiple image patches instead of a single training image as a prototype. This approach aims to clarify the semantic meaning of prototypes, thereby improving visual explanations. The authors demonstrate that ProtoConcepts can be integrated with various prototype-based classifiers without compromising accuracy on benchmark datasets.

### Strengths and Weaknesses
Strengths:
1. The authors effectively argue that relying on a single image patch as a prototype can hinder user comprehension of the represented concept.
2. The adaptability of ProtoConcepts across different prototype-based classifiers, such as ProtoPNet and TesNet, is well-documented.

Weaknesses:
1. The idea of using multiple image patches for concept visualization is not novel within the XAI community, as it has been explored in existing works like TCAV and ACE, which are not adequately discussed in the paper.
2. The evaluation of interpretability lacks rigor, raising concerns about the potential cherry-picking of examples. The absence of source code further complicates the assessment of the method's effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the discussion of existing concept-level explanation methods, such as TCAV and ACE, to contextualize their contributions better. Additionally, we suggest conducting a carefully designed human user study to provide empirical evidence of the interpretability of ProtoConcepts, as this would strengthen the claims made regarding its effectiveness. Furthermore, clarifying whether the examples presented are cherry-picked or using a fixed order for examples would enhance the credibility of the interpretability claims. Lastly, including a detailed analysis of computational costs and a clearer explanation of the prototype selection process would address significant concerns raised by reviewers.