ID: XKxupAK5Zz
Title: Fast Graph Condensation with Structure-based Neural Tangent Kernel
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for graph condensation called GC-SNTK, reformulating the problem as a Kernel Ridge Regression (KRR) task using a Structure-based Neural Tangent Kernel (SNTK). This method eliminates the need for complex nested loops in bi-level GNN optimization, achieving significant efficiency improvements—up to 61.5x faster than state-of-the-art baselines—while maintaining comparable or superior performance. The authors provide clear descriptions, equations, and comprehensive experiments to support their claims.

### Strengths and Weaknesses
Strengths:
- The motivation for the problem is clearly articulated, with well-designed figures and discussions that enhance reader understanding.
- The transformation of the graph condensation problem into a KRR task is a novel and interesting insight, supported by existing theories.
- The approach is clearly described, making it easy to adapt, with well-explained equations and intuitive illustrations.
- Experimental results demonstrate a clear efficiency advantage over existing baselines while maintaining similar or better performance.

Weaknesses:
- The inclusion of KRR alongside four other GNN approaches in average performance calculations in Section 4.5 is questionable, as KRR's performance may not be directly comparable to GNN models. If non-GNN models are considered, more should be included, and separate averages for GNN and non-GNN approaches should be reported.
- The results for GCond based on bi-level training on GCN should be supplemented with results from bi-level optimization on SGC to strengthen the baseline comparison.
- The experimental datasets are limited to citation networks, necessitating the inclusion of diverse datasets.
- The authors claim that bi-level methods suffer from training instability but do not verify stability in their method.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the average performance calculations by separately reporting GNN and non-GNN results if KRR is included. Additionally, consider adding results from bi-level optimization on SGC to Table 3 to enhance the baseline comparison. It would also be beneficial to include a wider variety of experimental datasets beyond citation networks to strengthen the findings. Lastly, we suggest verifying the stability of their method in comparison to bi-level methods to substantiate their claims.