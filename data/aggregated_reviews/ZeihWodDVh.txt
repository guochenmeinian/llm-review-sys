ID: ZeihWodDVh
Title: PureGen: Universal Data Purification for Train-Time Poison Defense via Generative Model Dynamics
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 4, 6, 4, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach, PureGen, for securing model performance against train-time data poisoning attacks by employing Energy-Based Models (EBM) and Denoising Diffusion Probabilistic Models (DDPM) for data purification. The authors propose that their methods, specifically PureEBM and PureGen, outperform existing defense strategies published in 2021/2022. The effectiveness of this method is evaluated on CIFAR-10, Tiny-ImageNet, and CINIC-10 datasets, demonstrating state-of-the-art performance. The authors clarify the setup for data poisoning, emphasizing that the poisoner can only manipulate a small subset of images with specific perturbations, which is distinct from adversarial attacks. They conducted extensive experiments, training over 3,000 classifiers and numerous EBMs and Diffusion models, showcasing the robustness of their findings and exploring various implementation steps and practical considerations, including the impact of training distribution shifts and the use of generative models on poisoned data.

### Strengths and Weaknesses
Strengths:  
1. The paper is well-structured and presents a clear methodology, making it easy to follow.  
2. The exploration of different variants of the proposed method adds value to the research.  
3. Comprehensive experiments show that the proposed defenses outperform baselines, providing a thorough examination of various data poisoning scenarios.  
4. The authors conducted extensive experiments, training over 3,000 classifiers, which supports the robustness of their findings.  
5. The work addresses a novel area in the literature regarding purification-type defenses against train-time data poisoning, filling a gap in existing research.  

Weaknesses:  
1. The novelty of the approach is questioned, as adversarial purification has been previously explored, and the paper does not sufficiently distinguish its contributions from existing literature.  
2. Some sections, such as the discussion on EBM models and the relevance of certain equations, lack depth and clarity.  
3. The computational burden of Langevin iterations is acknowledged but may deter practical application.  
4. There is a perceived lack of clarity regarding the novelty of the contributions, as some reviewers expressed concerns about the originality of the work.  
5. The authors' responses to reviewer critiques may come across as dismissive, particularly regarding the necessity of additional experiments and the characterization of poisoning methods.

### Suggestions for Improvement
We recommend that the authors improve the discussion in Section 3.1 by providing a more in-depth analysis of EBM models in the context of their method. Additionally, we suggest clarifying the novelty of their approach within the existing literature to strengthen their contribution. A comparative analysis of publicly available pre-trained DDPM models versus in-house trained models could also enhance the paper's findings. Furthermore, addressing the implications of defense-aware poisoning strategies and the potential for attackers to adapt to their purification methods would significantly bolster the robustness of their claims. We also recommend improving clarity in their responses to reviewer critiques to foster constructive dialogue and providing further justification for the number of experiments conducted to address concerns about sufficiency. Lastly, it may be beneficial to explicitly differentiate between the methodologies for data poisoning and adversarial attacks to avoid misunderstandings, and we encourage the authors to consider incorporating higher-order solvers to improve the efficiency of their purification methods.