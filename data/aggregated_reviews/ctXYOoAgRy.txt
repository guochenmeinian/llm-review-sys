ID: ctXYOoAgRy
Title: How do Large Language Models Handle Multilingualism?
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive investigation into how large language models (LLMs) manage multilingualism through a proposed three-stage workflow termed MWork: understanding, task-solving, and generating. The authors introduce a novel method called Parallel Language-specific Neuron Detection (PLND) to identify language-specific neurons without labeled data. Their experiments reveal that only 0.13% of neurons are language-specific, and deactivating these neurons significantly impacts multilingual task performance. The authors validate their hypothesis through rigorous experimentation across multiple languages and models.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and clearly presented, making complex concepts accessible.
- The contributions of PLND and the MWork hypothesis are fundamentally relevant to LLM interpretability and analysis.
- Extensive experiments are conducted across diverse natural language understanding tasks.

Weaknesses:
- Claims about LLM behavior are broad and primarily based on two similar models, raising questions about generalizability to larger models.
- The assertion that 400 documents suffice for language-specific neuron tuning lacks robust comparative analysis, potentially overstating findings.
- The classification of neurons as language-specific may overlook the existence of language-agnostic neurons, which could affect the conclusions drawn.

### Suggestions for Improvement
We recommend that the authors improve the paper by adjusting the text to reflect the limitations of their findings, particularly regarding the generalizability of results to larger models and the implications of context length on performance. Additionally, we suggest conducting more thorough comparisons to substantiate the claim about the sufficiency of 400 documents for tuning. It would also be beneficial to explore the presence of language-agnostic neurons and their impact on the analysis, potentially refining the classification of neurons used in the study.