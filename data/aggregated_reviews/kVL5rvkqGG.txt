ID: kVL5rvkqGG
Title: Repurposing Language Models into Embedding Models: Finding the Compute-Optimal Recipe
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the fine-tuning of Pythia LLMs for information retrieval (IR) applications, examining the impact of model sizes, FLOPs, fine-tuning methods (e.g., LoRA, bias tuning), and hyperparameters on loss and downstream applications, as measured by MTEB. The findings reveal that larger models do not always yield better loss, which is a counterintuitive result. Additionally, the paper proposes a new algorithm for optimizing model configurations based on computational budgets, analyzing the relationship between loss functions, parameters, and training tokens.

### Strengths and Weaknesses
Strengths:  
- The paper conducts extensive experiments and analyses, providing valuable insights for practitioners developing embedding models from LLMs.  
- It introduces a significant computation optimization problem and offers a comprehensive analysis of fine-tuning techniques with clear visualizations.  
- The authors provide code and additional evaluation results in the appendix, enhancing the paper's utility.

Weaknesses:  
- The practical relevance of the analyses is questionable, particularly regarding the term "Compute-Optimal," which overlooks inference costs.  
- The generalization ability is limited, focusing solely on Pythia and contrastive fine-tuning without exploring other models.  
- The paper lacks theoretical support for observed phenomena and does not adequately address the relationship between data quantities and performance.  
- Some figures are difficult to interpret, and the selection of datasets for evaluation may not be representative.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their explanations regarding l_r and l_c in lines 134 and 135, and provide citations or motivations for the loss used. Additionally, consider addressing the limitations of using the English partition of the BAAI BGE dataset, and clarify the performance metrics in figures to avoid confusion. We also suggest including a discussion on inference costs in the limitations section to enhance the paper's practical relevance. Finally, acknowledging related studies in the literature review could strengthen the context of the research.