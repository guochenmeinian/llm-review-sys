ID: RgNXKIrWyU
Title: Reusing Pretrained Models by Multi-linear Operators for Efficient Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 5, 6, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 2, 5, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Mango, a novel approach that utilizes a small model as an initializer for larger models to expedite training. Mango learns a full mapping of weights from the smaller model to the larger one using low-rank factorization, constraining each sub-mapping to rank 1. The authors demonstrate that this method can be learned in as few as 100 steps, resulting in faster convergence across various architectures, including vision transformers, BERT, and GPT models.

### Strengths and Weaknesses
Strengths:
- Well-written with strong motivating examples.
- The theoretical foundation for low-rank factorization is clearly articulated.
- Compelling results across three model types, showing significant time savings in convergence.

Weaknesses:
- Results are limited to small to base size transformers, lacking exploration of larger models, which are more resource-intensive.
- The experimental section lacks clarity on how acceleration ratios are measured and contains potentially contradictory statements regarding training steps and epochs.
- Missing background on tensor ring matrix product, which is central to the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by providing essential details on how the acceleration ratio is measured and clarifying any contradictory statements regarding training steps and epochs. Additionally, we suggest exploring the application of Mango to larger models, such as initializing a ViT-G from a ViT-H, to enhance the paper's impact. Furthermore, incorporating non-linear operations into Mango could improve training efficiency, and a discussion on the potential CO2 emissions reduction achieved by Mango would be beneficial. Lastly, we encourage the authors to compare their method with related works to contextualize their contributions better.