ID: JTwxylP6U9
Title: Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 5, 5, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called Diffusion-Based Projected Gradient Descent (Diff-PGD) for generating realistic and stealthy adversarial samples to deceive neural network models. The authors propose using a diffusion model to guide the optimization process, ensuring adversarial samples remain close to the original data distribution while being effective. Diff-PGD allows customization for various attack scenarios, including digital, physical-world, and style-based attacks, and separates adversarial loss optimization from other surrogate losses, enhancing stability and controllability. The authors demonstrate that adversarial samples generated with Diff-PGD show improved transferability and anti-purification capabilities compared to traditional gradient-based methods.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel framework, Diff-PGD, integrating diffusion models into adversarial sample generation, a unique approach not extensively explored in prior research.
- It provides a thorough description of Diff-PGD and its principles, with well-structured and clear communication of key concepts.
- The framework improves the stealthiness of adversarial samples while maintaining effectiveness, supported by convincing experimental evaluations.

Weaknesses:
- The vulnerability of DNNs/ViTs to adversarial examples is well-known; the paper lacks exploration of the proposed attack's performance against hardened models, such as adversarially trained models.
- The computational expense of Diff-PGD is significantly higher than regular adversarial attacks, with marginal advantages noted in low-iteration regimes.
- Many experimental results are based on limited examples, raising concerns about the robustness of claims regarding the proposed methods.
- The comparison with baseline methods is not comprehensive, lacking quantitative metrics like attack success rates and comparisons with GAN-based adversarial attack methods.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of the proposed method by conducting experiments on transformer-based models, such as ViT, and providing a performance report under different L-p constraints. Additionally, addressing the computational overhead of Diff-PGD is crucial, as it may hinder practical applications. The authors should also expand the comparison with more baseline methods, particularly GAN-based approaches, to substantiate the advantages of Diff-PGD. Finally, clarifying the evaluation metrics used in the experiments and ensuring a larger sample size for quantitative results would strengthen the claims made in the paper.