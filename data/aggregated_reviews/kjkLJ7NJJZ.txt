ID: kjkLJ7NJJZ
Title: Offline Minimax Soft-Q-learning Under Realizability and Partial Coverage
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 7, 7, 6, 5, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents algorithms based on soft Q-learning for offline reinforcement learning (RL) with finite-sample guarantees, requiring only single-policy concentrability and realizability, while avoiding the Bellman completeness assumption. The authors propose a minimax optimization approach to solve the offline minimax problem, establishing sample complexity bounds under partial coverage and realizability assumptions. The algorithms demonstrate convergence properties and provide guarantees for estimating the entropy-regularized Q-function.

### Strengths and Weaknesses
Strengths:
- The algorithms require weaker assumptions compared to prior works, focusing on single-policy concentrability and realizability without Bellman completeness.
- The proposed methods are implementable and theoretically sound, offering finite-sample guarantees and convergence properties.
- The paper is well-written and presents a clear exposition of the algorithms and their theoretical foundations.

Weaknesses:
- The connection between dual variables $l(s,a)$ and importance weights $w(s,a)$ is unclear, and a comparison with existing literature, such as Zhu et al. 2023, is needed.
- The convergence rates are slower than $1/\sqrt{N}$, which is less optimal than some recent works.
- The assumptions made for the regularized problem lack clarity regarding their relationship to standard assumptions for the original problem.
- The appendix lacks detail, making it difficult for readers to follow the proofs, and some equations are overly complex.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relationship between the dual variables $l(s,a)$ and the importance weights $w(s,a)$, and provide a detailed comparison with Zhu et al. 2023. Additionally, we suggest enhancing the exposition of the assumptions for the regularized problem to clarify their relevance to standard assumptions. The authors should also consider addressing the slower convergence rates and explore ways to optimize these bounds. Finally, we encourage the authors to expand the appendix with more detailed steps in the proofs and simplify overly complex equations to enhance readability.