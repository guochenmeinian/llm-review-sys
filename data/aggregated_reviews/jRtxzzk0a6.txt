ID: jRtxzzk0a6
Title: Kraken: Inherently Parallel Transformers For Efficient Multi-Device Inference
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 6, 5, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a modification to standard transformer architectures aimed at reducing allreduce operations in tensor parallelism. The authors propose that each shard of the hidden state and attention layer can operate independently, allowing allreduce to occur only after Feed-Forward Network (FFN) layers, which can overlap with attention calculations. Experiments with models up to 760M demonstrate accuracy comparable to GPT-2, with a projected 1.5X speedup in context encoding for 175B models. The paper also introduces Kraken, designed to reduce communication costs during inference by overlapping collective operations with computations.

### Strengths and Weaknesses
Strengths:
- The proposal is straightforward yet impactful, addressing a significant issue in reducing communication overhead during inference.
- The architecture maintains model accuracy while achieving notable speed improvements.
- The design and methodology are robust, with clear explanations enhancing the credibility of the work.

Weaknesses:
- Accuracy results are limited to models up to 760M, raising uncertainty for larger models where the proposal is crucial.
- The evaluation benchmarks focus primarily on language tasks, neglecting vision applications.
- The paper lacks detailed comparisons with existing baselines and related works, limiting its contextual understanding.
- The requirement to train from scratch poses a significant cost, especially for large language models (LLMs).

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including a more diverse set of benchmarks, particularly in vision tasks, to demonstrate the versatility of their approach. Additionally, a detailed comparative analysis with existing methods, including ablation studies and sensitivity analyses regarding inter-device bandwidths, would strengthen the paper. Clarifying the necessity of re-training from scratch and exploring the possibility of designing Kraken as a plug-in for existing Transformer models would enhance its practicality. Lastly, addressing how Kraken integrates with widely adopted tools like DeepSpeed and FlashAttention would further solidify its relevance in the field.