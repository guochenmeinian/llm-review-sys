ID: hgF8In32gL
Title: Text encoders bottleneck compositionality in contrastive vision-language models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a significant examination of language information loss in the text encoders of contrastive language-image pretraining models (CLIP). The authors introduce a text-decoding probing method to evaluate the encoded CLIP text vectors and their ability to reconstruct original captions, revealing that these vectors struggle with spatial and temporal relations but can capture simple attributes. The study also highlights that the text encoders act as performance bottlenecks, impacting image-text matching. Two novel datasets, CompPrompts and ControlledImCaps, are introduced to facilitate this investigation.

### Strengths and Weaknesses
Strengths:
- The proposed probing method for text reconstruction is innovative and insightful, potentially guiding future research on CLIP-like models.
- The introduction of two new evaluation benchmarks is valuable for assessing information loss and its effects on model performance.
- The paper provides a comprehensive analysis of the limitations of current text encoders in vision-language models.

Weaknesses:
- The paper does not effectively draw conclusions from the experiments on the CompPrompts dataset, particularly regarding the EM scores.
- The reliance on the T5 decoder for text recovery may introduce discrepancies that affect the evaluation of text encoder performance.
- The Controlled-ImCaps experiment lacks control over vision encoder differences, which could confound the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their experimental results, particularly in Table 2, by restructuring it to highlight supporting data and including average values. Additionally, we suggest exploring other CLIP variants, such as DeCLIP, ALBEF, and BLIP, to provide a broader context for their findings. The authors should also consider using alternative decoder-only models, like GPT-2, in the Text-only Recovery experiment. Furthermore, we encourage a more detailed analysis of the T5 decoder's reconstruction process and its implications for the performance evaluation of text encoders. Lastly, conducting more ablative experiments in the Controlled-ImCaps study could help clarify the impact of varying vision encoder capabilities on the results.