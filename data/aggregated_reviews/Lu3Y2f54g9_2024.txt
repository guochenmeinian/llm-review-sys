ID: Lu3Y2f54g9
Title: Representation Learning of Structured Data for Medical Foundation Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 7, 4, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method, UniStruct, for integrating structured and unstructured medical data through a custom tokenization approach inspired by subword tokenization techniques. The authors propose a multimodal medical foundation model that combines clinical text and structured data, achieving notable performance improvements on internal datasets. However, limited improvements on public datasets raise questions about the method's generalizability. The paper is well-written, with clear explanations of the model components, though some aspects of the methodology and evaluation require further clarification.

### Strengths and Weaknesses
Strengths:
- The methodology is thorough, particularly the custom tokenization approach for medical codes, which effectively captures relational dynamics.
- The experiments validate improvements on both internal datasets and the EHRSHOT public benchmark, demonstrating significant gains in medical code assignment tasks.
- The paper provides valuable insights into enhancing representation learning for structured medical data, addressing a critical gap in current LLM architectures.

Weaknesses:
- The evaluation lacks in-depth comparisons with existing models, particularly regarding how UniStruct performs against other multimodal approaches.
- Clarity on the scalability of the model and its generalization to larger clinical environments is insufficient.
- The introduction of custom tokenization could benefit from more intuitive examples, and the section on downstream prediction tasks is brief, lacking detailed performance measurement explanations.

### Suggestions for Improvement
We recommend that the authors improve the evaluation section by including more comprehensive baseline comparisons with existing multimodal models to clarify UniStruct's performance. Additionally, a more detailed discussion on the model's scalability and generalization to larger datasets would strengthen the findings. We suggest expanding the explanation of the custom tokenization process with intuitive examples and providing a clearer breakdown of performance measurement on specific downstream tasks.