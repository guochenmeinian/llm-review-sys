ID: Wl2optQcng
Title: Personalized Federated Learning via Feature Distribution Adaptation
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 5, 6, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents pFedFDA, a personalized Federated Learning (FL) method that conceptualizes global representation learning as a generative modeling task. The authors propose an algorithm that combines global knowledge through server aggregation with local knowledge via client-specific training and distribution estimation, enhancing personalized performance in data-scarce settings. The method adapts global generative classifiers to local feature distributions and is validated through experiments on benchmark datasets, demonstrating its robustness and applicability.

### Strengths and Weaknesses
Strengths:
1. The method of decomposing model training into shared representation learning and personalized classifier training is innovative and promising for handling non-i.i.d. data in FL environments.
2. The paper provides strong empirical evidence through comprehensive experiments, showcasing the superiority of pFedFDA in challenging distribution shifts and data scarcity scenarios.
3. The writing is clear and the paper is well-structured, making it easy to follow the authors' arguments and experimental results.

Weaknesses:
1. The method appears similar to traditional personalized federated learning, with the shared backbone being a weighted summation of client contributions, raising questions about its novelty.
2. The performance improvements are minimal in some cases, as seen in Table 2 where pFedFDA is significantly outperformed by FedBABU and pFedME.
3. The reliance on local distribution may lead to challenges when new clients with different distributions are introduced or when local distributions change over time.
4. The algorithm's additional computational cost could increase with larger local data, particularly in calculating local covariance matrices.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the distinction between their method and existing generative models used in personalized federated learning. Additionally, consider exploring other statistical measures beyond mean and covariance to describe data distributions. We suggest conducting further evaluations with lower Dirichlet values in experiments under extreme data scarcity to provide a more comprehensive assessment. To enhance efficiency, the authors should investigate methods to reduce the computational burden on clients, such as having the server calculate the covariance matrix once. Lastly, we encourage the authors to address the limitations regarding robustness against heterogeneity and the strong assumptions in Theorem 1 by considering skewed or long-tailed distributions in future work.