ID: 780uXnA4wN
Title: An Efficient High-dimensional Gradient Estimator for Stochastic Differential Equations
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 4, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel algorithm for estimating gradients of machine learning objectives based on stochastic differential equations (SDEs), particularly in the context of overparametrized SDEs. The authors propose a method that reduces computational complexity, making it scalable with respect to the number of parameters. The paper also formulates an efficient, unbiased gradient estimator for objective functions resembling stochastic optimal control cost functions. The methodology leverages the Feynman-Kac PDE and pathwise differentiation to derive first and second-order derivatives.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a creative approach to reducing computational costs in PDE constrained optimization, which has been challenging due to high simulation costs.  
- The writing is clear, and the motivations for the proposed methods are well-articulated.  
- The rigorous proofs and careful treatment of the interchange of limits and expectations enhance the paper's credibility.

Weaknesses:  
- The comparison to reinforcement learning (RL) is misleading, as RL focuses on model-free estimators, while the proposed method relies on model-based SDEs.  
- The relevance of the work to the ML/AI community is questionable, with a lack of compelling examples that demonstrate its applicability.  
- The paper is primarily targeted at a mathematical audience, which may not align with the NeurIPS readership.  
- The appendix is disorganized, making it difficult to follow the proofs, and there is a lack of a dedicated limitations section.

### Suggestions for Improvement
We recommend that the authors improve the clarity and organization of the appendix to facilitate understanding of the proofs. Additionally, we suggest including a limitations section to explicitly outline the constraints and assumptions of the proposed method. To enhance relevance to the ML/AI community, the authors should provide more applicable examples beyond the quadratic cost function. Furthermore, we encourage the authors to address the concerns regarding the smoothness requirements induced by the Feynman-Kac theorem and to provide theoretical or empirical analyses on the convergence rate of the estimation with respect to the number of parameters. Lastly, we advise the authors to clarify the assumptions made in the paper, particularly Assumption 2, and to consider including numerical simulations to strengthen their contributions.