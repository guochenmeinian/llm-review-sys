ID: puMfaHb1hY
Title: G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation framework for natural language generation (NLG) systems, termed G-EVAL, which utilizes Chain-of-Thought (CoT) prompting as an intermediate step in evaluations. The authors demonstrate that G-EVAL, using GPT-3.5 and GPT-4, achieves better correlations with human evaluations compared to traditional metrics like ROUGE. The study includes extensive experiments on summarization and dialogue generation tasks, revealing insights into potential biases of LLMs towards their own generated text.

### Strengths and Weaknesses
Strengths:
- The G-EVAL framework offers an innovative approach to NLG evaluation, leveraging LLMs for improved performance.
- Extensive experimental validation shows G-EVAL's superiority over existing methods.
- The analysis of LLM biases provides valuable insights for future research.

Weaknesses:
- The paper lacks a comparison of G-EVAL against other evaluators using the same LLMs, which is crucial for assessing its benefits.
- Concerns about the reliability and stability of GPT-4 as an evaluation metric are not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the paper by including a comparison of G-EVAL with at least one other text generation evaluator using the same LLMs to better evaluate the framework's contributions. Additionally, the authors should address the stability of GPT-4 and GPT-3.5 as metrics, possibly by incorporating alternative LLMs for fine-tuning. Furthermore, we suggest explicitly detailing the evaluation scales used by human evaluators and considering the QWK metric for comparing human and automated scores.