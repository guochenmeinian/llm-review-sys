ID: ZvQ4Bn75kN
Title: Video Diffusion Models are Training-free Motion Interpreter and Controller
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a training-free framework for motion control in video diffusion models, introducing the MOtion FeaTure (MOFT) derived from pre-trained model features. The authors demonstrate that MOFT effectively encodes and manipulates motion information, offering high interpretability and generalizability across various architectures. The framework shows competitive performance in generating natural motion, applicable in video motion control and point-drag manipulation.

### Strengths and Weaknesses
Strengths:
- The training-free approach leverages pre-trained diffusion models, significantly reducing resource requirements.
- MOFT provides a clear and interpretable method for understanding and manipulating motion information.
- The method is versatile and robust, applicable across different video generation models.

Weaknesses:
- The paper lacks clarity on the training process, particularly regarding the definition of loss functions and which stages are trained.
- The PCA analysis is based on a limited number of videos, which restricts the generalizability of results.
- The motion differences in qualitative videos appear subtle, raising questions about the effectiveness of comparisons with other methods.
- The scalability of the method to longer videos or higher resolutions is inadequately explored.
- The optimization process and the concept of "content correlation information" require clearer explanations.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the training process, specifying which components are trained and which are not. Additionally, expanding the PCA analysis to include a broader range of videos would enhance the generalizability of the findings. We suggest providing runtime and resolution metrics to better understand computational requirements. The authors should also consider including more complex motion scenarios in their experiments and clarify the optimization process in detail. Lastly, we advise using identical sample images for comparisons in Figure 6 to better visualize the capabilities of MOFT versus DIFT.