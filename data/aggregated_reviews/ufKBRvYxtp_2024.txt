ID: ufKBRvYxtp
Title: Sample-Efficient Agnostic Boosting
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 6, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 1, 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new agnostic boosting algorithm that improves sample complexity by reusing samples across multiple rounds of boosting. The authors propose two algorithms: the first requires \(O(\log(|B|)/(\gamma^3\varepsilon^3))\) samples and \(O(\log(|B|)/(\gamma^2\varepsilon^2))\) calls to the weak learner, while the second achieves \(O(\log(|B|)/(\gamma^3\varepsilon^3)+\log^3(|B|)/(\gamma^2\varepsilon^2))\) samples and \(O(1/(\gamma^2\varepsilon^2))\) calls. The theoretical contribution of improving sample complexity is significant, supported by a sophisticated analysis that extends to infinite base learning classes. The paper also discusses implications for agnostic learning halfspaces and reinforcement learning, with empirical evaluations suggesting improvements over previous algorithms.

### Strengths and Weaknesses
Strengths:
- The paper introduces significant technical novelties, particularly in sample reuse across boosting iterations.
- The theoretical analysis is rigorous, addressing subtle stochastic dependencies and effectively managing inherent complexity.
- The writing style is clear, with a high signal-to-noise ratio.

Weaknesses:
- The presentation lacks clarity, especially in Section 5, which is dense and could benefit from more intuitive explanations.
- The proposed procedure involves a large number of parameters, which may hinder practical usability. While the proofs provide valid settings for these parameters, a more explicit treatment of these choices would enhance accessibility.
- The empirical results are limited, with insufficient information on computational resources and dataset comparisons, potentially alienating the broader machine learning community.
- Some contributions, such as implications for halfspaces and reinforcement learning, may not be sufficiently original or clearly delineated from prior work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 5 by providing more intuitive explanations of the algorithmic choices and the role of sample reuse. Additionally, consider including an informal version of Lemma 9 in the main text, as it is referenced frequently. We suggest expanding the discussion on the potential-based framework mentioned in Line 102 to provide readers with a better understanding. Furthermore, it would be beneficial to include more comprehensive empirical results, including comparisons to additional datasets and previous algorithms, particularly [BCHM20]. Lastly, ensure that the computational resources used in experiments are clearly documented to enhance reproducibility. To improve accessibility, we recommend providing a clearer treatment of parameter choices or reducing the number of parameters in the main text, and addressing the readability of the paper to mitigate the complexity that may deter wider engagement with the boosting literature. Additionally, we suggest revising the references in the first sentence for accuracy and clarifying the empirical risk minimizer's complexity in the table, ensuring consistent terminology throughout the manuscript.