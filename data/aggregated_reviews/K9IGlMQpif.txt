ID: K9IGlMQpif
Title: SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 4, 7, 5, 5, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a data selection method called Small to Large (S2L) aimed at enhancing data efficiency in supervised fine-tuning (SFT) of large language models (LLMs) for specific domains. The authors propose using the training trajectories of smaller models to cluster data and select samples, demonstrating effectiveness in tasks such as mathematical problem-solving and clinical text summarization. Experimental results indicate that S2L can outperform traditional methods and even yield better performance than training on the full dataset.

### Strengths and Weaknesses
Strengths:
- The contribution is unique, focusing on SFT data selection in the LLM era, which is crucial for real-life applications.
- The method is intuitive and supported by theoretical guarantees regarding convergence and data characteristics.
- Extensive experiments validate the method's effectiveness across various compute budgets and model families.

Weaknesses:
- The paper overlooks related works in data selection, such as influence function-based and reinforcement learning approaches.
- Generalizability is questionable, as the small model used may not accurately represent larger models, lacking theoretical guidance on model architecture and parameter size.
- The title and framing of the contribution are overly broad, given the limited domain testing and the marginal novelty of using proxy models for data selection.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related works, specifically addressing influence function-based and reinforcement learning approaches. Additionally, the authors should clarify the generalizability of their method by providing theoretical guidance on the small model's architecture and parameter size. It would also be beneficial to refine the framing of their contribution to accurately reflect the scope of their findings. Finally, conducting experiments with larger models beyond the 7B scale and exploring the impact of different learning rate schedulers and optimizers on clustering effects would enhance the study's robustness.