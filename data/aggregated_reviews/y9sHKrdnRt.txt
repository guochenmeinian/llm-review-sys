ID: y9sHKrdnRt
Title: MC-DiT: Contextual Enhancement via Clean-to-Clean Reconstruction for Masked Diffusion Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 5, 5, 5, 5, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MC-DiT, a novel training paradigm for Diffusion Transformers (DiT) aimed at improving contextual information extraction in image generation. By employing a clean-to-clean mask-reconstruction approach, the authors address the limitations of traditional noisy-to-noisy reconstruction. The design includes two complementary branches of DiT decoders to mitigate model collapse, supported by theoretical and empirical analyses. Experiments on the ImageNet dataset demonstrate that MC-DiT achieves state-of-the-art performance in both unconditional and conditional image generation tasks.

### Strengths and Weaknesses
Strengths:
- The introduction of MC-DiT provides a significant advancement in utilizing clean-to-clean reconstruction for contextual information extraction.
- The authors present a thorough theoretical and empirical analysis, particularly regarding mutual information, which bolsters the validity of their claims.
- The method achieves superior results in image generation, as evidenced by state-of-the-art FID scores on the ImageNet dataset.

Weaknesses:
- The complexity of the MC-DiT model may hinder its practical adoption and implementation.
- The paper lacks sufficient experimental details regarding training time, inference time, and memory usage.
- Visualization of generated images is limited, necessitating an expansion to better illustrate result diversity and quality.
- The marginal improvements of MC-DiT-XL / 2-G compared to MDT-XL / 2- raise questions about its efficiency.
- Clarity is needed regarding the differences between MC-DiT and other masked diffusion transformers, as well as the nature of the contextual information being utilized.

### Suggestions for Improvement
We recommend that the authors improve the presentation of generated images by including results at a resolution of $512\times 512$ to adequately showcase diversity and quality. Additionally, providing detailed comparisons with other state-of-the-art methods would enhance the paper's impact. Clarifying the rationale behind the two extra EMA branches and their role in preventing model collapse is essential. We also suggest including more comprehensive experimental details regarding training and inference times, as well as memory usage. Finally, further ablation studies could elucidate the impact of hyperparameters on performance, addressing ambiguities noted in the current analysis.