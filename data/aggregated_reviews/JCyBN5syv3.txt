ID: JCyBN5syv3
Title: SimGen: Simulator-conditioned Driving Scene Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 9, 5, 8, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SimGen, a framework designed to generate diverse driving scenes by integrating real-world and simulator data through a cascade diffusion pipeline. The authors introduce the DIVA dataset, which comprises extensive real-world and simulated driving footage, enhancing the diversity of training data for autonomous driving tasks. The framework aims to address the sim-to-real gap and improve the generation of controllable and realistic autonomous driving images.

### Strengths and Weaknesses
Strengths:
1. The paper tackles a significant challenge in autonomous driving by effectively utilizing both real-world and simulator data to generate diverse driving scenes.
2. The DIVA dataset is well-constructed, offering a rich variety of appearances, weather conditions, and layouts, which can greatly benefit the research community.
3. The overall presentation is smooth, with clear writing and effective visualizations that enhance understanding.

Weaknesses:
1. The framework primarily simulates front-view cameras, neglecting the use of surround-view cameras essential for modern perception systems in self-driving cars.
2. The necessity of conditioning on the simulator is questionable, as simulators can render accurate semantics and depth without requiring the proposed CondDiff approach.
3. The generated scenarios are limited to the asset bank of the simulator, restricting the diversity of generated categories and potentially limiting the framework's applicability in broader contexts.
4. The quality and utility of the DIVA dataset remain unclear, with insufficient metrics provided for the annotations and performance comparisons with existing datasets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how the DIVA dataset is utilized in training, specifying its role in both the CondDiff and ImgDiff processes to enhance readability. Additionally, we suggest conducting a scalability analysis for data augmentation experiments to evaluate performance improvements with larger synthesized datasets. It would also be beneficial to include a high-level system workflow diagram to clarify the training and inference procedures. Lastly, we encourage the authors to provide more detailed metrics regarding the quality of the dataset annotations and to clarify the definition of safety-critical scenarios to strengthen their claims.