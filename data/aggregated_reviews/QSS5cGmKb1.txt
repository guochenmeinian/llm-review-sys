ID: QSS5cGmKb1
Title: STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 9, 6, -1, -1, -1, -1
Original Confidences: 4, 2, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmarking dataset, STARK, that integrates relational and textual knowledge bases across three domains relevant to product search and scientific applications. The authors propose a novel pipeline for synthetically generating queries, which are validated by human reviewers, and conduct retrieval experiments using various models. The study emphasizes the use of semi-structured databases to enhance information retrieval with LLMs.

### Strengths and Weaknesses
Strengths:  
- The integration of unstructured and structured data in the evaluation task is a significant contribution, bridging a gap in retrieval system evaluation.  
- The systematic construction of the benchmark dataset is well-articulated, with sufficient details provided, including query templates for reproducibility.  
- The use of multiple LLMs for generating answers enhances diversity and demonstrates the effectiveness of semi-structured databases in retrieval tasks.  

Weaknesses:  
- The experimental design lacks results from sparse models and only employs one dense retrieval model, leading to potentially biased conclusions.  
- The clarity of the technical processes, particularly in mapping user queries to the semi-structured database, is insufficiently detailed.  
- The dataset's limited availability restricts its contribution to the research community, and the paper does not adequately address the limitations or societal impacts of the work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup, particularly regarding how chunks are defined in multivector retrieval and the specifics of the LLMs used for generating answers. Additionally, including results from sparse retrieval models like BM25 and dense models such as ANCE and TAS-B would provide valuable baselines for comparison. We suggest that the authors consider using human-generated queries as seeds for synthetic queries to enhance diversity. Furthermore, it would be beneficial to clarify the proprietary status of the dataset in the paper to better inform the research community about its availability.