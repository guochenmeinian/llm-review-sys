ID: Iq7v0sZw2H
Title: Debiasing Pretrained Generative Models by Uniformly Sampling Semantic Attributes
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for learning a fair latent distribution to ensure that a pretrained generator produces samples with equal likelihood across different attributes. The authors propose maximizing the entropy of \(P_E^{\lambda}\) and utilizing a generative model to distill this fair distribution, guaranteeing an optimal solution that achieves fairness. The method is evaluated on various image datasets, demonstrating superior fairness regarding shapes, ages, and races. Additionally, the paper addresses the challenge of mitigating biases in pre-trained generators without retraining, using classifiers to create a fair distribution of noise vectors.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, organized, and presents a compelling motivation for addressing fairness in generative models.
- The proposed method is theoretically sound and computationally efficient, as it does not require retraining the generative model.
- Extensive experiments on multiple datasets yield promising results, outperforming existing debiasing methods.

Weaknesses:
- The experiments do not quantify the approximation error or tradeoffs of each component, such as the accuracy of the learned \(Q^{\lambda}\) or the distance of \(P_E^{\lambda}\) from the optimal distribution.
- The approach shows performance degradation with Progressive GAN on the CelebA-HQ dataset, attributed to the over-representation of certain attributes in the latent space.
- The empirical study is limited to GANs, lacking exploration of other generative models like VAEs or diffusion models, which may have different dynamics.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including a quantitative analysis of the approximation errors and tradeoffs associated with each component of the proposed method. Additionally, it would be beneficial to conduct experiments with other generative models, such as VAEs and diffusion models, to provide a more comprehensive evaluation of the proposed approach. Furthermore, addressing the performance degradation observed in Progressive GANs and discussing relevant baselines would enhance the paper's depth and context. Lastly, we suggest reorganizing sections A.6 and A.7 for improved readability.