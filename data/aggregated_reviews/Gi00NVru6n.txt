ID: Gi00NVru6n
Title: CorDA: Context-Oriented Decomposition Adaptation of Large Language Models for Task-Aware Parameter-Efficient Fine-tuning
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 7, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CorDA, a context-oriented decomposition adaptation method for large language models (LLMs) that initializes LoRA adapters using singular value decomposition (SVD) results from pre-trained weights. The authors propose two adaptation modes: knowledge-preserved adaptation, which aims to retain general knowledge, and instruction-previewed adaptation, which enhances performance on specific tasks. Extensive experiments demonstrate that knowledge-preserved adaptation maintains performance on general knowledge tasks, while instruction-previewed adaptation outperforms baseline PEFT methods.

### Strengths and Weaknesses
Strengths:
1. The experimental results, particularly for instruction-previewed adaptation, are robust, showing superior performance compared to other PEFT methods and nearly matching full fine-tuning.
2. The method is conceptually intuitive, leveraging decoupled components to capture context effectively.

Weaknesses:
1. The paper lacks theoretical support for some claims, particularly regarding the covariance matrix's role and the justification for performing SVD on $WC$.
2. The knowledge-preserved mode is not compelling, as it may hinder target task performance, raising questions about its emphasis alongside instruction-previewed adaptation.

### Suggestions for Improvement
We recommend that the authors improve the theoretical justification for the covariance matrix's connection to task context and clarify the rationale behind the SVD application on $WC$. Additionally, consider providing a more comprehensive comparison with other PEFT methods focusing on initialization techniques. It would be beneficial to include visualizations of the covariance matrix and a clear discussion on the data selection strategy for $C$. Finally, we suggest addressing the hyperparameter selection for $r$ in Equations 4 and 5 to enhance practical applicability.