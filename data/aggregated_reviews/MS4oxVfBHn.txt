ID: MS4oxVfBHn
Title: UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-World Document Analysis
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 8, 6, 6, 6, -1
Original Confidences: 4, 4, 4, 3, -1

Aggregated Review:
### Key Points
This paper presents the Unstructured Document Analysis (UDA) benchmark suite, aimed at evaluating Retrieval-Augmented Generation (RAG) systems on real-world unstructured documents. The UDA dataset comprises 2,965 documents and 29,590 expert-annotated Q&A pairs from diverse domains such as finance, academia, and general knowledge. The benchmark assesses various components of RAG workflows, including data parsing, indexing, retrieval, and question answering strategies. The study also explores advanced techniques like Chain-of-Thought reasoning and Code Interpreters, emphasizing their potential to enhance answer quality for complex queries.

### Strengths and Weaknesses
Strengths:
- The UDA dataset is comprehensive, featuring a diverse array of documents and Q&A pairs that enhance its applicability to real-world scenarios.
- The benchmark allows for detailed evaluation of different RAG components, facilitating granular analysis of their impact on performance.
- The paper provides valuable insights into the effectiveness of various parsing methods and RAG strategies, particularly in comparison to long-context models.

Weaknesses:
- The paper lacks an analysis of bad cases, such as noise sensitivity and hallucination issues.
- Some datasets, like PaperTab and PaperText, do not improve response results with human-annotated context, raising questions about their inclusion in the benchmark.
- The significance of bolded data in tables is not explained, and the methodology for identifying incomplete documents lacks detail.
- Results without RAG for comparison are missing, which is necessary to validate the effectiveness of RAG data.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by incorporating necessary human assessments alongside automatic metrics, as metrics like span-level F1-score do not fully align with human evaluation. Additionally, we suggest providing a detailed explanation of the significance of bolded data in tables and clarifying the methodology for retrieving incomplete documents. It would also be beneficial to include results without RAG for comparison to validate the effectiveness of the proposed methods. Finally, we encourage the authors to enhance the parsing of datasets like PaperTab to ensure well-parsed content is available for evaluation.