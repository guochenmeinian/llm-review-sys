ID: jMJVFP4BH6
Title: Towards Neuron Attributions in Multi-Modal Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 8, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Neuron Attribution Method (NAM) designed for multimodal large language models (MLLMs). The method consists of two steps: first, it employs image segmentation and a pretrained attribution algorithm, Diffusers-Interpret, to assign relevance scores to the model's final hidden state; second, it attributes this hidden state to specific neurons in the feed-forward network of the base LLM. The authors evaluate NAM against several baselines, demonstrating its effectiveness in identifying semantically relevant neurons and its utility for image editing tasks. Additionally, the authors contribute to the field of explainable large language models (LLMs) and express gratitude for the insightful comments received, acknowledging that the suggestion to include additional experiments has enhanced the rigor of their approach.

### Strengths and Weaknesses
Strengths:
- NAM effectively differentiates modality-specific neurons, enhancing the understanding of how MLLMs process multimodal content.
- The method is efficient, as it does not require backpropagation, and shows superior performance in several evaluation tasks, including cross-sample invariance.
- The authors provide a detailed description of the NAM algorithm, which aids reproducibility.
- The authors demonstrate a commitment to improving their work based on feedback, and the inclusion of additional experiments has strengthened the paper's rigor.

Weaknesses:
- The paper suffers from readability issues, with excessive notation and confusing figures, particularly Figure 2, which complicates comprehension.
- The evaluation is limited to only two MLLMs (GILL and NExTGPT), raising concerns about the generalizability of the findings.
- The method relies on advanced segmentation models, which may restrict its applicability to scenarios lacking such resources.
- The current score remains borderline, indicating potential areas for improvement.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by reducing notational density and enhancing figure captions, particularly for Figure 2. Additionally, expanding the evaluation to include a broader range of MLLMs and modalities, such as audio or video, would strengthen the generalizability of the claims. The authors should also consider conducting ablation studies to isolate the impact of different components of the NAM algorithm and address the assumptions regarding the encoding of semantic knowledge in FFN neurons versus self-attention blocks. Furthermore, engaging in further discussion during the rebuttal period could be beneficial in clarifying and enhancing the paper's contributions. Finally, a discussion on the computational costs associated with NAM compared to other methods would provide valuable context for its efficiency claims.