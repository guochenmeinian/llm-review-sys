ID: d7spHwemKX
Title: OntoTune: Ontology-Driven Self-training for Aligning Large Language Models
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents OntoTune, an ontology-driven self-training framework aimed at aligning large language models (LLMs) with domain-specific knowledge through hierarchical conceptual structures like ontologies. The authors propose that OntoTune can reorganize LLM knowledge using in-context learning, thus avoiding the need for large-scale domain-specific corpora. The framework is evaluated in the medical domain using SNOMED CT, achieving state-of-the-art performance in hypernym discovery and medical domain QA tasks while maintaining the general knowledge of the seed models.

### Strengths and Weaknesses
Strengths:
- OntoTune effectively integrates ontological knowledge into LLMs without extensive domain-specific corpora, marking a significant advancement in domain adaptation.
- The framework demonstrates superior results compared to existing methods like TaxoLLaMA, particularly in hypernym discovery and medical QA tasks.
- The paper is well-structured, with robust evaluations and interesting findings.

Weaknesses:
- The generalizability of the framework is limited, as evaluations are primarily conducted in the medical domain, raising questions about applicability to other domains.
- There is insufficient evaluation of how well the LLM aligns with ontological knowledge beyond hypernym discovery.
- The technical originality is questioned, particularly regarding its relevance to the Semantics and Knowledge track.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their work by conducting evaluations across various domains to assess the applicability of OntoTune. Additionally, the authors should provide a more detailed analysis of the performance inconsistencies observed in OntoTuneâ€™s variants (SFT, DPO, and SFT+DPO) across different tasks. It would also be beneficial to explore the integration of multi-modal ontological knowledge, as well as to clarify the definitions of "small-scale" and "high quality" ontologies, including references or metrics to operationalize these claims. Finally, we suggest that the authors consider evaluating the model's alignment with ontological knowledge through more complex tasks beyond hypernym discovery.