ID: i8JaxY7tDI
Title: $\textit{Read-ME}$: Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 4, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Read-ME, a framework for transforming pre-trained large language models (LLMs) into smaller Mixture-of-Experts (MoE) models with minimal training costs. The authors propose decoupling the router from the MoE backbone, enabling pre-gating and lookahead scheduling to enhance memory management and batching during inference. Empirical results demonstrate that Read-ME achieves significant improvements in latency and task performance compared to existing models, while also addressing the limitations of traditional per-layer routing schemes.

### Strengths and Weaknesses
Strengths:
1. The paper effectively bridges algorithmic advancements with system-level optimizations, ensuring that architectural improvements translate into real-world performance gains.
2. The introduction of a pre-gating, shared router is a notable innovation that allows for pre-computation and lookahead scheduling, addressing inefficiencies in traditional routing systems.
3. The expert-aware batching and optimal expert caching algorithms are well-designed, showing clear improvements in mean and tail latency.
4. Comprehensive experimental results validate the proposed approach, demonstrating significant improvements in MMLU performance and latency reductions.

Weaknesses:
1. The experimental evaluation is limited, primarily focusing on the LLaMA-2 model, which raises concerns about scalability and generalization to other model types and larger scales.
2. The analysis of the computational costs of the auto-regressive router compared to traditional routers lacks detail, necessitating a breakdown of its overhead and impact on inference latency.
3. The paper could benefit from a more detailed explanation of the pre-gating and batching algorithms, potentially including pseudocode or flow diagrams for clarity.
4. The motivation for pruning LLMs into smaller MoEs is unclear, as it raises questions about the advantages over smaller dense models.
5. The evaluation does not compare Read-ME with existing offline routing approaches, which could diminish the perceived contributions of the work.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by including tests on more diverse datasets and larger models to demonstrate scalability. Additionally, a detailed breakdown of the computational overhead introduced by the auto-regressive router compared to traditional routers should be provided to clarify its impact on inference latency. We suggest enhancing the explanation of the pre-gating and batching algorithms with pseudocode or flow diagrams to aid reproducibility. Furthermore, addressing the motivation behind pruning LLMs into smaller MoEs versus smaller dense models would strengthen the paper's rationale. Finally, including comparisons with existing offline routing approaches would provide a more comprehensive evaluation of Read-ME's contributions.