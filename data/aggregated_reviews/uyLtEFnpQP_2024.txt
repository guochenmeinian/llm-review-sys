ID: uyLtEFnpQP
Title: Du-IN: Discrete units-guided mask modeling for decoding speech from Intracranial Neural signals
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 4, 8, 7, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Du-IN, a speech decoding framework utilizing self-supervised pre-training and classification stages to decode spoken words from stereoElectroEncephaloGraphy (sEEG) data. The authors hypothesize that multi-variate representations within specific brain regions can enhance neural processing capture. They introduce a well-annotated Chinese word-reading sEEG dataset and conduct experiments on 12 subjects, focusing on classifying 61 Chinese words. The proposed model achieves state-of-the-art performance and includes a novel dataset that will be publicly available, potentially serving as a benchmark for future research. The authors differentiate Du-IN from existing models like LaBraM by utilizing region-level embeddings through depthwise convolution rather than channel-level embeddings.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and well-structured, making it easy to follow.
- The introduction of a novel sEEG dataset is significant, addressing the scarcity of such resources in the field.
- The manuscript provides a clear differentiation between Du-IN and LaBraM, highlighting the importance of region-level embeddings.
- The empirical analyses demonstrate the effectiveness of the proposed Du-IN framework, particularly the use of temporal masking and the innovative solution for the sEEG learning framework.

Weaknesses:
- The algorithmic novelty is limited, as the pre-training framework closely resembles existing methods like BEIT and VQVAE, with only minor modifications for sEEG data.
- Comparisons with other pre-training approaches are insufficient, lacking references to more recent methods like TS-TCC, Biot, and Neuro-BERT, and only a few baseline comparisons are provided.
- The decoding task appears to focus on well-established methods, with limited novelty in decoding a restricted vocabulary, and framing speech decoding as word classification may be overly simplistic.
- The model does not adequately address the variability of sEEG signals across different subjects, which could impact cross-subject performance.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their approach by clearly articulating the unique contributions of Du-IN compared to existing models and exploring more complex formulations for speech decoding. Additionally, we suggest expanding the comparative analysis to include more recent pre-training methods for neurological signals and ensuring that all baseline models are optimized for the specific decoding tasks. We encourage the authors to explicitly model multiple subjects in their framework to address potential representation conflicts and enhance cross-subject performance. Furthermore, we advise providing detailed reproduction information, including hyperparameters for all models compared, to enhance the credibility of their results. Lastly, we urge the authors to consider the ethical implications of their work, particularly regarding the invasive nature of sEEG recordings.