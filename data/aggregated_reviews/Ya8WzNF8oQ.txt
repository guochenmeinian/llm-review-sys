ID: Ya8WzNF8oQ
Title: Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 3, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a convergence rate analysis for a variant of policy-gradient learning of tabular-softmax policy models under a finite-horizon MDP setting with a discounting factor $\gamma=1$. The authors propose an epoch-by-epoch parameter update strategy, where learning occurs through a stochastic gradient ascent process for each epoch, assuming fixed returns in future epochs. The paper aims to analyze RL algorithms under an undiscounted setting, addressing the limitations of existing performance bounds derived under discounted MDP settings. Additionally, the authors analyze tabular softmax policy gradients (PG) for discounted Markov Decision Processes (MDPs) in the stochastic setting, emphasizing that previous analyses often focus on regularized cases or do not adequately address the stochastic nature of the problem. They argue that using structural information from dynamic programming can lead to more efficient algorithms, contrasting this with the common brute force approach of training all parameters simultaneously.

### Strengths and Weaknesses
Strengths:  
- The motivation to analyze RL algorithms under undiscounted settings is relevant, as many real-world problems are indeed undiscounted MDPs.  
- The results on convergence and convergence rates for policy gradient methods with softmax policy are sound and complete.  
- The paper is well-written and easy to follow, particularly the clear derivation of proofs in Section 4.  
- The paper addresses a significant gap in the analysis of stochastic tabular softmax PG for discounted MDPs.  
- The authors provide a theoretical framework that leverages the intrinsic nature of contextual bandit problems to propose more efficient algorithms.  

Weaknesses:  
- The policy gradient algorithm analyzed is not the standard version, which updates parameters for all decision epochs simultaneously, raising concerns about its practical relevance.  
- The epoch-wise updating strategy leads to analyses that appear to be straightforward adaptations of existing methods, potentially lacking novelty.  
- The interpretation of mathematical results, particularly regarding the factor $c_h$ in Theorem 3.8, is unclear, as it may not be a constant with respect to $H$.  
- The assumption that state distributions $\mu_h$ are fixed for all epochs is unreasonable, as they depend on the algorithm.  
- The reliance on a brute force training approach is critiqued, as it overlooks the problem's structure, potentially leading to inefficiencies.  
- The analysis primarily considers nonnegative rewards, which limits its applicability.  
- The paper may not sufficiently clarify the practical implications of its theoretical findings for practitioners.

### Suggestions for Improvement
We recommend that the authors improve the relevance of their algorithm by considering the standard policy gradient approach, which updates parameters across all epochs simultaneously. Additionally, we suggest providing a clearer distinction between new and existing results, particularly in Theorem 2.2. The authors should clarify the dimension of $\theta$ in Equation (5) and address the notation in Lemma 2.1 regarding the expectation operator. Including a numerical example to illustrate the complexity bounds of Theorem 4.4 would enhance the paper's practical relevance. Furthermore, we encourage the authors to discuss the implications of assuming fixed state distributions and explore the extension of their results to stochastic policy gradient methods. Finally, we recommend that the authors improve the clarity of how their theoretical insights can be translated into practical applications for practitioners and provide more references or context regarding existing analyses in the stochastic setting to strengthen their claims about the novelty of their work.