ID: GP30inajOt
Title: Retraction-free optimization over the Stiefel manifold with application to the LoRA fine-tuning
Conference: NeurIPS
Year: 2024
Number of Reviews: 20
Original Ratings: 5, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a retraction-free Riemannian optimization scheme on Stiefel and oblique manifolds for parameter-efficient fine-tuning (PEFT) in the LoRA style. The authors exploit landing flows on Stiefel manifolds, providing theoretical results on convergence alongside numerical experiments. They argue for the Stiefel manifold due to its similar geometry and the straightforward establishment of landing theory, while acknowledging the potential of the Grassmann manifold for future exploration. The proposed method aims to enhance the efficiency of fine-tuning while avoiding the computational burden associated with retractions. Additionally, the authors address concerns regarding the clarity of Table 1 and the accuracy of performance claims in their conclusions.

### Strengths and Weaknesses
Strengths:
- The method effectively combines Riemannian optimization advantages without the need for retraction, showcasing novelty in the context of PEFT.
- The theoretical analysis demonstrates convergence to critical points on the manifold and provides explicit guidance on setting the penalty parameter.
- Robust experimental validation indicates that the proposed method outperforms baseline algorithms on most datasets.
- The authors provide a clear rationale for choosing the Stiefel manifold over the Grassmann manifold, emphasizing its advantages in optimization.
- They demonstrate a willingness to revise their work based on feedback, particularly regarding the clarity of Table 1 and the accuracy of their performance statements.

Weaknesses:
- The theoretical framework is primarily developed for a function defined on $St(d,r)$, while the practical objective function for LoRA fine-tuning complicates direct application, as it must be minimized over $St(d,r) \times \mathbb{R}^{r \times m}$.
- The choice of step size remains unspecified, and the dependence of Lipschitz constants on $A$ raises concerns about the practical applicability of the convergence analysis.
- Some experimental results lack clarity and definition, and the manuscript does not adequately discuss the limitations of the proposed method.
- There remains ambiguity in the presentation of dataset pairs and evaluators in Table 1.
- The authors' initial performance claims may overstate their method's superiority across all datasets.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework to better align with the practical objective function used in LoRA fine-tuning, addressing the implications of non-compactness in the space of $A$. Additionally, we suggest providing a clear definition of the step size and exploring practical methods, such as backtracking line search, to ensure convergence. Clarifying the experimental results and their metrics, as well as discussing the limitations of the method more thoroughly, would enhance the manuscript's overall quality. We also recommend improving the clarity of Table 1 by clearly differentiating between dataset pairs and evaluators, ensuring that metrics are appropriately aligned. Furthermore, we suggest revising the conclusion to accurately reflect that while their method outperforms others on most datasets, it does not do so universally. Finally, we encourage the authors to explore the Grassmann manifold further, as it may enhance their optimization framework.