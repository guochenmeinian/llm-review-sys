ID: iJAOpsXo2I
Title: CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 9, 8, 6, -1
Original Confidences: 5, 3, 3, -1

Aggregated Review:
### Key Points
This paper presents CTIBench, a new benchmark for evaluating the performance of Large Language Models (LLMs) in Cyber Threat Intelligence (CTI). The benchmark assesses LLMs' capabilities through tasks such as multiple-choice questions, root cause mapping, vulnerability severity prediction, and threat actor attribution. The authors propose a framework that could guide future development in CTI tasks and enable further research. The results from assessing current LLMs provide valuable insights into their strengths and weaknesses in handling complex CTI tasks.

### Strengths and Weaknesses
Strengths:
- The benchmark tasks are well-defined and cover a range of essential CTI activities, providing a comprehensive evaluation framework for LLMs.
- The construction of the dataset is sound, and the experimental section offers insights into the performance of various LLMs.
- The paper is well-organized, clearly written, and cites relevant prior work, making it easy to follow.

Weaknesses:
- The scope of the submission is unclear, as it appears more as a dataset of tasks rather than a robust benchmark due to the absence of a software framework for consistent testing.
- The benchmark covers a limited subset of CTI tasks, which may not fully represent the domain's complexity.
- There is a lack of discussion regarding the potential misuse of LLMs in generating false intelligence, and the evaluation metrics may not capture the full performance spectrum of LLMs in practical CTI tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the benchmark's scope by providing a software framework for consistent and reproducible testing. Additionally, the authors should expand the discussion on how this benchmark aligns with the practical skills of cybersecurity professionals and consider including a comparative evaluation of CTIBench performance against human experts. To enhance the relevance of the tasks, we suggest incorporating real-world scenarios that reflect the complexities faced by CTI professionals, such as fill-in-the-blank questions instead of multiple-choice questions. Finally, addressing the potential data leakage bias and the implications of LLMs generating misleading intelligence would strengthen the paper's contribution to the field.