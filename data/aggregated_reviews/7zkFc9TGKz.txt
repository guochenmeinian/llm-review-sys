ID: 7zkFc9TGKz
Title: LD2: Scalable Heterophilous Graph Neural Network with Decoupled Embeddings
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel graph neural network (GNN) model called LD2, designed to optimize learning on large-scale heterophilous graphs. The authors propose a framework that decouples node feature embedding from topology embedding, utilizing a low-dimensional adjacency embedding and long-distance feature embedding through pre-computation. This approach aims to enhance performance and efficiency during mini-batch training. The authors provide theoretical support, complexity analysis, and extensive experimental results demonstrating LD2's effectiveness compared to existing baselines.

### Strengths and Weaknesses
Strengths:
1. The problem addressed is significant, as model efficiency on large-scale heterophilous graphs is underexplored.
2. The paper is well-organized, with thorough time complexity analysis and logical explanations of model components.
3. Experimental results indicate promising performance improvements in test accuracy and computational cost, achieving up to 15Ã— speed enhancement.

Weaknesses:
1. Comparisons between LD2 and other models are flawed due to differing programming languages, impacting fairness in performance evaluation.
2. The main contribution of LD2 appears limited, as it simplifies existing techniques without adequately comparing precomputation embeddings to established node embedding methods like node2vec.
3. The authors neglect to include a relevant baseline from the literature.

### Suggestions for Improvement
We recommend that the authors improve the fairness of comparisons by ensuring all models are implemented in the same programming language. Additionally, the authors should include a comparison of the embeddings obtained from precomputation with established node embedding methods to clarify the trade-offs involved. Furthermore, addressing the omission of the baseline [1] is essential for a comprehensive evaluation. Lastly, providing access to the experimental code would enhance reproducibility and allow for a deeper understanding of the model's mechanisms.