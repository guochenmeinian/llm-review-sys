ID: SihQ9bBLWa
Title: Annotations Are Not All You Need: A Cross-modal Knowledge Transfer Network for Unsupervised Temporal Sentence Grounding
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for unsupervised temporal sentence grounding (TSG) that utilizes transfer learning from image-noun and video-verb pairs, aiming to reduce reliance on expensive video-query annotations. The authors propose a Cross Modal Knowledge Transferring (CMKT) network that models appearance and action information in videos, achieving state-of-the-art performance on two challenging datasets. However, the introduction misrepresents the novelty of the work by not adequately citing prior research on unsupervised TSG.

### Strengths and Weaknesses
Strengths:
- The approach effectively reduces the need for large-scale training data by leveraging cross-modal alignment knowledge.
- The introduction of a copy-paste method enhances the model's ability to generalize across complex videos.
- Comprehensive experiments demonstrate significant performance improvements over existing unsupervised methods.

Weaknesses:
- The introduction lacks citations of relevant prior works on unsupervised TSG, leading to an inflated perception of novelty.
- The model diagram is overly complicated, negatively impacting readability.
- The methodology section is excessively detailed, obscuring key information and lacking organization.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the model diagram by simplifying its design and increasing font size. Additionally, we suggest that the authors reorganize the methodology section to highlight crucial information and consider moving less critical content to the appendix. It is also essential to address the lack of citations for previous works on unsupervised TSG to accurately contextualize the contributions of this paper.