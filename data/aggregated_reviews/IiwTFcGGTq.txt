ID: IiwTFcGGTq
Title: On the Adversarial Robustness of Out-of-distribution Generalization Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 7, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the adversarial robustness of out-of-distribution (OOD) generalization algorithms. It evaluates existing OOD methods, demonstrating their vulnerability to adversarial attacks, and provides a theoretical analysis of adversarial robustness in unseen domains. The authors propose two algorithms, AERM and RDANN, inspired by their theoretical findings, and conduct extensive experiments to validate their effectiveness.

### Strengths and Weaknesses
Strengths:
- The topic is novel, addressing the critical issue of adversarial robustness in OOD generalization, which is significant for machine learning reliability.
- The theoretical analysis is robust, covering various settings and providing comprehensive insights that inform the algorithm design.
- Experimental results show significant improvements in OOD adversarial robustness, with interesting observations consistent with theoretical predictions.
- The paper is well-written, with detailed explanations of methods and proofs that enhance reader comprehension.

Weaknesses:
- The experiments are limited to three datasets (RotatedMNIST, ColoredMNIST, and VLCS), lacking diversity in the DomainBed benchmark, which could be addressed by including at least one additional dataset.
- There is insufficient discussion regarding the relationship to relevant literature, particularly the missing references to works that explore the intersection of adversarial and OOD generalization.
- The clean accuracy drop on VLCS for the proposed methods is not adequately discussed, raising questions about the underlying reasons for this phenomenon.

### Suggestions for Improvement
We recommend that the authors improve their experimental section by incorporating results from at least one additional dataset in the DomainBed benchmark to enhance the robustness of their findings. Additionally, we suggest that the authors provide a thorough discussion of the relationship between their work and existing literature, particularly addressing the relevant studies that have explored adversarial robustness in OOD settings. Furthermore, we encourage the authors to elaborate on the observed decrease in clean accuracy on VLCS, providing insights into why this occurs compared to the other datasets. Lastly, we advise renaming AERM to align with established terminology in the field, such as "adversarial training," to clarify its relationship to existing methods.