ID: sSX9wLMSJT
Title: Probabilistic Active Few-Shot Learning in Vision-Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 8, 7
Original Confidences: 5, 4

Aggregated Review:
### Key Points
This paper presents a technically sound exploration of active learning and uncertainty estimation in Vision-Language Models (VLMs). The authors leverage Bayesian methods, particularly the Laplace approximation, to estimate uncertainty post-hoc, demonstrating mathematical rigor in their Gaussian approximation over cosine similarities and the Generalized Gauss-Newton (GGN) approximation. The experimental evaluation is comprehensive, addressing various datasets, models, and settings. The study innovatively combines active learning with probabilistic embeddings and emphasizes the role of uncertainty in support set selection, introducing a Wasserstein distance-based method for candidate selection. This approach could enhance the efficiency and reliability of few-shot learning, which is vital in scenarios with limited labeled data.

### Strengths and Weaknesses
Strengths:
- The use of Bayesian modeling is well-suited for active learning, and the combination with contemporary neural network tasks is commendable.
- The choice of the Laplace approximation is appropriate and well-justified.
- The appendix effectively covers necessary background information and presents derivations clearly.
- The experimental scope is extensive, and the writing is generally clear.

Weaknesses:
- The empirical results show only a slight advantage over the entropy-based baseline, and there is a lack of comparison with other potential methods in the literature.
- Minor issues include a reference error in line 69 (should refer to Appendix B1), a missing reference in line 335, and an undefined term ($\hat{H}$) in line 341.

### Suggestions for Improvement
We recommend that the authors improve the empirical results section by including comparisons to other methods, particularly non-Bayesian approaches, to provide a more comprehensive evaluation of their technique. Additionally, we suggest exploring other types of posterior approximations, such as a diagonal Laplace approximation over the entire network. It would also be beneficial to present general (unweighted) accuracy alongside class-weighted accuracy to give a fuller picture of the model's performance.