ID: 4Ggw1DsgRQ
Title: Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for optimizing post-training quantization (PTQ) in Large Language Models (LLMs), specifically focusing on 4-bit weight and 8-bit activation (W4A8) quantization. The authors propose activation-quantization-aware scaling (AQAS) and sequence-length-aware calibration (SLAC) to enhance computational efficiency and mitigate accuracy losses. Additionally, a new data format, dINT, is introduced to address underflow issues. The evaluation spans various LLMs (OPT, LLaMa) and tasks, demonstrating the effectiveness of the proposed methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, with clear visualizations that aid understanding.
- It provides valuable insights into the dynamic range of different layers and their variations with sequence lengths, contributing to future research on LLM quantization.
- The experimental design is comprehensive, showcasing the effectiveness of the proposed methods across multiple tasks.

Weaknesses:
- There are inconsistencies in the description and arrangement of figures, particularly in Figure 1(b), which lacks clarity.
- The distinction between AQAS and SmoothQuant is limited, with similar results observed across experiments.
- The SLAC method is inadequately described and underrepresented in experiments, raising questions about its overall contribution.
- The paper claims a twofold increase in hardware efficiency without providing corresponding experimental evidence.
- Comparisons with existing methods, such as AWQ, are missing, and some experimental comparisons may be unfair.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 1(b) by visually distinguishing the min-max ranges with different colors or markers. Additionally, the authors should provide a detailed description of the SLAC method, including its formula and implementation process. To strengthen the paper, we suggest including experiments that specifically demonstrate the claimed improvements in hardware efficiency. Furthermore, the authors should clarify the unique contributions of their work compared to similar investigations in the field and ensure that SLAC is included in a broader range of experiments for a comprehensive evaluation.