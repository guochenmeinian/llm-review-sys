ID: mfJifAlRMY
Title: Revisiting Referring Expression Comprehension Evaluation in the Era of Large Multimodal Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 6, 6, 6, -1
Original Confidences: 4, 4, 3, -1

Aggregated Review:
### Key Points
This paper presents a new benchmark for referring expression comprehension (REC) called Ref-L4, aimed at evaluating modern large multimodal models (LMMs). The authors analyze existing REC benchmarks, highlighting issues such as high annotation error rates and overly simplistic referring expressions. Ref-L4 addresses these shortcomings by offering a larger sample size, diverse object categories, lengthy referring expressions, and a rich vocabulary. The evaluation of 24 LMMs on Ref-L4 reveals biases and areas for improvement in model generalization.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and easy to follow, with extensive data.
- It meticulously cleans the original RefCOCO dataset, enhancing the assessment of LLM-based models.
- The new dataset features complex language descriptions and a richer vocabulary, improving its utility for REC evaluations.
- Human verification was conducted to ensure the accuracy of annotations.

Weaknesses:
- The reliance on GPT-4 for generating referring expressions may introduce noise if inaccuracies occur.
- There is insufficient detail on converting segmentation masks to bounding boxes, raising concerns about accuracy.
- Defining localization success solely at a single IoU threshold (0.5) overlooks the continuity of prediction quality.
- The models' performance advantage on the COCO dataset may be influenced by training data bias, necessitating further analysis.
- A lack of comparison with human performance limits the assessment of model capabilities.

### Suggestions for Improvement
We recommend that the authors improve the data generation methods beyond GPT-4 to enhance quality and reduce noise. Additionally, providing specific details on the conversion process from segmentation masks to bounding boxes would clarify accuracy concerns. The authors should consider using metrics such as average precision (AP) for a more comprehensive evaluation of localization performance. Further analysis of model generalization across new categories and scenes is essential, as is the inclusion of human benchmarks to assess practical model performance. Finally, expanding the dataset's diversity in linguistic and real-world contexts and incorporating additional REC datasets into comparative analyses would strengthen the benchmark's applicability.