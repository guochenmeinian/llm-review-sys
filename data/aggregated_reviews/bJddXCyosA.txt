ID: bJddXCyosA
Title: VisMin: Visual Minimal-Change Understanding
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents VisMin, a benchmark designed to evaluate fine-grained understanding in Vision-Language Models (VLMs) and Multimodal Language Models (MLLMs). It assesses models' abilities to identify subtle differences between similar images based on captions, focusing on changes in objects, attributes, counts, and spatial relationships. The benchmark is developed through an automated pipeline with human verification. Empirical findings indicate significant deficiencies in current VLMs regarding spatial relationships and counting, while fine-tuning models like CLIP and Idefics2 with the generated dataset enhances fine-grained understanding and image-text alignment.

### Strengths and Weaknesses
Strengths:
1. The introduction of VisMin effectively challenges models to detect subtle semantic differences, revealing significant shortcomings in VLMs and MLLMs.
2. The use of minimal-change image-text data for fine-tuning improves the fine-grained understanding capabilities of VLMs and MLLMs.
3. The dataset's curation pipeline is well-designed, yielding high-quality data beneficial for enhancing model performance.

Weaknesses:
1. Despite improvements in fine-grained understanding benchmarks, MLLMs showed decreased performance on POPE and MMMU, contradicting claims of enhanced text-image alignment. Additional benchmark results, such as TextVQA, MathVista, and MMBench, should be provided.
2. The explanation for poor performance due to the binary-choice task and computational constraints is insufficient; the authors could validate their method's effectiveness by using a smaller model like Qwen-1.8B.
3. The paper lacks zero-shot image classification results for VLMs, such as on ImageNet, to assess the impact of fine-tuning on zero-shot performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of notations and implementation details, particularly regarding how captions are selected for evaluation and the construction of the VisMin Instruct-IT dataset. Providing examples of rule-based methods used in dataset conversion would enhance understanding. Additionally, conducting a human evaluation to quantify noise in the dataset would be beneficial. Finally, we suggest including evaluation results for more recent models and justifying the inconsistent fine-tuning results observed in CLIP and Idefics2.