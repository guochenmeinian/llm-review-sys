ID: 0EfUYVMrLv
Title: Test-time Adaptation in Non-stationary Environments via Adaptive Representation Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a test-time adaptation (TTA) method called DART, which focuses on adapting to non-stationary environments by minimizing cumulative loss over time. DART employs multiple base learners reset at different periods, using dynamically learned weights for final predictions. The experimental results demonstrate DART's effectiveness in adapting to continual environmental changes on image corruption datasets. Additionally, the paper introduces Ada-ReAlign, an algorithm that utilizes a set of base learners with varying learning window sizes, combined with a meta-learner to adapt to changing distributions through entropy minimization.

### Strengths and Weaknesses
Strengths:
- Addressing continual adaptation in non-stationary environments is practical and relevant.
- The theoretical insights provided are beneficial and contribute to the understanding of the method.
- The transformation of continuous TTA to dynamic regret minimization is novel and versatile, allowing for effective handling of distribution shifts.

Weaknesses:
- Equations need accurate presentation; for instance, the argument of $\hat{L}_t$ in Eq. (4) appears incorrect, and softmax seems absent in the formulation of $\ell_\text{entro}^t$.
- More detailed explanations of theoretical analyses in Sec. 3.4 are needed, particularly regarding the origins of $\mathcal{O}(T^{-1/3})$ and $\mathcal{O}(T)$.
- Ada-ReAlign's requirement to maintain $\log T$ models increases memory consumption compared to baseline methods.
- The estimation of the distribution gap in Eq. (3) may be inaccurate with small batch sizes, necessitating evaluation with alternative metrics like optimal transport or MMD.

### Suggestions for Improvement
We recommend that the authors improve the accuracy of the equations presented, particularly ensuring the correct formulation of $\hat{L}_t$ and the inclusion of softmax in $\ell_\text{entro}^t$. Additionally, providing a more thorough explanation of the theoretical analyses in Sec. 3.4 would enhance clarity. It would be beneficial to compare memory consumption of Ada-ReAlign with baseline methods explicitly. We also suggest evaluating the distribution gap using alternative metrics to address potential inaccuracies. Finally, clarifying how to determine the number of base learners and the sliding window size based on the specific downstream problem would strengthen the practical applicability of the proposed methods.