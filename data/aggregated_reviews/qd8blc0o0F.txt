ID: qd8blc0o0F
Title: GRANOLA: Adaptive Normalization for Graph Neural Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GRANOLA, a novel graph-adaptive normalization layer for Graph Neural Networks (GNNs). The authors argue that existing normalization techniques, such as BatchNorm and InstanceNorm, inadequately address the unique characteristics of graph-structured data. GRANOLA normalizes node features by adapting to the graph's structure and utilizes Random Node Features (RNF) to enhance node representation. The authors provide theoretical backing for GRANOLA's design and demonstrate through empirical evaluations that it consistently outperforms existing normalization methods across various graph benchmarks.

### Strengths and Weaknesses
Strengths:  
* The paper introduces a normalization technique specifically designed for GNNs, effectively addressing a gap in the field.  
* It offers a solid theoretical foundation, including proofs of GRANOLA's adaptability to input graphs, enhancing the understanding of normalization in GNNs.  
* Extensive empirical results across multiple datasets show consistent performance improvements, reinforcing the credibility of the proposed method.  
* The paper is well-structured and clearly written, making it accessible to a broader audience.  

Weaknesses:  
* The focus on GNN normalization for graph-level tasks raises questions about its applicability to dominant models like Graph Transformers.  
* Implementation details regarding the sampling of RNF are unclear, particularly whether it is trained like other parameters or sampled randomly at each mini-batch or epoch.  
* The computational efficiency of GRANOLA is not adequately discussed, with significant increases in training and inference times noted.  
* The paper lacks sufficient evaluation against state-of-the-art methods, and the novelty of the approach is somewhat limited given prior work on adaptive normalization and random features.

### Suggestions for Improvement
We recommend that the authors improve the clarity of implementation details, particularly regarding the sampling and training of RNF. Additionally, we suggest conducting further experiments to evaluate GRANOLA's performance on more common backbone models like GCN and GAT. It would also be beneficial to include a more detailed discussion on the computational efficiency and performance-complexity trade-offs associated with GRANOLA. Lastly, we encourage the authors to emphasize the contribution of RNF in their findings and consider re-evaluating the framing of their work to highlight the improvements in expressivity rather than solely focusing on adaptive normalization.