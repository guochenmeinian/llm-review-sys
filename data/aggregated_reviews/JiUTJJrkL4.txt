ID: JiUTJJrkL4
Title: clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new suite of benchmarks for evaluating chat-oriented LLMs through two-player text-based games, assessing situated language understanding (SLU). The benchmarks include five games: taboo, a wordle clone, a drawing game, a picture matching game, and a knowledge exchange task, with an automated game master to enforce rules. The authors conduct experiments with various chat-oriented LLMs, revealing that models like GPT-4 outperform others. However, the paper lacks clarity on prompting methods and comparisons to existing benchmarks, leaving questions about the necessity and effectiveness of the proposed approach.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel framework for testing chat-oriented LLMs, filling a gap in evaluating SLU.
- The benchmark suite is open-source and allows for future expansions by the community.
- The test sets are created from scratch, minimizing overlap with LLM training data.

Weaknesses:
- There is no comparison to existing benchmarks, failing to clarify how this approach improves upon previous evaluations.
- The rationale for using two-player games is unclear, particularly when some games are inherently single-player.
- The absence of human validation raises concerns about the benchmarks' alignment with human judgments.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing a detailed comparison to existing benchmarks to clarify the advantages of their approach. Additionally, the authors should justify the choice of two-player games and explain the diagnostic intent behind transforming single-player games. We suggest incorporating human validation to assess the benchmarks' alignment with human performance. Furthermore, the authors should include qualitative analyses of failure modes for both older and newer models to better understand their performance. Lastly, we advise enhancing the clarity of figures and providing more context for terms like "incremental processing" to improve overall comprehension.