ID: c2LZyTyddi
Title: BIOT: Biosignal Transformer for Cross-data Learning in the Wild
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 5, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Biosignal Transformer (BIOT), a model designed for cross-dataset learning that accommodates varying channel numbers, sequence lengths, and missing values in biosignals. BIOT employs a tokenization module to convert multi-channel signals into sequences and utilizes a Linear Transformer Encoder for latent representation learning. The model is applicable for diverse tasks, including self-supervised and supervised training, and demonstrates superior performance compared to existing methods in biosignal classification tasks.

### Strengths and Weaknesses
Strengths:
- The design of token embeddings (segment, channel, and positional embeddings) shows originality within the Transformer framework.
- The methods are technically sound and comprehensible.
- BIOT effectively addresses challenges in biosignal modeling, such as mismatched channels, variable lengths, and missing values.

Weaknesses:
- Concerns exist regarding the scalability of BIOT to long sequences and numerous channels, as well as the inadequate modeling of spatial dependencies among channels.
- The paper lacks detailed explanations of token embeddings and does not sufficiently justify the choice of datasets or baseline models.

### Suggestions for Improvement
We recommend that the authors improve the discussion on scalability, particularly regarding long sequences and large channel numbers, and provide experiments to support this. Additionally, the authors should elaborate on the limitations of flattening channels into sequences, especially for graph-structured data. More details about token embeddings are necessary, including the computation of the energy vector, the dimensionality of the embedding table, and the functions used for positional embedding. We also suggest conducting more ablation studies to assess the impact of each component in BIOT, particularly normalization in the tokenization module and each token embedding. Lastly, the authors should clarify the motivation behind the dataset selection and consider including comparisons with reference models like SimCLR or MAEs.