ID: onr6HrKxn0
Title: DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a privacy preservation method for pre-trained language models by identifying and editing "privacy" neurons, specifically setting their activations to zero. The authors demonstrate this approach using the Enron dataset and BERT, showing that it can effectively preserve privacy without significantly compromising model performance. The DEPN (Detecting and Editing Privacy Neurons) method includes a gradient-based detector, an on-off neural privacy editor, and an aggregator for multiple sentences.

### Strengths and Weaknesses
Strengths:
- The method provides a straightforward solution for privacy preservation in language models.
- It is the first to apply neuron-based editing techniques to address privacy issues effectively while maintaining model performance.

Weaknesses:
- The evaluation is limited to only three types of private information in the Enron dataset; broader datasets are needed for a comprehensive assessment.
- The method's applicability to autoregressive language models, which are prevalent in real-world applications, is not addressed.
- The analysis of how batch size affects the editing process is insufficient, particularly regarding memorization effects.
- The comparison is limited to one baseline method, lacking a broader evaluation against various approaches in the literature.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by incorporating additional datasets, particularly those with diverse types of private information, such as in the biomedical domain. Additionally, the authors should extend their method to autoregressive language models to demonstrate its broader applicability. It would be beneficial to analyze the relationship between batch size and the effectiveness of memorization erasure more thoroughly. Finally, we suggest comparing the proposed method against a wider range of baseline approaches to strengthen the validation of their results.