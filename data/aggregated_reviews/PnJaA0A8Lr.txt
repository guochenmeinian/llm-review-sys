ID: PnJaA0A8Lr
Title: Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 7, 3, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of the trajectory alignment phenomenon during the edge of stability (EoS) in training neural networks, particularly focusing on the evolution of the tuple (residual, sharpness) when training on a single data point. The authors demonstrate that the residual and sharpness rapidly evolve onto a curve defined by "residual = Â± function(sharpness)" and remain on this curve during the final stages of training. This finding is supported by empirical evidence from real architectures and rigorous proofs in simplified settings. The paper also explores the alignment of gradient descent trajectories on a bifurcation diagram for specific neural network configurations.

### Strengths and Weaknesses
Strengths:
- The identification of the trajectory alignment phenomenon is a solid contribution, with rigorous establishment in simplified settings.
- The analysis improves upon prior work by tightening sharpness bounds and introduces concepts from control theory, providing useful insights into neural network optimization.

Weaknesses:
- The trajectory alignment phenomenon appears limited to training on one data point, with insufficient experimental evidence for its applicability to multiple data points.
- The claim that trajectory alignment is independent of network architecture and training data is not well-supported by experimental results, which indicate dependence on network width and data quantity.
- The paper lacks coverage of widely used loss functions, such as cross-entropy, raising questions about the generality of the findings.

### Suggestions for Improvement
We recommend that the authors improve the experimental evidence for the trajectory alignment phenomenon in settings with multiple data points, possibly by adopting the scaling trick from Chizat and Bach to enhance linearization accuracy in narrow networks. Additionally, we suggest clarifying the differences from Cohen et al. '21 regarding sharpness evolution during gradient flow to avoid confusion. The authors should also consider citing relevant prior works in the related work section and expanding the analysis to include more realistic datasets and loss functions. Lastly, addressing the limitations of the current results and providing a clearer definition of "reparameterization" would enhance the manuscript's clarity and rigor.