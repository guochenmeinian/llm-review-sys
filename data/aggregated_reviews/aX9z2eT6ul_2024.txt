ID: aX9z2eT6ul
Title: Unified Covariate Adjustment for Causal Inference
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for identifying causal estimands known as Unified Covariate Adjustment (UCA). The authors demonstrate that the UCA-expressible class encompasses a wide range of estimands identifiable by various adjustment methods, including back-door and front-door adjustments. Additionally, the paper proposes a scalable estimation strategy for the UCA-expressible class utilizing machine learning techniques and analyzes the error characteristics of these estimators, asserting that the proposed estimator achieves double robustness.

### Strengths and Weaknesses
Strengths:
1. The paper effectively illustrates the relationship between the UCA-expressible class and other classes of estimands through good examples.
2. It provides comprehensive identification and estimation strategies, presenting a complete causal inference methodology.
3. The theoretical analysis regarding the estimator's scalability and error characterization is robust.

Weaknesses:
1. The paper lacks necessary conditions for an estimand being not UCA-expressible, which could be beyond its scope.
2. It does not address the identification of causal estimands in the presence of unmeasured confounders, which may limit the applicability of UCA.
3. The claim regarding the estimator's double robustness may be questionable due to potential error accumulation as the index increases.
4. The proofs are relegated to supplementary material, lacking intuitive explanations in the main text.

### Suggestions for Improvement
We recommend that the authors improve the discussion on necessary conditions for an estimand being not UCA-expressible. Additionally, it would be beneficial to mention the limitations of UCA in the presence of unmeasured confounders at the end of Section 2. We suggest clarifying the conditions under which the estimator is doubly robust, particularly addressing the convergence rate for $\widehat{\mu}^i$. Furthermore, providing more intuitive explanations for the mathematical objects and symbols used, especially in Definition 1, would enhance clarity. Lastly, including a discussion on the limits of DML when the dimension grows and the sample size shrinks, as well as releasing the code for the estimator, would be valuable for the community.