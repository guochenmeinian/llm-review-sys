ID: kk3mSjVCUO
Title: ABEL: Sample Efficient Online Reinforcement Learning for Neural Theorem Proving
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 6, 5, 7, 7, 6
Original Confidences: 4, 3, 4, 3, 3

Aggregated Review:
### Key Points
This paper presents ABEL, a reinforcement learning framework designed for formal theorem proving that builds upon HTPS. ABEL integrates advanced language models and is trained on the LeanDojo dataset, demonstrating superior performance over state-of-the-art baselines, including DeepSeek Prover, GPT-f, and HTPS on the miniF2F and PutnamBench benchmarks. Key features include a proof tree structure for efficient proof searches, an online retraining mechanism, and a scalable online RL setup tailored for theorem proving. Experimental results show ABEL achieves state-of-the-art performance with significantly reduced computational resources and training examples.

### Strengths and Weaknesses
Strengths:
- ABEL achieves competitive theorem-proving performance with improved sample efficiency, requiring significantly fewer training examples than existing models.
- The integration of hypertree proof search and AlphaZero-style tree search enhances the framework's efficiency.
- The empirical results are compelling, establishing a new state-of-the-art on the PutnamBench benchmark while using 13x less compute than prior methods.

Weaknesses:
- The empirical evaluation is limited to MiniF2F and PutnamBench datasets, which restricts the demonstration of generalizability.
- The comparative analysis with existing systems lacks comprehensiveness, and the paper does not include ablation studies to clarify the impact of design choices.
- The writing and structure of the paper need improvement, as the system architecture and algorithm usage are not clearly presented.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper's writing and structure by including a clear system architecture diagram and algorithm examples to enhance comprehension. Additionally, conducting broader evaluations on additional datasets would strengthen the claims regarding generalizability. We suggest including ablation studies or a deeper analysis of key design choices and hyperparameters to better disentangle the contributions of various components. Finally, we encourage the authors to consider open-sourcing the code to facilitate reproducibility and further exploration by the research community.