ID: XjOj3ZmWEl
Title: ASIF: Coupled Data Turns Unimodal Models to Multimodal without Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 7, 4, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to obtain multimodal representations from pretrained unimodal models and a multimodal dataset, demonstrating its effectiveness on the zero-shot classification task. The authors propose mapping candidate images and texts into a shared representation space based on their similarities to an anchor multimodal dataset. The method aligns vision and language without requiring parametric model training, relying instead on relative representations derived from a support set of image-text pairs. Additionally, the paper introduces ASIF, an unsupervised method aimed at converting supervised encoders to open-vocabulary models without training, facilitating multimodal data handling and allowing for easy addition or removal of data without retraining. The performance of ASIF is discussed in comparison to existing models like CLIP and LiT, although some reviewers express skepticism regarding its empirical usefulness and competitive performance.

### Strengths and Weaknesses
Strengths:
- The proposed method is simple and intuitive, representing images or texts as vectors of similarities to an anchor set, requiring no gradient updates or training parameters.
- It allows for quick model adjustments through data handling without retraining, providing a degree of explainability in multimodal representations.
- The paper is clearly written, with a comprehensive discussion section.
- The introduction of ASIF emphasizes the ease of data modification without retraining and shows potential in settings with limited training resources, making it accessible for under-resourced audiences.
- The interpretability method is recognized as a valuable contribution.

Weaknesses:
- A larger multimodal dataset enhances zero-shot classification performance but results in larger representation vectors, which may complicate downstream tasks. More discussion on the limitations of representation vector sizes is needed.
- The method's sensitivity to the curated multimodal dataset is a concern; the authors did not explore the effects of using different datasets.
- Empirical results are weak, with performance significantly below established models like CLIP and LiT, raising questions about the method's scalability and generalization to other tasks.
- The empirical performance of ASIF is questioned, with some reviewers stating it remains at an "interesting idea" stage rather than demonstrating practical effectiveness.
- The current title may not adequately reflect the primary modalities discussed, leading to confusion regarding the scope of the work.
- The lack of experiments involving small training settings raises concerns about the overall usefulness and applicability of ASIF compared to existing models.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations and impacts of larger representation vectors, particularly regarding their applicability to generative tasks. Additionally, the authors should explore the effects of using different multimodal datasets and provide more experimental results to justify the influence of anchor sample selection. To enhance the paper's credibility, we suggest including comparisons with recent vision-language models and relevant baselines, such as K-nearest neighbors using the same visual representation. We also recommend improving the analysis of similarities between features from text and image encoders to provide convincing evidence of cross-modal communication. Furthermore, consider adopting other text encoders to strengthen the claims regarding modality compatibility. We suggest revising the title to more clearly indicate the primary focus on text and vision, as well as addressing the concerns about the empirical contributions by incorporating experiments that evaluate ASIF in both no-training and small-training settings. This could enhance the paper's significance and appeal to a broader audience. Finally, we encourage the authors to clarify claims regarding interpretability and data copyright issues, ensuring that the framework's advantages are clearly articulated.