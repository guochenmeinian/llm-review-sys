ID: QCY01LvyKm
Title: The iNaturalist Sounds Dataset
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 8, 8, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the iNatSounds dataset, a large and diverse collection of over 230,000 audio files from more than 5,500 species, aimed at advancing bioacoustic research. The authors conduct classification-oriented experiments to demonstrate the dataset's utility for pretraining models and its application in sound recognition tasks. The paper benchmarks various backbone architectures and provides baseline model scores, showcasing the dataset's potential for biodiversity monitoring and conservation.

### Strengths and Weaknesses
Strengths:
- The dataset is extensive, featuring contributions from 27,000 recordists, which enhances its diversity.
- The benchmarking of different backbone architectures is rigorous and well-structured, providing clear evaluation results.
- The paper is well-written and clearly organized, effectively communicating the dataset's significance and applications.

Weaknesses:
- Decisions regarding preprocessing, such as removing high-sampling-rate audio and converting to single-channel WAV files, limit the dataset's diversity and may not be justified.
- The weak labeling of some instances raises concerns about the dataset's utility as a validation benchmark.
- The relationship to prior work, particularly with comparable datasets like Xeno-Canto, is insufficiently addressed.

### Suggestions for Improvement
We recommend that the authors justify the decisions made in dataset preparation, particularly the removal of audio above 48kHz and the resampling to 22.05kHz, as these choices may unnecessarily limit the dataset's diversity. It would be beneficial to release a version of the dataset without preprocessing to preserve original audio quality. Additionally, we suggest including a comparison with Xeno-Canto and a more structured overview of existing bioacoustic datasets to better situate this work in the literature. To enhance the dataset's robustness, consider providing a subset with strong labeling and conducting analyses to evaluate the representativeness of the validation and test distributions. Finally, we recommend using class-averaged ROC-AUC as an evaluation metric and detailing the thresholding strategy used for metrics like F1 to provide clearer insights into model performance.