ID: NNtsO5L27J
Title: Efficient Low-rank Backpropagation for Vision Transformer Adaptation
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 4, 5, 6, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel training method called Low-rank Back Propagation via Walsh-Hadamard Transformation (LBP-WHT) aimed at enhancing the efficiency of backpropagation in Vision Transformers (ViTs). By projecting gradients into a low-rank space, LBP-WHT significantly reduces computational costs during training, as demonstrated through extensive experiments on classification and semantic segmentation tasks. The results indicate that LBP-WHT achieves a favorable efficiency-accuracy trade-off compared to existing methods like LoRA.

### Strengths and Weaknesses
Strengths:
1. The adoption of low-rank backpropagation for ViT adaptation is a novel contribution.
2. The method is straightforward and well-illustrated, particularly in Figure 2.
3. Impressive results show significant reductions in MFLOPs and enhanced performance on partially trained models.

Weaknesses:
1. Theoretical complexity reduction does not correlate with actual training speedup; training time or throughput metrics should be reported, especially for Table 2. The real speedup observed is only 1-2x, contrasting sharply with the MFLOPs reduction of 6-7x.
2. Memory cost during adaptation is not reported, which is critical for recent works, especially in low-memory scenarios.
3. The method does not offer advantages during inference and may introduce overhead in model parameters.
4. There is a lack of an ablation study combining LoRA with LBP-WHT to validate claims of complementarity.

### Suggestions for Improvement
We recommend that the authors improve the reporting of training time (GPU hours) and training throughput (images/s) in the main experiments to provide a clearer picture of the method's efficiency. Additionally, the authors should include memory cost analysis, particularly in the context of large language models and on-device training scenarios. To strengthen the paper, we suggest conducting experiments on larger datasets like ImageNet-1k and evaluating the proposed method on other architectures, such as MLP-Mixer and CNNs. Finally, we encourage the authors to provide an ablation study that demonstrates the benefits of combining LoRA with LBP-WHT, as well as a more thorough literature review that includes comparisons with methods like Gradient Compression.