ID: AwzbQVuDBk
Title: ProteinNPT: Improving Protein Property Prediction and Design with Non-Parametric Transformers
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 6, 6, 5, 5, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ProteinNPT, a transformer-based semi-supervised learning model that combines MSA transformer and Non-parametric transformer (NPT) to enhance property and mutation effect predictions. The authors demonstrate its potential in protein design tasks and introduce the extended benchmark dataset, ProteinGym, for model evaluation. By leveraging unlabelled natural sequences for pretraining, the model achieves informative embeddings, and the tri-axial attention mechanism facilitates learning relationships across data points. The results indicate that ProteinNPT outperforms baseline models across various downstream tasks.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant topic in protein engineering with a clear flow from motivation to solution.
- Experimental design supports claims, showing ProteinNPT's superiority over baseline models in multiple tasks.
- The authors effectively illustrate improvements in representation learning through comparisons with baseline methods.
- The introduction of novel datasets and comprehensive experimental analysis adds value to the bio-ML community.

Weaknesses:
- The model design is not clearly illustrated; the figures and annotations are overly simplistic, making it difficult to understand the integration of the MSA transformer.
- Many claimed novelties appear to be subtle changes or borrowed ideas from existing literature, limiting the perceived innovation.
- The model's performance in certain datasets is not adequately analyzed, particularly where it underperforms compared to other models.
- Presentation issues hinder comprehension, requiring readers to reference the appendix for critical information.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the model design by providing more explicit illustrations of how the MSA transformer is integrated and trained within ProteinNPT. Additionally, we suggest including a detailed discussion of the training data for the MSA transformer and how MSA sequences are generated. To enhance the novelty claims, the authors should clearly differentiate their contributions from existing methods. We also encourage the authors to analyze the performance discrepancies observed in Figures 5 and 6. Lastly, it would be beneficial to incorporate additional mathematical formulations in the main text to support the architectural explanations.