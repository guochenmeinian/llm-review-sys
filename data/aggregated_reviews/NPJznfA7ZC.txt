ID: NPJznfA7ZC
Title: Demystifying Prompts in Language Models via Perplexity Estimation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the hypothesis that the performance of a prompt is predicted by the extent to which the language model (LM) is familiar with the language it contains, using perplexity as a proxy measure. The authors find that lower perplexity correlates with better performance and propose a prompt selection method, SPELL, which generates and reranks prompts based on perplexity. The empirical results support the hypothesis, showing that negative correlations exist between LM perplexity and prompt performance across various tasks.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important research question regarding prompt effectiveness, particularly relevant as prompting becomes standard for adapting LMs to tasks.
- Experiments are well-designed, controlling for confounders, and the writing is clear and accessible.
- The proposed SPELL method shows performance improvements on some datasets, notably a +10 accuracy point gain on Newspop.

Weaknesses:
- The focus on perplexity may overlook other factors influencing performance; the relevance of prompts to tasks should be systematically considered.
- The experimental scope is limited, primarily focusing on word translation and text classification, which may not capture the full range of prompt performance.
- Inconsistencies in performance gains across datasets and LMs raise concerns about the generalizability of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the relevance of prompts to tasks, particularly in section 5, to address potential confounders. Additionally, expanding the experimental scope to include more complex tasks could provide more generalizable insights. Clarifying how perplexity is the primary influencing factor, as opposed to other prompt characteristics, would strengthen the argument. Finally, addressing the variability in performance across different settings and ensuring equal numbers of prompts in comparisons would enhance the robustness of the findings.