ID: PCjK8dqrWW
Title: WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 8, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents WorkArena++, a benchmark designed to evaluate web agents' capabilities in executing complex knowledge work tasks typical in enterprise environments. It includes a set of 682 tasks that assess advanced skills such as planning, decision-making, and information retrieval, with tasks categorized into two difficulty levels (L2 and L3). The evaluation reveals that state-of-the-art LLMs and VLMs struggle significantly, while human performance remains high. The authors provide a detailed analysis of the failure modes observed in these evaluations.

### Strengths and Weaknesses
Strengths:
1. The benchmark is well-designed, with clear documentation and rationale for each design choice.
2. The empirical evaluation is thorough, analyzing multiple LLM/VLM agents and comparing results with human performance, including detailed error analysis.
3. The paper is well-motivated and clearly written, addressing a significant gap in evaluating web agents in realistic enterprise settings.

Weaknesses:
1. The benchmark is limited to tasks performed exclusively on the ServiceNow platform, which may not represent the diversity of enterprise workflows.
2. There is a concern that the open-source nature of the benchmark may lead to overfitting in future LLMs, diminishing its utility as an evaluation tool over time.

### Suggestions for Improvement
We recommend that the authors improve the benchmark's relevance by incorporating tasks from a broader range of enterprise software platforms beyond ServiceNow to enhance generalizability. Additionally, we suggest isolating the task of interpreting vague descriptions into smaller, concrete steps as its own benchmark to clarify the distinction between L2 and L3 tasks. It would also be beneficial to discuss strategies for ensuring the long-term relevance of the benchmark as AI technologies evolve, including how it might be updated or expanded. Furthermore, providing more details on the demographics of human evaluators and their potential impact on task complexity perception would strengthen the paper. Lastly, consider exploring the implications of prompt engineering and the effects of learning on human evaluators in the context of model performance.