ID: ZoarR5QmFX
Title: Concentrate Attention: Towards Domain-Generalizable Prompt Optimization for Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies prompt optimization methods for fine-tuning language models, specifically addressing the domain generalization issue when the target domain is unknown. The authors present two empirical findings linking the generalization ability of prompts to attention patterns in later layers of the model. They propose new objectives for both soft and hard prompt optimization based on these findings. Experimental results on BERT-size transformers demonstrate substantial gains over vanilla methods in tasks such as sentiment classification and natural language inference (NLI).

### Strengths and Weaknesses
Strengths:
1. The paper pioneers the exploration of domain generalization in prompt optimization under practical conditions.
2. The connection between prompt generalization and attention patterns is insightful and valuable for future research.
3. The translation of findings into loss objectives presents nontrivial technical challenges, which the authors address effectively.
4. Empirical results show consistent and substantial performance improvements.

Weaknesses:
1. The main findings are primarily empirical, lacking a theoretical explanation for the generalizability of prompts.
2. The focus on small-scale transformers raises questions about the applicability of findings to larger models; further verification on open-source LLMs is needed.
3. The paper's presentation includes incorrect in-context citation formats.
4. The limited scope of tasks (only sentiment classification and NLI) restricts the generalizability of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the theoretical underpinnings of their findings to explain why certain prompts exhibit greater generalizability. Additionally, we suggest expanding the experimental scope to include larger models and a broader range of tasks beyond classification, such as open-ended generation. Furthermore, addressing the incorrect citation formats and providing clearer hyperparameter details for replication would enhance the paper's clarity and rigor.