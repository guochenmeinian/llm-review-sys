ID: T9jJsFUGtI
Title: Citance-Contextualized Summarization of Scientific Papers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for contextualized summarization of scientific papers, focusing on generating informative summaries based on the citances (citation texts) in the citing paper. The authors propose a method that grounds summaries in the context of citances, facilitating readers' access to relevant information. Additionally, the paper introduces a large dataset from the computer science domain, consisting of 4.6 million citation texts across over half a million academic papers. The proposed approach leverages Large Language Models (LLMs) to tailor summaries to the citances, addressing an important task in summarization.

### Strengths and Weaknesses
Strengths:
- The introduction of a new dataset, Context-SciSumm, which is valuable for the NLP community and research in summarization.
- The paper addresses a significant task in contextualized summarization, offering a novel perspective.
- The evaluation methodology is robust, considering various settings within the LLMs scope.

Weaknesses:
- The dataset lacks large-scale human-generated summaries, limiting its utility for training deep learning models and standard evaluation benchmarks.
- The focus on LLMs without comparison to standard summarization models or extractive baselines raises concerns about evaluation thoroughness.
- The claim of providing informative summaries tailored to citances is undermined by findings that the abstract is the most appreciated summary.
- The use of GPT-4 generated ground truth summaries, despite manual verification, raises questions about reliability and bias.
- The limited number of evaluation data points (15 papers and 25 citances) may not be representative enough for broader conclusions.

### Suggestions for Improvement
We recommend that the authors improve the dataset by including a larger number of human-generated summaries to enhance its applicability for training and evaluation. Additionally, the authors should consider including comparisons with standard summarization models or extractive baselines to provide a more comprehensive evaluation of their approach. Clarifying the interpretation of the finding that abstracts are preferred summaries would strengthen the paper's claims. We also suggest elaborating on the choice of using GPT-4 generated ground truth summaries to ensure reliability and minimize bias. Finally, increasing the number of evaluation papers while potentially reducing the number of citances could lead to more robust results.