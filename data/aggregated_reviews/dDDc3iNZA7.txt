ID: dDDc3iNZA7
Title: UniDSeg: Unified Cross-Domain 3D Semantic Segmentation via Visual Foundation Models Prior
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 4, 5, 4, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents UniDSeg, a universal approach that enhances the adaptability and generalizability of cross-domain 3D semantic segmentation. The authors propose a learnable parameter-inspired mechanism for off-the-shelf Visual Foundation Models (VFMs) with frozen parameters, which preserves pre-existing target awareness in VFMs. The method incorporates prompt-tuning concepts into domain generalization and adaptation tasks, utilizing Modal Transitional Prompting and Learnable Spatial Tunability to leverage 2D prior knowledge. Extensive experiments validate the proposed method's effectiveness across recognized tasks and datasets.

### Strengths and Weaknesses
Strengths:
1. The motivation for the research is clear and the proposed method is intuitive.
2. The introduction of prompt-tuning into the universal model for domain generalization and adaptation is innovative.
3. Extensive experimental results demonstrate that the proposed model outperforms baseline methods.

Weaknesses:
1. The method lacks theoretical analysis, particularly regarding the 3D-to-2D transitional prior.
2. The proposed Learnable Spatial Tunability appears similar to existing designs, raising questions about its novelty.
3. The experimental validation relies solely on the classification model CLIP, which may not be suitable for segmentation tasks.
4. The improvement in results is not sufficiently strong, and the motivation for using VFMs in segmentation tasks is not clearly articulated.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of Modal Transitional Prompting and clarify its relation to 3D-to-2D transitional prior. Additionally, we suggest providing a more detailed comparison with existing models like VFMSeg to elucidate the advantages of their approach. The authors should also consider validating their method on other VFMs designed for segmentation tasks, such as SAM or SEEM, to strengthen their claims. Furthermore, we encourage the authors to enhance the clarity of their writing, particularly in sections where terminology and concepts are introduced.