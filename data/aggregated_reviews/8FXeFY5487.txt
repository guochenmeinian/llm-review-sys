ID: 8FXeFY5487
Title: Enhancing Scalability of Pre-trained Language Models via Efficient Parameter Sharing
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a parameter-efficient approach utilizing a matrix product operator (MPO) to scale pre-trained language models (PLMs) to deeper structures. The authors propose a parameter-sharing architecture that employs an MPO-based initialization method, leveraging ALBERT's decomposition results. The approach aims to reduce model size while maintaining layer adaptability through a low-rank adapter. Extensive experiments demonstrate the effectiveness of the proposed methods across various settings, including fully-supervised, multitask fine-tuning, and few-shot learning.

### Strengths and Weaknesses
Strengths:
- The authors conduct comprehensive experiments, providing thorough evaluations across multiple settings.
- Theoretical analysis supports the stability of the proposed method in training deeper architectures.
- The writing is clear, facilitating reader comprehension.

Weaknesses:
- Clarity regarding the proposed methods is lacking, with essential details for result reproduction missing.
- Important baseline comparisons, such as RoBERTa-large, are absent, and the connection between bond dimension and adapter rank is unclear.
- The inference speed and efficiency improvements of the model are not adequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proposed methods by including essential details necessary for result reproduction. Additionally, the authors should provide a more explicit explanation of the "efficiency" of their model, including metrics such as real-time inference latency, memory consumption, or training time. It is also crucial to clarify how the rank for shared and layer-specific tensors is determined and to analyze the differences between MPOBERT and MPOBERT+. Furthermore, the authors should include missing baseline comparisons and conduct a more thorough ablation analysis of crucial hyper-parameters.