ID: BRqlkTDvvm
Title: BQ-NCO: Bisimulation Quotienting for Efficient Neural Combinatorial Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel MDP formulation named BQ-MDP, based on Bisimulation Quotienting (BQ), aimed at enhancing neural constructive solvers for combinatorial optimization (CO) problems. The BQ-MDP reduces the state space and captures the symmetry of sub-problems during solution construction. The authors propose a deep model, BQ-Transformer, trained via supervised learning, which demonstrates improved generalization performance across various problem sizes and distributions compared to existing baselines. Additionally, the authors introduce end-to-end models trained on instances with 100 nodes, showcasing (0-shot) generalization to instances with up to 4000 nodes. They compare their models to state-of-the-art end-to-end neural models, asserting superior generalization performance and emphasizing the generic applicability of their approach to five different problems.

### Strengths and Weaknesses
Strengths:
- The introduction of BQ into MDP formulations for neural constructive solvers is both novel and interesting.
- The method shows impressive generalization performance across problem sizes, outperforming several baselines.
- The rigorous formulation of BQ-MDP is applicable to a wide range of CO problems.
- The exploration of BQ-PerceiverG to reduce computational complexity is commendable.
- The proposed models exhibit strong generalization capabilities from 100 to 4000 nodes.
- The authors provide a comprehensive comparison with state-of-the-art models, demonstrating superior performance in various instances.
- The approach is generic, applicable to multiple problems, enhancing its versatility.

Weaknesses:
- Clarity issues exist in the mathematical formulations, particularly in Sections 2.1 and 3.1, such as the definition of the "step set Z" and the notation for partial solutions.
- The novelty of the model architecture may be limited, sharing similarities with previous works, and the claim regarding the elimination of the autoregressive decoder lacks compelling evidence.
- The reliance on imitation learning raises questions about the attribution of performance improvements to the architecture versus the quality of labeled data.
- Experimental comparisons are insufficient, with concerns about performance on TSP-100 and CVRP-100, lack of comparisons with POMO under similar settings, and unclear dataset specifications.
- Some referenced works focus on specific problems, limiting their applicability to the authors' claims.
- Certain comparisons with other models reveal that those models do not outperform the authors' baseline.

### Suggestions for Improvement
We recommend that the authors improve the clarity of mathematical formulations, particularly by providing explicit definitions and examples, such as toy TSP-5 instances. Additionally, the authors should address the novelty of their architecture by discussing its differences from previous works, particularly reference [4]. It would be beneficial to clarify the applicability of BQ-MDP to various combinatorial optimization problems beyond those tested and to report variances in experimental results. We also suggest including comparisons with other recently developed frameworks that enhance cross-size performance, as well as discussing the potential for using reinforcement learning in conjunction with the BQ-MDP framework. Furthermore, we recommend that the authors improve the related works section by explicitly discussing the limitations of the referenced models, particularly those that are problem-specific. Lastly, we suggest that the authors consider integrating insights from the meta-learning frameworks proposed in references [7] and [8], as these could enhance the adaptability of their models. Finally, the authors should ensure that the appendix and supplementary materials are correctly provided for review.