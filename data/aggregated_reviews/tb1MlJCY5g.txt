ID: tb1MlJCY5g
Title: KALM: Knowledgeable Agents by Offline Reinforcement Learning from Large Language Model Rollouts
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents KALM (Knowledgeable Agents from Language Model Rollouts), a method that fine-tunes a language model (LLM) to generate imaginary rollouts for offline reinforcement learning (RL). KALM aims to bridge the gap between LLMs and action-taking agents by translating textual goal descriptions into environment rollouts, which are then utilized for training. The authors demonstrate KALM's effectiveness on robotic manipulation tasks, achieving superior performance compared to baseline methods.

### Strengths and Weaknesses
Strengths:
- The approach of using LLMs to generate imaginary rollouts is innovative and beneficial for decision-making.
- The empirical results validate the method's effectiveness, particularly in out-of-distribution scenarios.
- The paper is well-structured and clearly articulates the proposed method.

Weaknesses:
- The limited context length of the LLM raises concerns about generating complete trajectories for complex tasks, and details on trajectory lengths in experiments are lacking.
- The reliance on environment-built-in reward functions may restrict applicability to certain tasks.
- The paper does not adequately discuss related work in model-based offline RL, which could provide a more comprehensive context for KALM's contributions.
- The evaluation of KALM against baselines may not be entirely fair due to the additional information provided by the hidden vector generated by BERT.

### Suggestions for Improvement
We recommend that the authors improve the discussion of model-based offline RL methods like MOPO, MoREL, and COMBO to contextualize KALM's contributions. Additionally, clarifying the evaluation process and ensuring that all baselines are compared fairly, considering the use of hidden vectors, would strengthen the analysis. We also suggest including a comparison against RL methods that leverage prior knowledge from LLMs, as this could enhance the paper's motivations. Lastly, addressing the limitations related to the LLM's context length and potential hallucinations in generated rollouts would provide a more robust understanding of KALM's capabilities.