ID: 3LKuC8rbyV
Title: Langevin Unlearning: A New Perspective of Noisy Gradient Descent for Machine Unlearning
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 8, 5, 5, 7, -1, -1, -1
Original Confidences: 3, 3, 1, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel unlearning framework called Langevin unlearning, which is based on noisy gradient descent and provides privacy guarantees for approximate unlearning. The framework unifies differential privacy with the unlearning process, demonstrating benefits such as approximate certified unlearning for non-convex problems, complexity savings compared to retraining, and support for sequential and batch unlearning. The theoretical foundation relies on the log-Sobolev inequality, establishing a linear rate of convergence for the unlearning step, with explicit constants provided for convex and strongly convex settings. The authors also extend their framework to handle multiple deletions.

### Strengths and Weaknesses
Strengths:
- The paper is a strong contribution, offering original theoretical advances and exploring second-order effects of the framework.
- It provides the first provable approximate unlearning guarantees in the nonconvex setting, which is significant for future research.
- The organization and writing are generally clear, with rigorous theoretical analysis and empirical evaluations.

Weaknesses:
- Clarity issues arise from the use of advanced technical tools that may not be accessible to a broader audience; key intuitions behind the log-Sobolev inequality need clearer explanations.
- The improvements from Langevin unlearning over retraining are only logarithmic, raising questions about their significance in experiments.
- The absence of experiments in the nonconvex case limits the practical applicability of the framework.
- The presentation could benefit from clearer notation and a more intuitive connection between theoretical results and empirical evaluations.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing detailed explanations of the log-Sobolev inequality and its implications, including simple examples and explicit values of the LSI constant. Additionally, we suggest including experiments in the nonconvex case to establish benchmarks for the proposed framework. The authors should also explore the computational complexity with stochastic gradient steps and clarify the significance of the logarithmic improvements in their experiments. Finally, we encourage the authors to summarize the main claims in a conclusion section for better coherence.