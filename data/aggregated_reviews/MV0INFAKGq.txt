ID: MV0INFAKGq
Title: Tanimoto Random Features for Scalable Molecular Machine Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 7, 7, 7, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two methods for approximating the Tanimoto kernel using random feature maps, addressing the lack of scalable random feature approximations for this widely used kernel in cheminformatics. The authors propose random features derived from hashing and sketches, providing theoretical proofs of optimality and error bounds. They clarify that the Tanimoto kernel, while appearing linear, is fundamentally non-linear and lacks a simple finite-dimensional feature map. The empirical evaluation demonstrates the methods' effectiveness across various molecular fingerprint datasets, with the proposed feature map approximating the kernel without statistical bias and with low variance.

### Strengths and Weaknesses
Strengths:
1. The proposal of two novel random features for Tanimoto coefficients.
2. Theoretical analysis on variance and approximation error for the proposed features.
3. Insightful computational experiments validating the efficiency of the methods.
4. Clear exposition and background information, making the work accessible to a broader audience.
5. Detailed responses to reviewer queries, clarifying technical aspects and addressing misunderstandings.
6. The proposed method fills a gap in the literature by offering random feature approximations for the Tanimoto kernel, which has not been previously addressed.

Weaknesses:
1. The computation of T_MM is unexpectedly slow, raising concerns about the efficiency of the proposed methods.
2. Uncertainty regarding the correctness of the error bounds, which requires clarification on whether the bounds are too loose or if there are experimental bugs.
3. The relevance of hand-crafted kernels compared to deep learning embeddings is questioned, particularly in terms of efficiency and scalability.
4. Some reviewers expressed confusion regarding the motivation for using random features for a kernel that seems linear.
5. Concerns about the clarity of notation and the presentation of key concepts, particularly in sections 3 and 4.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing more rigorous definitions and explanations of key concepts, such as "random feature maps" and "data-dependent sketches." Additionally, addressing the computational efficiency of T_MM and clarifying the error bounds will strengthen the paper. We suggest that the authors improve the clarity of notation throughout the paper, particularly in sections 3 and 4, to avoid ambiguity regarding terms such as "independent copies," "hash functions," and the role of various symbols. Explicitly stating the assumptions regarding the hash function and its impact on the results would also be beneficial. Furthermore, a more extensive investigation into the advantages of hand-crafted kernels over deep learning embeddings should be included, along with a discussion of the relevance of the DOCKSTRING task as a benchmark. Finally, we encourage the authors to provide a more detailed discussion on the significance of their method in comparison to existing approaches, especially regarding the variance of earlier methods, and to consider comparing their methods against a broader range of baselines, including other kernels, to better contextualize their contributions.