ID: BAjjINf0Oh
Title: Oracle-Efficient Differentially Private Learning with Public Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 8, 7, 6, 8, -1, -1, -1
Original Confidences: 2, 3, 3, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents methods for private learning utilizing public unlabeled data, addressing inefficiencies in previous approaches that relied on constructing an $\alpha$-covering and employed an exponential mechanism, which resulted in exponential running times. The authors propose new private and oracle-efficient algorithms with polynomial complexity, leveraging an Empirical Risk Minimization (ERM) oracle. The main results indicate that when the Gaussian complexity of the hypothesis class is small, a polynomial-time algorithm can be achieved, particularly with well-structured hypothesis classes. The algorithms are designed to be simple and practical, with high accuracy guarantees under certain conditions.

### Strengths and Weaknesses
Strengths:  
- The paper significantly advances the field by designing private algorithms that achieve oracle efficiency, filling a crucial gap in the literature.  
- The proposed algorithms are straightforward and applicable in practice, requiring only a polynomial number of calls to the ERM oracle.  
- The results hold under slight distribution shifts between public and private data, enhancing their applicability.

Weaknesses:  
- The intuitions behind the algorithm design are not clearly articulated, making it challenging to fully appreciate the results.  
- There is insufficient discussion on sample complexity, particularly regarding the term $n \ge \Omega(1/\alpha^{14})$ in Theorem 2.  
- The paper lacks clarity on the motivation for focusing on Gaussian complexity over VC dimension and whether results can be extended to the latter.  
- The sample complexity, while polynomial, is notably higher than that of previous work, and the reliance on exact ERM oracles raises questions about the validity of conclusions with approximate minimizers.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the intuitions behind their algorithm design to enhance understanding of the results. Additionally, it would be beneficial to include a detailed discussion on the role of public data in the learning process, addressing how it aids learning when private data alone is insufficient. We also suggest elaborating on the implications of the sample complexity bounds, particularly regarding the dependence on $\sigma$ and the necessity of assuming a smooth data distribution. Finally, clarifying the motivation for using Gaussian complexity and discussing potential extensions to VC dimension would strengthen the paper.