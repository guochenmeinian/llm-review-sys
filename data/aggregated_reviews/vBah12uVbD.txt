ID: vBah12uVbD
Title: Conformalized Credal Set Predictors
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 4, 8, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for predicting credal sets in classification tasks, generating conformal credal sets that are valid with high probability without assumptions on the model or distribution. The method applies to both first- and second-order predictors, with experiments on ChaosNLI and CIFAR10-H demonstrating its effectiveness in practical uncertainty quantification. The authors define various score functions based on distances in the probability space and likelihood. They also explore the impact of aleatoric uncertainty and justify the use of synthetic datasets to illustrate the behavior of credal sets, acknowledging limitations in the medical domain due to the unavailability of images in the Google Dermatology DDX dataset.

### Strengths and Weaknesses
Strengths:  
1. The paper is well-written, with clear demonstrations despite the presence of many formulas.  
2. The proposed method interestingly combines imprecise probability and conformal prediction.  
3. The experimental design on small label size datasets is comprehensive.  
4. The visualization of credal sets within a 2-D simplex is a notable feature that enhances the paper's appeal.  
5. The authors provide a clear rationale for the use of synthetic data to demonstrate the potential of their work.  
6. Limitations are discussed thoroughly.

Weaknesses:  
1. The technical novelty is moderate, as the method closely resembles a previous conformal prediction approach on ambiguous labels, and obtaining a probability interval is not very novel.  
2. The contribution and impact of the work are perceived as moderate, particularly due to the lack of focus on the medical domain, which is a critical application area.  
3. Computational complexity restricts the method's application to large-scale datasets.  
4. Access to ground-truth probability labeling is often unreasonable, complicating model calibration.  
5. The bounded noise assumption for the labeling process may not be practical.  
6. The limitations imposed by the unavailability of images in the Google Dermatology DDX dataset hinder a comprehensive exploration of the topic.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how their method compares to existing uncertainty representation methods, such as Bayesian neural networks and deep ensembles, to elucidate the added gains in uncertainty estimation. Additionally, the authors should address the computational cost of generating credal sets, as the time taken at test time may be a significant drawback. It would also be beneficial to provide a clear strategy for deriving class predictions from the generated credal sets and to evaluate the quality of uncertainty estimation using downstream tasks like Out-Of-Distribution detection. Furthermore, we suggest that the authors improve their exploration of the medical domain by incorporating the Google Dermatology DDX dataset into the camera-ready version of the paper, if feasible, and address how inherent labeling uncertainty influences conformal credal sets in medical machine learning models to strengthen the paper's contribution. Finally, we recommend that the authors clarify the implications of the bounded noise assumption and explore the potential applications of their method in practical scenarios.