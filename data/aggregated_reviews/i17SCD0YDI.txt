ID: i17SCD0YDI
Title: KEBAP: Korean Error Explainable Benchmark Dataset for ASR and Post-processing
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Korean Error Explainable Benchmark Dataset for ASR and Post-processing (KEBAP), a novel benchmark for evaluating ASR systems at both speech and text levels. The authors identify significant limitations in current evaluation methods, proposing a comprehensive approach that includes 37 speech-level error types and 13 text-level error types, addressing noise environments and speaker characteristics. The evaluation method is applied to commercial ASR systems like Google Cloud ASR and CLOVA, concluding with a comparative analysis of errors across these systems.

### Strengths and Weaknesses
Strengths:
- The introduction of a new evaluation method for identifying weaknesses in ASR systems.
- Clarity in the research problem, hypothesis, and potential community benefits.
- The paper is well-written and easy to understand.
- The proposed method enhances explainability and identifies errors beyond traditional metrics.

Weaknesses:
- Some aspects, such as the annotation of difficulty levels and potential biases in human recordings, require clarification.
- The paper lacks a discussion on scaling the method to other languages and does not include an ablation study.
- There is a need for benchmarking against other existing models to establish a baseline.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding how difficulty levels are annotated and address any potential biases in human speaker recordings and background noise addition. Additionally, consider including a discussion on scaling the method to other languages and conducting an ablation study to strengthen the setup. Finally, we suggest benchmarking against other models to provide a more comprehensive evaluation of KEBAP's effectiveness.