ID: h024LpF3bZ
Title: Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 7, 5, 8, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SPLAT, a benchmark designed to evaluate and elicit lateral thinking abilities in Large Language Models (LLMs) through 975 graded situation puzzles. The authors propose a multi-turn player-judge evaluation framework that aims to reduce reliance on stronger evaluation models. Experimental results indicate that applying SPLAT's data and reasoning processes to other lateral thinking benchmarks, such as RiddleSense, leads to performance improvements in LLMs.

### Strengths and Weaknesses
Strengths:
1. The paper addresses an important gap in evaluating lateral thinking, which is crucial for creativity and problem-solving.
2. The methodology is rigorous, with careful data collection and a well-designed multi-turn player-judge framework.
3. The paper is well-structured and clearly written, effectively illustrating key concepts and results.

Weaknesses:
1. The dataset is small compared to existing benchmarks, limiting its robustness.
2. The reference answers are restricted to one per question, which may not capture the open-ended nature of lateral thinking.
3. There is minimal discussion of potential biases in the dataset and a lack of ablation studies to isolate the impact of different components of the SPLAT framework.

### Suggestions for Improvement
We recommend that the authors improve the dataset by providing multiple reference answers for each question to better reflect the nature of lateral thinking. Additionally, we suggest conducting ablation studies to clarify which components of the SPLAT framework contribute to performance improvements. It would also be beneficial to discuss potential biases in the dataset and consider expanding the dataset to include more puzzles for a more comprehensive evaluation. Finally, we encourage the authors to explore ways to automatically generate new situation puzzles to enhance scalability.