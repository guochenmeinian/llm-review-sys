ID: n3fPDW87is
Title: Robust Distributed Learning: Tight Error Bounds and Breakdown Point under Data Heterogeneity
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 6, 7, 8, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on distributed learning in the presence of Byzantine workers and data heterogeneity. The authors propose a new robustness notion, (G,B)-Gradient Dissimilarity, which accounts for additional error terms in the average gradient difference between workers. The theory is applied to least squares regression, demonstrating robustness of stochastic gradient descent against a small number of dishonest workers. The paper establishes upper and lower bounds for the ratio of Byzantine to honest workers, showing that the breakdown point is upper bounded by \( \frac{1}{2+B^2} \).

### Strengths and Weaknesses
Strengths:  
The paper introduces an interesting and novel notion of robustness in distributed learning, effectively capturing the dynamics of distributed gradient computation with Byzantine workers. The theoretical contributions, including a criterion for robustness against (G,B)-gradient dissimilarity, are compelling and well-justified. The clarity of presentation and the quality of experiments further enhance the paper's contributions.

Weaknesses:  
The relevance of the (G,B)-Gradient Dissimilarity definition to other distributed learning tasks remains unclear. The setting regarding honest and Byzantine workers is confusing, particularly concerning the knowledge honest workers have about each other. Additionally, while Theorem 2's results are interesting, they are not entirely novel, as similar results exist in prior literature.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the relevance of (G,B)-Gradient Dissimilarity to various distributed learning tasks. Additionally, the authors should clarify the conditions under which honest workers can identify each other, as this impacts the realism of the model. An ablation study on logistic regression to observe the effects of varying the constant \( B \) on the breakpoint would strengthen the findings. Finally, we suggest moving the proofs of Lemmas 1 and 2 to Appendix B.1.2 for better readability.