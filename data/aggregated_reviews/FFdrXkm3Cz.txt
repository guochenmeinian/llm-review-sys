ID: FFdrXkm3Cz
Title: On the spectral bias of two-layer linear networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 6, 5, 5, 7, -1, -1, -1
Original Confidences: 3, 3, 4, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the convergence of gradient descent for two-layer linear neural networks, focusing on the influence of initialization scale on the spectral structure of the trained network. The authors propose a variational characterization of loss minimizers and introduce a hidden mirror flow to track the evolution of singular values in the weight matrices. The findings reveal that the optimization process is affected by initial parameter values, particularly under small-scale initializations, leading to a low-rank structure in the hidden layer.

### Strengths and Weaknesses
Strengths: 
- The paper advances knowledge on the behavior of two-layer linear networks, providing rigorous mathematical proofs and clear communication of ideas. 
- The variational characterization and hidden mirror flow contribute original insights into the optimization dynamics and implicit bias of the network.
- The manuscript is well-structured and presents its findings in a comprehensible manner.

Weaknesses: 
- The analysis is limited to two-layer linear networks, which restricts its applicability and significance in the broader context of neural networks.
- There is insufficient discussion on the practical implications of the findings, particularly regarding how the observed low-rank structure can be utilized in real-world scenarios.
- The manuscript lacks a thorough exploration of the sensitivity of results to assumptions, particularly regarding the initialization of weights and the implications of using linear activation functions.

### Suggestions for Improvement
We recommend that the authors improve the manuscript by addressing the missing reference: E. Boursier et al., which discusses the low-rank structure of optimal solutions in nonlinear cases. Additionally, the authors should consider removing the orthogonality condition on \(W_1\) and clarify the dependency of Theorem 3.1 on the initialization of \(W_2\). A more detailed analysis of the sensitivity of results to assumptions, especially regarding the minimal norm solution in Theorem 3.2, would enhance the manuscript. Furthermore, we suggest that the authors explore the implications of their findings for practical applications and consider conducting more extensive experiments beyond the toy examples provided. Lastly, a discussion on the challenges of extending the analysis to deeper networks or networks with nonlinear activations would be beneficial.