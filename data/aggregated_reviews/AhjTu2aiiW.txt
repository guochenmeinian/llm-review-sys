ID: AhjTu2aiiW
Title: First-Explore, then Exploit: Meta-Learning to Solve Hard Exploration-Exploitation Trade-Offs
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called First-Explore to address the challenge of deceptive rewards in Meta-Reinforcement Learning (Meta-RL). The authors propose learning two distinct policies: one for exploration and another for exploitation, which are conditioned on a common context. The exploration policy provides feedback to the exploitation policy during training, and the optimal exploration cutoff is determined based on cumulative rewards. The method is tested on a bandit problem and a dark-treasure room environment, showing effectiveness in mitigating deceptive rewards compared to existing Meta-RL approaches.

### Strengths and Weaknesses
Strengths:
- The problem of deceptive rewards in Meta-RL is significant and worth addressing.
- Conditioning on different exploration contexts to determine the exploration cutoff is a simple yet elegant approach.
- The proposed method effectively mitigates deceptive rewards in the tested environments.
- The paper is generally well-written, clearly conveying the motivation behind the research.

Weaknesses:
- The choice of $\rho=4$ appears arbitrary; ablation studies on $\rho$ would clarify its impact on the proposed method.
- The experimental environments are simplistic, raising concerns about the method's performance in more complex, long-horizon tasks.
- The paper lacks comparisons with more recent methods, and the experimental results do not show substantial improvements over existing baselines.

### Suggestions for Improvement
We recommend that the authors improve the organization of the paper, particularly by ensuring that all figures are cited in the main text for clarity. Additionally, providing explicit mathematical formulas for the objectives of the exploration and exploitation policies would enhance understanding. We suggest conducting experiments in more complex environments, such as MiniGrid or Minihack, to better assess the scalability of the method. Finally, addressing the hyperparameter tuning of $k$ more clearly and including mean and standard deviation in the experimental results would strengthen the presentation.