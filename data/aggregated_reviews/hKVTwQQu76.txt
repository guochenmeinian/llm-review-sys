ID: hKVTwQQu76
Title: DFA-GNN: Forward Learning of Graph Neural Networks by Direct Feedback Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 4, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DFA-GNN, a framework that applies Direct Feedback Alignment (DFA) for training Graph Neural Networks (GNNs) without backpropagation. It addresses limitations of traditional backpropagation methods, such as inefficiency and scalability issues, by utilizing a forward training mechanism tailored for graph data. The authors demonstrate that DFA-GNN generally outperforms backpropagation and other training algorithms across various benchmarks, attributing improvements to a pseudo-error generation mechanism.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and organized, making it easy to follow.  
- DFA-GNN shows better performance than other non-backpropagation methods and even standard backpropagation methods.  
- The framework is robust against noise and various attacks, enhancing its reliability.  
- It improves training efficiency through parallel gradient computation.  

Weaknesses:  
- The novelty is limited as it primarily extends DFA principles to GNNs, building on prior work.  
- The paper lacks a discussion on the computational complexity of the proposed algorithm.  
- Performance improvements vary significantly across datasets, with insufficient explanation for this variation.  
- The dataset used for experiments is relatively small, and the computational cost per epoch is significantly higher than backpropagation.  
- The method is limited to node classification tasks, neglecting graph-level or edge-level tasks.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational complexity of DFA-GNN, particularly addressing the time complexity and how it scales with larger datasets. Additionally, the authors should provide a comparative analysis of overall training costs, including the number of epochs required for convergence across different methods. We encourage the authors to explore deeper GNN architectures and include a broader range of tasks beyond node classification. Furthermore, clarifying the limitations of backpropagation and the specific scenarios where DFA-GNN excels would enhance the paper's contribution. Lastly, improving notation clarity, especially in formulas, would benefit reader comprehension.