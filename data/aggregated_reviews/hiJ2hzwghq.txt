ID: hiJ2hzwghq
Title: BiasX: “Thinking Slow” in Toxic Content Moderation with Explanations of Implied Social Biases
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called BIASX aimed at assisting content moderators by providing free-text explanations of implied social biases in statements. The authors conduct a large-scale user study with over 450 participants, demonstrating that these explanations help in identifying subtly toxic content. However, the introduction of explanations increases labeling time and mental demand, which may pose challenges in practical applications. The study emphasizes the superiority of human-written explanations over machine-generated ones and highlights the need for further research on generating high-quality examples.

### Strengths and Weaknesses
Strengths:  
1. The paper is well-structured and addresses a significant issue in content moderation.  
2. The large-scale user study provides empirical evidence supporting the framework's effectiveness in identifying subtle toxicity.  
3. The nuanced interpretation of findings, including side findings regarding non-toxic examples, adds depth to the research.  

Weaknesses:  
1. Concerns exist regarding the accuracy of generated explanations, with only 60% being accurate for hard toxic content, potentially limiting practical utility.  
2. The design for answering the second research question lacks rigor, particularly regarding the correlation between "thinking carefully" and "labeling time."  
3. The potential biases in AI-generated explanations and the need for clearer definitions of subtle versus overt toxicity are inadequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the design of the study to address the labeling time biases by calculating individual labeling times during the qualification stage. Additionally, the authors should justify the role of crowd workers as moderators more thoroughly, including any predefined standards they should follow. To enhance the framework's practical utility, we suggest that the authors explore methods to improve the accuracy of generated explanations and address the challenge of identifying high-quality examples. Furthermore, we encourage the authors to provide clearer definitions of subtle versus overt bias and elaborate on the implications of biases in AI-generated explanations. Finally, sharing the code used in the study would enhance reproducibility.