ID: wSpIdUXZYX
Title: Pretraining Codomain Attention Neural Operators for Solving Multiphysics PDEs
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CoDA-NO, an innovative neural operator architecture that utilizes attention mechanisms to capture interactions among different physical variables in coupled PDE systems. The authors demonstrate the model's effectiveness on fluid-structure interactions, particularly in Rayleigh-BÃ©nard (RB) convection systems, and report that pre-trained CoDA-NO outperforms FNO by an average of 20% on few-shot learning tasks. The study includes experiments on the Navier-Stokes equation and its coupled version with the elastic wave equation, showcasing the model's performance against various benchmarks. The authors acknowledge that their results are preliminary due to limited data generation and hyperparameter tuning but assert that CoDA-NO shows significant improvements over models trained from scratch across multiple settings.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel neural operator architecture based on attention mechanisms, showing superior performance on NS and NS+EW benchmarks.
- CoDA-NO demonstrates substantial improvements in performance, with reported gains of 20% to 94% over scratch-trained models in various configurations.
- The architecture's ability to adapt to multi-physics scenarios without altering the backbone is a notable advantage.
- The authors are pioneers in effectively applying masked training for PDE learning.
- The mathematical formulation is clearly articulated, aiding reader comprehension.
- The study addresses a significant multiphysics problem, enhancing its relevance.
- Empirical evidence supports the model's effectiveness for few-shot finetuning, with a 36% average improvement and a 43% gain on the PDEbench dataset.

Weaknesses:
- The limited scope of experiments on only two specific PDEs restricts generalization to broader multiphysics problems.
- The results are preliminary, with challenges in data generation and hyperparameter tuning due to time constraints.
- Pretraining does not consistently yield advantages, as evidenced by comparable performance of scratch-trained models in certain scenarios.
- Clarity in the presentation of the architecture and methodology is lacking, which may hinder accessibility for readers.
- The use of L2 loss is problematic, aggregating outputs of different physical meanings and potentially obscuring accuracy.
- Lack of prediction visualizations diminishes interpretability of results.
- Absence of a Fourier Neural Operator benchmark limits fair performance evaluation.
- The FNO model's parameter count significantly exceeds that of CoDA-NO, complicating performance comparisons.
- CoDA-NO exhibits higher inference times compared to baseline models.

### Suggestions for Improvement
We recommend that the authors improve the breadth of their experimental evaluation by including additional PDE systems, such as the Rayleigh-Benard convection system, to validate the general applicability of CoDA-NO. We suggest clarifying the motivation behind the combination of positional encoding, self-attention, and normalization layers to enhance understanding. Additionally, we encourage the authors to provide more details on self-supervised pretraining in Appendix B.1, such as the masked ratio, and to expand the evaluation metrics beyond L2 errors to include relative L2 error and infinity norm. We also recommend improving the clarity of the architecture by providing a high-level explanation in the introduction, incorporating pseudocode, and enhancing figures to convey the procedure more effectively. Furthermore, we suggest relocating some background information to the appendix to focus on motivation and high-level procedures in the main text, thereby making the paper more accessible. Finally, addressing the high inference times through optimization strategies would strengthen the practical applicability of CoDA-NO.