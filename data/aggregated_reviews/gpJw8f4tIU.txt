ID: gpJw8f4tIU
Title: Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ConSpec, an algorithm designed to learn prototypes of critical states in reinforcement learning (RL) tasks using contrastive learning. ConSpec aims to enhance long-term credit assignment by delivering intrinsic rewards based on the similarity of current states to learned prototypes. The authors evaluate the method across various environments, demonstrating its potential to improve performance in RL tasks.

### Strengths and Weaknesses
Strengths:  
- The paper introduces an interesting and important insight regarding long-horizon credit assignment and reward learning.  
- The experimental results appear solid, particularly in environments like Montezuma's Revenge, where ConSpec effectively identifies critical states.  
- The writing is clear, and the figures are well-drawn, facilitating understanding of the proposed method.  

Weaknesses:  
- The ConSpec algorithm is difficult to follow, particularly regarding the learning process of prototypes $h_i$.  
- The method assumes access to a "success" indicator, which may limit its applicability in general MDP settings.  
- The evaluation metrics used (e.g., #mini batches / #gradient steps) differ from standard RL benchmarks, raising questions about their necessity.  
- The reliance on a memory system may hinder generalization and scalability.  
- The definition of success and failure in trajectories is overly simplistic and may not adapt as the agent learns.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how prototypes $h_i$ are learned and utilized in experiments, addressing the inconsistencies noted in the abstract and Algorithm 1. Additionally, we suggest providing more detailed explanations of the success and failure definitions used in the experiments, as well as justifying the choice of evaluation metrics. It would also be beneficial to explore the implications of the memory system on generalization and to benchmark against stronger baselines, such as RAD, to enhance the robustness of the findings.