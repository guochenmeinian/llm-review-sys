ID: Ys8RmfF9w1
Title: Uncovering Neural Scaling Laws in Molecular Representation Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 31
Original Ratings: 7, 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the neural scaling laws in molecular representation learning (MRL), addressing a significant gap in the current research landscape. The authors examine various dimensions of MRL, including modalities, data distributions, and model capacities, revealing both surprising trends and consistent behaviors with other domains. They propose a transformer-based fingerprint encoder utilizing standard positional encodings, demonstrating its superior performance compared to traditional MLP-based encoders. The study confirms that MRL performance exhibits a power-law relationship with data quantity and explores the effects of data scaling using the PCQM4Mv2 dataset. The authors acknowledge limitations in their study, such as the focus on predictive tasks and the need for further exploration of generative tasks, as well as the limited investigation of pre-training effects and multitask learning settings.

### Strengths and Weaknesses
Strengths:
- The paper fills a critical gap in MRL research by exploring neural scaling laws, contributing significantly to the field.
- It provides a comprehensive analysis of various dimensions, supported by empirical data, and presents reasonable result interpretations that motivate future research directions.
- The transformer-based fingerprint encoder shows improved performance over MLP-based methods, enhancing the relevance of the findings.
- The authors effectively address reviewer concerns, clarify the rationale behind their architectural choices, and improve the usability of their codebase by adding a Jupyter notebook example and Sphinx documentation.

Weaknesses:
- The considered neural architectures are limited, lacking exploration of Graph Transformer models and equivariant graph representation learning methods, which have shown advantages in MRL tasks.
- The paper does not adequately address the performance degradation of the SMILES encoder, nor does it discuss the encoder choice for the fingerprint modality in sufficient detail.
- The manuscript contains grammatical errors and incomplete sentences that affect readability, and the discussion of related works appears sparse.
- The investigation of pre-training effects is currently limited, as only one method is applied, potentially leading to incomplete conclusions.

### Suggestions for Improvement
We recommend that the authors improve the diversity of neural architectures by including Graph Transformer models and expanding the exploration of equivariant graph representation learning methods to better understand their properties in MRL. Additionally, it would be beneficial to investigate the reasons behind the performance degradation of the SMILES encoder and provide hypotheses for this phenomenon. A more thorough discussion on the encoder choice for the fingerprint modality, including positional encodings and comparisons with MLP-based architectures, should be included. Furthermore, we suggest that the authors improve the investigation of pre-training effects by exploring multiple pre-training strategies across different modalities. The authors should ensure that the manuscript is free of grammatical issues and incomplete sentences to enhance readability. Lastly, we encourage the authors to elaborate on the limitations of their study, provide a more detailed review of related works in the field of molecular representation learning, and continue expanding their Sphinx documentation to include more examples, particularly for working with 3D modalities, to enhance usability for new users.