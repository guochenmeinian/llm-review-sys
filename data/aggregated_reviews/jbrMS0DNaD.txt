ID: jbrMS0DNaD
Title: INQUIRE: A Natural World Text-to-Image Retrieval Benchmark
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 7, -1, -1, -1, -1, -1, 4, -1
Original Confidences: 3, 4, -1, -1, -1, -1, -1, 4, -1

Aggregated Review:
### Key Points
This paper presents the INQUIRE benchmark, which includes 200 queries over the iNat24 dataset containing approximately 4.8 million images, aimed at evaluating multimodal vision-language models (VLMs) in text-to-image retrieval. The benchmark addresses two main challenges: full-rank retrieval, where models identify relevant images from all samples, and re-ranking, where models rank relevant images from a subset of top retrieved samples. The authors provide a comprehensive explanation of the data collection and annotation process, ensuring transparency and high-quality data for evaluation. The dataset comprises a substantial collection of natural images, with 149,022 labeled instances across 9,959 species, and the benchmark effectively illustrates the performance gap of existing models on challenging expert-level queries.

### Strengths and Weaknesses
Strengths:
- The dataset includes a vast amount of high-quality, human-annotated data covering a wide range of species, which is invaluable for advancing research.
- All data instances are real photographs of various natural species, making the dataset both challenging and useful for model evaluation.
- The benchmark is sufficiently challenging and beneficial for text-image retrieval research.
- The paper is well-structured, with clear sections and detailed evaluation results across various baseline methods, facilitating clearer comparisons.

Weaknesses:
- The benchmark's query dimension is limited to 200 queries, raising concerns about the expected variance in model performance and the comprehensiveness of the evaluation.
- The performance of models on FULLRANK is relatively poor, potentially due to the domain gap between pre-trained VLMs and the iNat24 dataset.
- The metrics used for evaluation are not intuitive and require verbose explanations, with a weak justification for avoiding Prec@k for full-rank retrieval.
- The task design is somewhat simplistic and lacks novelty, limiting the evaluation of model generalization capabilities.
- The results and discussions are somewhat superficial, lacking deeper insights into model performance and query complexity.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by expanding the query set to enhance its comprehensiveness and allow for variance analysis using bootstrap sampling. Additionally, we suggest discussing the metrics in the main text rather than relegating them to the appendix, and consider including the more intuitive Prec@k metric in the evaluation tables. Furthermore, we encourage a deeper exploration of the results, particularly regarding the potential of reranking and a quantitative assessment of query complexity. We also suggest enhancing the task design to incorporate more complexity and novelty, potentially by utilizing captions and high-quality image-text pairs for constructing expert models or exploring new research avenues. Lastly, addressing the placement of Table 2 and providing random choice and human performance results would enhance clarity and completeness.