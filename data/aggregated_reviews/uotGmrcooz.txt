ID: uotGmrcooz
Title: Optimal approximation using complex-valued neural networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 7, 5, 3, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the approximation rates of shallow complex-valued neural networks (CVNNs) with general non-polyharmonic activation functions. The authors establish upper bounds for the approximation error of polynomial and smooth functions by CVNNs, demonstrating that these bounds are tight under continuous weight selection. They also introduce a specific activation function that improves the bound and show that the rate bound is tight for standard sigmoid-type activations.

### Strengths and Weaknesses
Strengths:  
- **Contribution:** The paper is notable for being the first to establish optimal approximation rates for CVNNs with general non-polyharmonic activation functions, expanding beyond previous results limited to specific activations. The analysis of the optimality of convergence rates is comprehensive, proving tightness under continuous weight selection and identifying conditions for improved rates.
- **Quality and Clarity:** The paper is well-written, with clearly stated theorems and detailed proof sketches in the main text, supplemented by a carefully crafted appendix.

Weaknesses:  
- **Questionable Significance of the CVNN Model:** The practical and mathematical significance of CVNNs appears limited compared to real-valued neural networks, which are well-established models. The paper does not adequately justify the importance of CVNNs, merely referencing a few prior works without deeper context.
- **Limited Conceptual Novelty:** The paper primarily adapts existing methods from real-valued networks to the complex setting, resulting in results that feel incremental rather than groundbreaking.
- **High Technicality for a Conference:** The extensive technical detail, including 40 pages of proofs, may be more suitable for a journal rather than a conference setting, potentially limiting thorough review by NeurIPS reviewers.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the significance of the CVNN model, providing a clearer rationale for its importance in both mathematical and practical contexts. Additionally, the authors should clarify the implications of the continuous weight selection assumption, as it is not practical in general. Insights into the challenges faced when dropping this assumption would enhance the paper's credibility. Furthermore, we suggest that the authors elaborate on the similarities with existing results to highlight the novel contributions more effectively. Lastly, addressing the clarity of the constant $c(n,k)$ and its implications on the approximation bounds would strengthen the overall presentation.