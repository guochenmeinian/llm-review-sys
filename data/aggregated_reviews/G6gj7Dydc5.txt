ID: G6gj7Dydc5
Title: HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Hearing Enhanced Audio Response (HEAR) framework aimed at improving video-grounded dialogue (VGD) systems by enhancing audio perception. The framework consists of two components: Sensible Audio Listening (SAL), which predicts the relevance of audio to the question, and Reconstructive Listening Enhancement (RLE), which reconstructs masked audio frames based on context. The authors claim that their approach outperforms existing baselines on the AVSD@DSTC7 and AVSD@DSTC8 datasets.

### Strengths and Weaknesses
Strengths:  
- The HEAR framework is model-agnostic, applicable to various VGD systems, and improves performance specifically on audio-related questions without negatively impacting other queries.  
- The SAL component effectively directs attention to audio features, enhancing overall system performance.

Weaknesses:  
- The novelty of the proposed methods is seen as incremental, with similarities to existing works in audio-visual pre-training and masked reconstruction tasks.  
- The evaluation of SAL is limited, and its marginal improvements raise questions about its effectiveness in addressing the "deaf response" problem.  
- The overall loss function and its combination are not clearly explained, and the training process for SAL lacks clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 5 by making it larger and more concise, omitting repetitive elements found in Figure 3. Additionally, we suggest providing experimental evidence to support the claim regarding the limited number of audio-related questions and clarifying the training process for SAL, including whether it is trained independently. The authors should also elaborate on the differences between SAL and the Semantic Neural Estimator, and validate the effectiveness of their reconstruction methods against existing audio-visual pre-training techniques. Finally, we encourage the authors to provide a clearer explanation of the overall loss function and its integration within the framework.