ID: VazkRbCGxt
Title: Direct Consistency Optimization for Robust Customization of Text-to-Image Diffusion models
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 7, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel fine-tuning method called Direct Consistency Optimization (DCO) aimed at enhancing the robustness of personalized text-to-image (T2I) diffusion models. DCO introduces a training objective that minimizes the deviation between fine-tuned and pretrained models by optimizing a consistency function, allowing for efficient implementation through a noise-prediction loss approach. The authors propose that this method can effectively preserve pretrained knowledge during fine-tuning, thereby enabling seamless merging of models without interference.

### Strengths and Weaknesses
Strengths:
* The introduction of DCO represents a novel approach to fine-tuning T2I diffusion models, directly controlling deviation from pretrained models.
* The paper effectively combines concepts from constrained policy optimization and noise prediction to derive the DCO loss function.
* The results surpass baseline methods and demonstrate strong empirical performance.

Weaknesses:
* The DCO method shows only marginal improvements over existing methods, as indicated by figures 4 and 7.
* The experiments are limited in diversity, primarily focusing on specific datasets without broader human image variations.
* There is a lack of detailed implementation specifics, which raises concerns about reproducibility.
* Multiple references to existing works are missing, and the paper lacks sufficient details on the datasets used for training and evaluation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the limitations by discussing them more prominently in the main text. Additionally, it would be beneficial to include a broader range of experiments, particularly with diverse human images, to enhance the robustness of the findings. We suggest providing detailed information on the implementation specifics, including the codebase and computing resources, to facilitate reproducibility. Furthermore, we encourage the authors to include a loss landscape comparison when varying the amount of DCO $\beta$ and to highlight the highest scores reported in Table 1. Lastly, addressing the interplay between Textual Inversion and DCO in the experiments would provide valuable insights for the research community.