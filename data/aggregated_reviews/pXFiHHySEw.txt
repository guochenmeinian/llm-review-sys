ID: pXFiHHySEw
Title: Multi-Stage Predict+Optimize for (Mixed Integer) Linear Programs
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Multi-Stage Predict+Optimize framework aimed at solving optimization problems where parameters are revealed sequentially. The authors propose three neural network training algorithms specifically designed for mixed integer linear programs (MILPs) and empirically evaluate these methods against three benchmarks, demonstrating improved predictive performance over traditional approaches.

### Strengths and Weaknesses
Strengths:
- The proposed multi-stage framework is a significant extension of existing two-stage models, effectively addressing scenarios with progressively revealed parameters.
- The paper includes a robust empirical evaluation, clearly demonstrating the advantages of the proposed methods over classical techniques and discussing trade-offs in performance.

Weaknesses:
- The practical applicability of the framework is questionable, as the assumptions regarding stage-wise optimizations may not hold in real-world scenarios. The benchmarks used do not encompass more complex problems, limiting the framework's generalizability.
- The paper lacks sufficient references to existing literature on the integration of machine learning and optimization, which would help contextualize the work.
- Key details, such as termination criteria for the algorithms and the implications of certain assumptions, are inadequately addressed.

### Suggestions for Improvement
We recommend that the authors improve the contextualization of their work by including more references to relevant literature on machine learning applications in mixed-integer programming and stochastic programming. Additionally, the authors should clarify the assumptions made regarding feasibility in stage-wise optimizations and discuss the implications of these assumptions as limitations. It would also be beneficial to provide a more comprehensive set of benchmarks that reflect a wider variety of real-world applications. Finally, we suggest including discussions on the quality of the convex relaxation used in training and the potential impact on performance, as well as addressing the omission of termination criteria in the algorithms.