ID: GYjV1M5s0D
Title: FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FineMoGen, a diffusion-based motion generation and editing framework that synthesizes intricate motions aligned with fine-grained user descriptions. The authors propose a unique transformer architecture, Spatio-Temporal Mixture Attention (SAMI), which optimizes global attention by modeling spatio-temporal constraints and employing a sparsely-activated mixture-of-experts (MoE) for feature extraction. Additionally, they introduce the HuMMan-MoGen dataset to support research in fine-grained motion generation. Experimental results indicate that FineMoGen outperforms existing methods in motion quality.

### Strengths and Weaknesses
Strengths:
1. The introduction of a novel dataset significantly enhances research potential in human motion generation and aligns with intuitive human approaches.
2. The motivation for developing the SAMI model is robust and well-justified.
3. The zero-shot generation capability shows promise, with competitive results on benchmarks like HumanML3D and KIT-ML.

Weaknesses:
1. The relationship between SAMI and MEA is unclear, particularly regarding notations and the association between F' and B.
2. The factorization in Equation 4 lacks clarity, especially the description of G' as a time-varied signal and the formulation of G*.
3. Figure 2 inadequately represents the paper's primary contributions, focusing solely on the LLM module while omitting SAMI and MoE.
4. The motivation for the Sparse-activated Mixture-of-Experts is not well articulated, and its correlation with SAMI is unclear.
5. There are typographical errors, such as "vGiven" on Line 214.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relationship between SAMI and MEA, particularly in notation and explanations. The authors should provide a more detailed explanation of the factorization in Equation 4 and clarify the descriptions of G' and G*. We suggest revising Figure 2 to include SAMI and MoE to better represent the paper's contributions. Additionally, the authors should elucidate the motivation behind the Sparse-activated Mixture-of-Experts and its correlation with SAMI. Finally, we advise correcting typographical errors, such as "vGiven."