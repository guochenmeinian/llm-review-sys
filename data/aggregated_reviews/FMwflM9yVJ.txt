ID: FMwflM9yVJ
Title: CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CONTRASTE, a novel pre-training strategy utilizing contrastive learning to enhance performance in Aspect Sentiment Triplet Extraction (ASTE). The authors derive aspect-based prompts with masked sentiments and apply supervised contrastive learning (SCL) on the decoder-generated aspect-level sentiment embeddings. The proposed method demonstrates improved performance across various ABSA tasks, including ASTE, ACOS, TASD, and AESC. Additionally, the authors introduce generic placeholder-based templates for fine-tuning and a multi-task model, CONTRASTE-MTL, achieving state-of-the-art results on most ASTE benchmark datasets.

### Strengths and Weaknesses
Strengths:
1. The study addresses a cutting-edge problem in supervised contrastive pre-training for ASTE, which has been less explored.
2. The method is well-designed and the narrative is clear, with comprehensive and rigorous experiments supporting the proposed approach.
3. The results are reliable, and the paper is well-structured and written, with frank acknowledgment of limitations.

Weaknesses:
1. The related works section is poorly organized and lacks a comparative analysis with ChatGPT.
2. The motivation for the paper is unclear, and the mechanism of the OTD module is not well-explained, raising doubts about its effectiveness.
3. The description of the TCE module is insufficient, lacking deeper analysis and clarity on its operational mechanics during inference.

### Suggestions for Improvement
We recommend that the authors improve the organization of the related works section and include a comparative analysis with ChatGPT. Additionally, the authors should clarify the motivation behind their approach and provide a more detailed explanation of the OTD module's mechanism. We also suggest that the authors elaborate on the TCE module, addressing how it functions as a regressor, its prediction accuracy during inference, and the impact of training with randomly-generated pseudo labels on performance.