ID: wcvCSfrEho
Title: Towards Safe and Honest AI Agents with Neural Self-Other Overlap
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 10, 5, 9
Original Confidences: 3, 3, 3

Aggregated Review:
### Key Points
This paper presents a novel method, Self-Other Overlap (SOO) fine-tuning, aimed at reducing deceptive behavior in AI systems, addressing a critical challenge in AI safety. The authors propose that their approach draws inspiration from neuroscience to foster empathy, which is shown to be effective in both language models (LLMs) and reinforcement learning (RL) contexts. The results indicate significant reductions in deceptive behavior without compromising task performance, making the method a strong candidate for broader application. However, the paper's findings are primarily based on a single model, Mistral 7B, and a limited experimental scope.

### Strengths and Weaknesses
Strengths:
- The application of neural overlap as a fine-tuning mechanism for honesty is a novel and creative representation engineering technique.
- Significant reductions in deceptive behavior were observed in both LLMs and RL agents, providing compelling evidence for the approach's effectiveness.
- The reinforcement learning experiment was particularly interesting and demonstrated broader applicability of SOO.

Weaknesses:
- The reliance on a single model (Mistral 7B) and a narrow set of scenarios limits the generalizability of the findings.
- The lack of a formal definition of deception hinders the contextualization of results and the scope of the work.
- The experiments are confined to one RL environment, which restricts the robustness of the approach.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing a formal definition of deception to contextualize the results. Additionally, conducting deeper analysis and experimentation across a wider range of model families and sizes is essential for validating the findings. Investigating the thresholds and bounds of SOO, particularly whether there is a threshold where deceptive behavior consistently decreases, would enhance the theoretical basis of the work. 

Clarifying the 'burglar room' setup and its classification as deception is necessary to connect the experiment with real-world scenarios. The authors should also explore the relationship between empathy and deception more thoroughly, addressing the potential for increased empathy to lead to more subtle forms of deception.

We suggest including a comparison between fine-tuning and contrastive sampling techniques to support the argument for the efficiency of the chosen approach. Furthermore, exploring potential unintended behaviors arising from SOO fine-tuning, such as the risk of agents exploiting SOO-driven empathy for deceptive purposes, would provide a more comprehensive analysis of the method's limitations.

Lastly, for reproducibility, the authors should include specific experimental details, such as whether Mistral 7B was instruction-tuned and the activation layers used. Updating the definition of Representation Engineering to reflect its broader applications would also be beneficial.