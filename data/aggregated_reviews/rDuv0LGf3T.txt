ID: rDuv0LGf3T
Title: Prompting ChatGPT in MNER: Enhanced Multimodal Named Entity Recognition with Auxiliary Refined Knowledge
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a two-stage framework, PGIM, that leverages ChatGPT to generate auxiliary knowledge for enhancing Multimodal Named Entity Recognition (MNER). The first stage identifies K similar artificial samples from a training dataset to create in-context examples for ChatGPT, which then generates refined knowledge. This output is integrated into a transformer-based encoder and passed through a linear-chain CRF layer for final predictions. The authors demonstrate that PGIM outperforms the current state of the art, MoRe, across two datasets (Twitter 2015 and 2017) and conduct thorough experiments to analyze various aspects of their approach.

### Strengths and Weaknesses
Strengths:
- The authors propose a novel and effective method that achieves state-of-the-art results in MNER.
- The experimental validation is comprehensive, providing insights into the algorithmic decisions made.

Weaknesses:
- The paper lacks clarity regarding the number of manually annotated examples required for training, which is crucial for understanding the tradeoff between simplicity and annotation effort.
- The results against MoRe are close, and the comparisons do not adequately address the full capabilities of MoRe, particularly regarding efficiency and controllability.
- The MSEA module appears to have minimal impact on performance, raising questions about its utility.
- The annotation process and examples of predefined artificial samples are insufficiently detailed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the annotation process by specifying the number of examples needed for manual annotation and providing examples of this annotation. Additionally, we suggest articulating the advantages of PGIM over MoRe more explicitly, including a more comprehensive comparison across entity types rather than just final F1 scores. It would also be beneficial to explore the performance of PGIM using open-source alternatives to ChatGPT, such as alpaca-lora or llama2, for comparison. Including error bars in the experimental results would help assess variance, and conducting an error analysis against MoRe would provide deeper insights into the model's strengths and weaknesses. Lastly, we recommend adding experiments with Bert-base as the encoder to ensure fair comparisons with previous methods.