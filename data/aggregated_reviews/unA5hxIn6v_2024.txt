ID: unA5hxIn6v
Title: Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the training dynamics of a two-layer neural network in the mean-field limit, focusing on learning subspace-sparse polynomials with Gaussian data. The authors introduce a "reflective property" that is necessary for learning polynomials in finite time and propose a multi-phase gradient-based training algorithm. This work generalizes the merged staircase property (MSP) to a basis-independent setting, contributing to the understanding of SGD learnability in the context of neural networks.

### Strengths and Weaknesses
Strengths:
- The introduction of the reflective property is a significant contribution, providing a basis-free extension of MSP to Gaussian data.
- The manuscript is well-written, with detailed explanations and proofs, enhancing its clarity and accessibility.
- The results are independent of input dimensionality, making them applicable to high-dimensional data.

Weaknesses:
- The analysis is limited to mean-field dynamics up to finite time, which may overlook functions with higher leap/information exponents that do not satisfy the reflective property.
- The proposed learning algorithm is non-standard, requiring averaging over trajectories and a change of activation function, lacking a clear learning guarantee for discretized dynamics.
- The relationship between the reflective property and prior definitions of SGD hardness is not clearly articulated, necessitating further clarification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the relationship between the reflective property and existing definitions of SGD hardness, particularly in the context of leap complexity. Additionally, the authors should provide a more thorough discussion on the implications of the reflective property in relation to existing conditions for SGD learnability. It would be beneficial to clarify the discrepancies regarding the use of bias in the activation function and to include a definition of the Dirac delta measure as mentioned in the reviews. Finally, addressing the concerns about the necessity of the averaging and activation change in the training algorithm could strengthen the paper's contributions.