ID: E4P5kVSKlT
Title: On the Asymptotic Learning Curves of Kernel Ridge Regression under Power-law Decay
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 6, 8, 6, 5, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, 2, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a rigorous analysis of the asymptotic rates of kernel ridge regression (KRR), extending previous work by recovering known rates under less restrictive conditions on feature distributions. The authors characterize the bias term's rate of decay and provide a complete phase diagram of behaviors, including optimal scaling of the ridge parameter $\lambda \sim n^{-\theta}$. Empirical experiments on sinusoidal regression problems validate the theoretical findings. The study also discusses generalization error rates, bias-variance tradeoffs, and the implications of assumptions like eigenvalue decay and Holder continuity.

### Strengths and Weaknesses
Strengths:
1. The paper offers a clear and coherent analysis of kernel regression rates, particularly regarding the bias term and its decay under specific conditions.
2. It provides empirical evidence supporting theoretical claims, which is commendable for a theoretical paper.
3. The authors demonstrate a thorough understanding of the literature, effectively referencing relevant prior works.

Weaknesses:
1. The learning curves presented are not novel, as they replicate existing results in the literature.
2. The title is overly broad; a more descriptive title like "Rigorous analysis of bias and variance rates for KRR" is recommended.
3. Some relevant works are not cited, including those by Spigler et al. (2019), Canatar et al. (2021), and Simon et al. (2023).
4. Certain claims, such as the benign overfitting phenomenon in wide neural networks, lack sufficient explanation and supporting evidence.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the contribution section by providing a high-level overview before delving into technical details. Additionally, the authors should clarify the implications of the "embedding index" and the limitations of the source condition. It would also be beneficial to define terms like "overfitting" and "underfitting" earlier in the paper to enhance reader comprehension. Furthermore, we suggest addressing the missing references to relevant works and considering a more precise title that reflects the paper's contributions. Lastly, providing a more thorough discussion on the implications of results related to the generalization mystery of neural networks would strengthen the paper.