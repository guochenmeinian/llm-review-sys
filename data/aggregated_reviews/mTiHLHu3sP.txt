ID: mTiHLHu3sP
Title: GPT-RE: In-context Learning for Relation Extraction using Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach for relation extraction (RE) using large language models (LLMs) through in-context learning (ICL). The authors address shortcomings of ICL that contribute to the underperformance of LLMs compared to fully supervised models. They propose two methods: (1) task-aware representations for demonstration retrieval, emphasizing entity and relation relevance, and (2) gold label-induced reasoning to enhance demonstration quality. The approach is tested on four RE datasets (Semeval, TACRED, SciERC, ACE05), achieving state-of-the-art performance on Semeval and SciERC, and competitive results on TACRED and ACE05. The authors conduct multiple ablation studies to analyze the effectiveness of their methods.

### Strengths and Weaknesses
Strengths:
- The paper is well-organized and easy to follow.
- It addresses a significant performance gap between fine-tuned models and ICL for RE, proposing effective solutions that achieve state-of-the-art results.
- The comprehensive analysis through ablation studies demonstrates the potential of the proposed approach in various settings.

Weaknesses:
- The sampled test subsets for TACRED and ACE05 are smaller than expected, hampering model comparison, especially given TACRED's diverse relations.
- The paper lacks information on the distribution of relations in the test subsets, which is critical for evaluating the robustness of the results.

### Suggestions for Improvement
We recommend that the authors provide more information on the distribution of relations in the test subsets to enhance the credibility of their results. Additionally, we suggest that the authors clarify the definition of relation representation in section 2.4.2, specifically whether it includes context or solely the representations of two entities. Finally, we encourage the authors to strengthen their argument by offering additional evidence regarding the identified shortcomings of ICL, which would contribute to a more comprehensive understanding of the challenges in this area.