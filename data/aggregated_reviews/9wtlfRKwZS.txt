ID: 9wtlfRKwZS
Title: Global Convergence in Training Large-Scale Transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 7, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 1, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical investigation of the mean-field limit of Transformers, focusing on the convergence properties of gradient flow with weight decay as the model's width and depth approach infinity. The authors demonstrate that the dynamics of a discretized model approximates the Wasserstein gradient flow under specific regularity assumptions. They establish that if the Wasserstein flow weakly converges to a stationary distribution, the discrete model also converges with small risk. The paper extends existing analyses of two-layer neural networks and ResNets to Transformers, addressing the complexities introduced by both feedforward and attention layers. Additionally, the authors explore training dynamics and propose results related to global convergence guarantees for large-scale Transformers, although concerns are raised regarding the completeness and accuracy of these claims, particularly in relation to Corollary 4.1 and the asymptotic nature of results. The authors also validate Assumptions 3 and 4, showing that certain components exhibit local Lipschitz continuity, which is essential for establishing convergence.

### Strengths and Weaknesses
Strengths:
- The paper provides a significant extension of mean-field analyses to Transformers, which is challenging due to the dual architecture of feedforward and attention layers.
- The generality of the assumptions allows for a broad application, including softmax mechanisms and in-context learning.
- The analyses are novel and detailed, particularly the global convergence analysis presented in Section 4.
- The paper is well-organized and clearly written, tackling an important problem in the theoretical understanding of Transformer models.
- The authors provide a rigorous mathematical foundation for their claims, particularly regarding local Lipschitz continuity and the universal approximation property.

Weaknesses:
- The error bounds in Theorem 3.1 are excessively large, limiting their practical relevance, with hidden constants that are super-super-exponential in time, necessitating clearer justification.
- The convergence analysis in Theorem 4.1 similarly suffers from large constants and requires a sufficiently large time horizon, complicating the practical application of the results.
- Corollary 4.1 is deemed incorrect and requires major revision.
- The results are primarily asymptotic, limiting their practical applicability.
- The paper lacks specific convergence results for Transformer models, failing to define them adequately in theorems.
- The tightness of the recurring rate $L^{-1}+\sqrt{\log L/M}$ is not established, raising questions about the utility of the provided bounds.
- The full proof could not be examined within the rebuttal period, leaving some concerns unresolved.
- There is a need for additional experimental validation to support the theoretical claims made in the paper.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the error bounds in Theorem 3.1 by justifying the dependency on time more transparently, possibly by bounding $\phi_T$. Additionally, we suggest that the authors clarify the assumptions in Theorem 4.1 to ensure they are practical and checkable. It would be beneficial to address the tightness of the rate $L^{-1}+\sqrt{\log L/M}$ and provide a comparison with existing analyses for ResNets or other ODE systems. Furthermore, we encourage the inclusion of a summary paragraph in the main manuscript to highlight the key novelties and a brief overview at the beginning of each section to guide readers through the proofs. We recommend improving Corollary 4.1 and other theorems by ensuring that the dependencies for all constants are clearly specified. Please prove that Assumptions 1-4 are satisfied for Transformer networks in the form of (2.3), and represent parameters in these assumptions with clear bounds. If including ReLU activation, demonstrate its compliance with Assumptions 1-4 and Proposition 3.1. Providing heuristic examples to clarify when and why the assumptions hold would strengthen the paper's arguments. Lastly, we suggest including experiments to investigate the parameter sizes necessary for ensuring global convergence, as this would enhance the practical applicability of their theory and validate the theoretical claims.