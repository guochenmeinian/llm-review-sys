ID: 3AxESAk0Re
Title: STAIR: Learning Sparse Text and Image Representation in Grounded Tokens
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to learning interpretable text and image representations by introducing a sparse embedding space, STAIR, where each dimension corresponds to a (sub-)word from the BERT vocabulary. The authors propose a multi-stage training strategy aimed at effectively aligning image and text representations while grounding them in meaningful (sub-)words. Empirical results indicate that STAIR outperforms the CLIP model, which utilizes dense embeddings, in terms of interpretability and performance across various tasks, including image-text retrieval and zero-shot visual classification.

### Strengths and Weaknesses
Strengths:
- The problem of learning interpretable image and text representations is significant, enhancing the transparency of large vision-language models (VLMs).
- The proposed sparse embedding space and multi-stage training strategy are both reasonable and effective.
- The experiments are well-designed, and results convincingly validate STAIR's advantages in interpretability and effectiveness.
- Limitations are acknowledged, with discussions on potential solutions and future directions.
- The paper is well-written and easy to follow.

Weaknesses:
- There is no quantitative evaluation supporting the claim that STAIR is more efficient than CLIP in text-image retrieval.
- The interpretability of STAIR may not be necessary for image-text retrieval, as dense embeddings might suffice.
- It is unclear how STAIR handles out-of-vocabulary concepts, raising questions about its advantages over dense embeddings.
- The performance improvement's attribution to either the sparse embeddings or the training recipe lacks clarity, and there is a need for ablation studies.
- The multi-stage training procedure is not described in sufficient detail, and the reliance on a closed-source 1B image-text pair dataset may hinder reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how STAIR compares with CLIP regarding actual retrieval speed. Additionally, the authors should provide more information on the impact of regularization weights $\lambda$ on interpretability and explore any trade-offs between interpretability and retrieval performance. It would be beneficial to include details about the training cost, such as total training time and infrastructure used. Furthermore, we suggest conducting ablation experiments to clarify whether performance gains stem from the sparse embeddings or the training recipe, and to enhance the description of the multi-stage training process to better elucidate its contributions.