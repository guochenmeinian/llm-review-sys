ID: Nr1XSeDzpn
Title: On the Convergence to a Global Solution of Shuffling-Type Gradient Algorithms
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the convergence of shuffling Stochastic Gradient Descent (SGD) to a global solution for a class of non-convex functions, utilizing relaxed non-convex assumptions such as the average PL condition and generalized star-smooth-convex condition. The authors claim that their analysis maintains the same computational complexity as shuffling SGD in the convex setting while providing a novel complexity bound for achieving convergence to a global minimizer.

### Strengths and Weaknesses
Strengths:  
- The paper is clear, well-written, and provides a comprehensive background on the convergence of shuffling SGD.  
- The authors introduce more relaxed assumptions compared to previous works, carefully explaining their contributions.  
- The theoretical results are stated clearly, and experiments on real datasets are implemented, demonstrating the convergence guarantees under new assumptions.

Weaknesses:  
- The class of non-convex functions considered lacks interpretable or standard definitions, raising skepticism about its strength compared to previously studied classes.  
- The analysis appears straightforward and closely resembles prior works, with the main distinction being the introduction of the average PL condition.  
- The assumptions may not be well-motivated, and the convergence rates achieved are similar to those of incremental gradient methods, suggesting potential for improvement.

### Suggestions for Improvement
We recommend that the authors improve the clarity and motivation behind the class of non-convex functions considered, possibly by providing examples of functions that satisfy the relaxed conditions but not the PL condition. Additionally, we suggest that the authors clarify the tightness of the bounds under the proposed assumptions and explore whether faster convergence rates can be achieved. Furthermore, addressing the concerns regarding the choice of increasing learning rates and their practical implications would strengthen the paper. Lastly, we encourage the authors to refine the notation in Section 4 for consistency and clarity.