ID: yvN3PilD1S
Title: Automatic Instruction Data Selection for Large Language Models via Uncertainty-Aware Influence Maximization
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the UniMax framework for selecting instruction data to enhance the capabilities of Large Language Models (LLMs). The approach utilizes graph influence maximization to capture complex inter-dependencies within instruction data, employing a self-supervised graph learner and an uncertainty-aware influence function. Experimental results validate the effectiveness of the proposed method across various datasets.

### Strengths and Weaknesses
Strengths:
- The paper innovatively reframes instruction data selection as an influence maximization problem, providing a new perspective.
- The methodology includes theoretical proofs for properties like monotonicity and submodularity, ensuring a solid foundation for the greedy algorithm used in selection.
- Extensive experiments across multiple datasets confirm the effectiveness of the approach, with valuable insights from parameter sensitivity analysis and ablation studies.

Weaknesses:
- The reliance on a proxy graph structure learner may introduce biases, as it might not accurately capture relevant relationships.
- The framework does not adequately address the impact of noisy or incorrect instruction examples, which could degrade performance in practical applications.
- The core assumption that maximizing influence leads to an optimal subset lacks rigorous proof, relying instead on indirect empirical validation.

### Suggestions for Improvement
We recommend that the authors improve the clarity around the uncertainty score in Stage 1 of Figure 2 and its correlation with Stage 2. Additionally, please provide more details on the complexity of the graph learner and its generalizability to different datasets. We suggest conducting further experiments to assess the framework's robustness to noisy or incorrect instruction examples and to analyze the variance over multiple runs. A deeper exploration of the theoretical guarantees for the greedy selection algorithm and the impact of hyperparameter choices would also enhance the paper's rigor. Finally, we encourage the authors to consider real-world applications and the challenges associated with them.