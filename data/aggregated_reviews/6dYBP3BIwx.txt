ID: 6dYBP3BIwx
Title: CogVLM: Visual Expert for Pretrained Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 4, 6, 8, 6, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CogVLM, an open-source visual language foundation model that integrates visual and linguistic features while maintaining the capabilities of a pretrained large language model. The authors propose a visual expert module that operates within the attention and feedforward layers, allowing CogVLM-17B, derived from Vicuna-7B, to achieve state-of-the-art performance across 17 cross-modal benchmarks. The contributions include the introduction of the CogVLM model, validation of the visual expert module's effectiveness, and public availability of the model weights and dataset used in the supervised fine-tuning (SFT) phase.

### Strengths and Weaknesses
Strengths:
1. CogVLM achieves state-of-the-art performance across multiple benchmarks, demonstrating excellent capabilities.
2. The paper is well-organized with clear expression.
3. The motivation to address performance degradation in LLMs during multimodal training is meaningful.

Weaknesses:
1. The introduction of a new set of QKV and FFN nearly doubles the model parameters; consideration of using LoRA (Low-Rank Adaptation) for parameter efficiency is warranted, along with additional experiments to support this.
2. There is a lack of ablation studies comparing the impact of different RoPE (Relative Positional Encoding) strategies on visual tokens.
3. The prompting differences between tasks are unclear, raising concerns about user control over outputs like bounding box information.
4. Limitations and broader impacts of the model are not discussed, despite their likely existence.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the prompting differences between tasks to ensure users can effectively control outputs. Additionally, we suggest incorporating results from the representative VLM benchmark MME in Table 2 and discussing the potential for including text data during multimodal training to maintain LLM capabilities. The authors should also consider adding results for CogVLM on the Video-MME benchmark and conducting an ablation study on the RoPE strategies. Finally, a thorough discussion of the limitations and broader societal impacts of CogVLM is essential for a comprehensive understanding of its implications.