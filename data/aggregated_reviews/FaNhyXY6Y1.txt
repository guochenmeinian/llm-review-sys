ID: FaNhyXY6Y1
Title: Artemis:  Towards Referential Understanding in Complex Videos
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 7, 7, -1, -1, -1, -1
Original Confidences: 5, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Artemis, a video-language model designed for video-based referential understanding, capable of describing targets indicated by bounding boxes in videos. The authors introduce the VideoRef45K dataset, which includes 45,000 video question-answer pairs, to train the model. Artemis is evaluated on the HC-STVG benchmark, outperforming adapted image-based models. The model also demonstrates capabilities in general video question answering and multi-round video understanding.

### Strengths and Weaknesses
Strengths:
1. The establishment of the VideoRef45K dataset significantly contributes to the field by providing referential pretraining data.
2. The model architecture's target-specific feature branch is well-motivated.
3. The adaptation of image-based referring models to video as baselines is reasonable and effective.
4. Artemis shows significant performance improvements over adapted baselines on HC-STVG.
5. The model's ability to enhance reasoning capabilities in video-language tasks is demonstrated through its performance on general video question answering.
6. Artemis can integrate with existing video-language models for advanced video understanding tasks.

Weaknesses:
1. The RoI selection process clusters object bounding boxes without considering visual content, potentially losing valuable information when objects change states.
2. The paper does not compare Artemis with video-language models like Video-LLaVA and Video-ChatGPT, which could be adapted for video referring tasks, limiting the demonstration of the RoI feature branch's effectiveness.
3. Concerns arise regarding the accuracy of Multi-Object Tracking (MOT) in complex scenarios, as the paper does not provide detailed discussions or examples of performance in challenging conditions.
4. The temporal relationships among tracked RoIs are not deeply explored, raising questions about the model's understanding of temporal dynamics.
5. Comparisons with other models may lack fairness due to differences in training data and methodologies.

### Suggestions for Improvement
We recommend that the authors improve the RoI selection step by incorporating visual content analysis alongside bounding box coordinates to avoid losing critical information. Additionally, we suggest that the authors compare Artemis with adapted video-language models like Video-LLaVA and Video-ChatGPT to validate the effectiveness of the RoI feature branch. Providing performance metrics of HQTrack in complex tracking scenarios would strengthen the paper. Furthermore, a more in-depth exploration of temporal relationships among RoIs and a careful ablation study to clarify the contributions of different model components are essential. Lastly, we encourage the authors to consider human evaluation studies to assess the accuracy and relevance of predicted captions in relation to video-question pairs.