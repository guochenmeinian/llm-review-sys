ID: BtCrHwiBHP
Title: Fully Unconstrained Online Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 6, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 3, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the first $\tilde{O}(G \lVert w_\star \rVert \sqrt{T} + \lVert w_\star \rVert^2 + G^2)$ guarantee for online convex optimization without prior knowledge of bounds on gradients or comparator norms. The authors propose a new parameter-free bound that closely matches the optimal regret upper bound $\lVert w_\star \rVert G \sqrt{T}$, achieved under known conditions. The paper also generalizes the method to a frontier of bounds using parametric regularization and provides matching lower bounds.

### Strengths and Weaknesses
Strengths:
- The paper offers a comprehensive discussion and comparison with previous work, enhancing understanding of the problem.
- The new guarantee presents appealing properties, including symmetry between $G$ and $\lVert w_\star \rVert$ without dependence on $T$.

Weaknesses:
- The paper lacks a discussion of previous results that are incomparable with the new bound, which could enhance context.
- The presentation is dense, assuming a high level of prior knowledge from readers, particularly regarding recent research and techniques.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by providing more context for the sequence of reductions, especially in Protocols 2 and 3. Explicitly defining terms like $h_t$ and $a_t$ earlier would enhance understanding. Additionally, a dedicated related works section would help contextualize the results better. Addressing the questions regarding the stochastic case and the implications of the bounds when $w_\star$ or $g_t$ are small would also strengthen the paper. Finally, consider extending the method to work with quadratic losses to eliminate the dependence on $G$.