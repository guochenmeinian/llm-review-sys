ID: kuwz9k061u
Title: Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the usage of parametric knowledge from pretrained large language models (LLMs) in downstream tasks. The authors propose a framework to quantify and extract knowledge from pretrained LLMs, focusing on knowledge retrieval through downstream task performance. The experiments reveal a gap in knowledge utilization, particularly in out-of-domain scenarios, indicating that models struggle to leverage their parametric knowledge effectively. The study also examines whether scaling the number of model parameters improves knowledge utilization, concluding that the gap persists even in larger models.

### Strengths and Weaknesses
Strengths:
- The paper presents a novel framework for analyzing knowledge acquisition and utilization in pretrained language models, potentially illuminating the extent to which pretrained knowledge can be leveraged during fine-tuning.
- The systematic experimental design allows for an evaluation based solely on the model's learned knowledge, avoiding confounding factors.

Weaknesses:
- The evaluation is limited to encyclopedic knowledge from a single dataset (Wikipedia), which may restrict the generalizability of the findings. The robustness experiment may not accurately capture the model's inability to utilize pretrained knowledge due to potential overfitting during fine-tuning.
- The identification of knowledge gaps is not novel, and the conclusions drawn would benefit from additional supporting evidence and exploration of diverse datasets and probing methods.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by incorporating additional datasets (e.g., WebNLG) and types of knowledge (e.g., commonsense) to enhance the robustness of their findings. Further investigation into the diversity of prompts used in the experiments is essential to avoid overfitting and preserve pretrained knowledge. Additionally, we suggest exploring various probing methods beyond relation prediction to strengthen the conclusions regarding knowledge utilization. A qualitative error analysis would also provide valuable insights into the model's performance.