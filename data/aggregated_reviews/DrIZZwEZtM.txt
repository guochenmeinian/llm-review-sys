ID: DrIZZwEZtM
Title: Gaussian Differential Privacy on Riemannian Manifolds
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 7, 6, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extension of Gaussian Differential Privacy (GDP) to general Riemannian manifolds, proposing a Riemannian Gaussian mechanism that achieves GDP. The authors develop a Markov Chain Monte Carlo (MCMC) procedure for Riemannian manifolds with constant curvature and provide a numerical algorithm to compute the privacy budget on these manifolds. The paper addresses the challenge of finding a suitable scaling parameter, $\sigma$, for the noise, with equations that become more tractable under certain structural assumptions on the manifold.

### Strengths and Weaknesses
Strengths:
- The extension of GDP to Riemannian manifolds is timely and significant, given the growing interest in non-linear data structures in statistics and machine learning.
- The proposed mechanism is a natural consideration for achieving privacy in the context of manifolds, which is more challenging than the Laplace mechanism.

Weaknesses:
- The experimental setup is limited, with comparisons only between the Riemannian Laplace mechanism and the GDP mechanism, lacking other relevant baselines such as DP-Riemannian Optimization and Riemannian K-Norm Gradient mechanisms.
- Experiments are conducted solely on 2-dimensional and 3-dimensional manifolds, necessitating the inclusion of additional manifolds, such as hyperbolic manifolds, and variations in parameters $n$ and $d$.
- The assumption of constant curvature for the numerical algorithm may not encompass a sufficient variety of manifolds, limiting practical applicability.
- Some definitions and proofs within the paper are unclear or incomplete, affecting the overall soundness of the results.

### Suggestions for Improvement
We recommend that the authors improve the experimental setup by including comparisons with additional baselines, specifically the DP-Riemannian Optimization and Riemannian K-Norm Gradient mechanisms. Furthermore, it is essential to extend the experiments to include at least one additional manifold, such as a hyperbolic manifold, and to vary the parameters $n$ and $d$. Additionally, we suggest clarifying the motivation behind the constant curvature assumption and addressing the completeness of definitions and proofs, particularly regarding Definition 3.1 and the proofs of Theorems 3.1 and A.1. Finally, we encourage the authors to provide real-world numerical examples to illustrate the practical implications of their work.