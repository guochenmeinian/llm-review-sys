ID: Vc39mtFSUW
Title: ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ReLLa, a framework aimed at enhancing Large Language Models (LLMs) for zero-shot and few-shot recommendation tasks by addressing the lifelong sequential behavior incomprehension problem. The authors propose two main strategies: Semantic User Behavior Retrieval (SUBR) for zero-shot recommendations and Retrieval-enhanced Instruction Tuning (ReiT) for few-shot scenarios. Extensive experiments validate ReLLa's effectiveness, demonstrating superior performance compared to existing baselines, even with limited training samples.

### Strengths and Weaknesses
Strengths:
1. The paper clearly identifies a unique problem in recommendation systems regarding LLMs' inability to extract useful information from lengthy user behavior sequences.
2. The methodology is innovative, combining SUBR and ReiT to enhance LLM performance effectively.
3. The writing is clear, and the presentation is well-structured, making it easy to follow.

Weaknesses:
1. The novelty of the approach may be limited, as the core feature SUBR resembles existing data augmentation techniques.
2. The empirical results lack evaluation across a broader range of backbone LLMs, raising questions about generalizability.
3. The few-shot settings may not be comparable to traditional methods due to the larger number of training shots used, which could skew results.

### Suggestions for Improvement
We recommend that the authors improve the novelty discussion by clarifying how SUBR differs from existing methods. Additionally, we suggest providing more experimental results on various popular LLMs and stricter few-shot settings to enhance the robustness of the findings. It would be beneficial to analyze the computational complexity of ReLLa relative to existing models and address potential biases and limitations more thoroughly. Finally, we encourage the authors to explore the applicability of SUBR and ReiT beyond recommendation tasks to broaden the impact of their work.