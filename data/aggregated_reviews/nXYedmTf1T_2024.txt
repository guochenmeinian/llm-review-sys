ID: nXYedmTf1T
Title: Calibrated Self-Rewarding Vision Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 8, 5, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to addressing hallucination in Large Vision-Language Models (LVLMs) through the Calibrated Self-Rewarding (CSR) method. CSR enables models to self-generate candidate responses, evaluate rewards, and curate preference data for fine-tuning, emphasizing visual input and reducing resource requirements compared to existing methods. The authors demonstrate the effectiveness of CSR through comprehensive experiments across multiple benchmarks, showing improvements in performance and reduced hallucinations.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant issue in LVLMs, providing a simple yet effective method that could benefit the community.
- The empirical evaluation is thorough, covering twelve benchmarks and demonstrating CSR's effectiveness against various baselines.
- The paper is well-organized and clearly written, with theoretical explanations that enhance its credibility.

Weaknesses:
- The ablation study lacks an analysis of the weight of instruction-following and image-text alignment scores, which is crucial for understanding their impact on CSR.
- The methodological contribution is somewhat incremental, primarily adapting self-rewarding techniques from NLP to the vision-language domain without substantial novelty.
- Some empirical results show only marginal improvements, particularly on popular benchmarks like VQAv2 and GQA.

### Suggestions for Improvement
We recommend that the authors improve the ablation study by including an analysis of the weight of instruction-following and image-text alignment scores to clarify their contributions to CSR. Additionally, exploring the impact of different image-text alignment scoring approaches would be beneficial. It would also enhance the paper to distinguish between comprehensive, general VQA, and hallucination scores in the main ablation results. Clarifying the terminologies in Figure 2 and improving the overall clarity of Figures 1 and 2 would aid in reader comprehension. Finally, we suggest investigating the use of diverse examples in different iterations to potentially yield better results.