ID: 00uVk06eVK
Title: On the Noise Robustness of In-Context Learning for Text Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 7, 5, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called Local Perplexity Ranking (LPR) aimed at improving the robustness of in-context learning (ICL) for text generation tasks in the presence of noisy annotations. The authors argue that noisy annotations negatively impact performance, particularly in text generation, and propose LPR to replace noisy examples with less noisy neighbors based on perplexity. Experimental results demonstrate that LPR enhances noise robustness across various datasets and selection methods.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach (LPR) that effectively addresses the issue of noisy annotations in ICL, challenging previous assumptions about robustness in classification tasks.
- The methodology is well-structured and clearly described, with proper definitions and metrics.
- Empirical results show that LPR improves performance across multiple selection methods in the presence of noise.

Weaknesses:
- The absence of code limits reproducibility and verification of results by the broader research community.
- There is no analysis of the types of errors made by LPR compared to baseline methods, which could provide insights into its strengths and weaknesses.
- The evaluation is limited to short-form, closed-domain tasks, which may not reflect real-world scenarios where noisy annotations are more prevalent.

### Suggestions for Improvement
We recommend that the authors improve the paper by including comparisons with simpler methods, such as a chain of thought, to strengthen their claims. Additionally, we suggest conducting experiments on long-form, open-domain question-answering tasks to better justify the motivation for LPR. It would also be beneficial to provide a clear illustration of how LPR is conducted and to analyze the types of errors made by LPR versus baseline methods. Finally, we encourage the authors to address the absence of code to enhance reproducibility.