ID: GgV6UczIWM
Title: A distributional simplicity bias in the learning dynamics of transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates whether transformers trained on natural language data using masked language modeling (MLM) exhibit a "simplicity bias," learning increasingly complex interactions between input tokens during training. The authors introduce a novel framework to study the learning dynamics of transformers, developing a method to create "clones" of datasets that capture token interactions up to a specified order using factored attention layers and quadratic $x^2$ activation functions. They demonstrate that a BERT-like transformer sequentially learns higher-order interactions, providing theoretical analysis and empirical evidence on the many-body interactions present in natural text.

### Strengths and Weaknesses
Strengths:  
- The proposed hypothesis is significant, aligning with cognitive science perspectives on language evolution and simplicity bias.  
- The paper is well-organized, with clear illustrations of methods and experiments.  
- The methodology, including dataset cloning and factored attention, is convincing and aligns well with the investigated hypothesis.  

Weaknesses:  
- The architecture used is uncommon, limiting the broader applicability of the findings; the title may be better suited as "BERT-like Language Models."  
- Higher-order interactions are challenging to sample, creating a bottleneck for larger-scale applications.  
- The empirical evidence is limited to one model and dataset, necessitating validation across diverse language tasks.  

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by testing on a wider array of language tasks, such as machine translation. Additionally, clarifying the term "last layer" in line 163 is essential to avoid confusion. To enhance the paper's clarity, we suggest defining "factored attention" in a preliminary section and addressing the undefined notations throughout the manuscript. Furthermore, we encourage the authors to reconsider the framing of their hypothesis, potentially changing the title to "A bias towards low-order interactions in the early learning dynamics of BERT-like language models" to better reflect the content.