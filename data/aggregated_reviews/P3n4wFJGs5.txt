ID: P3n4wFJGs5
Title: Window-Based Distribution Shift Detection for Deep Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 6, 6, 5, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 3, 3, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for detecting distribution shifts in a window of samples using a coverage-based detection algorithm, alongside a methodology for detecting out-of-distribution samples utilizing an uncertainty estimator, $\kappa_f$. The authors propose a framework that computes deviation from a coverage generalization bound and apply a selective classification approach to monitor shifts in pre-trained models. The method is evaluated on ImageNet, demonstrating computational efficiency and high accuracy across various distribution shifts, including adversarial attacks and data corruption. The experimental design includes a mixed window of in-distribution (ID) and out-of-distribution (OOD) data, enhancing the evaluation of the methodology while avoiding information leakage from the test data.

### Strengths and Weaknesses
Strengths:
- The paper introduces an original perspective on window-based drift detection, which is underexplored in existing literature.
- The proposed algorithm exhibits low time and space complexity while maintaining good performance.
- The theoretical formulation is clear and well-structured, making it accessible.
- Extensive experimental benchmarks and a comprehensive ablation study on hyperparameters are provided.
- The methodology demonstrates versatility through the integration of various uncertainty estimators.
- The experimental design reflects realistic application scenarios by incorporating mixed data windows.
- The authors effectively address reviewer concerns and provide clarifications that enhance understanding.

Weaknesses:
- The choice of confidence-rate function may limit the method's effectiveness, as some models can produce over-confident predictions for out-of-distribution data.
- The motivation behind the coverage bound violations is unclear, raising questions about its superiority over traditional t-tests.
- The contribution regarding limiting coverage without controlling risk appears less useful.
- The experimental setup lacks clarity, particularly regarding the creation of test windows and the mixture of in-distribution and out-of-distribution samples.
- There is a lack of benchmarking against stronger single-instance baselines, which raises concerns about the robustness of the proposed method.
- The writing style in certain sections, particularly regarding the segmentation of objectives and limitations, could be improved for clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup, specifically addressing how windows are created and whether they can contain both in-distribution and out-of-distribution samples. Additionally, we encourage the authors to elaborate on the claim that "detection power might lay within lower coverages" and provide more details on the novelty of Algorithm 1, Lemma 4.1, and Theorem 4.2 in relation to existing literature. It would also be beneficial to include experiments with different in-distribution datasets to assess the generalizability of the results. Furthermore, we suggest discussing the limitations of the method in greater detail, particularly regarding its performance across various types of distribution shifts. We also recommend improving the clarity of the paragraph that segments their objectives and limitations into two separate paragraphs (L.26-41) to avoid information overload. Lastly, consider including the two-sided version of the algorithm in an appendix, as it may be of interest for future research on different types of drift or attacks, and benchmark against stronger single-instance baselines to address concerns regarding the method's performance.