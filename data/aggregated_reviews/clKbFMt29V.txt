ID: clKbFMt29V
Title: Self-Supervised Visual Acoustic Matching
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 5, 6, 5, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a self-supervised approach for visual acoustic matching (VAM), enabling the transformation of audio clips to match the acoustics of a target visual environment using only unpaired data. The method employs a conditional GAN framework and introduces an "acoustic residue" metric to optimize the dereverberation process. The authors demonstrate that their approach outperforms existing methods across multiple datasets, supported by a human perception study.

### Strengths and Weaknesses
Strengths:
- The proposed method is original, leveraging existing techniques and a novel acoustic residue metric to function without paired data.
- The evaluation includes a human perceptual study, validating the effectiveness of the approach.
- The presentation is clear, and supplementary audio demos enhance understanding.

Weaknesses:
- The evaluation metrics, primarily MSE on magnitude spectrograms, may not align well with human perception; alternatives like MSE on log spectrograms or raised magnitude could improve evaluation.
- The focus on clean speech limits the method's applicability to real-world scenarios with diverse audio sources and multiple speakers.
- Some relevant references in audio-only acoustic matching are omitted, and the design of the human perception study may have flaws that affect the conclusions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in sections where references to figures and sections are made, ensuring all cited content is present and easily locatable. Additionally, we suggest including a table summarizing the data used in each training step to enhance understanding of the experimental design. The authors should also consider providing results for varying levels of reverberation and separately for seen and unseen images to better demonstrate the effectiveness of the LeMARA approach. Furthermore, we encourage the authors to explore the impact of the De-biaser component using supplementary metrics like SRMR and to clarify the motivation for using the MetricGAN approach in their methodology. Lastly, addressing the limitations regarding the evaluation metrics and including comparisons with existing paired data methods would strengthen the paper's contributions.