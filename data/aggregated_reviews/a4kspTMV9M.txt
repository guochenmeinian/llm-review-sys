ID: a4kspTMV9M
Title: A Specialized Semismooth Newton Method for Kernel-Based Optimal Transport
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 3, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an implementation of Vacher et al. (2021) utilizing a Semi-Smooth Newton (SSN) scheme to reformulate the optimization problem as a root-finding problem (Proposition 3.1). The authors provide convergence guarantees (Theorem 3.3) indicating a $O(1/\sqrt{T})$ convergence rate, where $T$ is the number of iterations, and propose an efficient method to reduce the cost per iteration. The authors also focus on approximating optimal transport (OT) using a Sum of Squares approximation, leveraging the SSN method to achieve statistical guarantees and computational efficiency. Additionally, the paper emphasizes the trade-off between statistical efficiency and computational cost, clarifying the differences between kernel-based OT and entropic OT, particularly regarding their computational complexities and scaling limitations. Numerical experiments validate that their method outperforms the one proposed in Vacher et al. (2021).

### Strengths and Weaknesses
Strengths:
- The topic of scalable kernel-based OT is highly relevant, addressing the inefficiencies of current implementations.
- The proposed method reduces the dependency on the number of iterations, achieving $O(1/\epsilon^2)$ iterations for precision $\epsilon$.
- The algorithm is practical, with clear instructions for implementation, and the theoretical investigation is rigorous, providing global and local convergence rates.
- The authors demonstrate a clear understanding of the trade-offs involved in kernel-based OT and provide a novel approach using SSN methods.
- They are responsive to reviewer feedback and committed to improving the clarity of their presentation, particularly in the introduction and background sections.

Weaknesses:
- The authors may oversell their work; while the proposed method requires $O(1/\epsilon^2)$ iterations, previous methods require $O(\sqrt{n}\log(n/\epsilon))$, making it less efficient for high precision.
- The cost per iteration is vaguely estimated and lacks explicit detail, which is crucial for practical efficiency.
- The writing is confusing, particularly regarding computational efficiency, and should be clarified in a theorem or proposition.
- The paper lacks formal proofs or precise theorems to substantiate claims regarding the complexity of their algorithm, particularly in relation to poor conditioning.
- The empirical contributions alone may not justify acceptance without stronger theoretical backing.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding efficiency, explicitly stating the implications of the $O(1/\epsilon^2)$ iterations compared to previous methods. Additionally, the authors should provide a detailed discussion on the cost per iteration, ideally including a precise estimate. The writing should be revised for clarity, particularly in the computational efficiency section, and it would be beneficial to include a self-contained introduction to SSN methods in the main body of the paper. Furthermore, we suggest that the authors clarify the stopping criteria used in their experiments and provide a more thorough comparison with existing methods, both theoretically and experimentally. We also urge the authors to formally prove or state a precise theorem demonstrating that their method does not suffer from poor conditioning to validate the claimed complexity of \(O(n^{3}/\epsilon^{2})\). Finally, contextualizing the scaling limitations of their method compared to existing approaches, such as Vacher et al., will enhance the paper's impact.