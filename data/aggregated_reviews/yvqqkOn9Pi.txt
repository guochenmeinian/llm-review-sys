ID: yvqqkOn9Pi
Title: Would I have gotten that reward? Long-term credit assignment by counterfactual contribution analysis
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 6, 6, 4, 8, 6, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, 1, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Counterfactual Contribution Analysis (COCOA), a reinforcement learning credit assignment method that builds on Hindsight Credit Assignment (HCA). The authors propose a new approach to estimating action contributions by using rewarding outcomes defined as functions of future states, actions, and rewards. COCOA aims to reduce variance in policy gradient estimators compared to traditional HCA methods, particularly in scenarios where high variance is a concern. The paper includes theoretical analysis and experiments demonstrating COCOA's effectiveness in a key-to-door environment.

### Strengths and Weaknesses
Strengths:
- The paper provides a novel approach to credit assignment that is well-motivated and theoretically sound.
- Extensive experimentation illustrates COCOA's ability to reduce variance in specific scenarios, particularly when hindsight models are known.
- The writing is clear and accessible, making the concepts understandable to a broader audience.

Weaknesses:
- The paper lacks investigation into the accuracy required for hindsight models to effectively reduce variance, which is crucial for practical applications.
- Comparisons to other relevant methods, such as the doubly robust control variate and return-conditioned HCA, are insufficient, limiting the evaluation of COCOA's performance.
- The experiments are primarily conducted in a single, synthetic environment, raising questions about the generalizability of the results to more complex settings.

### Suggestions for Improvement
We recommend that the authors improve the investigation into the accuracy of hindsight models necessary for variance reduction, as this is critical for understanding practical limitations. Additionally, including comparisons with relevant baselines, such as the doubly robust control variate and return-conditioned HCA, would provide a clearer context for COCOA's performance. The authors should also consider evaluating COCOA in more diverse environments to assess its scalability and effectiveness in complex scenarios. Finally, revising the writing for clarity and accessibility would enhance the paper's impact on the reinforcement learning community.