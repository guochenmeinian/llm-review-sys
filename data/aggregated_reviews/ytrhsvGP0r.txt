ID: ytrhsvGP0r
Title: Epidemic Learning: Boosting Decentralized Learning with Randomized Communication
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 7, 5, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 1, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a decentralized learning algorithm that utilizes randomized communication topologies to optimize non-convex functions. The authors propose two algorithms, EL Oracle and EL Local, which are shown to converge asymptotically faster than existing methods. The theoretical analysis demonstrates the advantages of transient iterations and linear speed-up, supported by empirical results on the CIFAR-10 dataset.

### Strengths and Weaknesses
Strengths:
- The paper clearly communicates its contributions and presents novel algorithms in the context of decentralized optimization.
- The theoretical analysis is technically sound and provides interesting insights into convergence rates.
- The experimental results validate the theoretical claims, showing improved convergence compared to static topologies.

Weaknesses:
- There is ambiguity regarding the bounded quantity in (3) and its comparison to the rates in Theorem 1, which requires clarification.
- The practical implications of the proposed algorithms are not well contextualized, particularly concerning the fixed number of neighbors $k$ and its impact on convergence.
- The title "Epidemic Learning" may be misleading and lacks informativeness for the intended audience.
- The experiments are limited to a single dataset, raising concerns about the generalizability of the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the analysis, particularly regarding the bounded quantity in (3) and its relation to Theorem 1. Additionally, addressing the practical applicability of the fixed neighbor count $k$ and exploring varying $s$ values in experiments would enhance the robustness of the findings. We suggest revising the title to better reflect the content and appeal to the decentralized learning community. Finally, conducting experiments across multiple datasets would strengthen the paper's contributions and validate the proposed algorithms' effectiveness in diverse scenarios.