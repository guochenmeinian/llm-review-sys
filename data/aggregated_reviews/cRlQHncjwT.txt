ID: cRlQHncjwT
Title: Generative Forests
Conference: NeurIPS
Year: 2024
Number of Reviews: 33
Original Ratings: 7, 6, 5, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 1, 4, 2, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a generative model for tabular data known as Generative Forests (GF), which utilizes an ensemble of trees for sampling, data imputation, and density estimation. The authors introduce a training algorithm called GF.BOOST, which is simple to implement and effective across various tasks. The method partitions the input domain by considering all possible intersections of the supports of the leaves across the ensemble, achieving finer partitioning than a single tree. The performance of GF is evaluated against competing methods in synthetic data generation, missing data imputation, and density estimation, with a focus on metrics such as Optimal Transport (OT) distance, Coverage, and Density. The authors clarify that their method is not designed for supervised learning tasks, which limits the applicability of certain metrics.

### Strengths and Weaknesses
Strengths:
- The approach represents a significant advancement in generative modeling for tabular data, leveraging ensemble methods and decision tree induction.
- The methodology is robust, with comprehensive experiments across diverse datasets, showcasing the model's versatility and demonstrating significant performance advantages over Forest Flow and Adversarial Random Forests.
- The paper is well-organized and clearly written, facilitating understanding and reproducibility, and the authors have made their implementation code available.
- The inclusion of new evaluation metrics enhances the robustness of the analysis.

Weaknesses:
- The novelty of the proposed solution is questioned, particularly in relation to existing works that tackle similar tasks.
- The mathematical notations can be complex and may benefit from simplification for clarity.
- The explanation of the training algorithm lacks thoroughness, particularly regarding computational scaling with the number of trees and iterations.
- The evaluation setup is criticized for lacking comparisons with key methods such as ARF (FORDE) and TabDDPM, and for relying solely on the OT metric, raising concerns about its effectiveness across datasets with varying feature types.
- The experiments primarily utilize standard ML benchmarks, lacking real-world datasets with missing values for comparison against strong baselines.
- Some aspects of the methodology remain unclear, such as the implications of using KL divergence as a loss function and the strategy for ensuring diversity among trees.

### Suggestions for Improvement
We recommend that the authors improve the empirical comparisons by including recent approaches such as diffusion models and flow matching, as well as comparisons with ARF (FORDE) and other relevant methods like TVAE or CTAB-GAN to strengthen their claims. Additionally, incorporating more metrics for the LifeLike experiment and ensuring consistency in comparisons and datasets across different methods would provide a more comprehensive assessment. 

We suggest clarifying the computational cost for training and inference compared to Generative Trees and elaborating on the parameter selection process, particularly the impact of the duplicate_K parameter on performance. A dedicated section discussing limitations, particularly regarding multiclass scenarios and specific data types where the model may struggle, would enhance the paper's depth. Finally, we encourage the authors to explore and report on multiple evaluation metrics beyond the Optimal Transport distance, including the use of log-likelihood for assessing model quality, and to provide actual training times for their algorithm to address scalability concerns more effectively.