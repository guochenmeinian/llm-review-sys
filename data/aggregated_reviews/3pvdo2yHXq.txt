ID: 3pvdo2yHXq
Title: Speech-enriched Memory for Inference-time Adaptation of ASR Models to Word Dictionaries
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel inference method for adapting ASR systems to rare word dictionaries using a combination of k-nearest neighbor (KNN) retrieval and text-to-speech (TTS) techniques. The authors propose an algorithm that biases ASR model decoding towards predefined rare words while maintaining performance on other words, utilizing a trie for exact matches and a backup beam. The method is training-free and demonstrates effectiveness across various architectures, including encoder-decoder models and Neural Transducers, with comprehensive evaluations on multiple datasets.

### Strengths and Weaknesses
Strengths:
- The organization of memory as a trie and the use of a backoff beam elegantly prevent performance degradation on non-rare words.
- The methodology is robust, supported by extensive experiments across diverse datasets, including medical and synthetic entity-rich datasets.
- The proposed method is applicable to different ASR architectures without requiring retraining, showcasing efficiency during inference.

Weaknesses:
- The latency of the proposed approach and its comparison with other methods are not adequately addressed, leaving uncertainty about its speed relative to neural network-based adaptations.
- Comparisons to existing contextual biasing methods are limited, and the superiority of the proposed method over previous works is unclear.
- The explanation of the method, particularly in section 3.1, is difficult to follow, which may hinder understanding.

### Suggestions for Improvement
We recommend that the authors improve the clarity of section 3.1 by providing more specific details on how alignments are obtained. Additionally, we suggest including a quantitative analysis of the latency of the proposed approach compared to other methods to substantiate claims of efficiency. It would also be beneficial to clarify the comparisons with existing methods, particularly regarding the performance of PRISM relative to Le et al. 2021a. Finally, addressing the questions raised about model complexities and the nature of results in Table 2 would strengthen the experimental section.