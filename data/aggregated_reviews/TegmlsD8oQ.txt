ID: TegmlsD8oQ
Title: 4M: Massively Multimodal Masked Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 5, 9, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 5, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multi-modal pre-training scheme called 4M, which utilizes a single encoder-decoder architecture to integrate various modalities, including text, images, geometric, and semantic data. The authors propose a joint representation of these modalities through modality-specific tokenizers and employ multi-modal masked training. The extensive experiments demonstrate that 4M effectively addresses numerous vision tasks, supports fine-tuning for unseen tasks, and enables multi-modal controllable generation.

### Strengths and Weaknesses
Strengths:
- The writing is clear and accessible.
- The approach is scalable across data, architecture, and training objectives.
- The experimental methodology includes well-designed ablations that justify architectural choices and the impacts of input modalities.
- Comprehensive experimentation showcases capabilities such as zero-shot generalization and multi-modal generation.

Weaknesses:
- The paper lacks quantitative evaluation of generation capabilities and comparisons with state-of-the-art methods.
- There is insufficient discussion on the robustness of the approach concerning dataset quality, which is critical for scaling.
- The novelty of the work is questioned, as it appears to combine existing methods without significant innovation.
- The performance results, particularly in fine-tuning, are not competitive compared to stronger pre-trained methods.

### Suggestions for Improvement
We recommend that the authors improve the quantitative evaluation of 4M's generation capabilities and include comparisons with existing state-of-the-art methods. Additionally, we suggest that the authors elaborate on the robustness of 4M concerning dataset quality, particularly in relation to low-quality data. It would also be beneficial to provide more diverse quantitative results on downstream tasks and clarify the novelty of the proposed approach. Finally, consider discussing the implications of tokenization on performance across different domains, as well as conducting out-of-distribution analysis.