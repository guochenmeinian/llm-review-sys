ID: Fj07R03qkz
Title: IAEval: A Comprehensive Evaluation of Instance Attribution on Natural Language Understanding
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation framework for Instance Attribution (IA) methods, focusing on four properties: sufficiency, completeness, stability, and plausibility. The authors evaluate this framework on four datasets across three IA methods: feature similarity (FS), gradient similarity (GS), and representer point selection (RPS). Results indicate that FS and GS perform well, while RPS faces performance issues. The framework aims to provide a comprehensive assessment of IA methods' effectiveness.

### Strengths and Weaknesses
Strengths:
- The design of each evaluation metric is well-motivated and documented, enhancing clarity and organization.
- Evaluations across four datasets, including two Chinese datasets, improve the generality of the findings.
- The thoughtful design of experiments and insightful case studies, particularly regarding RPS, aids reader comprehension.

Weaknesses:
- The IA methods are not model-agnostic, necessitating consistent results across different models for the same language to justify the evaluation framework's effectiveness.
- The plausibility measure is debated among reviewers, with concerns about its necessity and the adequacy of its current design, which may not accurately reflect human understanding.
- The stability evaluation raises doubts regarding its implementation, with potential errors stemming from the selection of top N similar examples and the choice of similarity metrics.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by demonstrating consistent results across different models for the same language to validate the framework's applicability. Additionally, we suggest reconsidering the inclusion of the plausibility measure, as its current design does not effectively capture human interpretability. If human annotations are unavailable, this property should be omitted. Furthermore, we encourage the authors to clarify their claims regarding the evaluation metrics and address the concerns raised about the stability evaluation, particularly regarding the control of predictions and the similarity metrics used.