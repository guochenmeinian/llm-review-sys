ID: 7WoOphIZ8u
Title: Derivatives of Stochastic Gradient Descent in parametric optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 7, 4, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 1, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of stochastic optimization problems where the objective depends on a parameter, specifically analyzing the derivatives of SGD iterates with respect to that parameter. The authors establish convergence rates under various assumptions and settings, utilizing an inexact SGD sequence for their proofs. The results are supported by numerical experiments, although the experiments primarily involve synthetic data.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written, with a convincing technical flow of ideas.  
- It provides rigorous convergence guarantees for the derivatives of SGD under certain assumptions.  
- The connection to inexact SGD is notably novel and well-illustrated through numerical experiments.

Weaknesses:  
- The motivation for the research question is not sufficiently articulated, particularly in lines 35-36.  
- The title may be misleading, as it suggests a focus on algorithmic derivatives rather than derivatives of iterates concerning an additional parameter.  
- The assumptions used in the theoretical analysis are perceived as too strong, and the practical significance of the theory, especially regarding stochastic hyperparameter optimization, is unclear.  
- The numerical illustrations lack clarity, particularly regarding the stochastic gradient estimates and the treatment of the independent parameter $\theta$.

### Suggestions for Improvement
We recommend that the authors improve the motivation section to clarify the significance of the research question. Additionally, consider revising the title to better reflect the content, such as "Derivatives in parametric optimization and their behavior along SGD iteration." We suggest enhancing the clarity of the numerical experiments, ensuring that the stochastic gradient estimates are explicitly defined and that the role of $\xi$ is clarified. Furthermore, we encourage the authors to discuss the implications of their assumptions more thoroughly and explore the possibility of relaxing them to enhance the practical relevance of their findings.