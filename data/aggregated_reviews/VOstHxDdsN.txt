ID: VOstHxDdsN
Title: Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to optimize hard text prompts for generative models using efficient gradient-based optimization. The authors propose a method that automatically generates hard text-based prompts for both text-to-image and text-to-text applications, enabling users to create and combine image concepts without prior prompting knowledge. The paper emphasizes the benefits of hard prompts over soft prompts and demonstrates the method's effectiveness in tuning language models for classification.

### Strengths and Weaknesses
Strengths:
1. The approach is efficient, allowing optimization on smaller language models before transferring to larger models.
2. The method consistently outperforms other gradient-based optimization baselines across four datasets and achieves performance comparable to CLIP Interrogator, using only the CLIP model for prompt discovery and a total of 8 tokens.
3. The proposed method can be adapted for style transfer by extracting shared style characteristics into a single hard prompt.

Weaknesses:
1. The generated prompts, while relevant, often lack coherent English sentences, indicating a limitation in creating full sentences despite finding relevant words.
2. Longer prompts do not necessarily yield better results with Stable Diffusion, leading to potential overfitting and requiring tedious hyperparameter tuning.
3. The contribution of the work is limited, primarily applying existing methods without significant differentiation from related work. The presence of gibberish prompts complicates interpretation, and the explanation regarding emojis in prompts lacks clarity.

### Suggestions for Improvement
We recommend that the authors improve the coherence of generated prompts to enhance their interpretability. Additionally, addressing the issue of longer prompts leading to overfitting is crucial; we suggest exploring strategies to mitigate this while maintaining prompt quality. The authors should clarify the differences between their work and existing methods, particularly in terms of contributions. Including more diverse baselines in the evaluation and providing negative qualitative examples would strengthen the analysis. Lastly, we encourage the authors to present a more rigorous examination of the efficiency of their method compared to others, including visual comparisons and detailed discussions on trade-offs.