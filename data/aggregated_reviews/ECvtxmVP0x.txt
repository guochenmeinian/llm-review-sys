ID: ECvtxmVP0x
Title: Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attribution
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 8, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to enhance the interpretability of vision-language models (VLMs) through a multi-modal information bottleneck (M2IB) objective, utilizing CLIP, a state-of-the-art VLM, to compress irrelevant features while retaining significant visual and textual information. This method generates attribution maps without requiring ground truth labels, demonstrating improved attribution accuracy across various high-stakes datasets and outperforming traditional CAM-based methods. Additionally, the authors propose a novel attribution method for transformer-based models, showing its effectiveness through extensive empirical comparisons with various baselines, including KernelSHAP and Saliency. The evaluation indicates that their method outperforms or matches established methods like Chefer et al. across multiple metrics, with clarifications provided in the rebuttal regarding hyperparameter selection and the train/validation split.

### Strengths and Weaknesses
Strengths:
1. The work addresses a critical issue in the interpretability of VLMs, providing a method that does not depend on ground truth labels for generating attribution maps.
2. The proposed method shows superior performance compared to multiple baselines, reinforcing its validity.
3. The paper is well-structured, offering clear methodological details that facilitate understanding.
4. A comprehensive evaluation is conducted using multiple faithfulness metrics, demonstrating the robustness of the proposed method.
5. The authors have actively addressed reviewer concerns by adding new baselines and clarifying methodological details.

Weaknesses:
1. The hypothesis regarding the relationship between image and text encodings is unclear, as the model does not establish a unique mapping between representations.
2. Initial confusion regarding hyperparameter selection and the train/validation split may hinder reader comprehension.
3. The method involves numerous hyperparameters for tuning, yet lacks a clear analysis on how to optimize them for different models and datasets.
4. The computational expense of tuning hyperparameters for each image-caption pair raises concerns about practicality.
5. The inclusion of GradCAM as a baseline is questioned, as it is architecture-specific and may not align with the paper's objectives.
6. The claim that the method captures all relevant objects in both modalities is questionable, as the model may rely on limited information for specific pairs.
7. The paper contains several typographical errors and could benefit from improved writing clarity.
8. The use of standard error instead of confidence intervals for reporting results could be clarified.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the hypothesis regarding image and text encodings to strengthen the theoretical foundation of their approach. Additionally, providing a detailed analysis of hyperparameter tuning would enhance the usability of the method across various datasets. It would be beneficial to address the computational concerns by discussing the efficiency of the tuning process. We also suggest revising the manuscript to correct typographical errors and enhance overall writing quality. Furthermore, we recommend improving clarity regarding hyperparameter selection and the train/validation split in the manuscript to prevent future confusion. Consider incorporating a detailed explanation or table regarding bottleneck computation in the final paper. It would also be beneficial to justify the inclusion of GradCAM as a baseline more thoroughly, given its architecture-specific nature. Lastly, please ensure that the representation of statistical measures, such as standard error, is clearly defined in the final version of the paper.