ID: UMywlqrW3n
Title: Getting MoRE out of Mixture of Language Model Reasoning Experts
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Mixture-of-Reasoning-Experts (MoRE) framework that combines specialized language models to enhance generalization across various question answering (QA) tasks, specifically targeting factual, multihop, mathematical, and commonsense reasoning. The authors aim to improve interpretability and selective answering by employing an answer selector to determine the best response or abstain from answering. The framework demonstrates superior performance compared to individual specialized models across twelve datasets.

### Strengths and Weaknesses
Strengths:
- The motivation for generalizable QA is well-articulated, and the proposed idea is reasonable.
- Thorough evaluation across twelve datasets, including human studies, supports the effectiveness and interpretability of the method.
- The paper is well-written and addresses a timely problem in the field.

Weaknesses:
- The QA type coverage is not representative, and the authors do not adequately explain their choices of QA types.
- The performance of the MoRE framework is not consistently superior to single expert models, particularly in factual and multihop tasks, raising questions about its complexity and contributions.
- The experiments are limited to Codex and random forest, lacking diversity in tested models and classifiers.

### Suggestions for Improvement
We recommend that the authors improve the explanation of their QA type selections and clarify the overlaps between factual and multihop reasoning. Additionally, the authors should conduct experiments with a wider variety of language models and classifiers, such as boosting algorithms and BERT, to validate the effectiveness of the proposed method. Furthermore, we suggest moving the results of Appendix A.1 and Table 4 back to the main paper, as they are critical to justifying the importance of the answer selector. Lastly, providing more specific information about the MoRE-Codex Selector setting and the generation of confidence scores for predictions from different experts would enhance clarity.