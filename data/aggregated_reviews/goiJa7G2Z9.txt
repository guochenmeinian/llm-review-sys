ID: goiJa7G2Z9
Title: A Knowledge-Injected Curriculum Pretraining Framework for Question Answering
Conference: ACM
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents KICP, a knowledge base question-answering approach that emphasizes the pretraining stage. The authors propose a method that includes knowledge injection, knowledge adaptation, and curriculum reasoning to enhance the performance of language models on knowledge graph-based applications. Experiments conducted on four datasets demonstrate that KICP outperforms several baseline models, including traditional and knowledge-enhanced language models.

### Strengths and Weaknesses
Strengths:
- The approach effectively utilizes knowledge graphs to improve language model performance on question-answering tasks.
- The implementation of Curriculum Learning allows for gradual complexity in training, enhancing the model's reasoning capabilities.
- The method is well-documented with high-quality writing and detailed formalizations.

Weaknesses:
- The paper lacks comparisons with knowledge graph question-answering methods based on representation learning techniques, such as EmbedKGQA.
- There is insufficient discussion on the generalizability of the approach to other reasoning types beyond multi-hop reasoning.
- The writing is at times convoluted, making it difficult to follow, and there are inconsistencies in terminology, such as using "triplets" instead of "triples."

### Suggestions for Improvement
We recommend that the authors improve the comparison with existing KG-based pretraining methods by discussing their differences and possibly including them as baselines. Clarifying the base language models used in the experiments will support claims of fair comparisons. Additionally, running experiments with multiple random seeds and including statistical significance tests would strengthen the claims of significant improvements. Expanding the ablation study to cover more design choices would provide deeper insights into the method's components. Furthermore, acknowledging state-of-the-art approaches like retrieval augmentation and multi-LM collaboration in the related work section would enhance the paper's context. Lastly, we suggest revising the writing for clarity and consistency, particularly in terminology.