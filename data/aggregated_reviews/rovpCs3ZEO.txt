ID: rovpCs3ZEO
Title: FEDMEKI: A Benchmark for Scaling Medical Foundation Models via Federated Knowledge Injection
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 8, 7, 7, 8, -1
Original Confidences: 5, 5, 4, 4, -1

Aggregated Review:
### Key Points
This paper presents FedMEKI, a federated learning-based platform designed to integrate medical knowledge into foundation models while preserving data privacy. The authors curate a comprehensive dataset from seven publicly available medical sources, covering eight diverse medical tasks across seven modalities. They implement 16 benchmark approaches to validate the FEDMEKI platform, demonstrating its capability to enhance medical foundation models by allowing them to learn from a broader spectrum of medical knowledge without direct data exposure. The writing quality is high, and the paper effectively explores the scalability of medical foundation models within a federated framework.

### Strengths and Weaknesses
Strengths:
- The research addresses a timely and critical challenge in medical data privacy.
- The platform is well-documented, user-friendly, and supports reproducibility.
- Extensive experiments validate the platform's effectiveness across various medical datasets and tasks.
- The paper is clearly written and well-structured, facilitating reader comprehension.

Weaknesses:
- There is a noticeable performance trade-off when injecting medical knowledge into foundation models, with federated scaled models performing worse on some tasks.
- Performance on zero-shot inference tasks is unsatisfactory, indicating a need for improvement.
- The communication and synchronization overhead is high, and the dataset division rationale requires clarification.

### Suggestions for Improvement
We recommend that the authors improve the efficiency and performance of the training algorithms to better handle the complexities of federated learning. Additionally, developing more efficient communication protocols could reduce overhead and improve synchronization across clients. The authors should provide more explanations regarding the lower performance of specific tasks, such as MedVQA and the sepsis task compared to others. Furthermore, we suggest that the authors clarify the criteria used for dividing the dataset into training and validation tasks to ensure practical applicability. Lastly, we encourage the authors to maintain and update the GitHub repository, enabling contributions from other users to foster a collaborative development environment.