ID: mtkbTUVaV1
Title: A Fully Analog Pipeline for Portfolio Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 5, 7
Original Confidences: 5, 4

Aggregated Review:
### Key Points
This paper presents a method for solving the Markowitz mean-variance portfolio optimization problem using algorithms compatible with analog hardware, specifically continuous Hopfield networks and equilibrium propagation (EP). The authors formulate the optimization problem as minimizing risk for a given level of expected returns, with the covariance matrix and expected returns estimated from historical data. They employ a low-rank approximation method using a linear autoencoder, consisting of a linear encoder and decoder, trained via EP. The resulting continuous Hopfield network approximates the covariance matrix and provides a solution to the optimization problem.

### Strengths and Weaknesses
Strengths:  
- The paper explores an interesting application of analog computing, which is valuable for dissemination at the workshop.

Weaknesses:  
- The framing of the problem and the application of EP are unclear, particularly regarding the misleading presentation of Eq. 1, which suggests that \(E:=w^\top \cdot \Sigma \cdot w\) is the outer objective when it is actually the inner objective constrained by minimizing the reconstruction error \(\ell := || X - A \cdot B \cdot X ||_F^2\). The notation for free variables and parameters is confusing, making it difficult to discern the variables of the outer optimization problem and the precise definition of the energy function \(E\).  
- The statement regarding equilibrium propagation finding the gradient of the loss with respect to the input should be attributed to Scellier (2021), preceding the reference to Van Der Meersch et al. (2023).

### Suggestions for Improvement
We recommend that the authors improve the clarity of the problem framing, particularly by addressing the misleading nature of Eq. 1 and clearly distinguishing between the inner and outer objectives. It would be beneficial to remind readers in Section 2 that the covariance matrix and expected returns must be estimated from historical data. Additionally, we suggest reorganizing Section 3 to follow Section 5 for better flow, or clarifying the distinction between the sections focused on optimizing \(w\) and those on approximating \(\Sigma\) and \(\mu\). The authors should also clarify how the soft penalty in Eq. 4 affects the solution and whether the normalized outputs in the unconstrained optimization problem yield valid solutions to the original problem. Lastly, we recommend explaining earlier that the nonlinear activation function \(g\) must be increasing for \(E\) to be a Lyapunov function, and clarifying the applicability of the method in Appendix C to analog systems.