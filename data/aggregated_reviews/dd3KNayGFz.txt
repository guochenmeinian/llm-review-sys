ID: dd3KNayGFz
Title: Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 4, 4, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a differential privacy framework for graph neural networks (GNNs) that addresses privacy leakage in both graph topology and node attributes. The authors propose a model, Differentially Private Decoupled Graph Convolutions (DPDGC), which decouples graph convolution from node attributes, aiming to provide provable privacy guarantees while maintaining utility. The framework highlights the limitations of standard differential privacy techniques in graph learning and introduces a relaxed notion of node-level data adjacency for varying degrees of privacy.

### Strengths and Weaknesses
Strengths:
- The paper tackles the critical issue of graph differential privacy, essential for protecting sensitive user information.
- The theoretical analysis supports the proposed model, enhancing its credibility.
- Experimental evaluations on seven node classification datasets demonstrate the framework's performance.

Weaknesses:
- The presentation is challenging to follow, potentially hindering reader comprehension.
- The proposed method exhibits poor performance, particularly with a privacy budget of Îµ=16, which is insufficient for many datasets.
- Some relevant literature is not cited, indicating a lack of thorough research review.
- The decoupling method's novelty is questionable, as similar concepts exist in prior work.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation to facilitate reader understanding. Additionally, the authors should conduct a more comprehensive review of related works to include relevant literature. We suggest providing a detailed analysis of the privacy guarantees and discussing the implications of the results on various datasets. Furthermore, we encourage the authors to explore alternative privacy mechanisms beyond the current framework and to clarify the training process of the MLP in Table 3. Lastly, addressing the computational complexity and potential for distributed implementation would enhance the paper's contributions.