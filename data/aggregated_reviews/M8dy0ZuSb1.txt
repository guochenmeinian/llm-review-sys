ID: M8dy0ZuSb1
Title: Improving robustness to corruptions with multiplicative weight perturbations
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called Data Augmentation via Multiplicative Perturbations (DAMP), aimed at enhancing the robustness of deep neural networks (DNNs) against image corruptions. The authors propose a weight-based approach that utilizes random multiplicative weight perturbations, improving generalization on corrupted images without sacrificing accuracy on clean examples. The work explores the relationship between DAMP and existing methods such as Adaptive Sharpness-Aware Minimization (ASAM), demonstrating competitive performance across various datasets and architectures. Additionally, DAMP can be interpreted as performing variational inference, with its objective function analogous to the expected log-likelihood term of the evidence lower-bound (ELBO) used in variational inference, suggesting pathways to scale variational methods for large models and datasets.

### Strengths and Weaknesses
Strengths:
- The paper introduces a fresh perspective on achieving robustness against corruptions.
- DAMP is simple and exhibits nearly the same complexity as SGD while achieving competitive results.
- Theoretical foundations support the proposed method, and the connection between DAMP and variational inference is clearly articulated, enhancing the theoretical understanding.
- Comprehensive evaluations across multiple datasets and architectures show that DAMP often outperforms ASAM at a lower computational cost.
- The proposal to scale variational approaches for larger models and datasets is a significant contribution.

Weaknesses:
- The novelty of DAMP is questioned, as similar concepts have been explored previously, such as variational dropout.
- The theoretical foundation is perceived as weak, with assumptions not well-explained.
- The fixed nature of the variational distribution may limit the flexibility of the approach, potentially affecting performance in certain scenarios.
- Experimental results are limited to specific types of corruption, and comparisons with other augmentation techniques are insufficient.
- The method's performance on larger models and datasets remains unclear, and the trade-off between training time and accuracy raises concerns.

### Suggestions for Improvement
We recommend that the authors improve the contextualization of DAMP by comparing it with related works, particularly those addressing model robustness through weight perturbations. Additionally, conducting experiments on a broader range of corruption types, including natural and adversarial corruptions, would strengthen the findings. Clarifying the advantages of DAMP over ASAM and exploring its compatibility with data augmentations would also enhance the paper. Furthermore, we suggest that the authors elaborate on the implications of keeping the variational distribution fixed, particularly regarding its impact on model performance, and provide a more detailed explanation of the theoretical assumptions and their implications for modern architectures. Finally, clarifying how DAMP can be effectively scaled for various applications would strengthen the paper.