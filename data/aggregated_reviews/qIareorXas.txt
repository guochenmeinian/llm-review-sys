ID: qIareorXas
Title: Conformal Graph-level Out-of-distribution Detection with Adaptive Data Augmentation
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CGOD, a novel framework for graph-level out-of-distribution (OOD) detection that extends conformal prediction (CP) to ensure statistical guarantees for detection results. The authors propose an adaptive non-conformity scoring mechanism powered by data augmentation, generating multiple non-conformity scores that are aggregated to enhance robustness. The framework is validated through experiments on six dataset pairs, demonstrating superior performance in accuracy and reliability compared to existing methods.

### Strengths and Weaknesses
Strengths:
- The proposed framework effectively addresses limitations of prior approaches, providing statistical guarantees for detection results.
- The introduction of an innovative aggregated non-conformity score function enhances the robustness of the detection process.
- Comprehensive theoretical and experimental analyses support the framework's efficacy.

Weaknesses:
- The paper lacks a dedicated section discussing its limitations and future work directions.
- Clarity issues exist in key sections, particularly regarding the ablation study and model notation.
- Limited dataset coverage is noted, with no justification for excluding datasets referenced in prior work.
- The practical significance of the statistical guarantees is underexplored, and dataset statistics are not provided, hindering evaluation of scalability and generalizability.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing a dedicated section discussing the limitations of the framework and potential future work. Additionally, clarify the linear operation in Section 5.5 and ensure that the ablation study results are easily interpretable, specifying which models correspond to the rows in Table 3. We suggest including reasoning for the selection of dataset pairs and conducting experiments on all 10 pairs mentioned in reference [24] to strengthen validation. Furthermore, discuss the generalizability of the proposed metrics and provide detailed statistics for the datasets used to enhance the experimental evaluation.