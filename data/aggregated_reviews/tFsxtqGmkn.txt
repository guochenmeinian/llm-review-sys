ID: tFsxtqGmkn
Title: Maximum State Entropy Exploration using Predecessor and Successor Representations
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 8, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for maximum state entropy exploration in environments without rewards, proposing $\eta\psi$-Learning, which combines predecessor and successor representations to maximize entropy over states sampled in a single trajectory. The algorithm is evaluated against MaxEnt in toy domains and continuous control tasks, demonstrating significant performance improvements.

### Strengths and Weaknesses
Strengths:
- The paper introduces a simple yet effective method for learning non-Markovian policies that maximize state entropy from a single trajectory.
- The method shows considerable improvement over MaxEnt, a representative algorithm for maximum state entropy exploration with Markovian policies.
- Ideas are clearly presented with effective visualizations that enhance understanding.

Weaknesses:
- The scalability of the procedure to more complex domains, such as image-based inputs, is unclear.
- The method is only compared against MaxEnt, which is not the state-of-the-art for state entropy maximization in continuous control tasks.
- The predecessor representation appears unnecessary for action selection, and the evaluations in continuous control tasks remain limited to grid world environments.

### Suggestions for Improvement
We recommend that the authors improve the discussion on how to scale their approach to high-dimensional domains, particularly in image-based tasks. Additionally, including comparisons with standard exploration baselines, such as epsilon-greedy exploration and count-based intrinsic motivation approaches, would provide a more comprehensive evaluation of the proposed method. Clarifying the role of the predecessor representation in action selection and expanding the experiments to include more challenging continuous control tasks would also enhance the paper's contribution. Finally, addressing the limitations of the proposed approach explicitly would add significant value.