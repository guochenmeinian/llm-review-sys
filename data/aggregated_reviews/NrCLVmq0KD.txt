ID: NrCLVmq0KD
Title: LLM aided semi-supervision for efficient Extractive Dialog Summarization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for extractive summarization of customer-agent dialogs, utilizing large language models (LLMs) to generate pseudo-labels for dialogue data. The authors propose framing summarization as a question-answering problem, leveraging a large language model to create pseudo-labeled data, which is then used to fine-tune a summarization model. The approach shows effectiveness on the TWEETSUMM dataset, achieving performance comparable to state-of-the-art models while using only 10% of the labeled data.

### Strengths and Weaknesses
Strengths:
- The paper proposes a novel approach to extractive summarization using LLMs, demonstrating effectiveness on the TWEETSUMM dataset.
- The method is easy to follow and provides insights into the use of pseudo-labels for semi-supervised learning.

Weaknesses:
- The method relies on a closed-source model for generating pseudo-labels, raising concerns about the reproducibility and efficiency of the approach.
- The synthesized dataset does not guarantee purely extractive outputs, as the generative nature of GPT-3.5 may lead to non-extractive summaries.
- The writing contains several typos and formatting issues, and some notations lack clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of notations in Section 2.1 and ensure consistency in terminology, particularly regarding the use of "labeled" and "labelled." Additionally, the authors should address the efficiency of their method, particularly the need for multiple training iterations, and provide further analysis on the performance impact of using GPT-3.5 as the labeler and evaluator. A case study and a more comprehensive ablation study are also suggested to strengthen the paper's contributions.