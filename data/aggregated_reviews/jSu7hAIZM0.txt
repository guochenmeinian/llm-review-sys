ID: jSu7hAIZM0
Title: Preserving Privacy Through Dememorization:  An Unlearning Technique For Mitigating Memorization Risks In Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Dememorization framework aimed at mitigating memorization issues in language models (LLMs) through a reinforcement learning approach. The authors propose using a prefix as input to compute a negative BERTScore, which serves as a reward signal to enhance training. The method is designed to encourage the model to generate continuations that differ from the training data, thereby addressing privacy risks while maintaining model performance.

### Strengths and Weaknesses
Strengths:
- The paper is generally well-written and easy to follow.
- The proposed method shows improved privacy properties compared to the baseline unlearning (UL) method on certain metrics.
- The approach is robust to variations in the number of samples to unlearn and increases in prompting context length.

Weaknesses:
- The methods section lacks sufficient detail and does not adequately highlight the innovations related to the title.
- There is limited comparison with other baselines, primarily focusing on a single UL approach.
- The clarity of presentation is poor, with confusing tables and insufficient explanation of key concepts such as the KL penalty and loss function.
- The absence of theoretical proof or guarantees regarding the effectiveness of the dememorization process raises concerns about the claims made.

### Suggestions for Improvement
We recommend that the authors improve the methods section by providing a comprehensive description of the proposed approach, including mathematical details such as the KL penalty and loss function. Additionally, the authors should clarify the intuition and motivations behind their method, particularly regarding the repeated use of the "12 red street" example. To enhance clarity, we suggest revising the layout of tables and ensuring that all figures are readable. Furthermore, including comparisons against additional unlearning baselines would strengthen the paper's contributions. Lastly, we encourage the authors to address the theoretical aspects of their claims to substantiate the effectiveness of their dememorization framework.