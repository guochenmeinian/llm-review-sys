ID: AHXzIJewwr
Title: An Interpretable Answer Scoring Framework
Conference: ACM
Year: 2024
Number of Reviews: 3
Original Ratings: -2, -1, 1
Original Confidences: 3, 3, 2

Aggregated Review:
### Key Points
This paper presents a composite measure of answer quality as a weighted sum of multiple features, including a knowledge score based on entity matching with a knowledge base (KB), a structure score derived from unspecified presentation-level features, and a content score reflecting coverage of entities in the ground truth answer. The authors propose a heuristic design for weight selection. However, the experimental setup consists merely of average scores on a subset of queries from the YahooQA dataset, lacking an exploration of correlation with human judgment.

### Strengths and Weaknesses
Strengths:
- The proposed interpretable scoring system evaluates answers based on knowledge, content, and structure, enhancing the quality and reliability of answers generated by LLMs.
- The paper emphasizes the importance of explainability in information retrieval systems, providing a transparent evaluation framework.
- The methodology, including the use of a knowledge graph, heuristic features for structure, and entity alignment for content, is well-structured and thoroughly explained.

Weaknesses:
- The presentation has significant gaps, particularly in explaining the design of the Structure Score and the rationale behind weight choices.
- There is no substantial experimental setup to justify the necessity of the measure, nor is there a clear connection to generative retrieval.
- The paper lacks real-world applicability examples, sufficient replication guidance, and does not address potential scalability issues for large datasets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the Structure Score design and provide a more robust justification for weight selection. Additionally, incorporating a comprehensive experimental setup that correlates with human judgment would strengthen the paper. The authors should also include case studies or user feedback to demonstrate real-world applicability and address scalability concerns for practical applications. Lastly, providing clearer implementation details and tools for replication would facilitate adoption by other researchers.