ID: Y4mBaZu4vy
Title: The Importance of Being Scalable: Improving the Speed and Accuracy of Neural Network Interatomic Potentials Across Chemical Domains
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 5, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Efficient Graph Attention Potential (EGAP), a novel architecture for Neural Network Interatomic Potentials (NNIP) aimed at enhancing scalability and efficiency. The authors explore scaling strategies and utilize optimized self-attention mechanisms, evaluating EGAP on the MD22 and OC20 datasets, claiming state-of-the-art performance and improved efficiency over existing models.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a significant challenge in NNIPs by focusing on scalability and efficiency.
2. The model illustration is clear and visually appealing, facilitating understanding of the proposed architecture.
3. The investigation into scaling strategies for NNIP models is valuable for future research.

Weaknesses:
1. Dataset split inconsistency: The authors' use of a 95:5 train-test split on the MD22 dataset deviates from previous works, making results incomparable to baseline models.
2. Inconsistent dataset usage: The claim that the total dataset was used for training and evaluation is misleading, as it reduces the test set size compared to baseline models.
3. Limited experimental scope: The evaluation on the OC20 dataset is restricted to the 2M split of S2EF, neglecting larger splits that would better demonstrate efficiency.
4. Inconclusive performance difference: The performance metrics on the OC20 2M split are unconvincing due to the prioritization of energy over force metrics.
5. Lack of scalability experiments: The absence of experiments demonstrating model performance as size increases undermines scalability claims.
6. Result discrepancies: Reported results do not align with those in previous studies, raising concerns about accuracy and fairness in comparisons.

### Suggestions for Improvement
We recommend that the authors improve the dataset split procedure to align with baseline models, particularly for the MD22 dataset. Additionally, we suggest providing scalability experiments that demonstrate how EGAP performs as model size increases. Testing on larger splits of the OC20 dataset (S2EF-All or S2EF-All+MD) would better showcase efficiency advantages. The authors should clarify discrepancies between their reported results and those in original papers, especially for the MD22 dataset. Lastly, we encourage the authors to justify their claim regarding the minimal impact of dataset size on model performance, given the differing train/test splits compared to baseline models.