ID: uq176Mm0LD
Title: Scene-adaptive Knowledge Distillation for Sequential Recommendation via Differentiable Architecture Search
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: 3, 4, 5, 3

Aggregated Review:
### Key Points
This paper presents AdaRec, a knowledge distillation framework that utilizes differentiable neural architecture search (NAS) to adaptively compress the knowledge of a teacher model into a student model for sequential recommender systems. The authors introduce a target-oriented knowledge distillation loss and a cost-sensitive loss to guide the architecture search and constrain model size. The main contribution lies in the adaptive selection of the student network architecture, differing from existing methods that use fixed structures. The findings indicate improvements in both accuracy and throughput across three real-world datasets.

### Strengths and Weaknesses
Strengths:
- The authors are pioneers in combining knowledge distillation and NAS for sequential recommender systems, allowing adaptive compression of deep models.
- The method is model agnostic, applicable to various deep network architectures, and demonstrates significant improvements in inference speed and accuracy.
- The paper is well-written, with clear methodology and extensive experiments providing valuable insights.

Weaknesses:
- The absence of source code may hinder reproducibility.
- The choice of hyper-parameters for teacher models lacks clarity, and the authors have increased embedding dimensions and self-attention blocks without justification.
- Missing related work section and quantification of additional training time overhead.
- The paper does not include FLOPs in the results, nor does it clarify the hardware used for speed-up quantification.

### Suggestions for Improvement
We recommend that the authors improve reproducibility by providing source code. Clarifying the choice of hyper-parameters for teacher models and investigating AdaRec's performance on models with smaller parameter sizes would enhance the study. Including a related work section and quantifying additional training time overhead would strengthen the paper. We also suggest incorporating FLOPs in Table 2 and detailing the hardware platform used for inference speed-up. Lastly, including Pareto frontier trade-off plots for accuracy versus parameter count/throughput would provide deeper insights into performance constraints.