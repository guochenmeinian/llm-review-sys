ID: PqfPjS9JRX
Title: The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the covariance matrix of transformer networks at initialization, characterizing the limiting distribution through a stochastic differential equation (SDE) in the infinite depth-and-width limit. The authors propose minor modifications to the attention and MLP formulations, resulting in a shaped attention mechanism that enables stable training without normalization layers. Preliminary numerical results suggest that this modified architecture can achieve non-trivial performance. 

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and presents a solid analysis of the self-attention and feedforward blocks, making only minor modifications for tractability.  
- The theoretical contributions are novel and useful for hyper-parameter tuning, with promising experimental evidence supporting the effectiveness of the shaped attention.

Weaknesses:  
- The analysis is limited to network initialization, and the experimental results are not convincing due to the use of only two random seeds, which raises concerns about the reliability of the findings.  
- The shaped attention can yield negative values and values greater than 1, deviating from the proper probability mass functions (PMFs). This raises questions about the normalization property and its implications for practical deployment.

### Suggestions for Improvement
We recommend that the authors improve the experimental validation by conducting more extensive experiments with a larger number of random seeds to confirm the stability and performance of the proposed architecture. Additionally, we suggest exploring alternative formulations for the shaped attention to maintain the normalization property and avoid negative values. Addressing the analysis beyond initialization in future work would also strengthen the contribution. Furthermore, we encourage the authors to clarify how their approach could be leveraged to study training and generalization, particularly in relation to inference challenges.