ID: vjAORqq71s
Title: Newton Losses: Using Curvature Information for Learning with Differentiable Algorithms
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 2, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an alternative backpropagation scheme for deep learning that integrates a preconditioned step on the loss with a gradient step on a least square objective. The authors investigate two preconditioning methods: using the Hessian or the empirical Fisher. Experimental results indicate that the proposed plugin consistently enhances performance across various architectures and losses on two benchmarks. An ablation study on Tikhonov regularization and runtime comparisons are also included.

### Strengths and Weaknesses
Strengths:
- The paper features a robust experimental evaluation, analyzing two relevant benchmarks and considering multiple losses. The ablation studies and runtime comparisons provide a comprehensive view of the algorithm's performance.
- The proposed method shows clear performance gains across different settings, encouraging further research despite some theoretical ambiguities.

Weaknesses:
- The theoretical soundness of the approach is insufficient, lacking proofs or convergence guarantees. The mathematical formulation is confusing, with unclear definitions of variables and properties of the loss function.
- The experimental section is overly focused on problem descriptions rather than detailed comparisons, and there is a lack of convergence figures or methods to address vanishing/exploding gradients.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework by providing proofs or convergence guarantees to enhance the paper's rigor. Clarifying the mathematical formulation, particularly the definitions of variables and properties of the loss function, would also be beneficial. Additionally, we suggest including a more thorough comparison of the proposed methods with existing benchmarks and addressing the lack of convergence figures. Finally, demonstrating the vanishing/exploding gradient phenomena numerically could strengthen the discussion around these issues.