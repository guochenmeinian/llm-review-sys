ID: iN43sJoib7
Title: Are Self-Attentions Effective for Time Series Forecasting?
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 6, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel architecture, the Cross-Attention-only Time Series transformer (CATS), which challenges the conventional use of self-attention mechanisms in Transformers for time series forecasting. The authors propose that self-attention may not be as effective as simpler linear models, enhancing long-term forecasting performance through future horizon-dependent parameters as queries and past data as key-value pairs. Extensive experiments demonstrate that CATS achieves superior performance with lower mean squared error and fewer parameters compared to existing models.

### Strengths and Weaknesses
Strengths:  
- The introduction of CATS offers a fresh perspective on time series forecasting by innovatively removing self-attention mechanisms, enhancing computational efficiency and interpretability.
- The model's reduced parameter count and memory usage make it particularly suitable for large-scale applications.

Weaknesses:  
1. The reliance on an embedding-based cross-attention method lacks novelty and may not significantly advance the field.
2. The paper's expression is difficult to read, particularly in sections like 3.2, hindering comprehension of its contributions.
3. Concepts such as 'parameter sharing across horizons' and 'query-adaptive masking' may be overstated as novel, as they are standard in neural network architectures.
4. The central claim regarding self-attention's effectiveness is not convincingly supported, as the proposed solution modifies existing structures without introducing groundbreaking concepts.
5. Inconsistencies in training settings raise concerns about the fairness of comparisons with baseline models.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, particularly in complex sections like 3.2, to enhance reader comprehension. Additionally, we suggest providing stronger justification for the novelty of 'parameter sharing across horizons' and 'query-adaptive masking' to avoid overstating their significance. To address concerns about the central claim, the authors should consider including supplementary experiments that compare CATS with a broader range of models, including short-term forecasting datasets. Finally, we advise ensuring consistency in training settings to facilitate fair comparisons with baseline models.