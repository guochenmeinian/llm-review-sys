ID: 02Uc0G2Cym
Title: Robustness Guarantees for Adversarially Trained Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the optimization convergence of adversarial training in two-layer neural networks, proposing a reflecting loss to enhance attack strategies. The authors demonstrate that robust training with SGD on linearly separable datasets converges to a robust network in polynomial time. The theoretical analysis includes guarantees on the convergence of adversarial attacks and iteration complexity for training.

### Strengths and Weaknesses
Strengths:  
- The paper presents a clear and rigorous exploration of robust optimization, contributing significantly to the understanding of adversarial training in neural networks.  
- The empirical results complement the theoretical findings and extend to multi-class settings.  
- The proof sketch provides an intuitive explanation of the main result.

Weaknesses:  
- The condition on the attack strength $\nu$ is overly restrictive, limiting the distinction between clean and adversarial training.  
- Empirical results, particularly on CIFAR-10, do not show significant improvements, and the absence of error bars raises concerns about the statistical significance of the findings.  
- The use of the reflected loss function lacks sufficient motivation, and the analysis does not clarify its necessity or potential generalization to networks with bias terms.

### Suggestions for Improvement
We recommend that the authors relax the condition on the attack strength $\nu$ and provide deeper insights into its role and implications. Additionally, we suggest conducting further experiments to verify statistical significance by including error bars and evaluating performance under various attacks, such as CW and AutoAttack. The authors should also clarify the rationale behind the reflected loss function and explore its applicability to networks with bias terms. Finally, we encourage the authors to compare their results with existing literature, particularly the work by Allen-Zhu and Li, to contextualize their findings.