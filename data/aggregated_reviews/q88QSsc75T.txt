ID: q88QSsc75T
Title: Simultaneous Machine Translation with Tailored Reference
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 8
Original Ratings: -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to improve simultaneous machine translation (SiMT) by generating tailored references that accommodate varying latency constraints. The authors propose a "tailor" module, trained using reinforcement learning, to create non-anticipatory references from the ground truth. The approach aims to mitigate the anticipation problem inherent in SiMT models, demonstrating promising results across multiple datasets and policies.

### Strengths and Weaknesses
Strengths:
- The motivation for addressing the anticipation problem in SiMT is clear and relevant.
- Extensive experiments support the main claims, showing improvements in translation quality and reduced anticipation rates.
- The use of a reinforcement learning-based tailor module is a novel contribution that enhances the training process.

Weaknesses:
- The paper lacks clarity in several areas, including the definitions of motivation and the tailor module, and the rationale behind certain design choices.
- Missing baseline comparisons and experimental details, such as parameter settings and training schedules, hinder reproducibility.
- The complexity of the proposed method may not justify the modest improvements observed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation and definitions related to the tailor module. Additionally, it is essential to include relevant baseline comparisons, such as forward-translation and mixed loss approaches, to contextualize the proposed method. We suggest providing more detailed experimental settings, including batch size and learning rates, to facilitate reproducibility. Finally, clarifying the RL fine-tuning process and its implications on translation performance would strengthen the paper.