ID: 9FmolyOHi5
Title: Spike-driven Transformer
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Spike-driven Transformer that integrates the spike-driven paradigm of Spiking Neural Networks (SNNs) into the Transformer architecture, making it suitable for neuromorphic hardware. The authors propose the Spike-Driven Self-Attention (SDSA) module, which replaces traditional multiplication operations with a combination of mask and sparse addition, achieving linear complexity and up to 87.2× lower energy consumption compared to conventional self-attention. The introduction of membrane shortcuts allows spiking neurons to communicate via binary spikes, and the paper demonstrates competitive results across multiple datasets while reducing energy usage.

### Strengths and Weaknesses
Strengths:  
- The introduction of the SDSA module significantly enhances performance and energy efficiency, making the architecture more viable for neuromorphic chip deployment.  
- Extensive experimental results across various models and datasets showcase strong accuracy and energy efficiency.  
- The paper achieves state-of-the-art results on ImageNet while increasing binary spike-based computations in the Transformer.

Weaknesses:  
- The implementation details of membrane shortcuts are not thoroughly explored, raising questions about their necessity and optimization in the time domain.  
- The effectiveness of the spikformer-based framework without membrane shortcuts remains uncertain, as most literature combines both techniques, which weakens the argument for the proposed Transformer’s performance.  
- There is a noticeable performance drop compared to the original Transformer, and the paper lacks detailed analysis on how performance and energy consumption vary with the number of time steps.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the membrane shortcut's implementation on neuromorphic chips and clarify its necessity compared to the SEW shortcut. Additionally, providing results with more time steps and analyzing performance variations as time steps decrease would strengthen the argument for the SDSA's effectiveness. An expanded ablation study that includes combinations of membrane shortcuts with attention-free transformers could further validate the importance of SDSA. Lastly, enhancing the clarity of equations, particularly in the context of binary and floating-point distinctions, would improve the overall presentation.