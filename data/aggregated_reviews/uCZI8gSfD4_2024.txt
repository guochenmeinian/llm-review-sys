ID: uCZI8gSfD4
Title: Training Compute-Optimal Protein Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 6, 7, 7, -1, -1, -1, -1
Original Confidences: 3, 2, 2, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of the scalability of Protein Language Models (PLMs) and aims to derive scaling laws for optimal training configurations. The authors conduct extensive experiments to determine the optimal number of parameters, pre-training dataset size, and transferability between pre-training tasks based on computational budgets. Key findings include the impact of sequence diversity on perplexity, the establishment of scaling laws for both Masked Language Modeling (MLM) and Causal Language Modeling (CLM), and insights into the transferability of models pre-trained on different objectives. The authors validate their findings against state-of-the-art methods, showing improvements in downstream tasks.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written, effectively conveying its main findings and significance.
- It provides a comprehensive analysis of protein language model scaling and optimization of computational resources.
- The derived scaling laws offer valuable insights for determining optimal pre-training configurations for both generative and predictive tasks.

Weaknesses:
- Figure 2's extended x-axis may mislead readers to believe that scaling laws extrapolate beyond the supported range of 10e17 to 10e21 without sufficient data points.
- Sections 4.1 and 4.2 are difficult to follow, requiring multiple readings for clarity.
- Section 5.1 lacks a detailed discussion on the implications of results regarding protein sequence generation compared to state-of-the-art methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Sections 4.1 and 4.2 to enhance reader comprehension. Additionally, we suggest including a discussion section in the main body of the paper to outline key results and their implications, particularly in relation to previous work. This could involve detailing how their approach achieves performance improvements and the significance of their findings. Furthermore, we advise including more decimal places in reported numbers for clarity and addressing the confidence intervals for scaling exponents. Lastly, we encourage the authors to explore diverse sampling strategies in their experiments to provide a more comprehensive evaluation of model performance.