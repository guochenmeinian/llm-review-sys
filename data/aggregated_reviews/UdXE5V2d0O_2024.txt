ID: UdXE5V2d0O
Title: Direct Unlearning Optimization for Robust and Safe Text-to-Image Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for unlearning diffusion-based generative models, specifically targeting NSFW content removal in T2I models. The authors propose an image-based unlearning method that utilizes curated paired image data for preference optimization and introduces a regularization term to maintain the model's denoising capability. Experimental results indicate that the method effectively removes unsafe visual concepts without significant performance degradation on unrelated topics.

### Strengths and Weaknesses
Strengths:  
- The use of SDEdit for curating semantically similar paired images is novel and effective.  
- The model is well-explained, and the paper is well-structured, providing a clear overview of the problem and previous work.  
- The novel perspective of applying preference optimization to the unlearning problem is interesting and promising.  
- The authors demonstrate strong results in evaluations, particularly in defending against adversarial attacks.  

Weaknesses:  
- The main technical contribution is limited to a combination of SDEdit and DiffDPO, and the authors only fine-tuned Stable Diffusion v1.4, neglecting other popular T2I models.  
- The efficacy of the proposed method is heavily dependent on the quality and diversity of the synthesized image pairs, with concerns about overfitting due to a small dataset of only 64 pairs.  
- The explanation regarding the gradient ascent term's impact on the model's denoising ability is insufficient.  
- There is a missing detail in the experimental setup regarding how the authors merge models after applying DUO independently to four violence concept categories.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the explanation regarding the gradient ascent term and its effects on denoising ability, particularly in relation to the L_prior loss. Additionally, we suggest conducting comprehensive ablation studies to explore the impact of different sets of synthesized image pairs on model performance, thereby enhancing the credibility of the findings. Furthermore, incorporating a stronger, white-box attack methodology, such as UnlearnDiffAtk, would provide a more robust evaluation of the unlearned models. Lastly, please clarify the merging process of the four models resulting from the application of DUO to the violence concept categories.