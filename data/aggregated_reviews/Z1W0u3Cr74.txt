ID: Z1W0u3Cr74
Title: Revisiting Logistic-softmax Likelihood in Bayesian Meta-Learning for Few-Shot Classification
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a modification to the logistic-softmax function by incorporating a temperature parameter to enhance confidence in predictions within Bayesian meta-learning for classification tasks. The authors demonstrate that this adjustment not only addresses the confidence issue but also provides both theoretical and empirical advantages, as evidenced by their experiments. Additionally, they employ data augmentation techniques to facilitate a fully analytical mean-field inference method for the Bayesian meta-learning model.

### Strengths and Weaknesses
Strengths:
- The revised logistic-softmax function exhibits improved theoretical and empirical performance, validated through proofs and experiments.
- The potential applicability of the logistic-softmax function with temperature extends to broader research areas, including multi-class classification.
- The paper is well-written, ensuring clarity and ease of understanding.

Weaknesses:
- The performance of Bayesian meta-learning algorithms, particularly in few-shot classification, appears inferior compared to simpler algorithms like ProtoNet, especially when using the same architecture.
- The choice of Convnet as a backbone raises concerns about comparability with recent standard algorithms that utilize ResNet-12.
- The paper does not address critical questions regarding the performance of Bayesian meta-learning with complex training steps in few-shot learning scenarios.

### Suggestions for Improvement
We recommend that the authors improve the paper by addressing the performance comparison of Bayesian meta-learning algorithms with ResNet-12 as the backbone. Additionally, it would be beneficial to explore why Bayesian meta-learning struggles in few-shot learning despite its theoretical advantages. We also suggest that the authors discuss potential applications of the logistic-softmax function beyond meta-learning, as well as consider investigating other settings for their analysis. Furthermore, including a comparison with non-Gaussian processes like neural processes could enhance the paper's depth. Lastly, demonstrating the relationship between temperature variation and accuracy/uncertainty estimation in experiments would provide valuable insights.