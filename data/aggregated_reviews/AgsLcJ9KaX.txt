ID: AgsLcJ9KaX
Title: How do languages influence each other? Studying cross-lingual data sharing during LM fine-tuning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on cross-lingual sharing mechanisms during fine-tuning at the data level, utilizing the TracIn method to assess the influence of training languages on test predictions. The authors analyze how multilingual training data is leveraged by the XLM-R model, revealing that reliance on data from multiple languages increases as fine-tuning progresses. The findings suggest that training samples from other languages can reinforce and complement the knowledge acquired from the test language.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with clear presentation and comprehensive experiments covering various settings.
- It is the first study to adopt a training data-first approach to analyze cross-lingual sharing, providing valuable insights for the multilingual NLP community.
- The results indicate that cross-lingual representations may be more universal than previously suggested, enhancing understanding of multilingual model dynamics.

Weaknesses:
- The study is limited to the XLM-R model, raising concerns about the generalizability of the findings to other models or sizes.
- The computational cost associated with experiments is not clearly justified, potentially limiting the scope of the work.
- The paper could benefit from a more structured presentation, particularly in distinguishing methodology from experimental setup.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the computational cost discussion by including measurements such as estimated FLOP per experiment. Additionally, consider expanding the analysis to include other types of pre-trained language models to strengthen the conclusions. We suggest restructuring the paper to better separate the methodology from the experimental setup, starting with the main approach before detailing evaluation tasks and hyperparameters. Furthermore, please clarify the content of Table 1 and ensure that all figures, particularly Figure 4, are readable and well-presented.