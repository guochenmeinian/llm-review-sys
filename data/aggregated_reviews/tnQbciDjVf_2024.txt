ID: tnQbciDjVf
Title: TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 5, 3, 5, 5, -1, -1
Original Confidences: 5, 2, 4, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents the TransAgent framework, which enhances vision-language foundation models like CLIP through multi-source knowledge distillation from diverse, pre-trained expert models. The framework employs a mixture-of-agents gating mechanism to adaptively integrate knowledge from 11 heterogeneous agents, improving generalization without incurring additional inference costs. Experiments demonstrate its state-of-the-art performance across various visual recognition datasets, particularly under low-shot conditions.

### Strengths and Weaknesses
Strengths:  
1. The paper introduces an original approach to enhancing vision-language models by integrating knowledge from diverse pre-trained experts.  
2. The use of a mixture-of-agents gating mechanism and multi-source distillation is a unique contribution.  
3. Extensive experiments on 11 datasets validate the effectiveness of TransAgent, with detailed ablation studies enhancing understanding of its components.  
4. The writing is clear and well-structured, making the methodology and results easy to follow.  

Weaknesses:  
1. The claim of being the first unified transfer framework for generalizing vision-language models is overreaching, as similar work exists (e.g., CaFo).  
2. Some state-of-the-art methods are not adequately compared, including “PromptKD,” “DePT,” and others.  
3. The knowledge distillation technique appears too straightforward; advanced methods should be explored.  
4. Additional experiments are needed, such as training time comparisons and performance assessments with varying agent numbers.  
5. The novelty of applying knowledge distillation methods to CLIP is questioned, as it lacks significant technical contributions.  
6. Some figures are unclear, and a more organized reference format is required.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims regarding originality, specifically addressing the similarities with CaFo. Additionally, please include comparisons with the mentioned state-of-the-art methods in the related works section. We suggest exploring more advanced knowledge distillation techniques and conducting further experiments, such as training time comparisons and performance evaluations with varying numbers of agents. An ablation study focusing on the individual contributions of VAC, LAC, and MAC components would strengthen the paper. Lastly, we encourage the authors to enhance the clarity of figures and ensure a consistent reference format throughout the paper.