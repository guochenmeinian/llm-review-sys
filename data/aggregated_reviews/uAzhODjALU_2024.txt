ID: uAzhODjALU
Title: The Mamba in the Llama: Distilling and Accelerating Hybrid Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 4, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for distilling transformers into Mamba architectures to enhance inference speed post-training. The authors initialize Mamba states using pretrained transformer components and apply speculative decoding for multi-step predictions. Experimental results indicate that the distilled hybrid model achieves competitive performance compared to the teacher network, Zephyr-7B, after training on only 3B tokens, and outperforms Mamba 7B trained from scratch with 1.3T tokens.

### Strengths and Weaknesses
Strengths:
- The research addresses an important problem of accelerating training for Mamba-based hybrid models.
- The use of distillation from pretrained transformers is a novel approach.
- Results demonstrate that the Mamba-based hybrid model can learn efficiently with fewer iterations and samples.
- The paper is generally well-written with clear figures and an understandable method.

Weaknesses:
- The organization of the writing is poor, with unclear sections and unexplained symbols in equations.
- Some methodological settings are ambiguous, such as the fixed nature of A over time and the distinction between attention and trainable components.
- The experimental design is limited, lacking comparisons with Hybrid Mamba trained from scratch and other SSMs or linear RNNs, with benchmarks primarily focused on chat and academic tasks.
- The paper's scope is narrow, primarily focusing on Mamba without adequately addressing generalization to other models.

### Suggestions for Improvement
We recommend that the authors improve the organization and clarity of the writing, particularly in Section 2.1, by clearly stating derivations and assumptions. Additionally, clarify the methodological settings in Sections 2.2 and 2.3, and enhance Figure 1 with informative explanations. To strengthen the experimental section, include comparisons with Hybrid Mamba trained from scratch and other SSMs or linear RNNs. We also suggest discussing the limitations of focusing solely on Mamba and exploring the applicability of the method to smaller models, as this would provide a more comprehensive understanding of its effectiveness. Finally, please ensure that the speculative decoding algorithm is accurately specified to avoid confusion.