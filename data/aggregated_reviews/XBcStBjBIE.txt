ID: XBcStBjBIE
Title: ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 8, 8, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ChronoMagic-Bench, a novel benchmark for evaluating text-to-video (T2V) models in generating time-lapse videos, focusing on metamorphic amplitude and temporal coherence. The authors propose two new metrics, MTScore and CHScore, and introduce ChronoMagic-Pro, a large-scale dataset comprising 460k high-quality time-lapse video pairs with detailed captions. The dataset construction involves extracting video clips using curated prompts, scene change detection, and quality filters, alongside a comprehensive evaluation of existing models. The study also includes a fine-tuning strategy for the OpenSoraPlan v1.1 model, reporting improvements across all metrics but noting issues with flickering in generated videos, particularly a reduction in CHScore.

### Strengths and Weaknesses
Strengths:
- The introduction of ChronoMagic-Bench addresses a significant gap in T2V model evaluation for time-lapse videos.
- MTScore and CHScore are well-designed metrics that align with human perception, enhancing video quality assessment.
- ChronoMagic-Pro serves as a valuable resource for the research community, offering a diverse set of time-lapse videos.
- The study provides evaluation prompts and training samples for time-lapse videos with significant temporal changes and coherence.
- The adoption of visual tracking models for evaluating temporal coherence is innovative, and the results correlate well with human evaluations.
- The authors have effectively addressed many concerns raised during the rebuttal period, reporting improvements across all metrics after adopting MagicTime's training strategy.

Weaknesses:
- The evaluation is limited to open-source models, potentially overlooking the full spectrum of T2V models.
- The new metrics require further validation across a broader set of models and videos to strengthen their credibility.
- The dataset may benefit from increased diversity in video content and scenarios to better represent real-world complexities.
- The automatic metrics require further discussion and analysis to establish their efficacy.
- Details on systematic data collection are lacking, and the experiments and their analyses need improvement.
- The flickering artifacts in generated videos raise questions about the robustness of the fine-tuning strategy.
- There are minor issues with clarity, including typos and redundant statements, which detract from the overall presentation.

### Suggestions for Improvement
We recommend that the authors improve the visibility of the fine-tuning experiment by moving it from the appendix to the main article, as it is central to the paper's contributions. Additionally, exploring the performance of the fine-tuned model on non-time-lapse prompts could enhance the paper's relevance. A more comprehensive discussion on ethical considerations, including data privacy and potential biases, is necessary. We suggest providing public access to the dataset and implementation details to facilitate reproducibility. 

Furthermore, we recommend that the authors clarify the efficacy of ChronoMagic-Pro, particularly addressing the severe performance degradation observed in CHScore with OpenSoraPlan v1.1. The explanation and analysis for CHScore should be enhanced, including details on the tracking algorithm used and its robustness to the disappearance of tracking points in natural time-lapse videos. The authors should also justify the equal consideration of scales and impacts of each metric in CHScore.

We encourage the authors to provide a more thorough qualitative analysis of the proposed metrics alongside quantitative results to illustrate model performance interpretation. Ensuring the diversity of both ChronoMagic-Bench and ChronoMagic-Pro is crucial for data quality, and the authors should elaborate on the data-preprocessing steps, including the rationale for including low aesthetic score clips.

Additionally, the authors should clarify the fair comparison of experiments, addressing differences in resolutions, frame rates, and durations of generated videos. Human evaluation methods should be improved by expanding the number of prompts used and providing detailed explanations of the evaluators' backgrounds and outlier removal processes. Lastly, the authors should correct the information in the checklist and provide more details regarding video licenses, ensuring transparency about the sources and licensing of the videos used in both datasets.