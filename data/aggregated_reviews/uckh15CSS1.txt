ID: uckh15CSS1
Title: ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ExplainCPE, a novel medical Q&A dataset containing over 7,000 problems from the Chinese Pharmacist Examination. The authors evaluate the performance of several large language models (LLMs), including ChatGLM, ChatGPT, and GPT-4, on this dataset. The results indicate that most LLMs trained on general text perform poorly in this specialized domain, while GPT-4 achieves over 75% accuracy without specific fine-tuning.

### Strengths and Weaknesses
Strengths:
- The creation of ExplainCPE is a significant contribution, providing a new benchmark for evaluating medical LLMs.
- The dataset includes both correct answers and textual explanations, enhancing its utility for verifying LLM knowledge.

Weaknesses:
- The quality of the dataset is not adequately assessed, raising concerns about the correctness of labels and coherence of explanations.
- The evaluation lacks comparisons with domain-specific LLMs, which would better contextualize the benchmark's challenges.
- The paper is perceived more as a resource contribution rather than a novel methodological advancement, which may not align with the NLP application track.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of the dataset's quality by including expert assessments of the correctness and coherence of the labels and explanations. Additionally, we suggest incorporating comparisons with domain-specific LLMs to provide a clearer understanding of the dataset's utility. Furthermore, the authors should detail the sources and methods used for dataset collection to enhance its reliability and usefulness.