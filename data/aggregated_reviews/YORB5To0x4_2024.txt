ID: YORB5To0x4
Title: Universal Jailbreak Backdoors in Large Language Model Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 6, 6, 5
Original Confidences: 3, 4, 3

Aggregated Review:
### Key Points
This paper presents an investigation into the vulnerability of various alignment algorithms to jailbreak attacks in large language models (LLMs). The authors find that all alignment algorithms can be backdoored, with the susceptibility depending on the attack threshold. The study includes a comprehensive set of experiments, particularly highlighting the vulnerabilities of DPO and ORPO.

### Strengths and Weaknesses
Strengths:
1. The authors present intriguing observations on how different alignment techniques respond to poisoning jailbreak attacks.
2. The paper is well presented and fits well with the theme of the workshop.
3. Extensive experiments demonstrate the performance of different alignment methods.

Weaknesses:
1. The paper would benefit from providing more intuition behind the observed results, particularly regarding why IPO perfectly separates aligned behavior from poisoned behavior and what differentiates IPO from other approaches.
2. The analysis is limited to a single model, and extending this analysis across various LLMs would provide deeper insights.
3. The empirical nature of the study lacks conclusive results on robustness; formal statements of the mathematical problems and an analysis of robustness would enhance the paper.
4. It remains unclear how different alignment methods behave under jailbreaking attacks, necessitating an explanation of the parameters in alignment methods that affect robustness.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their findings by providing more intuition behind the results, particularly regarding the separation of aligned and poisoned behaviors. Additionally, we suggest extending the analysis to include multiple large language models to enrich the insights. To address the empirical nature of the study, we encourage the authors to formally state the mathematical problems and analyze robustness. Finally, we advise the authors to clarify how different alignment methods behave under jailbreaking attacks by explaining the relevant parameters that influence robustness.