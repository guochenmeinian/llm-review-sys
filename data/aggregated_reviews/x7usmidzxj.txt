ID: x7usmidzxj
Title: On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the convergence rates of the Adam optimizer under generalized affine variance noise and relaxed smoothness conditions, achieving a convergence rate of \(O(\text{polylog}(T)/\sqrt{T})\). The authors propose two probabilistic convergence rates, demonstrating results comparable to prior work. The study also explores high-probability convergence in non-convex settings, yielding a rate of \(\text{poly}(\log T/\delta)/T\).

### Strengths and Weaknesses
Strengths:  
1. The results are novel and significant, showing high-probability convergence of Adam under relaxed conditions, which is a solid contribution to the field.  
2. The proofs are correct, and the paper is well-structured and clearly presented.  

Weaknesses:  
1. The lack of numerical experiments diminishes the practical validation of the theoretical results.  
2. The contribution may be perceived as weak due to the reliance on previously established techniques, which affects the novelty of the findings.  
3. The choice of \(\beta_2=1-1/T\) may reduce Adam to SGD with momentum, raising concerns about the implications for the analysis.  
4. Some notations and terms, such as \(\nabla f(\textbf{x})\) and the meaning of "right parameter," require clarification.

### Suggestions for Improvement
We recommend that the authors improve the clarity of notations by providing detailed formulas for terms like \(\textbf{g}_t\), \(g(\textbf{x})\), and \(\nabla f(\textbf{x})\). Additionally, addressing the repeated citation of reference [10] in line 118 would enhance the paper's readability. We suggest including comparisons of the main results with related works in Section 5, as this would strengthen the discussion. Furthermore, incorporating simple numerical experiments would bolster the practical relevance of the theoretical findings. Lastly, we encourage the authors to explore the implications of using a smaller \(\beta_2\) and clarify the definitions of terms introduced in the proofs to aid reader comprehension.