ID: 01lhHg8H9p
Title: GLBench: A Comprehensive Benchmark for Graph with Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 6, 8, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GLBench, a comprehensive benchmark designed to evaluate GraphLLM methods across supervised and zero-shot learning scenarios. It aims to standardize experimental settings for fair comparisons among various models, including traditional Graph Neural Networks (GNNs) and Pretrained Language Models (PLMs). The benchmark provides insights into the performance of GraphLLM methods, revealing that LLMs as enhancers generally outperform traditional methods, while their role as predictors often leads to uncontrollable outputs. Notably, the study highlights the absence of a clear scaling law in model performance relative to size and emphasizes the importance of structural and semantic information in zero-shot learning.

### Strengths and Weaknesses
Strengths:
- GLBench is the first comprehensive benchmark for GraphLLM methods, facilitating fair evaluations across diverse datasets.
- The paper provides significant insights into the influence of LLM roles on performance, which could guide future integrations of GNNs and LLMs.
- The writing is clear and the code is well-organized, enhancing usability for further research.

Weaknesses:
- The benchmark lacks a deep exploration of specific challenges faced by different methods, such as the impact of imbalance issues in supervised settings.
- Some experimental conclusions, particularly regarding scaling laws, require more rigorous testing and clearer experimental designs to substantiate claims.
- The paper does not adequately discuss its relation to prior work, missing connections to relevant benchmarks in the field.

### Suggestions for Improvement
We recommend that the authors improve the depth of analysis regarding the specific challenges each method addresses, particularly by exploring how imbalance issues affect model performance at varying levels. Additionally, we suggest that the authors provide more rigorous experiments to substantiate claims about scaling laws, such as maintaining constant pre-training datasets and varying only model size. It would also be beneficial to include a clearer discussion of how GLBench relates to other relevant works in the field. Furthermore, enhancing documentation by aligning models and including missing datasets and experimental codes would significantly improve reproducibility. Lastly, merging Sections 3 and 4 could enhance readability and coherence.