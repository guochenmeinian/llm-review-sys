ID: XYDMAckWMa
Title: Explicit Flow Matching: On The Theory of Flow Matching Algorithms with Applications
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to training flow-based generative models by deriving a conditional flow matching objective function aimed at reducing variance and enhancing stability. The authors propose using a formula for the ground truth marginal velocity field, estimated through self-normalized importance sampling, to regress onto this field. The paper introduces several theoretical contributions, including a tractable form of the FM loss with smaller variance, an explicit integral expression for the vector field related to Flow Matching loss, and derived expressions for the flow matching vector field and score in specific cases. The authors assert that their rigorous analysis of SGD convergence demonstrates better training variance, and numerical experiments indicate improved learning results in fewer steps. However, the contribution is questioned due to similarities with existing work, and the assumption of Jacobian independence is discussed as a simplification that aids in deriving the vector field formula.

### Strengths and Weaknesses
Strengths:  
- The idea of reducing variance in flow matching is a valuable research direction.  
- The paper introduces a novel approach to the FM loss with rigorous variance analysis.  
- It provides a comprehensive family of formulas for the vector field and score, which are not strictly derived in existing literature.  
- The presentation of the proposed method is clear, and the two-batch-size algorithm is highlighted as an effective training strategy, supported by reduced variance in experiments.

Weaknesses:  
- The theoretical contributions can be simplified, and the notation is overly complicated, leading to confusion.  
- The proposed formula is not novel, as it has been previously established in the literature, and some contributions are perceived as minor or well-known, requiring further empirical validation.  
- The paper suffers from poor organization and clarity, making it difficult to follow.  
- The evaluation of the proposed method on tabular data and toy data has been found incorrect or misleading.  
- The authors' communication style could be more direct, particularly regarding the derivation simplifications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their theoretical contributions by simplifying the notation and presentation. Specifically, use standard probability notation instead of index notation to avoid confusion. Additionally, we suggest merging sections that can be consolidated to enhance readability. The authors should also address the limitations of their approach more thoroughly, particularly regarding the bias introduced by using self-normalized importance sampling. Furthermore, we encourage the authors to conduct a more comprehensive empirical study, particularly validating the two-batch-size algorithm and comparing their method against existing approaches in terms of training time and memory usage. Lastly, addressing the necessity of the Jacobian independence assumption more explicitly, correcting minor typographical errors, and ensuring consistent notation throughout the paper would enhance its professionalism.