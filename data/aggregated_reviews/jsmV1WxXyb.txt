ID: jsmV1WxXyb
Title: Statistically Profiling Biases in Natural Language Reasoning Datasets and Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for quantifying models' reliance on potentially spurious features in training datasets, specifically through the identification of biased features such as specific words and sentiment cues. The authors evaluate four models (FastText, ESIM, BERT, RoBERTa) across ten NLI and multiple-choice QA tasks, concluding that newer models are less reliant on dataset cues. They also assess ChatGPT, finding it more agnostic to these cues, with potential for further enhancement through prompt design and chain-of-thought prompting. The main contribution is the ICQ (I-See-Cue) framework, which identifies model-learned biases without additional test cases.

### Strengths and Weaknesses
Strengths:
- The proposed method is reasonable, easy to interpret, and complements previous work, suggesting methodological correctness.
- The extensive experiments across various datasets and models, including ChatGPT, provide valuable insights.
- The paper is well-written and presents a straightforward method with a comprehensive experimental setup.

Weaknesses:
- Claims lack grounding due to the absence of significance tests, raising concerns about the validity of conclusions drawn from limited samples.
- The evaluation methodology is difficult to follow, with unclear terminology and notation, particularly regarding the MSE formula and data split creation.
- The motivation for the specific method design is inadequately justified, and the practical usability of the evaluation technique is not demonstrated due to the absence of source code.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation methodology by providing more detailed descriptions and consistent terminology. Specifically, clarify the term "extracted training data" and the notation used in the MSE formula. Additionally, we suggest including significance tests to support claims made from the results, particularly in Table 3, to enhance the credibility of the findings. Furthermore, we encourage the authors to justify the motivation behind their method design more robustly and to provide source code or an anonymized repository to facilitate reproducibility. Lastly, addressing the computational costs associated with the proposed method would strengthen the paper.