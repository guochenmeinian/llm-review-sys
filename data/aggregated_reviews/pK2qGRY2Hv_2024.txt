ID: pK2qGRY2Hv
Title: The Limits of Transfer Reinforcement Learning with Latent Low-rank Structure
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates transfer in reinforcement learning by exploiting latent structures common across multiple Markov Decision Processes (MDPs). Specifically, it considers M source episodic MDPs with a shared low-rank structure in their transition matrices and one target episodic MDP with a similar low-rank structure. The authors derive minimal sample-complexity bounds for the source MDPs, leading to a regret bound for the target MDP that is independent of the state-action space size. The paper explores various low-rank assumptions, including those related to the Tucker rank of tensors, and establishes a transferability coefficient that aids in deriving sample-complexity lower bounds.

### Strengths and Weaknesses
Strengths:
- The paper presents novel results and a comprehensive exploration of low-rank assumptions on transition matrices.
- The introduction of the transferability coefficient $\alpha$ enhances understanding of transfer dynamics in reinforcement learning.
- The theoretical analyses are rigorous, providing sound foundations for the proposed algorithms.

Weaknesses:
- The paper is lengthy and technical, which may hinder readability; numerous typos and inconsistent notation further complicate comprehension.
- While the work builds on existing concepts, it lacks explicit discussion on how it diverges from traditional approaches, potentially limiting its perceived originality.
- The assumptions required for theoretical results may restrict practical applicability, and the implications of the proposed methods in real-world scenarios are not sufficiently highlighted.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by addressing the following specific points:
1. Clarify the proof of Theorem 1, particularly regarding the notation and the relationship between assumptions and the proof structure.
2. Ensure consistent notation throughout the paper, particularly for terms like the number of episodes and matrix ranks.
3. Provide explicit definitions for terms such as total variation distance and clarify the dimensions of matrices involved in proofs.
4. Highlight practical implications and applications of the proposed methods, including how they could integrate into existing RL systems.
5. Conduct numerical experiments to demonstrate practical performance gains against relevant baselines.
6. Include a notation list in the appendix to aid readability and understanding of the variables used in the analysis.