ID: VAEYN1pGX9
Title: Learning Counterfactual Explanations for Recommender Systems
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Learning to eXplain Recommendations (LXR), a model-agnostic framework for generating counterfactual explanations in recommender systems. LXR learns to identify key user data influencing recommendations and introduces new evaluation metrics for assessing explanation quality. The authors claim that LXR outperforms existing methods in performance evaluation, demonstrating its effectiveness in providing explanations.

### Strengths and Weaknesses
Strengths:
- The proposed method is efficient and model-agnostic, suitable for various recommender systems.
- The framework is well-explained, making it easy to understand.
- The authors provide publicly available source code, enhancing reproducibility.
- The introduction of new evaluation metrics is a valuable contribution.

Weaknesses:
- Some expressions in the text require clarification, and many references are outdated, primarily pre-2020.
- The method's independence from the target item raises concerns about its applicability in real-world scenarios.
- The efficiency of LXR is not thoroughly explored, lacking empirical and theoretical analysis.
- The evaluation metrics may be biased towards LXR's design, and there is no comparison with commercial baseline models.
- The paper lacks human trials to validate the explainability of the proposed metrics.

### Suggestions for Improvement
We recommend that the authors improve the clarity of certain expressions in the text and update the references to include more recent works. The baseline for comparison should preferably include the most recent models implemented in commercial environments. Additionally, we suggest providing a detailed analysis of the time complexity of LXR compared to other baselines and clarifying the motivation behind the method's independence from the target item. The authors should also consider conducting human trials to validate the effectiveness of the new evaluation metrics and address the potential biases in the evaluation process.