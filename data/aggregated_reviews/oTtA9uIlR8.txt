ID: oTtA9uIlR8
Title: Detecting Syntactic Change with Pre-trained Transformer Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to detecting syntactic changes in English texts from the 1830s to the 1990s using a fine-tuned BERT model. The authors demonstrate that their model can distinguish texts from different periods based solely on syntactic information, identify instances of syntactic change, and validate the historical rise of progressive constructions. The methodology includes a series of automatic and manual evaluations, which are well-structured and contribute to the clarity of the research.

### Strengths and Weaknesses
Strengths:
- The paper is straightforward and easy to follow, with a well-motivated problem statement.
- The experiments are clear and conducted at both word and sentence levels, enhancing the paper's interest.
- The approach promises to reduce the need for manual tagging, which is beneficial for historical text analysis.

Weaknesses:
- The paper lacks a concrete NLP application, which is particularly important for an NLP application track.
- The manual evaluation is under-detailed, making replication difficult.
- Some sections, particularly Section 3, are confusing and could benefit from clearer explanations.
- The reliance on a POS tagger trained on modern data raises questions about potential biases in the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 3 by consistently using terminology related to sequence taggers. Additionally, we suggest providing more detailed descriptions of the manual evaluation to facilitate replication. It would be beneficial to include a discussion on the implications of using a tagger trained on modern data for historical texts. Furthermore, we encourage the authors to clarify the significance of the results presented in Section 5.3 and to consider including examples of progressive versus non-progressive constructions in Section 6 to enhance understanding. Lastly, addressing the questions regarding code and data release would strengthen the paper's reproducibility.