ID: v5Un2QqnRf
Title: Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 7, 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Lumen, a large multimodal model (LMM) designed for versatile vision-centric tasks. Lumen employs a decoupled architecture that first learns a shared heatmap representation for various vision tasks, such as object detection and instance segmentation, which is then decoded for seamless task execution. The model also demonstrates strong capabilities in visual question answering (VQA). The authors provide a comprehensive evaluation of Lumen, focusing on its enhanced multimodal comprehension capabilities compared to previous versions and other generalist models. They propose that the improved performance in object detection and generalization tasks validates the value of LLMs in vision-centric applications. Experimental results indicate that Lumen outperforms existing LMMs on multiple benchmarks, showcasing the effectiveness of its proposed method.

### Strengths and Weaknesses
Strengths:
1. The paper effectively motivates the use of a unified heatmap representation to mitigate the inductive bias of individual visual tasks, enhancing visual perception without compromising general-purpose conversational abilities.
2. The two-phase training strategy is reasonable, reinforcing conversational capabilities by jointly tuning with VQA and vision-centric data.
3. The experiments are well-organized, demonstrating Lumen's superior performance on vision-centric tasks and comparable VQA abilities.
4. The authors provide a clear rationale for the experimental results, demonstrating the advantages of Lumen over previous versions and baseline models.
5. The paper addresses the reviewers' concerns with detailed explanations and proposed revisions, indicating a willingness to improve clarity and precision.

Weaknesses:
1. The organization of the paper requires reconsideration; the detailed training recipe is placed in the Appendix, which disrupts the clarity of the methods and results.
2. Inference speed is a concern, as Lumen generates only one heatmap per model forward pass, raising questions about efficiency in dense visual perception tasks.
3. The accuracy of predicting task-specific tokens as routers needs further discussion, particularly regarding the model's ability to adapt to varied task prompts.
4. The comparison with existing LMMs lacks strict alignment due to differing training data and methodologies, which may limit the robustness of the claims.
5. The terminology used, such as "complete vision-language interaction," may lead to confusion and requires rephrasing for clarity.

### Suggestions for Improvement
We recommend that the authors improve the organization of the paper by integrating the detailed training recipe into the main text for better clarity. Additionally, addressing the inference speed issue is crucial; the authors should explore methods to enhance efficiency in evaluating dense visual perception tasks. We also suggest providing more discussions and proofs regarding the accuracy of predicting task-specific tokens and the model's adaptability to different task formats. Furthermore, we recommend that the authors improve the clarity of terminology by rephrasing "specialized datasets" to "meticulously curated specialized datasets" and "complete vision-language interaction" to avoid confusion. Lastly, we suggest that the authors provide a more detailed comparison of training methodologies for LMMs to strengthen their claims regarding generalization and performance.