ID: tSWoT8ttkO
Title: Efficient Recurrent Off-Policy RL Requires a Context-Encoder-Specific Learning Rate
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 3, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RESeL, which enhances recurrent off-policy reinforcement learning (RL) in partially observable Markov decision processes (POMDPs) by applying a lower learning rate to the context encoder. The authors justify this approach through theoretical analysis and demonstrate strong empirical performance across various benchmarks, including classic POMDP tasks, meta-RL, and credit assignment. The paper emphasizes the importance of using different learning rates for the recurrent context encoder and policy networks, arguing that a single learning rate often leads to sub-optimal performance. Additionally, the authors provide a novel perspective on the instability in recurrent off-policy RL, asserting that the primary issue arises from excessively large output variations between consecutive updates of the RNN, rather than gradient instability. They demonstrate that advanced RNN architectures effectively mitigate gradient explosion, and their method offers greater stability than traditional gradient clipping. The theoretical findings indicate that output variation converges to a constant value, allowing for a balanced learning rate tailored for RNNs.

### Strengths and Weaknesses
Strengths:
- The problem of partial observability addressed is crucial and challenging, with significant performance gains demonstrated across a wide range of tasks.
- The paper is well-written and easy to follow, with clear communication of ideas and thorough experimental verification.
- The sensitivity analysis shows that a smaller learning rate improves stability by reducing action variation.
- The paper provides a clear distinction between gradient instability and output variation, enhancing the understanding of training instability in recurrent off-policy RL.
- Experimental results support the claims, showing that the proposed method is more stable than gradient clipping.
- The theoretical analysis offers valuable insights into the convergence behavior of output variations.

Weaknesses:
- The novelty of the proposed solution is questioned, as the idea of using different learning rates for different components is not new and has been previously explored in other works.
- The paper lacks rigorous hyperparameter tuning methods, relying instead on hand-tuning, which is insufficient for a work primarily based on empirical results.
- The theoretical understanding of why a smaller learning rate stabilizes training is not fully developed, and the connection to existing techniques like gradient clipping and truncated gradients is not adequately addressed.
- The scope of the results may not be sufficiently tied to off-policy RL, raising questions about the generalizability of the findings across different paradigms.
- The sensitivity analysis is limited, with results currently presented for only three tasks, which may not fully capture the method's robustness.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their contribution by providing a more comprehensive theoretical analysis of the stability benefits of using a lower learning rate for the context encoder. Additionally, we suggest that the authors conduct a more rigorous hyperparameter selection process, such as a grid search, to validate their findings. It would also be beneficial to include comparisons with existing techniques like gradient clipping and truncated gradients to demonstrate that these methods do not already mitigate the stability issues. Furthermore, we recommend that the authors improve the scope discussion by clarifying the relevance of their findings to off-policy RL compared to other paradigms, such as supervised learning. Finally, we suggest that the authors conduct a thorough sensitivity analysis across a broader range of tasks to better demonstrate the robustness of their method. Addressing these points will strengthen the paper's contributions and applicability.