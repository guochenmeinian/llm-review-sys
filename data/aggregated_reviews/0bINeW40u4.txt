ID: 0bINeW40u4
Title: Eye-gaze Guided Multi-modal Alignment for Medical Representation Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 7, 8, 5, -1, -1, -1
Original Confidences: 2, 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Eye-gaze Guided Multi-modal Alignment (EGMA) framework, which utilizes radiologists' eye-gaze data to enhance the alignment of medical visual and textual features. By synchronously collecting eye-gaze data during diagnostic evaluations, EGMA demonstrates improved generalization and achieves state-of-the-art performance in image classification and image-text retrieval tasks across four medical datasets. The study also explores the impact of varying amounts of eye-gaze data on model performance, showcasing the feasibility of integrating this auxiliary data into multi-modal alignment frameworks.

### Strengths and Weaknesses
Strengths:  
- The innovative use of eye-gaze data provides a fresh perspective on multi-modal learning in medical contexts, diverging from traditional reliance on annotated datasets.  
- The robust experimental design includes comparisons with state-of-the-art methods, effectively demonstrating the efficacy of the proposed framework.  
- Comprehensive visualizations and tables support the paper's claims, and the framework shows strong generalization across different datasets, indicating broader applicability.  
- The paper is well-written, with clear figures and descriptive text, and presents quantitative results that consistently outperform existing methods.

Weaknesses:  
- The reliance on eye-gaze data collection introduces a significant dependency that may limit scalability, particularly in resource-constrained settings.  
- The paper primarily focuses on classification and retrieval tasks; additional evaluations, such as lesion localization or segmentation, could provide a more comprehensive assessment of the framework’s effectiveness.  
- Potential privacy issues related to eye-gaze data are not adequately addressed, and the paper lacks detailed analysis on the impact of different components of the model.  
- The quality of eye-gaze data may vary, and the introduction of potentially noisy information could negatively impact representation learning.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the dependency of eye-gaze data collection, particularly addressing how to mitigate challenges in resource-limited settings. Additionally, including evaluations on lesion localization or segmentation tasks would enhance the assessment of the framework’s effectiveness. We suggest that the authors address privacy concerns more robustly, potentially through de-identification methods or alternative data representations. Furthermore, a more detailed analysis of the impact of varying amounts of eye-gaze data on model performance would provide deeper insights into the framework's robustness and adaptability. Lastly, clarifying the Mean(Max()) operation's role in fine-grained alignment and the motivation for the EGM module would strengthen the paper's technical clarity.