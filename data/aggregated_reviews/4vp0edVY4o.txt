ID: 4vp0edVY4o
Title: Continual Learning with Global Alignment
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive approach to continual learning by addressing task interference in transformer-based models. The authors propose a method called ‘global alignment’ to align data representations using task-specific compositions of pre-trained token representations. They conduct extensive experiments to validate their approach, which includes modifications to the transformer architecture and the introduction of learnable weights in self-attention layers. The results demonstrate the effectiveness of their methods in both task-incremental and class-incremental learning scenarios. Additionally, the authors have provided a rebuttal and shared a link to their code, which has led to most reviewers adjusting their scores upward, indicating that concerns were largely addressed.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to understand, with clear motivation and theoretical justifications for the proposed methods.
- The experiments validate the effectiveness of the proposed methods, showing compelling performance in task-incremental learning settings.
- The authors provide a clear examination of the causes of cross-task interference, enhancing the reader's understanding of the problem.
- The authors effectively clarified the complexities of non-linearity analysis, which was appreciated by reviewers.
- The comparison among Wire-Fixed, Wire-Neigh, and C-LoRA was well-articulated, enhancing understanding.

Weaknesses:
- The analysis of interference does not consider the activation function between layers, which may differ in linear versus non-linear cases.
- The rationale behind using neighborhood tokens and the computational cost of matching K tokens for each sample requires further clarification.
- The contribution of Controlled-LoRA compared to other methods is unclear, particularly regarding its performance and parameter efficiency as task numbers increase.
- One reviewer maintained a lower score, suggesting that not all concerns were fully resolved.
- The writing is at times unnecessarily complex, and some terms are not clearly defined, which may confuse readers.

### Suggestions for Improvement
We recommend that the authors improve the analysis in Section 3.2 by considering the activation functions between layers to differentiate between linear and non-linear cases. Additionally, please clarify the rationale for using neighborhood tokens and discuss the computational implications of matching K tokens. We suggest providing a more detailed comparison of Controlled-LoRA with other methods, particularly regarding its performance and parameter efficiency as the number of tasks increases. Furthermore, we encourage the authors to simplify the writing and ensure that all technical terms are clearly defined to enhance readability. Lastly, we recommend incorporating the comparison between Wire-Fixed, Wire-Neigh, and C-LoRA in the main text for enhanced clarity.