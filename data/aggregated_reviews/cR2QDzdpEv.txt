ID: cR2QDzdpEv
Title: Robust Reinforcement Learning from Corrupted Human Feedback
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a robust reward learning approach formulated as an $\ell_1$-regularized maximum likelihood estimation problem. The authors propose an alternating optimization algorithm that incurs minimal computational overhead compared to standard RLHF methods. The study addresses the issue of potentially corrupted preference labels in human feedback and demonstrates the approach's effectiveness through experiments in robotic control and natural language generation.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and includes comprehensive experiments across two domains.
- The method is straightforward, with a reasonable motivation and theoretical backing for its statistical convergence.

Weaknesses:
- The lack of experiments involving formal RLHF methods, such as Proximal Policy Optimization (PPO), limits the evaluation of the proposed method.
- The use of $\delta$ as a regularization method may seem trivial, and additional experiments on HH-RLHF or Ultrafeedback would strengthen the findings.
- The paper does not adequately discuss the limitations of the proposed method or the implications of using the Anthropic Helpful and Harmless dataset, which may not effectively demonstrate the impact of perturbation.

### Suggestions for Improvement
We recommend that the authors improve the paper by including experiments specifically focused on PPO to provide a more comprehensive evaluation of the proposed method. Additionally, conducting further experiments on HH-RLHF or Ultrafeedback would support the effectiveness of the approach. Clarifying the challenges associated with optimizing the parameter $\delta$ and discussing the potential issues of applying the method to real-world RLHF tasks would enhance the paper's depth. Lastly, we suggest filtering the dataset used in experiments to ensure cleaner results and possibly reporting the ranking accuracy when training DPO to strengthen the findings.