ID: DUFD6vsyF8
Title: Federated Natural Policy Gradient and Actor Critic Methods for Multi-task Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of reinforcement learning (RL) in a federated setting where agents receive local rewards and communicate through a networked graph. The authors propose federated natural actor-critic (NAC) and natural policy gradient (NPG) algorithms, providing convergence rate guarantees in both tabular and linear function approximation settings. The results indicate a relationship between the number of tasks and convergence rates.

### Strengths and Weaknesses
Strengths:  
- The presentation is mostly clear, and the theoretical results are solid, appearing to be the first in the federated setting.  
- The problem is well-motivated, and the convergence results are strong, particularly regarding the dependence on $T$ and the sizes of state and action spaces.  
- The paper provides the first rigorous analysis of the convergence rates for the proposed algorithms, offering insights into the impact of the spectral radius $\sigma$.

Weaknesses:  
- The term "multi-task RL" is misleading; the setting may be better described as "cooperative RL" due to potential misalignment in agents' reward functions.  
- The learning objective function is less general, as it does not adequately address finding local optimal policies for different tasks.  
- The convergence rate of federated NAC appears worse compared to federated Q-learning, raising questions about the underlying reasons.  
- The practical implementation of the mixing matrix $W$ for aggregating neighborhood information is unclear, and further explanation is needed.  
- The decision to place comparisons to related work in supplementary material is questionable, and the claim of novelty regarding global convergence guarantees may not be fully substantiated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the terminology by explicitly using "cooperative RL" instead of "multi-task RL." Additionally, the authors should consider revising the learning objective function to better accommodate local optimal policies for different tasks. It would be beneficial to provide a detailed explanation of the convergence rate discrepancies between federated NAC and Q-learning. We suggest including a discussion on the practical acquisition of the mixing matrix $W$ and providing examples to illustrate its application. Lastly, we recommend moving the consensus error bounds to the main paper to enhance the understanding of the convergence of local policies to the aggregated policy.