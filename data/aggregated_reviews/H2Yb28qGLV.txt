ID: H2Yb28qGLV
Title: Lo-Hi: Practical ML Drug Discovery Benchmark
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 9, 7, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel benchmark named "Lo-Hi" for drug discovery, focusing on two tasks: Lead Optimization (Lo) and Hit Identification (Hi). The authors propose a unique molecular splitting algorithm to address the Balanced Vertex Minimum k-Cut problem and critique existing benchmarks for failing to reflect realistic drug discovery scenarios. They suggest that their proposed datasets, including KDR-Hi, KDR-Lo, and Sol-Hi, better imitate real-life conditions and enhance machine learning applications in drug discovery. The authors find that GNNs excel in hit identification, while SVMs perform better in lead optimization. They also acknowledge dataset limitations and propose future work directions, including synthesizability and the complexities of drug discovery.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant topic for the ML-biomedical community, providing a clear and compelling introduction to the drug discovery process.
- The proposed benchmarks are grounded in real-world practices, enhancing their relevance and applicability.
- The introduction of diverse datasets (KDR-Hi, KDR-Lo, Sol-Hi) improves the evaluation of drug properties.
- The extensive supplementary material demonstrates considerable effort in justifying claims through detailed analysis.
- Clear efforts to improve documentation and accessibility through a README file on GitHub.

Weaknesses:
- The focus on only two target proteins (DRD2 and HIV) limits the benchmark's applicability to the diverse challenges in drug discovery.
- The manuscript lacks a thorough discussion of data limitations and the impact of outliers, which are critical in practical applications.
- Some claims about existing benchmarks are presented without sufficient context or acknowledgment of their potential utility.
- The scope of the benchmark remains narrow, lacking a more automated curation process for broader target inclusion.
- There is a lack of consensus on how to measure synthesizability, complicating the proposed feasibility benchmark.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including references that have previously identified flaws in drug discovery benchmarks, particularly from the ML perspective. Additionally, consider expanding the benchmark tasks to include a broader array of targets, utilizing resources like BindingDB to enhance practical relevance. We suggest that the authors provide clearer instructions for new users regarding dataset splits and validation processes within the manuscript itself, in addition to the README file. Furthermore, investigating the impact of outliers as a distinct task within the benchmark could significantly enhance its utility. Lastly, clarification is needed regarding the specific measures of synthesizability they are considering, particularly in relation to the SA score by Ertl & Schuffenhauer (2009), and outlining a detailed plan for incorporating a feasibility benchmark in the "Next Steps/Discussion" section would strengthen the manuscript.