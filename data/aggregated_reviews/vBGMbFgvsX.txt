ID: vBGMbFgvsX
Title: Going Beyond Heuristics by Imposing Policy Improvement as a Constraint
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 7, 6, -1, -1, -1, -1
Original Confidences: 4, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a constrained optimization approach for reinforcement learning (RL) that incorporates heuristic signals to enhance task performance. The authors propose the Heuristic-Enhanced Policy Optimization (HEPO) method, which ensures that the learned policy outperforms or matches the performance of policies trained solely with heuristic rewards. The method is evaluated on various robotic tasks, demonstrating consistent improvements in performance compared to traditional heuristic methods.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a straightforward and innovative constrained optimization approach that effectively leverages heuristic signals to improve task performance in RL.  
- The method simplifies the reward design process by dynamically balancing heuristic and task rewards without manual tuning, showing significant improvements across diverse tasks.  
- The derivation of the algorithm is presented clearly, and the experiments demonstrate that the proposed algorithm outperforms the baseline in terms of average return.  
- The statistical analysis of the experimental results is rigorous, and the ablation study highlights the importance of using the policy trained on the heuristic reward as a reference.  

Weaknesses:  
- The data collection process is unclear, particularly regarding the equal number of trajectories collected for both policies. The theoretical implications of this data sharing need clarification.  
- There is no theoretical analysis provided for the algorithm, including insights on convergence or a formal guarantee on the convergence rate.  
- The absence of a direct comparison with Extrinsic-Intrinsic Policy Optimization (EIPO) in the first experiment weakens the findings, as does the lack of results in the second experiment that raises questions about the validity of the reported data.  
- The paper does not explore how the performance of the algorithm varies with the quality of the heuristic or include comparisons with other reinforcement learning techniques such as reward shaping and policy cloning.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the data collection process and provide a theoretical analysis of the algorithm, including insights on convergence and the effects of heuristic selection and the alpha parameter. Additionally, we suggest including a direct comparison with EIPO and other reinforcement learning methods, as well as exploring the impact of heuristic quality on algorithm performance. Finally, merging the related work section with section 3.3 could enhance the paper's coherence.