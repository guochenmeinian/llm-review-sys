ID: zMeemcUeXL
Title: FAMO: Fast Adaptive Multitask Optimization
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel multi-task optimization method called FAMO, which aims to mitigate conflicts between task gradients while maintaining computational efficiency. The authors propose a dynamic convex combination of task losses, allowing for a balanced decrease in losses across tasks with minimal computational overhead. They demonstrate that FAMO achieves comparable performance to state-of-the-art methods while being significantly faster, especially as the number of tasks increases. Additionally, the authors introduce an approach to gradient manipulation through amortized computation, claiming to be the first to derive such a method. They clarify that the gradient of \( w \) in equation 10 is derived from the gradient of \( z \) multiplied by the derivative of softmax, with \( 1/\eta \) being subsumed into \( \beta \). The authors emphasize that FAMO considers "relative progress" rather than "absolute progress," which may lead to better outcomes.

### Strengths and Weaknesses
Strengths:
- The paper introduces a method that effectively addresses a known issue in multi-task optimization, providing strong empirical evidence of its performance.
- FAMO is shown to be more efficient than previous gradient manipulation methods, with competitive results across multiple benchmarks.
- The authors provide clear clarifications regarding the methodology and theoretical underpinnings of their approach.
- They acknowledge the limitations of amortized MGDA and articulate the advantages of their proposed method, FAMO.
- The paper is well-structured and clearly written, with a solid connection to existing literature.
- The authors demonstrate responsiveness to reviewer inquiries, enhancing the paper's clarity.

Weaknesses:
- The proposed method closely resembles MGDA, necessitating a more thorough discussion of their relationship.
- The claim regarding FAMO mitigating conflicts between gradients (CG) lacks sufficient theoretical or empirical support.
- Experimental sections lack detail, particularly regarding data splits and hyperparameter tuning.
- The use of training loss for hyperparameter optimization is questioned as potentially misleading, despite the authors' justification for fairness across methods.
- There are reservations about the need for additional results and discussions on non-amortized FAMO to fully understand the proposed ideas.
- The method's sensitivity to hyperparameters is concerning, and additional visualizations of loss and task weights during optimization would enhance clarity.
- Claims regarding FAMO's space complexity being $\mathcal{O}(1)$ are ambiguous and require further justification.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the connection between the proposed method and MGDA to clarify their relationship. Additionally, please provide more details in the experimental sections, including data split sizes and hyperparameter tuning processes. We suggest improving the theoretical and empirical evidence supporting the claim that FAMO mitigates CG. Furthermore, we encourage the authors to reconsider their hyperparameter optimization strategy, as relying on training loss may not accurately reflect generalization performance. It would be beneficial to include figures illustrating the losses and task weights throughout the optimization process. We also suggest including results from the non-amortized FAMO (log-MGDA) in their revised manuscript and providing relevant discussions to clarify the contributions of both approaches. Lastly, we advise the authors to clarify the claims regarding FAMO's space complexity and address the sensitivity of the method to hyperparameters more explicitly, as well as to revise claims about FAMO to avoid presenting it as an equal rate method for all tasks without necessary qualifications.