ID: FXJDcriMYH
Title: Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 6, 7, 7, -1, -1, -1
Original Confidences: 4, 2, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a systematic investigation into model growth techniques for large language models (LLMs), focusing on the depth-wise stacking operator, "Gstack." The authors aim to address three main challenges: the lack of comprehensive evaluation, untested scalability, and the absence of empirical guidelines. They analyze various growing techniques, concluding that stacking layers is the most effective method, supported by extensive experiments and evaluations across multiple NLP benchmarks.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with clear objectives and a comprehensive evaluation framework.
- It presents convincing results that demonstrate the effectiveness of the stacking technique.
- The experiments are thorough and well-designed, providing valuable insights for practitioners.

Weaknesses:
- The motivation behind the objectives is at times insufficient, particularly regarding the differences between decoder-transformer language modeling and BERT pretraining.
- The analysis of diminishing returns and growth timings is superficial, lacking depth compared to existing literature.
- The paper does not sufficiently explore the impact of model growth on final model accuracy or downstream task performance.

### Suggestions for Improvement
We recommend that the authors improve the discussion on why decoder-transformer language modeling differs from BERT pretraining and include comparisons with established results. Additionally, a more detailed analysis of diminishing returns and growth timings is necessary, potentially referencing existing studies that provide insights into these aspects. Finally, incorporating theoretical insights or rationale for the effectiveness of the Gstack operator could strengthen the paper's contributions.