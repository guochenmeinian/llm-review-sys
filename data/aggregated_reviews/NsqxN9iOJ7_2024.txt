ID: NsqxN9iOJ7
Title: Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 7, 5, -1, -1, -1
Original Confidences: 3, 3, 5, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new consistency-based framework for video diffusion model distillation, leveraging adversarial loss to enhance video quality and employing consistency distillation loss in the motion embedding space to effectively learn video motion patterns. The authors propose mixed trajectory distillation to improve alignment between training and inference phases. Experimental results indicate that the proposed approach yields more visually pleasing results compared to previous methods.

### Strengths and Weaknesses
Strengths:  
1. The proposed disentangled motion-appearance distillation is reasonable and effective.  
2. The generated results in Fig. 5 and supplementary materials are very promising.  
3. The quantitative comparisons in Tables 1 and 2 are convincing.  
4. The proposed disentangled motion distillation and mixed trajectory distillation are intuitive and novel.  
5. The experiments are thorough, conducted across various datasets, and demonstrate superior results in video diffusion distillation.  
6. The paper is well-written and easy to follow.  

Weaknesses:  
1. The adversarial loss is not stable; the authors could employ other methods such as perceptual loss.  
2. Motion jittering is observed in the supplementary video, likely caused by the teacher model; the authors could better discuss ways to alleviate it.  
3. In Fig. 6, there is no caption to indicate which results are from the proposed methods versus the designed two-stage baseline, leading to confusion about the differences between the first and second rows.  
4. The introduction of gaps between training and inference distillation inputs is not straightforward.  
5. The introduction to related work needs significant enhancement, particularly regarding the decoupling of appearance and motion, which has many related works.  
6. The conclusion that "learnable representation works the best" lacks specific analysis, which could be beneficial for future research.  

### Suggestions for Improvement
We recommend that the authors improve the stability of the adversarial loss by considering alternative methods such as perceptual loss. Additionally, the authors should provide a more thorough discussion on alleviating motion jittering in the supplementary video. To enhance clarity, we suggest adding captions in Fig. 6 to differentiate between the proposed methods and the baseline. Furthermore, we encourage the authors to clarify the introduction of gaps between training and inference distillation inputs and to significantly enhance the introduction to related work, particularly regarding the decoupling of appearance and motion. Lastly, we advise the authors to include a more detailed analysis of why "learnable representation works the best" to aid future research.