ID: kM5eGcdCzq
Title: The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data Only
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents RefinedWeb, a 5 trillion token dataset derived solely from Common Crawl, which addresses the scalability challenges of curating high-quality corpora for training large language models (LLMs). The authors challenge the conventional belief that a mixture of filtered web data and curated corpora is necessary for effective LLM training, demonstrating that properly filtered and deduplicated web data can match the performance of curated datasets. The paper includes a comprehensive evaluation of RefinedWeb, emphasizing its competitive performance against The Pile in various NLP tasks, despite lacking explicit domain-specific data. The authors argue that the broad nature of web data, combined with effective filtering and deduplication processes, enhances model capabilities. They acknowledge limitations in their evaluation setup for capturing performance in mathematics and coding tasks, asserting that domain-specific data is essential for such areas. Key contributions include the viability of using uncurated web data, methods for creating web datasets for LLMs, and the public release of a 500 billion token subset.

### Strengths and Weaknesses
**Strengths:**
1. The paper demonstrates better scalability by reducing reliance on curated corpora.
2. Models trained on extensive web-only datasets outperform those trained on curated corpora.
3. The release of the RefinedWeb dataset is a significant resource for the machine learning community.
4. The toxicity level of RefinedWeb is comparable to that of The Pile, indicating reasonable content quality.
5. The evaluation process is thorough, including various model training and comparison methods.
6. The paper is well-structured and provides a thorough evaluation of the dataset.
7. It effectively demonstrates that better data quality can lead to substantial improvements in model performance, even in domain-specific tasks.
8. The authors address reviewer concerns with detailed responses and revisions, enhancing the paper's clarity and depth.

**Weaknesses:**
1. The toxicity analysis does not address social biases or harmfulness, leaving room for further research.
2. The claim that models trained on RefinedWeb are equally capable as those trained on curated corpora lacks fine-grained performance breakdowns across evaluation tasks.
3. The evaluation setup does not adequately capture performance in mathematics and coding tasks.
4. The authors' rationale for the limited public release of the dataset could benefit from further elaboration.
5. The reasoning for releasing only a 600 billion token sample instead of the full dataset is not clearly articulated.

### Suggestions for Improvement
1. We recommend that the authors improve the bias analysis to include a deeper examination of social biases and harmfulness within RefinedWeb.
2. We suggest including a fine-grained breakdown of performance on each evaluation task to substantiate claims regarding model capabilities.
3. We encourage the authors to clarify the reasoning behind the partial release of the dataset and consider discussing potential resource constraints or commercial concerns openly.
4. We recommend improving the discussion on domain-specific performance by explicitly stating the necessity of incorporating coding and mathematics datasets for adequate model training.
5. We suggest including a contextual analysis of the compute and data savings associated with small performance gains in the main text to enhance the understanding of the results.
6. We encourage the authors to include results from larger models and discuss the implications of filtering and deduplication on performance, particularly in domain-specific tasks.