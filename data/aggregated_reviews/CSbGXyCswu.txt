ID: CSbGXyCswu
Title: Fine-Grained Human Feedback Gives Better Rewards for Language Model Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 7, 7, 6, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents *FINE-GRAINED RLHF*, an extension of reinforcement learning from human feedback (RLHF) that integrates more granular human feedback into language model (LM) training. Traditional RLHF typically utilizes scalar rewards for entire sequences, which can lead to reward sparsity and lack of insight into specific problematic areas. The authors propose training separate reward models for different feedback categories, generating feedback at a lower granularity, such as sentence-level. They evaluate this approach on detoxification and long-form question answering tasks, demonstrating that fine-grained RLHF outperforms traditional RLHF and supervised baselines in both automatic and human evaluations. The authors introduce a novel dataset, QA-Feedback, with fine-grained human feedback, and explore the customization of LM behaviors through weighted combinations of reward models.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and provides a thorough analysis of fine-grained RLHF, addressing a significant issue in RLHF related to reward sparsity.
- The proposed framework is intuitive and easy to implement, with potential implications for improving controllability in RLHF.
- The introduction of a new dataset for QA with fine-grained feedback enhances the research community's resources.

Weaknesses:
- The study is limited to a single LLM, and validation across other models would strengthen the findings.
- The detoxification task relies on closed-source APIs for evaluation, which may not provide an elegant setup; a human evaluation approach is recommended.
- The multi-reward aspect introduces complexity in weighting different feedback categories, and further analysis on optimal weighting strategies is needed.
- The size of the training dataset is relatively small, which may limit the robustness of the conclusions drawn.

### Suggestions for Improvement
We recommend that the authors improve the experimental validation by testing their approach on additional LLMs, such as LLaMA, to enhance generalizability. For the detoxification task, consider replacing the closed-source API evaluation with a more controllable generation task that incorporates human evaluation. Additionally, we suggest conducting a more detailed analysis on how to select weightings for the multi-reward model, potentially including a Pareto analysis and visualizations of trade-offs in terms of human reward. Lastly, addressing the limitations related to dataset size and computational complexity would strengthen the paper's contributions.