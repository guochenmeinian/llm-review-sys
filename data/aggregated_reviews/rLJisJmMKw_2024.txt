ID: rLJisJmMKw
Title: GenWarp: Single Image to Novel Views with Semantic-Preserving Generative Warping
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 6, 6, 4, 6, -1
Original Confidences: 3, 4, 3, 3, -1

Aggregated Review:
### Key Points
This paper presents a novel framework for novel view synthesis from a single image, leveraging a generative process that incorporates self-attention and cross-view attention mechanisms conditioned on warped coordinates derived from a monocular depth estimate. The authors propose a flexible architecture that avoids traditional warping and inpainting methods, achieving superior performance in generating high-quality novel views, particularly in challenging scenarios. The method demonstrates better FID, PSNR, and LPIPS scores than existing baselines on datasets like RealEstate10k and ScanNet.

### Strengths and Weaknesses
Strengths:
- The manuscript is well-written, with a comprehensive related work section and a straightforward methods section.
- The proposed approach effectively addresses issues of artifacts from depth errors and preserves semantic details during generation.
- Experimental results are robust, showing the method's effectiveness across various scenarios.

Weaknesses:
- The paper lacks apples-to-apples ablations, particularly comparing the proposed method with alternatives that do not use depth information or warping.
- Evidence for the claim that cross-attention effectively attends to corresponding points is limited, with only one example provided.
- The SD warp baseline could be improved to avoid unrealistic artifacts, such as black borders in generated images.
- The methodology section could benefit from additional diagrams and visualizations to clarify the workflow and attention mechanisms.
- The proposed depth-warping embedding may struggle with large camera movements or occlusions, potentially limiting its effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the paper by conducting apples-to-apples ablations to compare the proposed method with alternatives lacking depth information or warping. Additionally, we suggest providing more examples to substantiate the claim regarding cross-attention's effectiveness in attending to corresponding points. The authors should also consider enhancing the SD warp baseline to mitigate unrealistic artifacts and include more visual aids in the methodology section to clarify the process. Finally, a discussion on the limitations of the depth-warping embedding in handling large viewpoint changes would strengthen the paper.