ID: SyEwsV52Dk
Title: Evaluation Metrics in the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence to Sequence Tasks
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a hybrid evaluation of recent LLMs on three NLP tasks: text simplification, text summarization, and grammatical error correction. The authors find discrepancies between automatic metrics and human evaluations, arguing that automatic metrics are unreliable for system development. They demonstrate that rankings induced by ChatGPT as an evaluator align more closely with human judgments. The study highlights that model outputs often surpass gold references in quality.

### Strengths and Weaknesses
Strengths:
- The paper provides significant insights into the inadequacy of automatic evaluation metrics for LLM outputs and establishes GPT-4 as a competent evaluator.
- It presents a systematic evaluation across multiple LLMs, contributing valuable data for future research.

Weaknesses:
- The evaluation scale of 100 examples is limited, which undermines the strength of the claims made.
- The paper lacks detailed descriptions of experimental settings, including how GPT-4 was prompted and the overlap of samples between human and GPT-4 evaluations.
- There is insufficient analysis of inter-annotator agreement, and the novelty of the findings is questioned due to prior work on similar issues.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limitations of human evaluation as a gold standard, alongside the noted biases in model selection based on automatic metrics. Additionally, please clarify the experimental settings, including the instructions given to evaluators and the overlap of samples rated by GPT-4 and human evaluators. It would also be beneficial to analyze the agreement between GPT-4 and human annotators based on specific score ranges. Finally, consider addressing the ethical implications of using LLMs for evaluation in future research.