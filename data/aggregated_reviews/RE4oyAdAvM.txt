ID: RE4oyAdAvM
Title: Domain Adaptation for Conversational Query Production with the RAG Model Feedback
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for domain adaptation in conversational query generation, building on prior work that utilizes reinforcement learning (RL) with nonparametric retrieval signals. The authors propose a new reward function and implement two strategies to enhance stability in the RL training process. The adaptation pipeline consists of training a query generator on the source domain, training a RAG model on the target domain, and then training the query generator on the target domain using RL, where the reward is based on the parametric retrieval score from the RAG model. The main contribution is the introduction of the new reward function.

### Strengths and Weaknesses
Strengths:
1. The authors propose a more accurate RL feedback mechanism for query generation, reducing reliance on human annotations.
2. The paper includes detailed ablation studies supporting the effectiveness of the proposed strategies for training stability.
3. The experiments cover both English and Chinese, demonstrating thoroughness in various settings.

Weaknesses:
1. The proposed method lacks clarity and detail in certain areas, such as the processing of search results and the determination of pseudo queries from the source generator.
2. The evaluation of query generation on the target-domain dataset is insufficient, relying on manually annotated instances that lack reliability and do not robustly demonstrate effectiveness.
3. There is limited comparison with other RL methods, particularly those that utilize end-to-end performance as feedback.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proposed method by providing detailed steps for processing search results and determining pseudo queries. Additionally, the authors should enhance the evaluation methodology by ensuring the reliability of annotations and exploring additional RL methods for comparison. It would also be beneficial to include more comprehensive discussions on future directions and to address the novelty of the work by considering the integration of in-context learning methods and the latest LLMs in their experiments.