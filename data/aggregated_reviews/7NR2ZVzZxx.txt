ID: 7NR2ZVzZxx
Title: LogicBench: A Benchmark for Evaluation of Logical Reasoning
Conference: NeurIPS
Year: 2023
Number of Reviews: 21
Original Ratings: 5, 7, 6, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the LogicBench dataset, which encompasses propositional, first-order, and non-monotonic logics. Each data sample consists of triplets (context, question, answer) generated through GPT-3 and rule-based methods. The dataset aims to address the limitations of existing datasets by including a broader range of logic types and demonstrating through experiments that LogicT5, trained on LogicBench (Aug), outperforms T5-Large on LogicBench (Eval). The authors propose that using a consistent model architecture allows for a clearer understanding of the benefits derived from LogicBench. They report separate accuracies for different logical conclusions to illustrate model behavior effectively. The paper also discusses the quality of the dataset, emphasizing a "human-in-loop" approach for validating logical formations and grammar. Additionally, the authors have made updates to the README and clarified terminology in their tables.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and easy to follow.
- It introduces a novel QA dataset that systematically tests reasoning patterns, which is valuable for diagnosing errors in LLMs.
- The dataset includes non-monotonic reasoning, marking it as a significant advancement over existing benchmarks.
- The authors provide a thorough explanation of the dataset generation process and the validation of logical formations.
- The paper includes updates to the README and clarifications on terminology, enhancing clarity.
- The experiments demonstrate the potential of LogicBench to improve logical reasoning in downstream tasks.

Weaknesses:
- The experiments could benefit from comparisons with models trained on existing logic data.
- Some results, particularly in Table 5, lack clarity regarding the overall distribution of "Yes" and "No" responses.
- The dataset generation process may lead to variations in answers based on common sense, which is not adequately addressed.
- The paper's reliance on template-based data creation raises concerns about the authenticity of the logical reasoning capabilities being tested.
- The reported performance on LogicBench is only slightly above random accuracy, indicating potential limitations in model effectiveness.
- There is a lack of exploration into the subjective nature of answers that may vary according to common sense, which could enrich the dataset.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including comparisons with models trained on existing logic datasets. For Table 5, providing overall results or the ratio of "Yes" and "No" would enhance understanding of data distribution. Additionally, showing error bars on experimental results would clarify the variability in performance. The authors should also analyze the factors influencing dataset size and diversity, and clarify the meanings of "single-task" and "multi-task" in Table 6. Consistency in terminology, such as using "First-order" instead of "First Order," should be maintained throughout the paper. We suggest that the authors improve the dataset generation process by reformulating template-based inputs into human-like natural language to better assess LMs' understanding and reasoning capabilities. Furthermore, we encourage the authors to explore the implications of using negative instances in their logical inference rules to enhance the robustness of their findings. Lastly, addressing the concerns regarding the performance of LLMs in the context of Chain of Thought (CoT) reasoning would strengthen the benchmark's credibility and provide a more detailed discussion on limitations and potential societal impacts.