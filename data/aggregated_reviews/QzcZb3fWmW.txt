ID: QzcZb3fWmW
Title: Emergence of Shape Bias in Convolutional Neural Networks through Activation Sparsity
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: 7, 7, 7, 8
Original Confidences: 3, 3, 4, 3

Aggregated Review:
### Key Points
This paper presents a method to address the texture bias in deep neural networks (DNNs) by enforcing activation sparsity through a top-k selection mechanism. The authors demonstrate that this approach enhances shape sensitivity in neural representations, leading to improved performance in various tasks, including recognition and synthesis. The results indicate that the top-k operation can induce shape bias even without training, and it shows promise in applications such as style robustness and few-shot image synthesis.

### Strengths and Weaknesses
Strengths:
- The demonstration that shape encoding occurs with the proposed sparsification, even without training, is a significant contribution.
- The method is novel and effectively addresses a challenging problem in DNNs, showing good structural encoding capability and parts/subparts learning dynamics.
- The paper is well-presented, with clear explanations and a variety of compelling examples supporting the claims made.

Weaknesses:
- Some editorial improvements are needed, such as clarifying which numbers are bold in tables and refining certain text descriptions.
- The evaluation could benefit from larger-scale experiments and quantitative analyses, particularly in relation to existing solutions and benchmarks.
- It is unclear whether the top-k operation is applied uniformly across all layers, and additional comparisons with other methods would enhance clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their presentation by addressing editorial issues, such as the formatting of tables and refining text descriptions. Additionally, including experiments with ResNet variants and discussing the application of top-k operations to Vision Transformers would strengthen the paper. We also suggest exploring the impact of non-top-k units on experimental results and considering the application of different k values depending on layer depth to further investigate the hypothesis on sparsity rates.