ID: YYJojVBCcd
Title: Fairness without Harm: An Influence-Guided Active Sampling Approach
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 3, 4, 5, 6, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents Fair Influential Sampling (FIS), a method designed to enhance the Pareto Frontier of the fairness-accuracy tradeoff by sampling training data. The authors propose that FIS can maintain fairness while preserving accuracy by validating training data examples against a validation dataset, which does not require sensitive attribute annotations. The algorithm selects training data based on its influence on accuracy and fairness, claiming that it can reduce group gaps and mitigate distribution shifts. Theoretical analysis supports the assertion that acquiring additional data can improve fairness without degrading accuracy.

### Strengths and Weaknesses
Strengths:
- The proposed FIS method effectively addresses group fairness while safeguarding privacy by not requiring sensitive attribute annotations for the training dataset.
- The theoretical contributions are robust, providing upper bounds for generalization error and risk disparity, which enhances the credibility of the method.
- The approach is applicable across various sectors, as it does not rely on a single sensitive variable, thus broadening its usability.

Weaknesses:
- The success of FIS is contingent on the quality of the validation set; a poorly representative validation set may compromise accuracy and fairness.
- The reliance on specific influence measurements necessitates a thorough exploration of their sensitivity to different data distributions.
- The paper lacks a comprehensive discussion of fairness metrics, particularly the rationale behind selecting risk disparity as the primary metric.
- There are inconsistencies in the reported datasets used for experiments, and the method's dependence on a clean validation dataset poses practical challenges.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the choice of fairness metrics, particularly clarifying why risk disparity is selected and how it compares to other metrics like equalized odds. Additionally, the authors should address the challenges of obtaining a clean and informative validation set in real-world scenarios, including potential biases. It would be beneficial to provide empirical validation of the accuracy claims alongside fairness metrics and to clarify the data selection strategy in relation to the validation set. Lastly, we suggest enhancing the clarity of the presentation, particularly in Table 1, by defining fairness violation and explaining the highlighting scheme.