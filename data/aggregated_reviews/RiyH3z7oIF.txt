ID: RiyH3z7oIF
Title: Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 5, 5, 5, 5, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 3, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents two algorithms, Delayed-PSVI and Delayed-LPSVI, for learning optimal policies in linear Markov Decision Processes (MDPs) under delayed feedback. The authors aim to learn posterior distributions for value functions, incorporating information from both state transitions and rewards, while addressing challenges posed by weaker posterior concentration due to delays. Delayed-LPSVI employs noisy gradient descent for computational efficiency, and both algorithms achieve sublinear regret relative to the time horizon, which scales linearly with the mean feedback delay. Empirical results indicate that these algorithms yield higher returns more quickly compared to a modified optimism-based algorithm. Additionally, the authors introduce Langevin Monte Carlo (LMC) methods to enhance computational efficiency and tackle intractable posteriors in the presence of delays.

### Strengths and Weaknesses
Strengths:
- The paper addresses the under-explored topic of delayed feedback in reinforcement learning, providing thorough analysis and empirical evaluation of the proposed algorithms.
- The algorithms are reasonable and simple to implement, with theoretical results that include optimal worst-case regret bounds in terms of dimension and time horizon.
- The structured approach to analyzing key technical challenges related to delays in reinforcement learning is effective, and the incorporation of LMC methods is a significant contribution that bridges a gap in existing literature regarding approximate Bayesian inference.

Weaknesses:
- The motivation and logistics of the specific delayed feedback setting are unclear, particularly regarding how the agent can control feedback when all data from a trajectory is hidden until a random future time.
- The rationale for the delayed setting, especially the restriction on using state transition data for updates, remains unclear and may limit practical applicability.
- The distinction between the proposed algorithms and existing methods like [L]PSVI is not adequately justified, raising questions about the unique contributions of the new algorithms.
- The experimental setup lacks comparisons with other state-of-the-art methods that handle delayed feedback, and the examples provided for real-world applications do not convincingly illustrate the necessity of modeling delayed state transitions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation for the delayed feedback model and provide a more intuitive understanding of how it allows for feedback control. Additionally, we suggest that the authors clarify the rationale for the delayed setting, particularly the constraint on using state transition data for updates, to enhance understanding of its relevance. A rigorous justification of the baseline algorithm used for comparison and a clearer distinction between their approach and existing algorithms like [L]PSVI would strengthen the paper. Furthermore, we encourage the authors to refine their examples to better align with the proposed model, ensuring they accurately represent the challenges of delayed feedback in practical scenarios. Finally, including comparisons with other algorithms designed for delayed feedback in the experimental section would enhance the empirical contributions and robustness of the paper.