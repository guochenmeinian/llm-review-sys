ID: xkljKdGe4E
Title: Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification
Conference: NeurIPS
Year: 2024
Number of Reviews: 24
Original Ratings: 8, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a reassessment and comparative analysis of classic Graph Neural Networks (GNNs) and Graph Transformers (GTs) in the node classification task. The authors demonstrate that with proper hyperparameter tuning, GNNs can outperform state-of-the-art GTs across various datasets, asserting that their results represent the best performance on standard datasets. They emphasize that their study focuses on assessing GNNs rather than benchmarking GTs, ensuring a fair comparison by utilizing the same hyperparameter search space and training environments for both models. Extensive empirical results and ablation studies yield significant insights into GNN hyperparameters, including normalization, dropout, and residual connections.

### Strengths and Weaknesses
Strengths:
- The paper effectively shows that classic GNNs can achieve competitive performance in node classification with appropriate hyperparameter tuning.
- Robust evidence is provided that GNNs can outperform GTs across multiple datasets, highlighting their potential.
- Extensive ablation studies and detailed experimental results strengthen the validity of their findings and provide valuable insights into the impact of various GNN configurations.
- The evaluation includes a diverse set of datasets, enhancing the robustness of the findings.

Weaknesses:
- All results are presented numerically, lacking visualizations that could enhance readability.
- The experimental setup raises concerns regarding fairness in comparisons, as the tuning procedures for GTs were not clearly aligned with those for GNNs, and GTs have not been exhaustively tuned.
- There is a lack of statistical significance analysis regarding the performance differences between GNNs and GTs.
- Insights into why GNNs outperform GTs, particularly in the context of specific architectures like GraphGPS, are not sufficiently explored.

### Suggestions for Improvement
We recommend that the authors improve the paper by including visualizations of classification results for GNNs and GTs to enhance clarity. Additionally, the authors should ensure that the hyperparameter tuning for GTs is explicitly described and aligned with that of GNNs to facilitate fair comparisons. We suggest conducting a more thorough tuning of GTs to ensure a rigorous evaluation against GNNs. Furthermore, the authors should provide statistical significance analyses to substantiate their claims of GNN superiority. Finally, we encourage the authors to include more insights into the architectural differences and performance implications of GNNs versus GTs, particularly regarding the integration of GNNs into GT frameworks like GraphGPS.