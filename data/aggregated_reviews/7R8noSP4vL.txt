ID: 7R8noSP4vL
Title: Tempo Adaptation in Non-stationary Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 5, 4, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Proactive Tempo-control Model-based (PTM) framework for reinforcement learning (RL) in non-stationary environments, addressing the adaptation tempo alongside the classical exploration-exploitation trade-off. The authors propose two variants: PTM-T, which employs natural policy gradient and achieves sublinear dynamic regret, and PTM-G, which demonstrates empirical superiority over four existing baselines in Mujoco tasks. Additionally, the authors interpret non-stationarity in terms of wall-clock time rather than episode time, arguing that this perspective aligns better with real-world applications. They discuss the optimal number of policy iterations in relation to a time budget \( T \) and the variable \( K \), asserting that the optimal number of iterations remains constant regardless of the time budget, influenced by the environment's dynamics. The authors acknowledge the complexity of their theoretical analysis and writing, aiming to clarify these aspects in the camera-ready version while emphasizing the importance of both performance comparisons and component significance in their experiments.

### Strengths and Weaknesses
Strengths:
- The concept of tempo adaptation in non-stationary RL is novel and relevant for practical applications.
- The framework introduces a unique perspective on non-stationarity, which could have significant implications for real-world applications.
- The theoretical analysis is solid, providing rigorous guarantees for the proposed methods, and the authors provide clear explanations regarding the relationship between \( K \) and \( T \).
- Comprehensive experimental evaluations demonstrate the framework's effectiveness against various baselines, supported by relevant experiments and examples.

Weaknesses:
- The writing lacks clarity, making it difficult to appreciate the contributions fully. Key theoretical results are not clearly stated, and the main text is not self-contained, deferring important definitions and algorithm descriptions to appendices.
- The theoretical analysis is complex and difficult to follow, which may impede understanding of the main contributions. The problem formulation is unclear, particularly regarding the trade-off between data collection and policy optimization, and its relation to standard non-stationary RL setups.
- The assumption regarding the non-stationarity variable and the notation used throughout the paper are overly complex, hindering reader comprehension. Some reviewers feel that the authors have not fully addressed all concerns, impacting the overall acceptance of the paper.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing by clearly stating the main theoretical results in a consolidated theorem and ensuring that essential definitions and algorithm descriptions are included in the main text. Additionally, we suggest that the authors clarify the trade-off between data collection and policy optimization, and better explain the relationship of their formulation to existing non-stationary RL frameworks. Simplifying the notation and reducing the complexity of mathematical symbols would enhance readability. We also recommend that the authors improve the clarity of the theoretical analysis by reducing the number of notations and organizing the content more straightforwardly. Furthermore, we encourage the authors to provide a discussion on why non-stationarity is modeled only at the inter-episode level, especially given the novelty of their wall-clock-time approach, and to clarify the variable nature of \( K \) in their results. Finally, we suggest that the authors refine the presentation of their mathematical notations and theorems to enhance accessibility in the final version.