ID: 67o9UQgTD0
Title: Counterfactual Memorization in Neural Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 6, 7, 8, 6, 6, 6, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel concept of "counterfactual memorization," which quantifies the change in a model's predictions when a specific training example is omitted. The authors conduct experiments on three standard text datasets, revealing that rare memorized examples exist and that there is an inverse correlation between the number of duplicates and counterfactual memorization. The study extends this concept to "counterfactual influence," assessing how memorized examples impact predictions during inference. Additionally, the authors explore memorization in language models, distinguishing it from generalization and learning. They argue that existing literature often conflates memorization with learning, failing to define memorization distinctly. The paper also addresses ambiguities in image classification, highlighting that multiple correct labels can exist for the same image, similar to language modeling scenarios. The paper is well-structured, with clear definitions and insightful findings.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a significant exploration of memorization in neural language models.
- The definitions of counterfactual memorization and counterfactual influence are original and provide useful metrics for future research.
- The empirical methodology is sound, with adequate sensitivity analysis and clear mathematical formulations.
- The paper contributes valuable insights to the memorization literature by clarifying the distinction between memorization and generalization.
- It acknowledges the complexities of ambiguity in both image classification and language modeling, adhering to established metrics like top-1 accuracy and validation perplexity.
- The authors engage constructively with reviewer feedback, enhancing clarity on the topic of memorization.

Weaknesses:
- The research problem lacks a crisp definition, and the overall per-token accuracy metric may not adequately reflect the memorization of sensitive information.
- There are technically mistaken statements regarding counterfactual memorization, and the definitions of "common" and "rare" information are unclear.
- The experiments lack robustness, and the paper does not address related work in causal inference, which could enhance the research context.
- The definition of memorization based on 0-1 loss may not encompass all scenarios in language modeling, potentially oversimplifying the concept.
- The paper does not sufficiently address the implications of fully memorized models, which could limit the depth of the discussion on memorization.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the research problem and provide a more sophisticated metric for measuring memorization, particularly regarding sensitive information. It is essential to clarify the definitions of "common" and "rare" information and to ensure that the term "counterfactual memorization" is used consistently. We suggest including a section on related work to contextualize the findings within existing literature, particularly in causal inference. Additionally, the authors should consider experimenting with different backbone language models to broaden the applicability of their findings. We also recommend improving the clarity of the introduction by explicitly defining the types of memorization studied in the paper. Furthermore, consider expanding the discussion on the limitations of the 0-1 loss metric in capturing all aspects of memorization, particularly in the context of fully memorized models. Lastly, we encourage the authors to proofread the manuscript to correct typographical errors and enhance overall presentation.