ID: YrAxxscKM2
Title: Why Do We Need Weight Decay in Modern Deep Learning?
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 6, 5, 7, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the role of weight decay (WD) in modern deep learning, distinguishing its effects in over-training and under-training regimes. The authors propose that WD enhances implicit regularization in the over-training regime by controlling the noise scale of stochastic gradient descent (SGD), thereby improving generalization. In the under-training regime, WD modifies the effective learning rate, enhancing training stability, particularly in low-precision settings like bfloat16. The study provides a unified perspective on WD's impact on optimization dynamics rather than merely acting as a regularizer. Additionally, the authors present experiments validating their conjecture regarding snapshot ensembles and their alignment with the Exponential Moving Average (EMA) in function space, conducted within the ResNet18 on CIFAR-10 setting. The results indicate strong alignment in test accuracies and low Total Variation Distance (TVD) across various learning rates (LR) and weight decay (WD) combinations, with plans to extend these experiments to TinyImageNet in the revised manuscript.

### Strengths and Weaknesses
Strengths:  
- The paper offers valuable insights into WD's influence on optimization dynamics, moving beyond traditional views of regularization.  
- It effectively distinguishes between over-training and under-training regimes, revealing WD's varying roles in different contexts.  
- The experimental design supports the theoretical claims, demonstrating consistency between predictions and observations.  
- The additional experiments provide strong empirical support for the authors' conjecture, demonstrating a convincing alignment between snapshot ensembles and EMA.

Weaknesses:  
- The theoretical analysis relies heavily on approximations and lacks formal proofs, leaving room for inaccuracies.  
- The study focuses primarily on specific architectures like ResNet and GPT-2, limiting the generalizability of the findings.  
- There is insufficient guidance for practitioners on tuning WD across different model architectures or datasets.  
- The experiments were limited to CIFAR-10 due to time constraints, which may restrict the generalizability of the findings.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework by providing formal mathematical proofs for the proposed conjectures to enhance the robustness of their claims. Additionally, conducting experiments on a broader range of model architectures, including results from the TinyImageNet dataset, would strengthen the generalizability of the results. We suggest clarifying the assumptions made in the analysis, particularly regarding the constancy of the first derivative across data points and the implications of using WD in conjunction with other regularization techniques. Finally, enhancing the readability of figures and ensuring consistent terminology throughout the paper would improve overall presentation.