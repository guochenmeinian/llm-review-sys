ID: 3Yrfx7oYMF
Title: Instruction Embedding: Latent Representations of Instructions Towards Task Identification
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel task of instruction embedding aimed at identifying tasks from instructional samples, which supports downstream applications like data selection and demonstration retrieval. The authors introduce an instruction embedding benchmark (IEB) comprising 47k samples for training and testing, alongside a baseline method called prompt-based instruction embedding (PIE) that operates in both training-free and fine-tuning settings. The effectiveness of PIE is validated on the IEB.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and accessible.
- Data and code are available through supplementary materials.
- The task of instruction embedding is innovative, and the benchmark is a pioneering effort in this domain.
- The proposed baseline method is straightforward yet effective.
- The motivation is compelling, with promising downstream implications for various tasks.

Weaknesses:
- The use of GPT-4 for data synthesis and filtering may introduce noise.
- Overlapping tasks between training and testing sets raise concerns about model performance in real-world applications, especially for out-of-distribution tasks.
- The foundational models for PIE are large and inefficient, leading to significant computational overhead.
- The results section lacks robustness, particularly regarding the significance of findings and the absence of a random baseline.
- The dataset creation process is unclear, potentially leading to biases.
- The related work section is too concise and misses key references, such as "Description-Based Text Similarity."

### Suggestions for Improvement
We recommend that the authors improve the robustness of the results section by including a random baseline and addressing statistical significance with error bars. Clarifying the data creation process and providing a definition of what constitutes a "task" would enhance understanding. Additionally, we suggest adding a detailed description of EFT and IFT in Table 2, and expanding the related work section to include more comprehensive references, particularly on instruction tuning of LLMs. Finally, consider implementing manual validation of the IEB benchmark to assess data quality and discussing limitations related to data noise, model transferability, and computational overhead.