ID: 89AUi5L1uA
Title: SOFTS: Efficient Multivariate Time Series Forecasting with Series-Core Fusion
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 7, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SOFTS (Series-cOre Fused Time Series forecaster), an efficient multivariate time series forecasting model that innovatively addresses the gap between channel independence and channel correlation. Utilizing a centralized STAR (STar Aggregate-Redistribute) module, SOFTS aggregates a global core representation from all series, which is then redistributed and fused with individual series representations. The paper demonstrates SOFTS's superiority over state-of-the-art methods in performance and computational complexity, showcasing the STAR module's adaptability across various forecasting models.

### Strengths and Weaknesses
Strengths:
- SOFTS proposes a unique approach to handling channel correlations through the centralized STAR module, allowing for efficient aggregation and redistribution of series representations.
- The model's design is well-thought-out, with empirical results indicating that SOFTS outperforms existing methods.
- The paper is well-written and structured, providing clear explanations of the STAR mechanism and its integration into SOFTS.
- The proposed method has the potential to enhance forecasting accuracy and efficiency across various domains.

Weaknesses:
- Additional experiments evaluating SOFTS's robustness under varying conditions of distribution drift could provide deeper insights into its reliability.
- The paper lacks significant innovation, as the stochastic pooling method is not novel in deep neural networks, and the aggregate-and-dispatch concept has been previously studied.
- Some experimental results are unconvincing, with unclear dataset and metric choices for different ablation studies and a lack of statistical significance tests between results.

### Suggestions for Improvement
We recommend that the authors improve the discussion and analysis of their results, particularly in comparing SOFTS with iTransformer and other models. Clarifying the conditions under which SOFTS should be preferred over these models would enhance the paper's utility. Additionally, we suggest increasing the look-back window length of the input series to 512 and 720, and comparing performance with baseline models. Providing more results on abnormal channels and including training times for each method would also strengthen the paper. Lastly, we encourage the authors to move the discussion of limitations from the appendix to the main text and to characterize the datasets and tasks where SOFTS outperforms existing methods.