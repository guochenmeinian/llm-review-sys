ID: HcLFNuQwy5
Title: SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 6, 5, -1, -1
Original Confidences: 3, 4, 5, -1, -1

Aggregated Review:
### Key Points
This paper presents a new benchmark for scientific figure interpretation, consisting of 1000 human-curated figure-caption pairs from arXiv papers, along with additional subsets for fine-tuning and few-shot examples. The benchmark is evaluated using 27 LLMs, revealing that models like GPT-4o and Gemini-Pro 1.5 Pro perform below the human evaluation baseline. The authors also explore the models' performance variability through adversarial experiments. However, the task of "Scientific figure interpretation" may require a broader context beyond the figure-caption relationship, suggesting a need for further evaluation methods.

### Strengths and Weaknesses
Strengths:
- The benchmark is novel and well-constructed, providing a comprehensive test suite for multi-modal LLMs.
- Extensive experiments, including adversarial tests, demonstrate the robustness of the benchmark.
- The clarity of the curation process and experimental setup is commendable.

Weaknesses:
- The originality is limited as the dataset builds upon existing benchmarks like SciCap without proposing new methods.
- The current focus on CS-related questions restricts the dataset's applicability.
- There is a lack of error analysis and evidence supporting the usefulness of additional noisy data.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by including more generic and complex scenarios for evaluating LLM capabilities, such as captions and insights generation for scientific figures. Additionally, the authors should provide detailed statistics on dataset domains, types of figures, and caption complexity to enhance understanding. It would also be beneficial to conduct a thorough error analysis to identify challenging question categories and model error patterns. Finally, we suggest expanding the dataset to include a broader range of arXiv papers beyond CS-related questions to increase its utility.