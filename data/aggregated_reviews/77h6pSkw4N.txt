ID: 77h6pSkw4N
Title: DocSplit: Simple Contrastive Pretraining for Large Document Embeddings
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel contrastive pretraining method called DocSplit, which enhances document embeddings by utilizing positive examples from the same document and negative examples from unrelated documents. The authors demonstrate the effectiveness of their approach across various tasks, achieving state-of-the-art performance in document classification, few-shot learning, and retrieval tasks. The method is simple, model-agnostic, and applicable to both short and long documents.

### Strengths and Weaknesses
Strengths:
- The paper addresses a well-motivated problem with clear writing and reasonable experiments.
- The proposed method is practical and yields consistent improvements over existing models.
- The authors provide a thoughtful limitations section.

Weaknesses:
- The explanations regarding potential biases in summary generation are somewhat weak.
- The work is considered incremental and lacks novelty, with insufficient comparison to relevant baselines.
- The paper does not specify the exact contrastive objective/loss used, which is crucial for understanding its implementation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their explanations regarding the potential biases in summary generation, particularly in relation to the first K sentences in a news article. Additionally, we suggest that the authors explore the use of non-random negative samples to enhance their contrastive learning framework. It is also essential to provide a comprehensive comparison with relevant recent methods and baselines, as this will strengthen the paper's impact and novelty. Finally, we advise the authors to specify the exact contrastive objective/loss employed in their method to clarify its implementation.