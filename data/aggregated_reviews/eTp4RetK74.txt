ID: eTp4RetK74
Title: ASPEN: Breaking Operator Barriers for Efficient Parallelization of Deep Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 6, 6, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 2, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ASPEN, an opportunistic parallelism method that mitigates synchronization barriers in DNN graphs, allowing parallel compute resources to execute multiple data-paths independently with reduced overhead in a shared memory system. The authors achieve this by splitting operators/tensors into tiles and implementing decentralized executors that synchronize on-demand with a centralized information pool capturing dependencies. The evaluation demonstrates significant speedups on models like ResNet and BERT compared to existing frameworks.

### Strengths and Weaknesses
Strengths:
- The exploration of opportunistic parallelism is innovative and relevant, particularly in scenarios with high threading overhead, multiple data-paths, and limited independent dimensions in input data.
- The implementation of ASPEN is intuitive and efficient, with correctness and completeness proofs.
- ASPEN shows significant performance improvements in inference environments on heterogeneous/edge devices.

Weaknesses:
- The application of opportunistic parallelism is limited, as the scenarios for its effectiveness rarely coexist in modern ML workloads, particularly with GPUs that do not exhibit high threading overhead.
- The implementation details are sometimes unclear, particularly regarding the merging of tiles and the explanation of the Ready Pool.
- Evaluations lack comprehensiveness and may contradict claims about performance benefits, necessitating further ablation studies to clarify the conditions under which ASPEN outperforms other frameworks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of implementation details, particularly regarding the merging of tiles and the Ready Pool's functionality. Additionally, we suggest conducting more comprehensive evaluations, including ablation studies to analyze how batch size and model structure affect ASPEN's performance. It would also be beneficial to provide a detailed comparison with other frameworks, particularly in terms of memory consumption and performance on GPU architectures. Lastly, addressing the limitations related to GPU efficacy and the trade-offs in memory consumption during optimization would strengthen the paper.