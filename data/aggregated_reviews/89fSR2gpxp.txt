ID: 89fSR2gpxp
Title: Offline Behavior Distillation
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Offline Behavior Distillation (OBD) to synthesize expert behavioral data from sub-optimal reinforcement learning (RL) data, facilitating rapid policy learning. The authors propose two naive OBD objectives and introduce the Action-Value Weighted PBC (Av-PBC) objective, which optimizes the weighted decision difference, achieving superior distillation guarantees with linear discount complexity. Theoretical analyses and extensive experiments on D4RL datasets demonstrate that Av-PBC significantly enhances OBD performance and convergence speed compared to the naive objectives.

### Strengths and Weaknesses
Strengths:
1. The paper introduces a novel approach to distill vast sub-optimal RL data into a limited set of expert behavioral data, enhancing policy training efficiency.
2. The theoretical analysis is robust, providing a solid foundation for the proposed Av-PBC objective, with comprehensive empirical results across multiple datasets.
3. The problem setup and assumptions are well-articulated, and the application setting clarifies the scope of the research.

Weaknesses:
1. The dependency on an offline RL algorithm limits the claim that OBD improves training efficiency, as the training cost of offline RL is part of the proposed Algorithm 1.
2. Limited discussion on the impact of the initial dataset quality on Av-PBC's effectiveness restricts its practical applicability. The authors should provide insights on when Av-PBC guarantees performance.
3. The network architecture and data generation model details are unclear, necessitating additional information on the generating network alongside the policy network.
4. The experiments lack clarity regarding the number of offline datasets and their influence on performance, as well as comparisons of time and performance under varying generated data volumes.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the differences between Av-PBC and AvPBC in Section 1, ensuring that only the evaluated Av-PBC is referenced. Additionally, the authors should provide a thorough comparison of related works on offline data generation in RL, addressing whether this paper is the first in this domain. 

To enhance the discussion on dataset quality, we suggest the authors elaborate on the findings regarding the performance of Av-PBC across different dataset qualities, particularly in relation to state-action coverage and average trajectory return. 

Furthermore, we encourage the authors to clarify the semantics of the surrogate loss in the "Problem Setup" and provide examples of critical states discovered by Av-PBC to strengthen the paper's argument for explainable RL. Lastly, adding ablation studies and further corollaries could significantly bolster the paper's contributions.