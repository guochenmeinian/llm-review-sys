ID: 3CIQIYNGlp
Title: Exploring the Impact of Model Scaling on Parameter-Efficient Tuning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on parameter-efficient tuning (PET) methods, focusing on the impact of model scaling on various PET techniques and proposing the arbitrarily PET (APET) framework. The authors conclude that tunable parameter positions and quantities significantly influence model scaling, although some findings may seem obvious. The paper provides a thorough analysis of PET methods across different model scales, contributing useful empirical insights for researchers.

### Strengths and Weaknesses
Strengths:
- The paper is well written and structured, with solid experimental results that align with expectations from PET researchers.
- The presentation and visuals are commendable, enhancing the readability of the findings.
- It offers valuable empirical findings for researchers working with varying scales of language models.

Weaknesses:
- The technical contribution of the APET framework is limited, as it appears to be a combination of existing PET methods without significant novelty.
- Some sections lack clarity, particularly regarding the fixed number of tunable parameters in Section 5.2 and the analysis focusing primarily on "within group" comparisons.
- The paper does not adequately address the effectiveness of different insertion methods or explore a wider range of PEFT modules.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their explanations, particularly in Section 5.2, by detailing how the number of tunable parameters is fixed within the APET framework. Additionally, we suggest including a more comprehensive analysis that compares different PEFT modules, such as Compactor and IA3, to strengthen the technical contributions. Furthermore, we encourage the authors to provide detailed performance metrics for each experiment in the appendix rather than just averaged results. Lastly, addressing the concerns regarding the APET framework's novelty and its relation to existing methods would enhance the paper's impact.