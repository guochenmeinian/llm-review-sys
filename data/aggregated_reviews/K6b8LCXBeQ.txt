ID: K6b8LCXBeQ
Title: GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 5, 8, 6, -1, -1, -1
Original Confidences: 5, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GMAI-MMbench, a comprehensive multimodal evaluation benchmark designed for assessing general medical AI (GMAI) systems. It features an extensive dataset collection from 285 diverse clinical datasets across 38 imaging modalities and evaluates 50 large vision-language models (LVLMs). The evaluation reveals significant challenges, with the top model achieving only 52% accuracy.

### Strengths and Weaknesses
Strengths:
- The benchmark's construction from 285 datasets ensures broad representation across 38 medical image modalities.
- The lexical tree structure allows for customizable evaluations tailored to specific clinical needs.
- The extensive evaluation methodology provides valuable insights into the performance and limitations of current models.

Weaknesses:
- There are concerns regarding data leakage, as the benchmark includes a small proportion of private data, raising questions about the originality of the data seen by models.
- Ethical review and approval of private datasets are not demonstrated.
- The QA generation process lacks clarity regarding the prompts used for GPT-4V, which is crucial for benchmark quality.
- The incorporation of patient information in the evaluation design is insufficient, potentially undermining the benchmark's applicability to real clinical settings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the QA generation process by detailing the prompts used for GPT-4V. Additionally, we urge the authors to address the ethical review of private datasets and ensure that the benchmark incorporates patient information to reflect real clinical scenarios. Providing detailed visualizations or tables for performance across modalities and the distribution of multiple-choice options would enhance the benchmark's comprehensiveness. Finally, we suggest including open-source datasets and code to facilitate reproducibility and broader usability of the benchmark.