ID: iy4Of0w8ML
Title: GPEX, A Framework For Interpreting Artificial Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach using Gaussian Processes (GPs) to explain the outputs of Artificial Neural Networks (ANNs) by deriving an evidence lower-bound (ELBO) that encourages the GP's posterior to align with the ANN's output. The authors propose scalable training methods for GPs, addressing traditional scalability issues, and utilize two methods for explanation: identifying similar examples in the GP's kernel space and employing saliency maps. The manuscript also discusses the potential of GPs to enhance interpretability of neural networks, although the claim that "GP is a white-box model" is contested, as it depends on the similarity function used. The paper includes various experiments demonstrating the effectiveness of the proposed method, but essential information is relegated to supplementary materials, detracting from the manuscript's clarity.

### Strengths and Weaknesses
Strengths:
- The method is straightforward and intuitive, with GP matching being less aggressive in areas of high uncertainty.
- The approach addresses scalability issues associated with GPs, allowing for training with a large number of inducing points.
- The manuscript introduces promising ideas regarding the GP-ANN analogy, which is considered a high-priority research direction.
- The implementation is publicly available, enhancing reproducibility.

Weaknesses:
- The paper is overly dense, with critical information relegated to supplementary materials, making it difficult to grasp the full methodology.
- Key aspects, such as the biological dataset and scalability, are not adequately discussed in the main text.
- There is a lack of clarity in defining key concepts such as explainability and interpretability, complicating the understanding of the paper's goals.
- The explanation methods lack empirical comparisons with existing techniques, and the results are not quantitatively evaluated.
- The discussion on the explainability-complexity trade-off is insufficient, particularly regarding the use of complex DNN-kernels.
- Claims regarding the interpretability of GPs are vague and require more substantiation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the main text by integrating essential details currently found in the supplementary materials. It is crucial to define key terms like explainability and interpretability explicitly to enhance reader comprehension. Additionally, we suggest including a detailed discussion of the biological dataset and scalability within the main text, rather than relying on supplementary material. Conducting empirical comparisons with existing explanation methods to substantiate the claims made about the effectiveness of the proposed approach would also strengthen the paper. A more thorough discussion on the explainability-complexity trade-off, particularly regarding the use of DNN-kernels, is necessary. Finally, we recommend providing more detailed descriptions of experiments and results, especially in figures, to improve the overall presentation and understanding of the findings.