ID: qHrZszJSXj
Title: Don't be so Monotone: Relaxing Stochastic Line Search in Over-Parameterized Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 5, 5, 8, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 2, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a nonmonotone line search method, named POlyak NOnmonotone Stochastic (PoNoS), aimed at enhancing the optimization of deep learning models like Stochastic Gradient Descent (SGD) and Adam in over-parameterized settings. The authors relax the monotonic decrease condition of the objective function, allowing for larger step sizes and introducing a resetting technique that eliminates backtracking while maintaining a substantial initial step size. The paper demonstrates that the proposed method achieves convergence rates comparable to monotone methods and shows superior performance in experiments across various deep learning architectures.

### Strengths and Weaknesses
Strengths:
- **Originality:** The introduction of nonmonotone line search methods for deep learning is a significant contribution, extending previous work by Zhang and Hager (2004) and Vaswani et al. (2019).
- **Quality:** The rigorous proof of convergence rates and strong experimental results highlight the method's effectiveness in improving convergence speed and generalization properties.
- **Clarity:** The paper is well-structured and clearly explains technical concepts, with qualitative descriptions aiding reader comprehension.
- **Significance:** The method demonstrates improved computational efficiency and generalization compared to existing state-of-the-art algorithms.

Weaknesses:
- The method requires the selection of several parameters (e.g., $\eta_{\rm max}$, $c$, $c_p$, $\delta$, and $\xi$), and the impact of these choices on performance is inadequately addressed.
- The theoretical contributions do not exhibit improvements over existing methods, raising questions about the novelty of the results.
- The experimental evaluation lacks comprehensive comparisons, particularly regarding wall-clock time for certain models, and does not include test results for transformers, limiting claims of state-of-the-art generalization performance.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the influence of parameter choices on the performance of the proposed method, providing clarity on their significance. Additionally, we suggest enhancing the theoretical analysis to demonstrate how PoNoS outperforms existing methods, rather than merely matching their convergence rates. It would be beneficial to include more detailed experimental comparisons, particularly regarding wall-clock time and test performance for RBF kernel models and transformers, to substantiate claims of superior generalization. Lastly, we encourage the authors to clarify the necessity of interpolation in achieving theoretical results and to address the potential drawbacks of the proposed method more thoroughly.