ID: qCglMj6A4z
Title: Gradient Descent with Linearly Correlated Noise: Theory and Applications to Differential Privacy
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 6, 7, 8, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 5, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical study of gradient descent with linearly correlated noise, motivated by differential privacy applications, particularly the DP-SGD algorithm. The authors propose a new convergence analysis based on "restart iterations," which unifies previous analyses for perturbed gradient descent (PGD) and anti-PGD. Additionally, they introduce a refined matrix factorization method for noisy SGD, supported by empirical results demonstrating its advantages over prior methods.

### Strengths and Weaknesses
Strengths:
- The analysis method is based on a simple yet essential idea, enabling a unified approach for a class of gradient descent algorithms.
- The transition from a tighter bound to a new matrix factorization objective illustrates powerful empirical applications to private optimization.
- The paper is well-structured, educational, and presents clear theoretical results confirmed by numerical experiments.

Weaknesses:
- The novelty is somewhat limited to a specific class of algorithms, and the average case analysis for differentially private algorithms may not accurately reflect model performance.
- The paper lacks self-containment, assuming familiarity with prior works, which may hinder understanding.
- The proposed method appears somewhat "handmade," and the clarity of Example 3.3 could be improved.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Figure 1 and provide a more detailed diagram to enhance understanding. Additionally, we suggest including more background information on related works in the main text or supplementary materials to make the paper more self-contained. It would also be beneficial to discuss the implications of the hyper-parameter "\tau" in greater detail and provide heuristics for its selection in practice. Finally, we encourage the authors to elaborate on the benefits of the "restart" method in both theoretical and practical contexts.