ID: XOCbdqxAR2
Title: TD Convergence: An Optimization Perspective
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of the Temporal Difference (TD) learning algorithm from an optimization perspective, arguing that this viewpoint enhances understanding of TD learning and its convergence properties. The authors identify two forces—target force and optimization force—that influence TD convergence, particularly under the conditions of strong convexity and smoothness. Additionally, the paper discusses off-policy distributions \(D\) that can lead to faster convergence than on-policy distributions, emphasizing the importance of identifying suitable distributions given \(P\) and \(\Phi\). The authors aim to clarify conditions under which fixed points exist in more general settings beyond standard linear function approximation and address concerns regarding the Lipschitzness and strong convexity assumptions and their implications for convergence.

### Strengths and Weaknesses
Strengths:
- The paper effectively addresses the behavior of TD learning in complex settings, providing significant insights into its convergence properties.
- The exposition is clear, with well-organized proofs and a straightforward presentation of concepts, particularly the counterexample in Section 4.
- The exploration of alternative loss functions, such as Huber loss, adds depth to the theoretical framework.
- The authors effectively engage with reviewer feedback, addressing concerns and clarifying key points, which enhances the paper's strength.
- The discussion on off-policy distributions and their potential for improved contraction factors is insightful and valuable for future research.
- The authors' willingness to incorporate suggestions into the paper demonstrates responsiveness to reviewer input.

Weaknesses:
- The distinction between the proposed scheme and classical TD learning is not sufficiently clarified, particularly regarding the frozen target network.
- The discussion in Section 6 lacks detail on generalization beyond linear function approximation, and the conditions for convergence remain somewhat restrictive.
- The novelty of the optimization perspective is questioned, as many arguments rely on established results from prior works, with insufficient comparison to existing literature.
- The setting may be perceived as restrictive, lacking generalization beyond standard linear function approximation.
- The existence of fixed points is assumed without sufficient exploration of examples outside known cases, which could limit the paper's applicability.
- The ReLU example introduces nonsmoothness not fully covered by the assumptions, raising concerns about the robustness of the proposed methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the differences between their approach and classical TD learning, particularly in relation to the frozen target network. A more detailed discussion in Section 6 regarding the generalization of convergence conditions would enhance the paper's contribution. Additionally, we suggest that the authors provide a more thorough comparison with related works, especially those analyzing the target-based version of TD learning. Including empirical results to demonstrate the practical implications of their theoretical findings would strengthen the paper significantly. Furthermore, we recommend improving the clarity of the conditions for fixed point existence by providing examples beyond the standard cases discussed. Lastly, addressing the nonsmoothness introduced by the ReLU activation function, possibly through smoothing techniques or alternative activation functions like softplus, would ensure adherence to the assumptions made in the paper.