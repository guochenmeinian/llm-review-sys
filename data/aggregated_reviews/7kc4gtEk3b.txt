ID: 7kc4gtEk3b
Title: A Comprehensive Benchmark for Neural Human Radiance Fields
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark for evaluating human NeRF models, addressing the challenges of existing evaluation methods by establishing standardized metrics and experimental settings. The authors propose a unified evaluation framework that includes various datasets, specifically focusing on ZJU-MoCap, GeneBody, and HuMMan, to assess generalizability and animatability. They introduce a modified version of HumanNeRF aimed at enhancing animatability from monocular video and conducted extensive experiments on training frame numbers, intervals, and view selections, providing detailed analyses of methodology differences and their impacts on performance. The work emphasizes the importance of generalizability and cross-view comparisons in human NeRF evaluations.

### Strengths and Weaknesses
Strengths:
- The authors effectively identify and address the critical issue of unifying benchmarks in the rapidly evolving field of novel view rendering.
- The analysis of the impact of human mask and SMPL parameter estimation on performance is valuable for real-world applications.
- The paper includes well-designed experiments and a comprehensive summary of existing human NeRF methods, which can guide future research.
- The authors improved clarity in definitions and experimental settings, enhancing the overall presentation.
- A wide range of experiments was conducted, including cross-dataset generalization and training time ablation studies.

Weaknesses:
- The choices made in unifying benchmarking procedures lack sufficient justification, particularly regarding the selection of training and testing views.
- The paper does not include a limitations section, which is a significant omission.
- The GeneHumanNeRF baseline is described too briefly, and comparisons with other techniques are limited.
- Some experiments are still in progress due to time and resource constraints.
- The evaluation metrics could be better designed to account for frame consistency.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their benchmarking choices by providing detailed explanations of how training and testing views were selected, including whether they are spaced similarly or vary in difficulty. A table summarizing all experimental settings would enhance transparency and reproducibility. 

Additionally, the authors should expand the discussion on the separation of hard clothing and poses in the GeneBody dataset, ensuring that all design choices are well-motivated. Including masked PSNR as a metric in supplementary materials could provide a more comprehensive evaluation.

We suggest that the authors clarify the rationale behind the choice of 1 and 4 views for testing and provide insights into the performance variations concerning position and viewing angles. Furthermore, the authors should include a limitations section to address potential biases and the implications of their benchmarking choices. A more detailed description of the GeneHumanNeRF setup and comparisons with other methods would strengthen the paper's contributions. Lastly, we recommend conducting further experiments on the influence of inaccurate SMPL parameters across different datasets and consider designing a new metric that incorporates frame consistency and other experimental settings for a more comprehensive evaluation.