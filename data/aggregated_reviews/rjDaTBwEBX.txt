ID: rjDaTBwEBX
Title: In What Languages are Generative Language Models the Most Formal? Analyzing Formality Distribution across Languages
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation of two large language models (LLMs), XGLM and BLOOM, focusing on their responses to neutral, formal, and informal prompts across five languages. The authors investigate the relationship between prompt type and response formality, considering the model's size and training text types. The study is supported by well-planned experiments and provides an annotated dataset of LLM-generated outputs, contributing to the understanding of formality in multilingual contexts.

### Strengths and Weaknesses
Strengths:  
- The experiments are well-designed, targeting clearly specified variables and yielding a reasonable number of results.  
- The diagnostics of formality are linguistically accurate for each language.  
- The qualitative and quantitative analysis, particularly in Section 5 and Table 4, is insightful.  

Weaknesses:  
- The annotation process lacks reliability due to the involvement of only two annotators per language, and there is no profile provided for them.  
- Definitions of "informality" are inconsistent across languages, raising questions about calibration and interpretation.  
- There is insufficient analysis of inter-annotator agreement and the influence of pretraining corpus formality on model outputs.

### Suggestions for Improvement
We recommend that the authors improve the reliability of their results by involving more annotators and providing their profiles. Additionally, we suggest clarifying the definitions of "informality" across languages and addressing the calibration issues related to prompt types. A detailed analysis of the pretraining corpus and its impact on formality bias and preservation should be included to enhance the generalizability of the findings. Furthermore, we advise revising the terminology used in the paper to avoid confusion, such as differentiating between "incohesive" outputs and those that are simply too short to assess formality.