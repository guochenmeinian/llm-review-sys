ID: COPzNA10hZ
Title: Norm-based Generalization Bounds for Sparse Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 5, 7, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the derivation of generalization bounds for sparsely connected neural networks, particularly focusing on Convolutional Neural Networks (CNNs). The authors extend peeling arguments to establish a bound on the Rademacher complexity based on neuron connections and network weight norms. The proposed bounds are validated through numerical experiments on datasets like MNIST and CIFAR, with the claim of providing the first non-vacuous generalization bound based on Rademacher complexities for a three-layer network trained on MNIST. The authors clarify that the term $\beta$ is defined independently of the depth $L$, although concerns about the dependence on depth remain.

### Strengths and Weaknesses
Strengths:
1. The paper is well-structured and clearly written, demonstrating a comprehensive understanding of related work.
2. The proposed bounds improve upon existing bounds by leveraging modern proof techniques for Rademacher complexity.
3. The numerical evaluations of the bounds are appreciated, and the formalization of deep neural networks as directed acyclic graphs is elegant.
4. The paper makes a strong contribution by presenting a novel generalization bound that is non-vacuous on MNIST.
5. The authors demonstrate a willingness to clarify and improve their definitions and arguments based on reviewer feedback.

Weaknesses:
1. The bounds exhibit exponential scaling with network depth, raising concerns about their practical applicability.
2. The explicit dependence on the number of connections (e.g., filter size in CNNs) may limit the generality of the results.
3. The numerical experiments do not convincingly demonstrate the tightness of the generalization bounds, with significant discrepancies between the bounds and reported generalization gaps.
4. There are doubts regarding the vacuity of the bound, particularly since it is evaluated on MNIST, a multi-class dataset, while the bound appears to apply only to binary classification.
5. The source code computes the maximum pixel sum on raw data, neglecting a normalization factor, which may lead to vacuous bounds.
6. The bound is limited to homogeneous elementwise activation functions, excluding popular components like pooling layers.
7. Concerns regarding the exponential dependence on depth remain inadequately addressed, particularly in relation to practical architectures like ResNet-18/50.

### Suggestions for Improvement
We recommend that the authors improve the experimental validation by:
- Reporting the values of other generalization bounds in the same setting to illustrate the proposed bound's tightness.
- Varying the amount of regularization to observe its impact on the generalization gap, which could provide more insightful results than merely reporting the value of $\rho(w)$.
- Improving the clarity of the bound's interpretation by adding a paragraph after Proposition 3.1 to describe the different factors involved.
- Addressing the scaling with network depth, as the current presentation appears unfavorable.
- Including a comparison table listing numerical values of the bounds from prior works evaluated on the MNIST models to provide valuable insights.
- Clarifying the notation used, especially regarding the quantities in the proof of Proposition 5.3, and emphasizing the limitations of the bound concerning activation functions more explicitly.
- Correcting the evaluation of the bounds in the source code to account for the missing factors related to input dimensions, which may clarify discrepancies with prior works.
- Extending the peeling argument to CNNs or sparsely connected networks, as this could provide theoretical insights and potentially lead to better bounds in realistic settings.