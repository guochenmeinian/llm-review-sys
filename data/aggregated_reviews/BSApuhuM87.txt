ID: BSApuhuM87
Title: Beware of Model Collapse! Fast and Stable Test-time Adaptation for Robust Question Answering
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Anti-Collapse Fast (Anti-CF) test-time adaptation (TTA) for extractive question answering, addressing the instability and potential model collapse associated with TTA methods like TENT (Wang et al., 2021). The authors propose a regularization method that computes KL divergence between the adapted model's predicted probabilities and those of the source model, while tuning only a small number of parameters in a side block to enhance efficiency. Empirical results demonstrate that Anti-CF achieves competitive EM/F1 scores across various QA datasets while being faster than other adaptive models.

### Strengths and Weaknesses
Strengths:  
- The paper identifies the imbalance in label distribution as a significant cause of model collapse in TTA, providing valuable insights.  
- Anti-CF effectively uses the source model's outputs as soft labels to stabilize adaptations during testing.  
- The method shows stable performance improvements across different distribution shift scenarios, indicating its practical applicability.  

Weaknesses:  
- The novelty of the approach is questioned, as comparisons with existing methods like LAME and PEFT techniques are insufficiently detailed.  
- The performance of Anti-CF is only validated in the question answering task, raising concerns about its generalizability to other NLP tasks.  
- The use of KL divergence may limit performance potential, and the hyperparameter selection process lacks systematic exploration.

### Suggestions for Improvement
We recommend that the authors improve the discussion of how Anti-CF differs from and builds upon existing methods, particularly LAME and PEFT techniques, to clarify its novelty. Additionally, we suggest including comparisons with stronger TTA methods such as SAR, AdaNPC, and LAME to strengthen the evaluation. The authors should also provide a more systematic approach to hyperparameter selection, considering variations across different datasets and models. Finally, addressing the limitations imposed by KL divergence and exploring more flexible constraint methods could enhance the robustness of the proposed approach.