ID: HkC4OYee3Q
Title: SleeperNets: Universal Backdoor Poisoning Attacks Against  Reinforcement Learning Agents
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 7, 6, -1, -1, -1
Original Confidences: 3, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel backdoor attack framework against Reinforcement Learning (RL) agents, termed SleeperNets, which utilizes dynamic reward poisoning to address the limitations of static reward poisoning in previous works. The authors provide a theoretical analysis of the advantages of dynamic adversarial poisoning and conduct comprehensive evaluations demonstrating the effectiveness of SleeperNets against prior backdoor attacks. Additionally, the paper introduces a new threat model that allows adversaries to manipulate entire episodes rather than individual steps, supporting their claims with empirical results across four different environments.

### Strengths and Weaknesses
Strengths:
- The paper effectively highlights the drawbacks of static reward poisoning, motivating the need for dynamic reward poisoning.
- The theoretical analysis convincingly demonstrates how dynamic reward poisoning overcomes the limitations of static designs.
- The explicit threat model and theoretical investigation of prior models are commendable, providing clarity in the context of adversarial RL.
- The empirical evaluations, including detailed ablation studies, are broad yet manageable and support the authors' claims.

Weaknesses:
- The applicability of the proposed threat model is questionable; it is unclear in what real-world scenarios an attacker could execute the SleeperNets attack without direct software access.
- The authors do not investigate defenses against their attack, which is a significant limitation.

### Suggestions for Improvement
We recommend that the authors improve the discussion of real-world scenarios where an attacker could act in the proposed outer-loop manner without direct access to the training machine. Specific examples should be articulated, detailing how Algorithm 1 could be executed under such conditions. Additionally, we suggest that the authors conduct empirical investigations using out-of-distribution anomaly detection methods to assess the detectability of their attacks. Finally, we encourage the authors to explore defenses against their novel attack, as securing the training environment and developing test-time anomaly detectors are promising avenues for future work.