ID: b6IBmU1uzw
Title: CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 7, 6, 7, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CARES, a comprehensive benchmark designed to evaluate the trustworthiness of Medical Large Vision Language Models (Med-LVLMs) across five dimensions: trustfulness, fairness, safety, privacy, and robustness. The benchmark comprises approximately 41,000 question-answer pairs from 16 medical image modalities and 27 anatomical regions, revealing significant trustworthiness concerns in current Med-LVLMs, including factual inaccuracies, demographic biases, privacy breaches, and susceptibility to attacks. The study aims to standardize the evaluation of these models and drive the development of more reliable systems in healthcare.

### Strengths and Weaknesses
Strengths:
- The paper addresses a crucial topic in medical imaging, providing a well-motivated and comprehensive evaluation framework.
- CARES includes a large and diverse dataset, allowing for thorough evaluation across various medical specialties.
- The authors offer valuable insights into the performance of multiple Med-LVLMs, enhancing understanding of their trustworthiness.
- The paper is clearly written and well-organized, facilitating comprehension of its methodology and findings.

Weaknesses:
- The evaluation of open-ended questions relies on GPT-4, which may introduce bias and does not assess GPT-4's performance on the benchmark.
- The transformation of binary classification tasks into yes/no questions may lead to misleading accuracy metrics due to imbalanced label distributions.
- The number of evaluated models is limited, missing some state-of-the-art vision-language models.
- The evaluation metrics for trustfulness and fairness may not adequately capture model performance, and the safety evaluation's relevance is debatable.
- The benchmark lacks longitudinal analysis and does not consider the human-machine collaboration perspective in clinical settings.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of open-ended questions by incorporating human review to enhance annotation reliability. Additionally, expanding the evaluation dimensions to include more types of privacy data and attack scenarios would strengthen the benchmark. We suggest conducting an in-depth analysis of internal model mechanisms to better understand model behaviors and incorporating longitudinal comparisons to track trustworthiness changes over time. Furthermore, providing practical guidance for clinical applications based on evaluation results and considering the long-term impacts of Med-LVLMs on healthcare would enhance the paper's relevance. Finally, we encourage the authors to explore the human-machine collaboration perspective to assess how Med-LVLMs assist medical decision-making effectively.