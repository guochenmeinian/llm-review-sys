ID: DgB01RzOqo
Title: Multilingual Large Language Models Are Not (Yet) Code-Switchers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the effectiveness of Multilingual Large Language Models (LLMs) on code-switching (CSW) tasks, asserting that current models struggle to perform satisfactorily under zero-shot or few-shot conditions. The authors evaluate four LLMs (mT0, BLOOMZ, XGLM, and OpenAI's GPT-3.5-turbo) across four tasks: sentiment analysis, machine translation, summarization, and word-level language identification, using fine-tuned methods as baselines. The results indicate that fine-tuned multilingual models outperform LLMs significantly. The authors propose future research directions focusing on better data representation for CSW, adapting CSW optimization objectives, and developing socially aware NLP technologies.

### Strengths and Weaknesses
Strengths:
- The paper is well-motivated and empirically solid, highlighting the importance of CSW for inclusive NLP technologies.
- A comprehensive set of experiments across various tasks supports the authors' claims regarding LLM limitations.
- The writing is clear and the narrative logical, making the empirical studies easy to follow.

Weaknesses:
- The analysis of model outputs is shallow, lacking exploration of specific errors made by LLMs compared to fine-tuned models.
- The paper does not include recent open-source LLMs, leaving the potential performance of stronger models unexamined.
- There is insufficient detail on the training data used for fine-tuning PLMs, and the generalizability of results is questionable due to limited language evaluation.

### Suggestions for Improvement
We recommend that the authors improve the prompt templates used in their experiments by incorporating advanced prompting methods such as Chain-of-Thought prompting and Least-to-Most prompting, which may enhance task decomposition for CSW. Additionally, we suggest including recent open-source LLMs like LLaMA in the experiments to assess their impact on performance gaps. The authors should also provide a more in-depth qualitative analysis of model outputs, detailing the types of errors made by different models. Furthermore, discussing the nature and quantity of training data for each model would contextualize the results better. Lastly, clarifying the number of training samples used for fine-tuning PLMs would strengthen the comparison with LLMs.