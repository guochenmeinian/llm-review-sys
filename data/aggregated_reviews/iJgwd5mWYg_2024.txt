ID: iJgwd5mWYg
Title: Beyond Primal-Dual Methods in Bandits with Stochastic and Adversarial Constraints
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 4, 5, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of multi-armed bandits with cumulative cost constraints, focusing on designing a 'best of both worlds' algorithm that minimizes constraint violations in both stochastic and adversarial settings. The authors propose an adaptation of Exp-IX that operates within an optimistic feasible set, achieving $\tilde{O}(\sqrt{T})$ regret and violation guarantees without requiring knowledge of a Slater parameter. The main contribution is establishing these guarantees in a novel way, which is claimed to be the first of its kind without prior knowledge of the Slater parameter.

### Strengths and Weaknesses
Strengths:
- The paper is clear and well-written, with good presentation of the algorithm and results.
- The proposed algorithm achieves nearly min-max optimal regret bounds and provides valuable insights into the tension between empirical estimators and constraint violations.

Weaknesses:
- The results rely on a modified definition of the Slater parameter, which may oversimplify the problem and restrict the class of problems considered. The authors' claim that this definition is "slightly stronger" is disputed.
- Key literature is overlooked, particularly regarding similar algorithms that achieve comparable results without requiring Slater's condition, which limits the perceived contribution of this work.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the modified definition of the Slater parameter, clarifying its necessity for the guarantees and addressing its implications on the generality of the results. Additionally, we suggest that the authors provide a more thorough literature review, particularly including works such as [R1], [R2], [R3], and [R4], to contextualize their contributions more effectively. Furthermore, we encourage the authors to clarify how their setting generalizes Bandits with Knapsacks, as this connection is not immediately evident.