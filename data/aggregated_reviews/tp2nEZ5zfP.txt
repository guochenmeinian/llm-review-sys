ID: tp2nEZ5zfP
Title: NetHack is Hard to Hack
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 6, 7, 7, 7, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 2, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the performance gap between neural and symbolic agents in the NeurIPS 2021 NetHack Challenge. The authors hypothesize that the advantage of symbolic agents stems from hierarchical reasoning, which is absent in neural agents. To test this, they generated a new dataset from the winning symbolic agent, which includes both actions and higher-level strategic labels, and trained a neural behavior cloning agent with this data. The study examines the effects of model size, dataset size, neural architecture modifications, and reinforcement learning fine-tuning. Results indicate that hierarchical training significantly enhances neural agent performance more than merely increasing model capacity, although larger transformer models may overfit the augmented data. The authors also propose that symbolic agents utilize hierarchical policies and handcrafted heuristics, while neural agents lack necessary inductive biases for sparse rewards. The experiments demonstrate that hierarchical behavior cloning, larger Transformer architectures, and RL fine-tuning can enhance neural agent performance, though significant gaps to symbolic agents persist. The authors introduce a new large-scale dataset, HiHack, to support their analysis and provide insights into the limitations of current neural network methods in complex environments.

### Strengths and Weaknesses
Strengths:
- The hypotheses and claims are clearly articulated, and the experiments are well-designed.
- The paper is well-written, providing clarity on the research question and methodology.
- The contribution to understanding the performance gap in complex, long-horizon problems is significant, potentially inspiring further research.
- The experimental design is clever, with clear charts and obvious experimental effects.
- The methods are detailed for replication, and results are logically presented with useful visualizations.
- The conclusion effectively summarizes key takeaways and contributes significantly to research in sparse reward reinforcement learning and imitation learning.

Weaknesses:
- The small number of independent samples per model class (6) limits statistical power, necessitating inclusion of low-sample hypothesis testing results.
- The model's generalizability beyond NetHack is limited, raising questions about its applicability to other complex environments.
- The paper acknowledges that its contributions are not algorithmic but scientific, with limited novelty in the proposed methods.
- The findings, while interesting, are not groundbreaking and may not have broad applicability beyond the NetHack context.
- The low performance of transformers is noted, suggesting that the LSTM + transformer combination may be more effective in certain contexts.

### Suggestions for Improvement
We recommend that the authors improve the statistical analysis by including findings from low-sample hypothesis testing, such as t-tests, to better distinguish between noise and meaningful differences. Additionally, the authors should consider toning down broad conclusions and framing them as hypotheses, such as "Hierarchy hurts overfitting models," to reflect the limited experimental scope. Expanding the applicability of the methods beyond NetHack and addressing the assumptions regarding hierarchical labels would also enhance the paper's impact. We suggest adding more experiments and theoretical derivations to strengthen the contribution, including flat-transformer results in the main paper and discussing them to highlight the advantages of LSTMs/RNNs in specific cases. Better emphasizing the comparison to symbolic-only agents and other neural solutions within the paper, as well as clarifying the arguments regarding model overfitting in the main text, would improve clarity. Considering the implications of training efficiency and the potential benefits of training models to convergence for clearer analysis of architecture and data factors is also recommended. Finally, open-sourcing the code would enhance the reproducibility of their work.