ID: E2TJI6CKm0
Title: VeriX: Towards Verified Explainability of Deep Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the VERIX algorithm, which calculates optimal robust explanations for deep neural network (DNN) predictions. An explanation is considered robust if small perturbations of excluded features do not alter the model output, while it is optimal if perturbations to included features do change the output. The algorithm iterates over input features and employs neural network verification tools, returning both explanations and counterfactuals. The approach is evaluated on datasets like MNIST, GTSRB, and TaxiNet. The authors also propose a robustness-based method for computing minimal sufficient explanations, leveraging formal robustness checkers. Furthermore, the paper includes a comprehensive analysis of the trustworthiness of explanations generated by sound but incomplete verifiers, comparing complete and incomplete verification methods and highlighting a trade-off between explanation size and generation time. The authors introduce the concept of distance-restricted sufficient explanations and emphasize the importance of ensuring counterfactuals are derived from the data distribution, particularly in the context of perturbations applied to input features.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a clear algorithm for computing optimal robust explanations.
- It provides formal guarantees of soundness and precision, distinguishing it from heuristic methods like LIME and SHAP.
- The empirical evaluations effectively address various questions, including the effect of perturbation size and feature traversal order.
- The authors provide a clear comparison of complete versus incomplete verifiers, detailing the implications for explanation size and generation time.
- The introduction of distance-restricted sufficient explanations adds depth to the theoretical framework and connects it with established concepts in the field.
- The authors demonstrate responsiveness to reviewer feedback, committing to enhance clarity and detail in the final version.

Weaknesses:
- The scalability of the algorithm is a concern due to the repeated invocation of neural network verification tools, which are difficult to scale for large networks.
- The usefulness of the calculated explanations is questioned, with a need for a discussion on practical applications and data selection for developers.
- The algorithm's sensitivity to feature traversal order raises trust issues regarding the generated explanations.
- The lack of unique explanations is noted as a limitation, which may detract from the perceived trustworthiness of the results.
- The intersection of explanations is critiqued as potentially meaningless, raising concerns about the robustness of the proposed methods.
- There are questions regarding the effectiveness of perturbations in ensuring counterfactuals remain within the data distribution, indicating a need for further clarification.

### Suggestions for Improvement
We recommend that the authors improve the scalability of the VERIX algorithm by evaluating sound but incomplete tools based on techniques like abstract interpretation to demonstrate the trade-offs in explanation quality. Additionally, we suggest including a discussion on how developers can practically utilize these explanations and what data points should be selected for computation. It would also be beneficial to evaluate different traversal strategies to address the sensitivity issue and clarify how varying orders can affect trust in the explanations. Furthermore, we recommend that the authors improve the discussion on the trustworthiness of explanations by explicitly addressing the limitations of unique explanations. We suggest clarifying the implications of the intersection of explanations and its relevance to feature attribution. To enhance the robustness of counterfactuals, we recommend that the authors provide a more detailed explanation of how perturbations can be effectively constrained within the data distribution, potentially incorporating techniques such as GANs for distribution estimation. Lastly, we encourage the authors to provide quantitative results regarding explanation sizes when using sound but incomplete verifiers, as this would enhance the empirical evaluation.