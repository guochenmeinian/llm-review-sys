ID: YhAZqWhOnS
Title: Autodecoding Latent 3D Diffusion Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 7, 6, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for unconditional and text-conditional generative models of 3D shape and texture representations. The authors propose training a 3D diffusion model of radiance and RGB fields using 2D image and object mask supervision. The method consists of two stages: first, an auto-decoder is trained, followed by a 3D UNet that models a denoising diffusion process on latent features. At test time, random latent representations can be sampled and denoised to generate 3D radiance volumes for rendering from arbitrary viewpoints.

### Strengths and Weaknesses
Strengths:
- The authors propose a valid method for training 3D diffusion models from posed 2D images, with an interesting two-stage approach involving an auto-decoder and a 3D UNet.
- Extensive evaluation is conducted on five different datasets, demonstrating the method's applicability across diverse scenarios.
- The manuscript is well-written, with sound technical explanations and correct use of mathematical terms, supported by well-organized figures.

Weaknesses:
- The experimental evaluation raises questions, particularly regarding the high FID value of 52.71 for pi-GAN, which lacks coherence with prior works.
- A qualitative baseline comparison is missing, and a deeper discussion on the differences between generative model types would enhance the manuscript.
- The results are limited in quality, particularly in texture predictions, and a comparison with test-time optimization methods like Dreamfusion is warranted.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental results by providing intuition on the high FID score for pi-GAN. Additionally, including qualitative baseline comparisons for methods reported in Table 1 would strengthen the analysis. The authors should also discuss the limitations of the texture quality observed in Figure 5 and consider incorporating a text-based comparison with test-time optimization methods. Finally, addressing the importance of datasets containing multiple views of the same object would enhance the manuscript's depth.