ID: U6bhCLSPun
Title: Last-Iterate Convergent Policy Gradient Primal-Dual Methods for Constrained MDPs
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 7, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on policy searching for constrained Markov Decision Processes (CMDPs) through two Lagrangian-based methods: the regularized policy gradient primal-dual (RPG-PD) method and the optimistic policy gradient primal-dual (OPG-PD) method. Both methods are single-time-scale, demonstrating insensitivity to hyperparameter changes, and exhibit last-iterate convergence with theoretical convergence rates. The authors provide a comprehensive overview of related works and establish a new framework for analyzing policy-based primal-dual algorithms.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, with a commendable related works section.
- The authors are the first to present non-asymptotic rates for last-iterate convergence of single-time-scale primal-dual methods.
- The theoretical analysis is technically sound, and the last-iterate convergence guarantees are meaningful for CMDPs.
- Empirical studies show improved performance over existing algorithms, particularly in suppressing oscillations in utility values.

Weaknesses:
- The convergence of the proposed methods depends on the strong duality property of CMDPs, which may not hold for general parameterized policy classes.
- Numerical experiments are limited to tabular cases; comparisons with more baselines, such as primal methods like CRPO, are recommended.
- The paper may be overly lengthy for a conference proceeding, suggesting the need to remove redundant explanations or examples.
- The paper only considers a single constraint in CMDPs; generalization to multiple constraints is questioned.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definitions of "single-time-scale" and "two-time-scale" at the beginning of the paper. Additionally, the authors should address the potential generalization of results to multiple constraints and explore the application of a homotopic strategy to the RPG-PD method for better theoretical guarantees. Furthermore, we suggest including more numerical experiments beyond tabular cases and comparing results with additional baselines. Lastly, the authors should provide insights into the implementation challenges of their proposed methods and clarify the "optimistic" aspect of the OPG-PD method.