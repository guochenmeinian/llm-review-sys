ID: iFxWrxDekd
Title: Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 5, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the properties of stochastic gradient descent (SGD) through its continuous-time stochastic differential equation (SDE) approximation, focusing on invariant sets in the parameter space that SGD dynamics can attract. The authors derive rigorous conditions under which these invariant sets act as attractors and provide empirical validation of neurons collapsing towards these sets, particularly in two-layer linear networks. The concept of "stochastic collapse" is introduced, suggesting that SGD can lead to simpler subnetworks, which may enhance generalization.

### Strengths and Weaknesses
Strengths:
- The paper addresses the important question of the regularization properties of stochastic algorithms, formally characterizing invariant properties of SGD on deep networks and establishing conditions for attractivity.
- The formalism is novel and aids in understanding the empirical behavior of deep networks, particularly the influence of learning rates on generalization.
- The theoretical analysis is supported by empirical evidence, providing insightful perspectives on optimization dynamics.

Weaknesses:
- The paper uses "label noise gradient descent" and "stochastic gradient descent" interchangeably without clear distinction, which may lead to confusion regarding the applicability of results.
- The analysis primarily focuses on simple networks, limiting the generalizability of findings to more complex architectures.
- The literature review is insufficient, lacking references to closely related studies that could enhance the discussion on low-dimensional parameter spaces and their implications for generalization.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the terminology used, explicitly stating which algorithm the results apply to, and ensuring that the differences between label noise gradient descent and stochastic gradient descent are well articulated. Additionally, we suggest expanding the literature review to include relevant studies that discuss SGD's behavior in low-dimensional parameter spaces. To enhance the theoretical contributions, we encourage the authors to explore novel methods for improving generalization based on their findings, particularly regarding the scheduling of learning rates in a theory-guided manner.