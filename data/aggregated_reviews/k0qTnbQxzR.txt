ID: k0qTnbQxzR
Title: CogCoM: Train Large Vision-Language Models Diving into Details  through Chain of Manipulations
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 4, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CogCoM, a novel approach for training large Vision-Language Models (VLMs) utilizing the Chain of Manipulations (CoM) mechanism. This mechanism allows the model to solve visual problems step-by-step, drawing inspiration from human cognitive processes such as marking and zooming into images. CogCoM integrates manipulations like grounding, zooming, and OCR into its architecture, enabling it to address various visual challenges without external tools. The authors developed a robust data generation pipeline producing 70K high-quality samples, and CogCoM demonstrates state-of-the-art performance across multiple benchmarks.

### Strengths and Weaknesses
Strengths:
1. **Explainable Reasoning and Manipulation Mechanism**: CogCoM provides transparent and explainable reasoning through intermediate steps, enhancing its versatility in complex visual tasks.
2. **Data Generation Pipeline**: The efficient pipeline for generating high-quality training data is crucial for detailed visual reasoning.
3. **Superior Performance**: CogCoM achieves state-of-the-art results across nine benchmarks, showcasing its effectiveness.

Weaknesses:
1. **Design of Figures and Tables**: Figures are poorly designed; the first two figures are repetitive, and the font size in the second figure is too small. Captions for Tables 2 and 3 are too close to the tables.
2. **Lack of Discussion on Related Work**: The paper does not adequately discuss existing related work, such as LLAVA-Plus, which would provide necessary context and comparison.
3. **Data Quality Concerns**: The reliance on GPT-4 and existing models for data generation raises concerns about the quality of generated data, as there are no validation or filtering mechanisms.
4. **Insufficient Benchmarking**: The reported VQA benchmarks are not convincing, and comparisons with more recent models like LLaVA-1.5 and Monkey are lacking.
5. **Missing Results**: There are no test results for the math data included in the CoM dataset, and no results for multi-image multi-turn understanding are provided.
6. **Parameter Clarifications**: Some parameters, such as the maximum turns the model can accept, are not clearly explained.

### Suggestions for Improvement
We recommend that the authors improve the design of figures and tables for clarity and compliance with submission guidelines. Additionally, the authors should enhance the discussion of related work, particularly by citing and comparing with models like LLAVA-Plus, ViperGPT, and others. To address data quality concerns, implementing validation or filtering mechanisms during data generation is essential. Furthermore, providing results both with and without CoM data would clarify its impact. The authors should also include test results for the math data and multi-image multi-turn understanding capabilities. Lastly, clearer explanations of model parameters would enhance the paper's comprehensibility.