ID: xx3qRKvG0T
Title: BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 8, 4, 4, 7, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 3, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents BasisFormer, an end-to-end time series forecasting architecture that leverages learnable and interpretable bases. The authors acquire bases through adaptive self-supervised learning, treating historical and future sections of the time series as distinct views and employing contrastive learning. The architecture includes a Coef module that calculates similarity coefficients between time series and bases using bidirectional cross-attention, and a Forecast module that consolidates bases for accurate future predictions. Extensive experiments demonstrate that BasisFormer outperforms previous methods for both univariate and multivariate forecasting tasks.

### Strengths and Weaknesses
Strengths:
1. The use of adaptive self-supervised learning for basis acquisition is innovative and effectively addresses the problem of tailored modeling for time series data.
2. The Coef module's design allows for precise calculation of similarity coefficients, enhancing the model's predictive capabilities.
3. The paper is well-written and presents a carefully designed network architecture, supported by thorough ablation studies.

Weaknesses:
1. The claim of self-supervised learning potentially leading to weaker performance compared to supervised models raises skepticism; a comparison with other self-supervised time series prediction models is needed.
2. The motivation and necessity of the basis concept in time series forecasting are not clearly articulated, leaving questions about its essential role and verification methods.
3. The experimental results lack comparisons with other self-supervised learning methods and periodic basis approaches, which could strengthen the claims made.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the definition and necessity of bases in time series forecasting, as well as the benefits of basis learning. Additionally, we suggest including comparisons with other self-supervised learning methods to validate the performance claims. Furthermore, enhancing the explanation of Eq. 7 regarding the relationship between explainability and the smooth term, and using the common l2-norm would be beneficial. Lastly, providing additional visualizations of the attention distribution in the BCAB module could help in understanding the network's behavior.