ID: mYniPxMGLL
Title: Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new evaluation framework for text simplification, introducing the SALSA annotation framework and the LENS-SALSA metric. The authors collected 19,000 annotations on 840 simplified outputs, demonstrating the framework's utility through analyses of simplification models and automatic evaluation metrics. The proposed reference-free metric outperforms existing metrics, and the study highlights differences between human and model-generated edits.

### Strengths and Weaknesses
Strengths:
- The paper proposes a comprehensive evaluation framework and dataset that could significantly advance text simplification research.
- The thorough analysis of simplification models and human edits is commendable, providing valuable insights into edit types and their quality.
- The methodology is solid, and the proposed LENS-SALSA metric shows improved performance over existing metrics.

Weaknesses:
- The paper is overly dense and poorly structured, making it difficult to follow the core contributions. Key content is relegated to appendices, obscuring important findings.
- The evaluation framework lacks clarity regarding its applicability to different target audiences, as the definition of successful simplifications is not well articulated.
- Some edit types exhibit low inter-annotator agreement, raising concerns about the reliability of the analyses based on this data.

### Suggestions for Improvement
We recommend that the authors improve the paper's structure by moving Section 6 earlier to provide context for the data used. Additionally, consider breaking the paper into two parts to enhance clarity and focus on core contributions. It would be beneficial to define what constitutes a successful or failed edit more explicitly, considering the target audience's needs. Clarifying the relationship between the 21 edit types and the overall evaluation process is also essential. Finally, addressing the low agreement on certain edit types and justifying the use of only six annotators would strengthen the reliability of the findings.