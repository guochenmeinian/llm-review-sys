ID: A0xVOahTiw
Title: MaNtLE: Model-agnostic Natural Language Explainer
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MaNtLE, a model-agnostic natural language explainer designed to generate explanations for subsets of examples in structured classification tasks. The authors propose fine-tuning a T5 model on thousands of synthetic classification tasks paired with natural language explanations. The evaluation demonstrates that MaNtLE outperforms existing interpretability tools like LIME and Anchors in terms of classification utility and human evaluation metrics.

### Strengths and Weaknesses
Strengths:
- The method is simple yet effective, providing a new direction for language explanations without requiring human labeling.
- Comprehensive experiments are conducted, including comparisons with popular interpretability tools and evaluations of both automatic and human assessments.
- The approach is efficient at inference time, as it generates explanations without needing input perturbation.

Weaknesses:
- Comparisons to baselines are perceived as unfair, as LIME and Anchors were limited to fewer perturbations or training examples.
- The evaluation metrics used do not adequately measure how well MaNtLE explains model behavior, diverging from the claims made in related work.
- The paper lacks comparisons with other natural language explanation methods, focusing instead on feature attribution tools.

### Suggestions for Improvement
We recommend that the authors improve the comparison by including other natural language explanation methods, such as CAGE or WT5, to provide a more balanced evaluation. Additionally, we suggest clarifying the evaluation metrics to better reflect how well MaNtLE explains model behavior, rather than solely focusing on classification utility. It would also be beneficial to discuss the scenarios in which natural language explanations are more advantageous, particularly in relation to tabular datasets. Lastly, addressing the concerns regarding the low classification accuracy and the rationale behind the sampling strategy for automatic evaluation would strengthen the paper.