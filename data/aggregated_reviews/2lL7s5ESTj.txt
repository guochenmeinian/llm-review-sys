ID: 2lL7s5ESTj
Title: Replicability in Learning: Geometric Partitions and KKM-Sperner Lemma
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 7, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of the list-replicable coin problem and the geometric problem of constructing $(k,\varepsilon)$-secluded partitions of $\mathbb{R}^d$. The authors resolve the optimal trade-off between $k,\varepsilon$, and $d$, providing new upper bounds for the list-replicable coin problem that relate list size and sample complexity. The main results include an upper bound for any list size $L \in [2^d] \setminus [d]$, which uses $\tilde{O}(\frac{d^2}{\nu^2\log^2(L)})$ samples per coin, and a near-tight bound on the construction of $(k,\varepsilon)$-secluded partitions. The authors also introduce a neighborhood variant of the KKM/Sperner Lemma, which has implications for the study of algorithmic stability.

### Strengths and Weaknesses
Strengths:  
- The paper addresses a significant topic in learning theory, exploring the trade-off between sample complexity and algorithmic stability.  
- The results provide a new upper bound that interpolates between minimal overhead and optimal list size complexity.  
- The construction of secluded partitions is mathematically elegant and could inspire future research.  
- The paper is well-written, with clear proof intuition and a comprehensive overview of prior literature.  

Weaknesses:  
- The scope of the work may be limited for NeurIPS, as the implications for replicable learning appear somewhat marginal.  
- The upper bound result is closely related to prior work, raising questions about its novelty.  
- The connection between secluded partitions and list-replicability, while interesting, may not significantly advance the field.  
- Some results, particularly the neighborhood variant of Sperner's lemma, feel disconnected from the main focus on list-replicability.

### Suggestions for Improvement
We recommend that the authors improve the related work section to better contextualize their contributions within existing literature on replicable learning. Additionally, we suggest enhancing the discussion on the implications of their results for list-replicable learning, particularly in relation to other approaches beyond secluded partitions. Clarifying the connections between their geometric insights and broader learning theory concepts, such as differential privacy, would also strengthen the paper. Finally, addressing the presentation issues noted, such as typos and clarity in definitions, would improve the overall readability.