ID: cHTWkVH5ft
Title: Collaborative Retrieval for Large Language Model-based Conversational Recommender Systems
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents CRAG, a novel framework that integrates large language models (LLMs) with collaborative filtering (CF) to enhance conversational recommender systems (CRS). The authors claim that CRAG is the first to apply LLMs with CF specifically for CRS, demonstrating improved recommendation performance through experiments on the Reddit-v2 and Redial datasets. The framework includes a two-step reflection process, which significantly enhances recommendation accuracy, particularly for recently released movies. The paper is well-structured, with comprehensive ablation studies and clear documentation for reproducibility.

### Strengths and Weaknesses
Strengths:
- The integration of collaborative filtering with LLMs is novel and addresses key challenges in CRS.
- The methodology is solid, supported by extensive empirical results and ablation studies.
- The paper is well-organized, with clear explanations and diagrams aiding understanding.

Weaknesses:
- The claim of being the first to combine LLMs with CF is questionable, as similar ideas have been explored in other works.
- The experiments are limited to the GPT family, raising concerns about the generalizability of findings.
- Selected baselines are inappropriate, with only the Zero-shot LLM serving as a proper comparison.
- The paper lacks qualitative user studies to assess real-world effectiveness and user satisfaction.

### Suggestions for Improvement
We recommend that the authors improve the baseline comparisons by including more LLM-based conversational recommendation systems to strengthen the experimental results. Additionally, conducting experiments with other recent LLMs, such as LLaMA-3.2 and Qwen-2.5, would support the claim of combining state-of-the-art LLMs with CF. Clarifying the motivation behind the two-step reflection process and providing specific examples of how LLMs infer potential items would enhance the paper's clarity. Lastly, addressing the computational overhead of the two-step reflection process and its implications for real-time recommendations would be beneficial.