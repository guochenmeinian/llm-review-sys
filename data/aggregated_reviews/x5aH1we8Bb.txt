ID: x5aH1we8Bb
Title: Adv3D: Generating 3D Adversarial Examples in Driving Scenarios with NeRF
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 5, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to generating 3D adversarial examples for attacking 3D object detectors in driving scenarios using Neural Radiance Fields (NeRF). The authors propose techniques such as primitive-aware sampling and semantic-guided regularization to enhance the physical realism of the adversarial examples. The effectiveness of their method is validated through extensive experiments, demonstrating a significant reduction in detection performance across various 3D detectors.

### Strengths and Weaknesses
Strengths:
- The work introduces a novel application of NeRFs for generating adversarial examples, leveraging their differentiability for optimization.
- The manuscript is well-written and clear, with insightful analyses provided in sections discussing detector architecture robustness and adversarial training.
- Extensive experiments validate the proposed method's effectiveness and its potential as a data augmentation strategy.

Weaknesses:
- A critical comparison against relevant baselines, particularly mesh-based methods, is missing, which is essential given the similarities in the proposed setup.
- The evaluation of attack imperceptibility is insufficiently addressed, leaving questions about the discernibility of the attacks compared to clean samples.
- Some sections, particularly Section 4.4, lack elaboration, and the photorealism of certain adversarial samples is questionable.
- The practicality of the proposed attack in real-world scenarios is not convincingly demonstrated, and the need for NeRF in this context is debated.

### Suggestions for Improvement
We recommend that the authors improve the manuscript by including comparisons against relevant baselines, particularly the mesh-based baseline [43], to substantiate the necessity of their approach. Additionally, the authors should provide a detailed evaluation of the imperceptibility of their attacks, including metrics on how discernible the attacks are compared to clean samples. Clarifying the rendering process and addressing the photorealism of adversarial samples in Section 4.4 would enhance the overall robustness of the work. Furthermore, we suggest that the authors conduct real-world experiments to better demonstrate the practicality of their attacks and consider including additional related references that were overlooked.