ID: v1kpc060aC
Title: Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 6, 6, 7, 6, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 2, 1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel weighted robust aggregation framework designed to enhance Byzantine-robust training in asynchronous distributed machine learning systems. The authors quantify the difficulty in asynchronous scenarios by considering the number of Byzantine updates, which they argue is a more natural measure than the number of Byzantine workers. They extend the robust aggregation framework to include weights and develop appropriate rules and a meta-aggregator. By integrating this framework with a double momentum mechanism, the authors achieve optimal convergence rates for the first time in asynchronous Byzantine ML.

### Strengths and Weaknesses
Strengths:
- The paper proposes a significant solution for Byzantine robust training in asynchronous distributed systems, supported by rigorous research methods and reliable experimental results.
- It features a coherent structure, clear expression, and substantial implications for asynchronous distributed machine learning.
- The integration of a weighted robust framework with a double momentum mechanism is innovative, achieving optimal convergence rates.
- The paper is well-contextualized within existing literature on Byzantine-robust training.

Weaknesses:
- The experimental evaluation is limited, lacking comparisons with other weighting methods and spanning only a single dataset.
- There is insufficient explanation on how the weight coefficients are calculated within the framework.
- The paper does not discuss its limitations or potential negative social impacts, such as privacy issues or algorithmic biases.
- The complexity of the methodology may pose challenges for practical implementation.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluations by including comparisons with other weighting methods and testing on additional datasets to demonstrate the effectiveness of their approach. Additionally, providing a detailed explanation of how the weight coefficients are calculated would enhance clarity. The authors should also discuss the limitations of their method and its implications on system metrics, such as memory consumption and training speed. Finally, exploring the impact of non-uniform distributions of Byzantine updates could provide deeper insights into the robustness of the proposed method.