ID: SHyVaWGTO4
Title: Unlocking Deterministic Robustness Certification on ImageNet
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 5, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to certified accuracy in neural networks by introducing the Lipschitz Residual Network (LiResNet) and the Efficient Margin Maximization (EMMA) loss function. The authors claim that LiResNet facilitates the computation of the Lipschitz constant and stabilizes robust training by penalizing adversarial examples across all classes. Their experiments on CIFAR-10/100, Tiny-ImageNet, and ImageNet demonstrate state-of-the-art results in certified accuracy under $\ell_2$ perturbations at $\epsilon = 36/255$. Additionally, the paper argues that LiResNet improves clean accuracy over standard ResNet when specifically trained for certified robustness, achieving a better trade-off between expressiveness and Lipschitz bounds, resulting in less over-regularization and tighter certificates.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and easy to follow.
- The proposed method shows strong empirical performance and achieves impressive improvements over state-of-the-art results.
- The architecture's ability to maintain tighter bounds and reduce over-regularization is a significant contribution.
- Empirical results demonstrate LiResNet's superiority in certified robustness training.

Weaknesses:
- The authors overclaim the novelty of the Lipschitz Residual layer, failing to adequately discuss prior works such as the CPL and SLL layers.
- The efficiency and tightness of the approach are not convincingly demonstrated, particularly regarding the use of power iteration for Lipschitz constant estimation.
- The EMMA loss is presented as novel, yet it closely resembles existing methods, lacking sufficient differentiation in the main text.
- Experimental results are limited to a single perturbation level, which does not adequately showcase the robustness of the approach.
- The authors' claims regarding clean accuracy compared to ResNet lack clarity, particularly in defining the baseline for comparison.
- There is confusion regarding the architecture's performance outside of certified training contexts.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related works, particularly the CPL and SLL layers, to clarify the contributions of their Lipschitz Residual layer. Additionally, the authors should provide a more rigorous analysis of the efficiency of their approach, particularly regarding the power iteration method, and clarify how it compares to previous works. The authors should also include a comprehensive ablation study to demonstrate the impact of the EMMA loss compared to existing methods. Furthermore, we suggest presenting results across multiple perturbation levels to better illustrate the robustness of their approach. Lastly, we encourage the authors to revise the title to reflect the deterministic nature of their certification method, such as "Unlocking Deterministic Robustness Certification on ImageNet," and to improve the clarity of their claims regarding clean accuracy in relation to ResNet by explicitly defining the baselines used for comparison. Additionally, it would be beneficial to elaborate on the implications of the architecture's performance in non-certified training scenarios to avoid misunderstandings.