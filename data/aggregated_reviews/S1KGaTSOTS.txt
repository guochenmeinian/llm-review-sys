ID: S1KGaTSOTS
Title: ClusterFomer: Clustering As A Universal Visual Learner
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 6, 7, 6, 6, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ClusterFormer, a vision model that integrates recurrent cross-attention clustering and feature dispatching to enhance feature fusion accuracy. The authors claim that ClusterFormer outperforms baseline architectures across various tasks, including image classification, object detection, and segmentation. The model utilizes a recurrent cross-attention mechanism for clustering and center updates, contrasting with SLIC's distance-based center determination, allowing for more dynamic interaction among features. The authors provide a detailed computational cost analysis, indicating that while the FLOPs of the cross-attention mechanism are lower than those of the Swin Transformer in a single iteration, they increase with more recursions, aligning with Swin's costs after three iterations. The paper discusses the cross-attention mechanism through the lens of the E-M process and emphasizes the model's potential as a universal visual learner, while also addressing explainability by linking features to cluster centers, thereby enhancing transparency and facilitating intuitive understanding of model behavior.

### Strengths and Weaknesses
Strengths:
- The proposed method is evaluated on major tasks in the computer vision field, demonstrating significant accuracy improvements.
- The originality of explaining the cross-attention mechanism from the E-M process perspective is noteworthy.
- The paper achieves high scores on major benchmarks, demonstrating its effectiveness.
- The authors provide comprehensive details on computational costs and architecture, addressing reviewer concerns.
- The explainability of the model is enhanced through clustering, offering clear insights into feature interpretation.

Weaknesses:
- The technical novelty of the clustering module is limited, with similarities to existing methods such as hierarchical clustering, superpixel sampling networks, SSN, and HCFormer; a discussion on these relationships is needed.
- The experiments are conducted with relatively small backbones, and the paper lacks sufficient details for reproducing results, including network architectures and configurations.
- The claims regarding explainability and the model being a "universal vision model" are overstated, as adaptations are necessary for different tasks.
- Computational costs for training larger backbones are significant, requiring extensive resources.
- Some reviewers express confusion regarding the relationship between parameters, FLOPs, and the Swin Transformer.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the relationship between their method and existing clustering techniques, addressing the limitations in novelty. Additionally, the authors should provide detailed information on the architectures of ClusterFormer-tiny and -small, including the overall pipeline for detection and segmentation. It is crucial to report the FLOPs, inference latency, and memory budget for ClusterFormer to clarify its computational costs. We also suggest improving the clarity regarding the initialization of 'k' and its impact on performance, as well as providing a more detailed explanation of the parameter and FLOP comparisons with the Swin Transformer. Finally, we recommend revising the claims of explainability and the model's universality to reflect the need for task-specific adaptations, and exploring alternative architectures could strengthen the paper's contributions. Incorporating these insights into the revised manuscript will enhance its overall clarity and impact.