ID: uRewSnLJAa
Title: Self-Supervised Reinforcement Learning that Transfers using Random Features
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 5, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a self-supervised method for learning approximate multi-step Q-estimates using random features as bases for reward functions in target tasks. The authors propose RaMP, which facilitates fast adaptation to unseen reward functions by learning a set of basis multi-step Q-functions from offline data. The approach addresses limitations in both model-based and model-free reinforcement learning (RL) by implicitly learning transition dynamics without explicitly modeling future trajectories. The paper also explores unsupervised pre-training of RL agents using random reward functions, enabling faster fine-tuning on target tasks.

### Strengths and Weaknesses
Strengths:
- The methodology is novel, leveraging random features for Q-basis formation, which allows for efficient adaptation to downstream tasks.
- The problem formulation is impactful, particularly for generalist RL agents, and the paper is well-written and clear.
- The experimental evaluation is comprehensive, covering multiple environments and including ablation studies.

Weaknesses:
- The experimental evaluation lacks sufficient comparison against meta-RL benchmarks, which is critical given the context of the work.
- The paper's clarity suffers in certain sections, particularly regarding the relation to prior work on successor features and the assumptions made in theorem 3.1.
- The choice of offline dataset and the limited number of environments used in experiments raise concerns about the generalizability of the results.

### Suggestions for Improvement
We recommend that the authors improve the experimental evaluation by including comparisons against meta-RL methods, particularly since the proposed approach is evaluated on a meta-RL benchmark. Additionally, we suggest that the authors clarify the relationship to prior work on successor features and provide a more detailed discussion of the trade-offs associated with using random reward projections, especially in high-dimensional image-based environments. Including more diverse baselines, such as comparisons with IQL and goal-conditioned offline RL methods, would also strengthen the paper. Finally, we encourage the authors to enhance the clarity of sections that discuss the assumptions and implications of their theoretical contributions.