ID: 1g7HWFmvY0
Title: Small-scale adversarial perturbations expose differences between predictive encoding models of human fMRI responses
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 7, 8, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the adversarial robustness of encoding-decoding vision deep learning models in relation to fMRI brain activations. The authors propose that robustness against adversarial perturbations could identify the most "human-like" models, utilizing eight pre-trained and two adversarially trained models. Key findings reveal significant differences in models' sensitivity to adversarial attacks, although the impact of perturbation size is limited. The authors note that adversarial training enhances robustness but reduces fMRI prediction accuracy. They suggest that employing sparser mappings may improve robustness, though the relevance of this suggestion is unclear. Additionally, the paper emphasizes the trade-offs between model robustness and predictivity, proposing that layer selection should be based on existing knowledge while minimizing subjective choices. The authors assess sensitivity through the difference in predicted responses between unperturbed and perturbed images and acknowledge the limitations of their current approach, including the need for more comprehensive exploration of model layers and parameters.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, structured, and addresses a significant gap in the literature regarding adversarial sensitivity and brain prediction ability.
- It explores multiple encoding models and mapping metrics, providing valuable insights into model performance.
- The connection between sparsity and robustness is intriguing and relevant to current discussions in explainable AI.
- The authors maintain a rigorous methodological pipeline that minimizes subjective choices in layer selection.
- The study highlights the significant trade-off between adversarial robustness and predictive accuracy in fMRI response modeling.

Weaknesses:
- The interpretation of results lacks clarity, particularly regarding the implications of adversarial robustness for model quality.
- The analysis relies on averaged data from multiple brain regions without sufficient illustration of individual region performance.
- Recent advancements, such as the inclusion of the CLIP model, are not addressed.
- The choice of focusing on specific perturbations raises questions about the symmetry of the sensitivity metric.
- The lack of exploration of different layers and the use of a fixed Lasso alpha parameter may limit the insights into model performance.

### Suggestions for Improvement
We recommend that the authors improve the justification for using adversarial robustness as a litmus test for brain encoding models, clarifying its significance in evaluating model quality. Additionally, stronger justification is needed for why adversarial sensitivity serves as a proxy for learning the "right" features. We suggest exploring alternative regression methods like Lasso or Ridge to assess potential differences in trends, while also tailoring the Lasso alpha values within subjects and aggregating results to enhance robustness. Expanding the dataset size for training and hyperparameter tuning could improve model performance. Cross-validation across layers should be implemented to validate the choice of intermediate layers, rather than relying solely on a priori selections. Exploring voxel-level data instead of ROI averages may provide more informative insights. Lastly, the authors should clarify the adversarial sensitivity metric to ensure it accounts for both increases and decreases in predicted activity, and clarify the relationship between adversarial training and prediction performance, particularly regarding statistical significance in their results.