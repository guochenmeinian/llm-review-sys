ID: xRQxan3WkM
Title: The Implicit Bias of Adam on Separable Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the implicit bias of the Adam optimizer in the context of binary classification using a single-layer linear model on separable data. The authors demonstrate that, under the assumption of a zero stability constant $\epsilon$, Adam converges to the maximum $\ell_\infty$-margin solution, contrasting with the maximum $\ell_2$-margin solution achieved by gradient descent. The paper characterizes the convergence rate for various learning rates and includes numerical experiments to support its theoretical findings.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written and well-organized, making it easy to follow the argument and motivation. The proof sketch effectively conveys the theoretical conclusions.
- The results are novel, revealing an $\ell_\infty$-norm implicit bias of Adam, which differs from previous work that focused on $\ell_2$-norm solutions.

Weaknesses:
- The omission of the stability constant $\epsilon$ limits the characterization of its influence, which can be significant, as noted in prior studies. This raises concerns about the robustness of the conclusions drawn.
- The analysis is confined to a simple setting without stochastic sampling noise, which is often present in practical scenarios. The authors should consider repeating experiments from previous works to validate their claims.

### Suggestions for Improvement
We recommend that the authors improve the analysis by starting with a non-zero $\epsilon$ and treating $\epsilon=0$ as a special case to better capture its effect on implicit bias. Additionally, incorporating stochastic sampling noise into the model would enhance the practical relevance of the findings. The authors should also address the optimality of the convergence rates presented in Corollary 4.7 and clarify the implications of their results in comparison to adaptive learning rates in gradient descent.