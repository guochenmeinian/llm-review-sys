ID: phnGilhPH8
Title: FedFed: Feature Distillation against Data Heterogeneity in Federated Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 7, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach called Federated Feature Distillation (FedFed) to address data heterogeneity in Federated Learning (FL) while ensuring privacy. The authors propose partitioning data into performance-sensitive and performance-robust features, sharing only the former among clients to minimize privacy risks. The incorporation of differential privacy (DP) further enhances privacy protection. The paper includes theoretical analyses and empirical evaluations to support its claims.

### Strengths and Weaknesses
Strengths:
- The method effectively mitigates data heterogeneity in FL, demonstrating improved model performance.
- The simplicity of the approach allows for easy integration with existing FL algorithms.
- The incorporation of DP provides an additional layer of privacy protection.

Weaknesses:
- The evaluation of privacy leakage is insufficient, lacking quantitative measurements and comprehensive assessments of potential privacy attacks.
- The experiments are limited to vision datasets and a small number of clients, raising concerns about generalizability.
- The practicality of the method is unclear due to unquantified overheads related to computation, communication, and storage.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of privacy leakage by including quantitative metrics such as peak signal-to-noise ratio (PSNR) and Frechet inception distance (FID) to strengthen empirical evidence. Additionally, the authors should expand the evaluation to include a wider range of datasets and client numbers to assess generalizability. It is crucial to provide a more comprehensive discussion of the privacy-performance trade-off and to clarify the definitions of performance-sensitive and performance-robust features. Furthermore, we suggest that the authors address the additional communication costs and computational overheads associated with the FedFed method, particularly in realistic settings. Lastly, including more recent FL models as baselines would enhance the evaluation of the proposed method's effectiveness.