ID: 0HRRNEAQFp
Title: A General Protocol to Probe Large Vision Models for 3D Physical Understanding
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 5, 7, 7, -1, -1
Original Confidences: 4, 4, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation of large-scale vision models regarding their encoding of 3D properties in images. The authors propose a lightweight protocol that trains discriminative classifiers on features from pre-trained models to assess various physical properties of 3D scenes, including geometry, materials, support relations, lighting, shadow, occlusion, and depth. The study applies this protocol to models like CLIP, DINOv1, DINOv2, VQGAN, and Stable Diffusion, revealing that while Stable Diffusion and DINOv2 excel in certain properties, they struggle with occlusion and material properties. The findings suggest that different layers and time steps of these models are suited for different properties, which may enhance 3D understanding applications.

### Strengths and Weaknesses
Strengths:
1. The paper proposes a novel protocol for assessing 3D awareness in large-scale vision models, considering various physical properties in a lightweight manner.
2. The structure and writing are clear and easy to follow.
3. The investigation addresses a significant and interesting problem, offering valuable insights into the 3D awareness of large-scale models.

Weaknesses:
1. The effectiveness of the proposed probes in reflecting 3D understanding is unclear, as some properties may be identified with 2D clues. The use of linear SVM for binary questions may not adequately assess occlusion and depth.
2. There is a lack of baselines for linear probing, making it difficult to understand how a 3D-aware model trained with 3D data would respond to probes.
3. The paper shares similar goals with another study, and a comparison with that work is recommended.
4. Evaluating additional large vision models, such as SAM and MAE, would enhance the comprehensiveness of the study.

### Suggestions for Improvement
We recommend that the authors improve the clarity of how the proposed probes reflect 3D understanding, particularly for properties like material and occlusion. Additionally, providing upper and lower bounds for the probes would justify their effectiveness in reflecting 3D awareness. It would also be beneficial to include a comparison with the paper “Probing the 3D awareness of visual foundation models” to contextualize the findings. Finally, we suggest evaluating more large vision models, such as SAM and MAE, to broaden the analysis.