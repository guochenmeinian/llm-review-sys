ID: wm9JZq7RCe
Title: An Analysis of Tokenization: Transformers under Markov Data
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 6, 6, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the role of tokenization in transformer models, demonstrating that without tokenization, transformers struggle to achieve low cross-entropy loss on next-word prediction. The authors show that tokenization enables models to surpass the limitations of unigram models and provide a theoretical framework for understanding the information tokenizers contribute. The study includes empirical analyses comparing tokenized and non-tokenized models across various data-generating processes, particularly focusing on $k$th-order Markov processes.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, combining empirical observations with theoretical insights, enhancing understanding.
- It addresses a significant problem in NLP, contributing valuable theoretical work and interesting empirical results, particularly regarding BPE and unigram models.

Weaknesses:
- The focus on a rarely used tokenizer in Section 3.2 may limit the paper's applicability; guarantees for more common tokenizers like BPE would be more beneficial.
- The theoretical results are based on specific data distributions (kth-order Markov), raising questions about their relevance to real linguistic distributions.
- The presentation is hindered by numerous typos and unclear figures, making it difficult to follow the arguments.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their presentation by addressing typographical errors and enhancing the explanations of complex figures. Additionally, we suggest that the authors expand their discussion on the practical implications of their findings for current tokenization methods, particularly focusing on commonly used tokenizers like BPE. Finally, we encourage the authors to clarify the relationship between their theoretical results and real-world language modeling to strengthen the paper's relevance.