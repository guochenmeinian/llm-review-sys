ID: Of2xc2GVid
Title: On the Calibration of Large Language Models and Alignment
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the confidence calibration of large language models (LLMs) across three tasks: causal language modeling, T-REx (factual knowledge), and MMLU (multitask language understanding). The authors propose that calibration improves with larger model sizes and longer pre-training times, while instruction tuning negatively impacts calibration, and RLHF alters calibration without significantly degrading it. The analysis employs metrics such as Reliability Diagram and Expected Calibration Error (ECE) to evaluate model performance.

### Strengths and Weaknesses
Strengths:
- The paper addresses the important issue of calibration in LLMs, which is crucial for their safe deployment.
- It is well-written and accessible, providing a thorough investigation of the LLM pipeline from pre-training to instruction tuning and RLHF.
- The findings contribute new insights into the dynamics of calibration across different tasks.

Weaknesses:
- The evaluation methods, particularly for causal language modeling, are not convincing due to the lack of a ground truth for accuracy calculations.
- The performance on MMLU is too low to warrant discussions on calibration.
- The paper lacks performance metrics for models fine-tuned after instruction tuning and RLHF, making it difficult to assess their effectiveness.
- Conclusions drawn about overfitting and ECE are not sufficiently supported by evidence, and the correlation between semantic diversity and ECE lacks causal analysis.
- All experiments are based on a single run, raising questions about the robustness of the results.

### Suggestions for Improvement
We recommend that the authors improve the evaluation methods for causal language modeling to provide a clearer rationale for accuracy calculations. It would be beneficial to include performance metrics for models fine-tuned after instruction tuning and RLHF to contextualize calibration results. We suggest clarifying the connections between overfitting, ECE, and semantic diversity with more robust analyses. Additionally, conducting multiple runs with varied hyperparameters would enhance the reliability of the findings. Finally, tightening the text in the introduction could allow for a deeper exploration of the implications of poorly calibrated models.