ID: MLLp6AHQFs
Title: LOVM: Language-Only Vision Model Selection
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel task called Language-Only Vision Model (LOVM) selection, aimed at identifying the best-performing vision-language model for a specific downstream visual zero-shot application using only textual descriptions. The authors create a benchmark dataset and establish baselines through text generation and cross-modality transferability, demonstrating that model selection can be effectively achieved without relying on image datasets. Empirical results indicate that this method outperforms traditional image-based selection. Additionally, the authors conduct experiments addressing concerns about ground truth accuracy rates and the impact of using different large language models (LLMs) on performance metrics. They demonstrate that small errors in evaluation do not significantly affect the overall k_tau and R_5 rankings, even when introducing substantial noise, and that using a different LLM affects performance but does not alter the overall conclusions regarding the effectiveness of LOVM methods.

### Strengths and Weaknesses
Strengths:
- The introduction of the LOVM task is innovative and practically useful, allowing for model selection without the need for image datasets.
- Extensive experiments are conducted, evaluating 35 pre-trained VLMs across 23 datasets with various metrics.
- The paper effectively demonstrates the feasibility of cross-modality transferability for estimating VLM performance and provides insightful analyses of VLM behaviors.
- The authors conducted thorough additional experiments to address reviewer concerns, indicating engagement with the review process.
- Results indicate that small variations in evaluation do not drastically impact model rankings, supporting the robustness of the LOVM methods.

Weaknesses:
- The reliance on VLMs from the open-clip library limits the generalizability of LOVM to other architectures and pretrained datasets.
- The applicability of LOVM is restricted to VLMs with modality-specific encoders, potentially limiting its use as unified architectures gain popularity.
- The benchmark primarily consists of classification datasets, raising concerns about the conclusions drawn for other tasks with fewer classes.
- The impact of noise on correlation between models and ground truth performance could be more clearly articulated.
- The reliance on different LLMs raises questions about the consistency of results across models.

### Suggestions for Improvement
- We recommend that the authors expand the range of VLMs evaluated to include those pretrained with different objectives beyond CLIP or CoCa styles to assess the generalizability of LOVM.
- The authors should clarify the motivation for using text to predict accuracy on image datasets, possibly enhancing the introduction with supportive experimental results.
- We suggest including segmentation and detection tasks in future benchmarks to provide a more comprehensive evaluation of VLMs.
- The authors should address the small differences in ground truth accuracy rates in the supplementary material, as these may affect the reliability of Kendall's coefficient.
- We encourage the authors to quantify the efficiency of the proposed benchmark in terms of time and computational savings compared to traditional evaluation methods.
- We recommend that the authors improve the clarity of the discussion surrounding the impact of noise on model rankings and correlation with ground truth performance. Additionally, consider providing more detailed analysis on how different LLMs might affect the reproducibility of results in future studies.