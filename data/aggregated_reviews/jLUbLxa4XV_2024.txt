ID: jLUbLxa4XV
Title: Certified Adversarial Robustness via Randomized $\alpha$-Smoothing for Regression Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for enhancing certified robustness in regression models through randomized smoothing and an $\alpha$-trimming procedure. The authors propose a new probabilistic certificate bound applicable to regression models with unbounded outputs, addressing the limitations of prior work in this area. The framework focuses on certifying machine learning models in regression tasks, emphasizing the robustness of predictions under perturbations. The authors establish several propositions and theorems that clarify conditions for robustification and certification. Experiments demonstrate the effectiveness of the proposed approach on synthetic datasets and camera re-localization tasks, and key definitions, such as the accepted region for predictions, are clarified in the revised manuscript.

### Strengths and Weaknesses
Strengths:  
- The authors tackle an important problem of extending certified robustness from classification to regression, which is relevant to the Neurips audience.  
- The theoretical insights and rigorous proofs provided are commendable, and the certification bound is valid for any regression model with bounded or unbounded output.  
- Significant revisions have been made to clarify definitions and concepts, particularly regarding the accepted region and the relationship between outputs and regions in their figures.  
- The paper provides a solid foundation for understanding the robustness of regression models, with well-defined propositions and theorems.  
- The authors are responsive to reviewer feedback, incorporating suggestions to enhance clarity and depth.

Weaknesses:  
- The paper requires extensive revision for clarity, particularly in defining terminology and articulating the assumptions behind mathematical results.  
- Some definitions and explanations remain unclear, particularly regarding the differences between outputs within and outside specific regions in figures.  
- There is a heavy reliance on prior work, with limited novelty beyond the incorporation of $\alpha$-trimming.  
- The arbitrary choice of parameters in Proposition 1 raises concerns about the general applicability of the results.  
- Key terms such as "accepted region" and "bag" are not clearly defined, leading to confusion.  
- The potential for decreases in robustness, as highlighted by the reviewers, is not adequately addressed in simple terms.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing explicit definitions for all key terms, particularly "accepted region" and "bag." Additionally, we suggest including a more detailed discussion of how the proposed method differs from prior works, particularly those referenced in [17]. It would be beneficial to clarify the meaning of "accepting prediction" in the context of regression models and to provide straightforward examples for equation (12) to illustrate the advantages of the $\alpha$-trimming filter. Furthermore, we encourage the authors to enhance the explanations of Figures 4 and 5 to aid reader comprehension and to include comprehensive comparative results with other certified robustness methods. We also recommend that the authors state and prove Proposition 1 in its most generally applicable form to enhance its utility. It would be beneficial to explicitly mention the possibility of decreases in robustness due to the $\alpha$-trimming procedure, using straightforward language to ensure reader comprehension. Finally, including comparisons between smoothing parameters in classification versus regression around Theorem 3 would further enrich the manuscript.