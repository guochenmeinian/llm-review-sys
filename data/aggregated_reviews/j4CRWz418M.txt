ID: j4CRWz418M
Title: Are Large Language Models Good Statisticians?
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 7, 5, 9, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents StatQA, a benchmark designed for evaluating large language models (LLMs) in statistical analysis tasks, particularly focusing on hypothesis testing methods. The authors reveal that even state-of-the-art models like GPT-4o achieve only 64.83% accuracy, indicating significant room for improvement. The study highlights the differences in error types between LLMs and human performance, suggesting potential benefits from combining both. The benchmark comprises 11,623 examples and aims to promote further research in this area. Additionally, the paper is well-received, and the authors provide valuable feedback on the review process, leading to a clear recommendation for acceptance.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel benchmark for assessing LLMs in statistical analysis, addressing a significant gap in existing literature.
- It provides comprehensive evaluations of various LLMs, including comparisons with human performance, revealing insights into their strengths and weaknesses.
- The dataset construction covers a diverse range of statistical tasks and methods, enhancing its relevance.
- The authors' feedback on the review process is considered constructive and informative.

Weaknesses:
- The dataset relies solely on synthetic data generation, which raises concerns about quality and validation.
- Some sections lack clarity, particularly regarding the dataset construction pipeline and the criteria for categorizing question difficulty.
- The choice of models and methods used for evaluation is limited, potentially affecting the robustness of findings.
- The reviewer was initially unaware of the revision policy during the rebuttal period, which may indicate a need for clearer communication regarding submission guidelines.

### Suggestions for Improvement
We recommend that the authors improve the visibility of the benchmark link, possibly by placing it in a footnote on the first page. Additionally, consider including examples of LLM applicability errors in the main text rather than just the appendix. Expanding the benchmark to include causality aspects, similar to the QRData benchmark, could enhance its relevance. 

Clarifying the definition of accuracy in the experiments and including error bars in Table 2 would provide a better understanding of the significance of results. Furthermore, measuring precision and recall alongside accuracy could help identify data imbalances. 

We suggest using more powerful models for question refinement and expanding the candidate pool for human evaluations to ensure more significant performance metrics. Lastly, providing more details on the dataset construction process and the domain knowledge strategy would strengthen the paper's methodology. We also recommend improving clarity around submission and rebuttal policies to ensure all reviewers are fully informed.