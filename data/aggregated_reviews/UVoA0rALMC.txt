ID: UVoA0rALMC
Title: Few-shot Unified Question Answering: Tuning Models or Prompts?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into unified question-answering (QA) models for low-resource scenarios, focusing on the effectiveness of model tuning and prompt tuning. The authors demonstrate that prompt tuning can outperform model tuning in few-shot settings with appropriate initialization. They conduct a comprehensive analysis using 16 QA datasets, highlighting the benefits of parameter-sharing and simple knowledge transfer techniques for prompt initialization.

### Strengths and Weaknesses
Strengths:  
- The paper provides a systematic and comprehensive analysis, supported by a sufficient number of experiments.  
- It is well-organized and accessible, enhancing readability for a broad audience.  
- The findings on prompt tuning and parameter-sharing are significant for practitioners in QA, especially in low-resource contexts.  

Weaknesses:  
- The lack of comparison with larger language models (LLMs) such as chatGPT or Claude diminishes the paper's impact.  
- The investigation is limited to T5-base and T5-large models, raising questions about the generalizability of the findings.  
- Some claims, particularly regarding the effectiveness of prompt tuning, may not hold true with larger models, which could outperform the proposed methods.  

### Suggestions for Improvement
We recommend that the authors improve the paper by including comparisons with larger language models to strengthen the investigation's relevance. Specifically, evaluating the performance of prompt tuning and model tuning with models like LLaMA 6B/17B would provide valuable insights. Additionally, we suggest contextualizing results in Table 5 with existing literature and exploring alternative methods such as LoRA, which may offer more promising results than prompt tuning. Clarifying the definitions of tasks and including standard deviations in the core article would also enhance the clarity and robustness of the findings. Lastly, an explanation of PT(R) or PT(B) in Table 6 should be added for better understanding.