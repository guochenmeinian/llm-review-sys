ID: Z7Cz9un2Fy
Title: NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 4, 6, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a knowledge distillation approach, termed Neighbor Exitwise Orthogonal Knowledge Distillation (NEO-KD), aimed at enhancing the adversarial robustness of multi-exit networks. The authors propose two main strategies: (i) Neighbor Knowledge Distillation (NKD), which generates teacher predictions by averaging outputs from neighboring exits to improve robustness and reduce adversarial transferability, and (ii) Exit-wise Orthogonal Knowledge Distillation (EOKD), which distills clean data outputs to adversarial examples while promoting orthogonality among predictions. The experiments demonstrate that NEO-KD outperforms existing methods across various datasets, including MNIST, CIFAR-10, and Tiny-ImageNet, while also addressing the challenges of adversarial transferability.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and organized, with a clear explanation of the proposed approach and substantiated claims through extensive experiments.
2. NEO-KD achieves superior adversarial accuracy against various attack types across different budget setups.
3. The method effectively reduces adversarial transferability among exits in multi-exit networks.

Weaknesses:
1. NEO-KD exhibits low performance at later exits for smaller datasets like MNIST and CIFAR-10, and at early exits for mid-scale datasets like TinyImageNet.
2. The paper lacks comparisons with general defense methods, such as TEAT, and does not clearly articulate the advantages of distillation methods over other adversarial defenses.
3. The motivation and operation of EOKD, particularly the orthogonal labeling operation, are not sufficiently clarified, leaving some aspects confusing.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation and operation of EOKD, particularly regarding the orthogonal labeling operation and its relation to adversarial transferability reduction. Additionally, we suggest including comparisons with general defense methods like TEAT to better contextualize the advantages of NEO-KD. It would also be beneficial to demonstrate results using larger datasets, such as ImageNet, to validate the efficacy of the proposed method on a broader scale. Finally, please clarify the baseline settings used in the experiments, particularly the choice of utilizing the last exit for distillation.