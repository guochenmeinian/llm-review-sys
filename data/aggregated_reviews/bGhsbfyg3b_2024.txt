ID: bGhsbfyg3b
Title: Opponent Modeling with In-context Search
Conference: NeurIPS
Year: 2024
Number of Reviews: 24
Original Ratings: 4, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Opponent Modeling with In-context Search (OMIS) algorithm, which integrates in-context learning and in-context search within a multi-agent reinforcement learning (MARL) framework. The authors argue that their approach addresses significant challenges in opponent modeling by providing theoretical guarantees for generalization and demonstrating superior performance against existing baselines. OMIS is evaluated against SP-MCTS, showing consistent outperformance across various environments, particularly in the Predator Prey and OverCooked settings. The authors provide aggregated performance metrics using IQM and 95% confidence intervals, reinforcing the statistical significance of their findings. They also clarify the novelty of OMIS, emphasizing its unique application of in-context learning as a sequence-to-sequence problem, which has not been previously applied in this domain.

### Strengths and Weaknesses
Strengths:
- The paper focuses on a critical challenge in multi-agent reinforcement learning: generalizing to unseen opponent policies during testing, enhancing adaptability and robustness.
- OMIS demonstrates superior performance compared to SP-MCTS across multiple environments and settings.
- The inclusion of hyperparameter tuning for SP-MCTS allows for a fairer comparison.
- The theoretical analysis demonstrates unique properties of the OMIS algorithm, including performance guarantees for both seen and unseen opponent policies.
- The literature review is comprehensive, showcasing a deep understanding of the opponent modeling problem.
- The empirical evaluation is thorough, encompassing diverse environments and comparing against several baselines, with statistical validity strengthened by the use of IQM and confidence intervals.

Weaknesses:
- The work lacks novelty, as the combination of in-context learning-based pretraining and in-context search is not sufficiently distinct from existing methods.
- Some reviewers express concerns about the novelty of applying in-context learning within a common MARL framework, suggesting that it may not be sufficiently innovative.
- OMIS relies on strong assumptions regarding access to environment dynamics, which may provide an unfair advantage over baselines.
- The experiments are relatively simple, raising concerns about OMIS's scalability in more complex environments, particularly its reliance on extensive pre-training and computational resources for online inference.
- Despite improvements, concerns about the clarity of writing and the significance of the method remain.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their approach by clearly articulating how their method differs from existing techniques. Additionally, we suggest that the authors clarify the significance of their method in the context of existing literature to strengthen the argument for its novelty. The authors should address the strong assumptions regarding environment dynamics, potentially by comparing OMIS with model-based approaches like MCTS. To enhance scalability, we encourage the authors to conduct experiments in more complex environments to evaluate performance under realistic conditions. Furthermore, we recommend improving the clarity of the writing by providing a more detailed revision plan that addresses specific areas of ambiguity, such as the roles of components in the algorithm and the definitions of key terms. Simplifying the notation for partial trajectories by using time-slice indexing would also be beneficial. Lastly, we suggest ensuring that IQM is plotted aggregated over tasks to provide a clearer overview of performance across different settings and providing a statistical significance analysis of results to strengthen the empirical findings.