ID: jGyMUum1Lq
Title: Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 5, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents modifications to the conventional mIoU metric for semantic segmentation, aiming to address biases towards large objects and improve evaluation granularity. The authors propose fine-grained mIoU metrics, including class-wise, image-wise, instance-wise, and worst-case metrics, and conduct extensive experiments across various datasets and architectures to validate their claims. Additionally, the paper includes a comprehensive benchmark study evaluating 15 modern neural network architectures on 12 diverse natural and aerial segmentation datasets. The authors argue that their work falls within the NeurIPS Datasets and Benchmarks Track, providing systematic analyses of existing systems and advocating for fine-grained mIoUs to address the limitations of conventional mIoU. They clarify their rationale for training models from scratch to ensure fair comparisons and highlight the importance of a comprehensive codebase for reproducibility.

### Strengths and Weaknesses
**Strengths:**
- The motivation for the new metrics is clear and well-articulated.
- Comprehensive evaluations of multiple segmentation models across diverse datasets are conducted.
- The proposed metrics are technically sound and provide richer statistical insights, potentially guiding future algorithm designs.
- The thoroughness of the benchmark study and the identification of mislabels in existing datasets contribute valuable insights to the field.

**Weaknesses:**
- The modifications to the mIoU metric are considered trivial, lacking substantial novelty to attract a wider audience.
- The rationale for training models from scratch is unclear, leading to inconsistencies in the paper's narrative.
- Limitations of the proposed metrics are not adequately discussed, particularly regarding their impact on performance evaluation.
- Concerns exist about the potential limited adoption of these metrics due to their reliance on the prior IoU paradigm and the clarity of their training methodology.
- The choice to exclude null classes and the focus on typical datasets may limit the broader applicability of their findings.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the proposed metrics to better convince the audience of their significance. Additionally, clarify the reasoning behind training models from scratch to maintain a consistent storyline. A more thorough discussion of the limitations of the proposed metrics is necessary, particularly concerning their computational costs and the exclusion of non-ground truth classes. We also recommend improving the clarity of the rationale for excluding null classes, especially in safety-critical contexts, to address concerns about false positives and negatives. Visual examples should be included to illustrate the limitations of traditional metrics, and a clearer differentiation between semantic and panoptic segmentation metrics is advised. Furthermore, consider exploring a more diverse set of datasets in future research, including those from medicine, biology, and engineering, to enhance the breadth of their work. Lastly, ensuring that the GitHub link is fully functional and that the code is readily accessible by the camera-ready deadline will further support the reproducibility of their study.