ID: BC1IJdsuYB
Title: Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a technique called Selective Amnesia, enabling controllable forgetting of concepts in pretrained deep generative models. The authors propose a continual learning approach that combines Elastic Weight Consolidation (EWC) and Generative Replay (GR) to retrain models while promoting the forgetting of specific examples. The technique is evaluated across various datasets, including simple models and large-scale applications like Stable Diffusion, demonstrating its potential for addressing trustworthiness issues in generative models.

### Strengths and Weaknesses
Strengths:
1. The manuscript addresses a critical challenge in generative AI, focusing on content safety and the prevention of harmful outputs.
2. The application of continual learning principles to generative model safety is innovative and enhances the manuscript's contribution.
3. The diverse selection of datasets for empirical evaluation validates the model's robustness and versatility.
4. The quality of exposition is high, presenting a clear narrative and comprehensive model description.

Weaknesses:
1. The quantitative evaluation is limited to simpler datasets, necessitating extension to more challenging datasets for a comprehensive performance perspective.
2. The selection of the parameter $q$ lacks thorough analysis and appears arbitrary, requiring more detailed explanation.
3. The experimental results for Stable Diffusion do not outperform baseline methods, particularly in erasing concepts.
4. The proposed method promotes replacing concepts rather than enforcing true forgetting, which may lead to unintended associations.

### Suggestions for Improvement
We recommend that the authors improve the quantitative evaluation by extending it to more complex datasets and incorporating human assessments for accuracy. Additionally, a thorough discussion on baseline models and state-of-the-art comparisons would enhance the context of their work. The authors should provide a detailed analysis of the selection process for $q$ and its implications. Furthermore, we encourage the authors to clarify how their method ensures true forgetting rather than mere replacement of concepts, potentially by exploring techniques that automatically identify suitable surrogate datasets.