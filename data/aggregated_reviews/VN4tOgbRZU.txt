ID: VN4tOgbRZU
Title: Something for (almost) nothing: improving deep ensemble calibration using unlabeled data
Conference: NeurIPS
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4

Aggregated Review:
### Key Points
This paper presents ν-ensembles, an algorithm designed to enhance the accuracy and calibration of deep ensembles while maintaining computational efficiency. The authors propose generating random labels for unlabeled data, allowing each ensemble member to train on different labelings, which promotes diversity among predictions. Through PAC-Bayesian analysis and empirical evaluations on CIFAR-10 and CIFAR-100, the authors demonstrate that ν-ensembles outperform standard ensembles in calibration metrics, both in-distribution and out-of-distribution.

### Strengths and Weaknesses
Strengths:
- ν-ensembles achieve significantly better calibration across all metrics compared to standard ensembles, indicating improved reliability in predicted probabilities.
- The Mutual Information (MI) of ν-ensembles is lower, suggesting enhanced diversity among ensemble members.
- Improvements in calibration and diversity are consistent across various architectures and datasets.
- The method maintains O(1) memory cost and competitive training time, allowing for efficient parallel training.

Weaknesses:
- ν-ensembles do not significantly outperform standard ensembles in terms of accuracy.
- Calibration improvements diminish with larger training sets, indicating a potential limitation in scalability.
- The analysis would benefit from including more complex datasets to validate findings further.
- The choice of small, similar datasets limits the generalizability of the results.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the influence of the size of the unlabeled dataset, the presence of noise, and equal class representation in the unlabeled data. Additionally, including out-of-distribution unlabeled data could provide further insights. Clarifying the significance of the colors in Table 1 and adding a legend in Figure 2 would enhance the manuscript's clarity. Furthermore, it would be beneficial to explore whether the unlabeled data is also utilized by other methods and how this affects performance compared to training solely on labeled data. Lastly, consider including results for MLPs on CIFAR and bolding the best method in results tables for clarity.