ID: 9JFSJitKC0
Title: Spectral-Risk Safe Reinforcement Learning with Convergence Guarantees
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 7, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a spectral-risk-constrained reinforcement learning (RL) algorithm, termed spectral-risk-constrained policy optimization (SRCPO), which employs a bilevel optimization framework. The outer problem optimizes dual variables derived from spectral risk measures, while the inner problem identifies an optimal policy given these dual variables. The authors claim that this is the first method to guarantee convergence to an optimum in the tabular setting, supported by empirical results demonstrating superior performance in continuous control tasks.

### Strengths and Weaknesses
Strengths:
1. The problem of spectral risk measure-constrained RL is well-motivated and applicable to safety-critical scenarios such as healthcare and finance.
2. The paper is well-written and easy to follow, with clear introductions to complex notions.
3. The authors provide a general algorithm framework with convergence guarantees for a family of spectral risk-constrained RL problems.
4. Empirical evaluations demonstrate the proposed algorithm's performance superiority compared to other methods.

Weaknesses:
1. The approach of handling risk-sensitive RL through inner and outer problems is not novel, as it has appeared in prior works.
2. The format of Algorithm 1 lacks clarity, particularly regarding specific algorithm steps such as policy updates and distribution updates.
3. The theoretical guarantees for the overall performance of the spectral risk measure-constrained RL problem are not provided.
4. The paper does not discuss non-asymptotic rates for convergence or sample complexity guarantees.
5. Some definitions and notations are unclear, and there are minor typographical errors.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Algorithm 1 by including specific steps for policy updates and distribution updates. Additionally, the authors should provide theoretical guarantees for the overall performance of the spectral risk measure-constrained RL problem, including comments on convergence rates and sample complexity. We also suggest that the authors clarify the definitions and notations used throughout the paper to avoid confusion. Lastly, addressing the novelty of the loss function and exploring its generalizability could strengthen the paper's contributions.