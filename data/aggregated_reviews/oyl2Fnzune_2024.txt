ID: oyl2Fnzune
Title: Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 7, 3, 8, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Uni-Med, a medical generalist foundation model aimed at multi-task learning across six medical tasks. The authors propose a Connector-Mixture-of-Experts (CMoE) module to address the tug-of-war problem in multi-modal optimization, achieving competitive performance without task-specific fine-tuning. Extensive experiments and ablation studies validate the model's effectiveness and efficiency in handling large-scale multi-modal medical data.

### Strengths and Weaknesses
Strengths:
- The introduction of the CMoE module is novel and effectively addresses the tug-of-war problem at the connector level.
- Comprehensive ablation studies validate the CMoE module's effectiveness across various configurations.
- Uni-Med demonstrates impressive performance with minimal computational overhead, showcasing its efficiency.

Weaknesses:
- Certain configurations in the ablation studies, such as a high number of projection experts, may lead to overfitting, which requires further discussion and mitigation strategies.
- The interpretation analysis focuses solely on visual features across tasks, neglecting the analysis of visual features across different medical image modalities.
- The related work section lacks comprehensive references to existing evaluations of medical vision-language models, such as [1,2].
- The model currently supports only 2D images, while many medical imaging modalities, like CT and MRI, are 3D.
- Evaluation metrics for the report generation task should include RadGraph Score and RadCliQ, as BLEU and ROUGE do not fully capture semantic accuracy.

### Suggestions for Improvement
We recommend that the authors improve the discussion on overfitting in the ablation studies by including strategies to mitigate it. Additionally, the authors should consider expanding the interpretation analysis to include visual features from various medical image modalities. We also suggest enhancing the related work section with more comprehensive references to relevant evaluations of medical vision-language models. Furthermore, the authors should explore extending the model's capabilities to support 3D images and incorporate more appropriate evaluation metrics for the report generation task.