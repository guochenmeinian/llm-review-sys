ID: I3IuclVLFZ
Title: FedLPA: One-shot Federated Learning with Layer-Wise Posterior Aggregation
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FedLPA, a novel one-shot federated learning method that utilizes layer-wise posterior aggregation to address challenges posed by heterogeneous data distributions among clients. The authors propose using layer-wise Laplace approximation to infer posteriors from local models, allowing for accurate global model training without requiring additional datasets or exposing private label information. Extensive experiments demonstrate that FedLPA significantly outperforms state-of-the-art methods, particularly in non-IID scenarios.

### Strengths and Weaknesses
Strengths:
1. FedLPA achieves strong performance with only a single communication round, minimizing communication overhead and privacy risks, especially on heterogeneous data.
2. The method does not require additional public datasets.
3. The paper includes a convergence analysis that shows a linear convergence rate for global model optimization.
4. The originality of using layer-wise posterior aggregation is a significant contribution to one-shot federated learning.

Weaknesses:
1. The reliance on multiple layers of approximation introduces potential errors that could lead to suboptimal global models; a more thorough theoretical analysis of these errors is needed.
2. FedLPA requires more computation than simpler methods like FedAvg, which may limit its practicality.
3. Storing and transmitting block-diagonal Fisher matrices increases memory usage and communication costs, which could be significant for large models or numerous clients.
4. The implementation complexity of layer-wise posterior aggregation may hinder real-world application without clearer guidelines.
5. The scalability of FedLPA to large datasets or many clients is not adequately addressed, necessitating evaluations of computational and communication overheads.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of approximation errors and their impact on model performance. Additionally, providing detailed guidelines for practical implementation would enhance accessibility. The authors should also address scalability concerns by evaluating the method's performance with larger datasets and more clients. A deeper discussion on potential privacy risks and mitigation strategies is necessary, particularly in federated learning contexts. Finally, expanding the experimental scope to include more complex datasets and neural network architectures would strengthen the empirical support for FedLPA.