ID: DDvcWpZNgl
Title: PIEClass: Weakly-Supervised Text Classification with Prompting and Noise-Robust Iterative Ensemble Training
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a weakly supervised text classification method that utilizes zero-shot prompting of pre-trained language models (PLMs) to generate pseudo labels. To mitigate noise from pseudo label generation, the authors propose an iterative ensemble training approach that combines two PLM fine-tuning strategies. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on seven datasets, closely matching fully-supervised baselines on two datasets.

### Strengths and Weaknesses
Strengths:
1. The approach effectively addresses the noise issue in weakly supervised learning, a significant challenge in pseudo label generation.
2. The paper is well-organized and clearly illustrated, enhancing reader comprehension.
3. The experiments are comprehensive, including ablation studies and an analysis of pseudo label quality over iterations.
4. The dual PLM fine-tuning strategy for co-training is innovative, yielding noticeable performance improvements.

Weaknesses:
1. The rationale behind certain methodological choices is inadequately explained.
2. The practicality of the method is questioned due to the need for hyperparameter tuning, which may complicate real-world application.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology section to enhance understanding. Additionally, addressing the concerns regarding hyperparameter sensitivity across different datasets is crucial; consider using a fixed set of hyperparameters for all datasets or employing a small labeled dataset for validation. Furthermore, the authors should provide a complete algorithm outlining the entire process and ensure that key concepts, such as head-token fine-tuning and the role of ELECTRA, are adequately explained in the main text. Lastly, it would be beneficial to analyze the impact of less-confident predictions in subsequent iterations.