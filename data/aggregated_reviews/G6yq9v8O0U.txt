ID: G6yq9v8O0U
Title: Factorized Tensor Networks for Multi-Task and Multi-Domain Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Factorized Tensor Network (FTN) aimed at addressing multi-task learning (MTL) and multi-domain learning (MDL) by incorporating task/domain-specific low-rank tensors into a shared backbone network. The authors claim that FTN achieves comparable accuracy to independent networks while requiring fewer additional parameters, thus facilitating efficient knowledge transfer across tasks. The method is validated through experiments on various datasets, demonstrating its effectiveness in maintaining performance while reducing storage costs. Additionally, the paper compares FTN with post-training low-rank factorization methods, particularly focusing on EfficientNet and ResNet-50 backbones, showing that FTN significantly outperforms low-rank approximations in terms of accuracy, even with fewer parameters. The analysis indicates that while increasing the rank (R) in low-rank factorization reduces approximation error, it does not achieve the accuracy levels of FTN. The authors also discuss the implications of latency-efficient MTL methods and their computational trade-offs.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and easy to follow.
- The experimental design effectively showcases the method's applicability across different scenarios, including dense-prediction tasks and image classification.
- FTN is simple to implement and demonstrates strong parameter efficiency, scaling well with multiple tasks and domains.
- The paper provides a thorough empirical evaluation across various tasks and architectures, showcasing the advantages of FTN.
- The authors effectively address concerns regarding the application of their method to transformers and express intent to explore diffusion models in future work.
- The experimental results on Visual Decathlon are promising, enhancing the paper's credibility.

Weaknesses:
- The novelty of FTN is limited, as it closely resembles existing parameter-efficient tuning methods, particularly low-rank adaptation techniques.
- The paper lacks sufficient comparisons with other relevant methods, such as adaptor-based MTL techniques, which could clarify FTN's unique contributions.
- There is a notable absence of recent literature on modular learning and factorized knowledge that could enhance the discussion of FTN's context and relevance.
- The experimental evaluation is somewhat narrow, missing comparisons with additional baselines and datasets, which could strengthen the findings.
- There are reservations about the novelty of the proposed method in light of existing related work.
- The approximation error remains significant even with larger ranks, which limits the effectiveness of low-rank methods.

### Suggestions for Improvement
We recommend that the authors improve the novelty section by providing a more comprehensive analysis of FTN's distinct advantages compared to existing methods like K-Adaptation and SSF. Additionally, it would be beneficial to include comparisons with adaptor-based MTL methods to better contextualize FTN's performance. We suggest incorporating recent advancements in modular learning and factorized knowledge to enrich the literature review. Expanding the experimental evaluation to include more baselines, diverse backbone models, and additional datasets would also enhance the robustness of the findings. Finally, clarifying the computation of parameter costs and addressing the similarities with existing methods like LoRA would strengthen the paper's contributions. We also recommend that the authors improve the discussion on the novelty and significance of FTN compared to existing methods, incorporating insights from the experiments and analyses provided in the rebuttal, and consider including results from diffusion model experiments in future versions to further strengthen their findings.