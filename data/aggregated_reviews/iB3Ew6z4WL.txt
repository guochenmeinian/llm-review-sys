ID: iB3Ew6z4WL
Title: MultiMoDNâ€”Multimodal, Multi-Task, Interpretable Modular Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 7, 4, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MultiModN, a modular network designed to address multimodal multitask problems while maintaining interpretability. The architecture includes separate encoder modules for each modality and decoder modules for each task, allowing it to handle missing modalities effectively. The authors conducted experiments across 10 tasks in 3 diverse domains, comparing MultiModN to P-fusion under identical preprocessing conditions. Results indicate that MultiModN achieves comparable performance to P-fusion while demonstrating greater robustness against missing not at random (MNAR) scenarios. 

### Strengths and Weaknesses
Strengths:
1. The study addresses the critical issue of developing robust and interpretable models for multimodal tasks, emphasizing the importance of interpretability and robustness in real-world applications.
2. Experiments are conducted in a controlled manner, ensuring fair comparisons with confidence intervals included, convincingly demonstrating the model's interpretability and robustness.
3. The paper is well-organized, with clear methodology and detailed experimental descriptions, supported by visually accessible figures and tables.

Weaknesses:
1. The experiments are limited to the same settings and preprocessing as P-fusion, raising questions about the model's performance under different conditions or baselines.
2. Clarity issues exist regarding the architecture, particularly concerning decoder parameter sharing, prediction combination across modalities, and handling missing modalities.
3. Empirical results are weak, with the proposed method sometimes underperforming compared to simpler fusion baselines.

### Suggestions for Improvement
We recommend that the authors improve the paper by including a small proof-of-concept experiment to demonstrate the model's performance under varied settings and preprocessing steps. Additionally, we suggest enhancing clarity in the methodology section by explicitly detailing decoder parameter sharing, how predictions are combined, and the handling of missing modalities. Furthermore, we encourage the authors to compare their approach with recent multimodal multitask architectures, particularly those utilizing transformers, and to discuss the implications of using RNNs versus transformers in their design. Lastly, we recommend adding experiments that showcase the benefits of modularity, such as adapting to new tasks or modalities over time.