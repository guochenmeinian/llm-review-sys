ID: 8K6ul0hgtC
Title: How does PDE order affect the convergence of PINNs?
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1
Original Confidences: 2, 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of the gradient flow (GF) convergence of Physics-Informed Neural Networks (PINNs) for general \(k^{\text{th}}\) order partial differential equations (PDEs) using a power-of-ReLU activation function. The authors extend the work of Gao et al. by achieving tighter bounds and demonstrating that the network width necessary for convergence increases exponentially with the power \(p\) of the ReLU activation function. They determine that the optimal power \(p\) is \(k + 1\) and highlight that GF convergence deteriorates with increasing dimensions. To mitigate these issues, the authors propose a variable splitting strategy as an effective order-reduction approach.

### Strengths and Weaknesses
Strengths:
- The paper addresses a relevant and challenging problem in the training of PINNs, providing theoretical guidance on improving the training process.
- The theoretical contributions, including the extension of GF analysis to higher-order PDEs and the quantification of convergence constants, are significant.
- The manuscript is well-written and easy to follow, with a submitted code for reproducibility, although it currently does not function properly.

Weaknesses:
- The experiments utilize Adam optimization instead of gradient descent (GD), which does not validate the theoretical results based on GF. The authors are strongly advised to include GD experiments to enhance the paper's quality.
- The assumptions underlying GF are overly restrictive, limiting practical applicability. The authors should address concerns regarding training conditions and the gap between theory and practice.
- The presentation of the lower bound \(m > C\) lacks sharpness, potentially leading to misleading conclusions. The authors should clarify this in the introduction and avoid suggesting a direct connection between theoretical results and practical optimization struggles.
- The numerical experiments do not vary network width \(m\) to empirically verify the theoretical relationship regarding the impact of \(p\) on convergence.

### Suggestions for Improvement
We recommend that the authors improve the experimental validation by including results using gradient descent to substantiate their theoretical claims. Additionally, addressing the restrictive assumptions of GF in practical scenarios would enhance the manuscript's applicability. Clarifying the implications of the lower bound \(m > C\) early in the introduction is crucial to avoid misinterpretation. Furthermore, varying the network width in numerical experiments would provide a more comprehensive understanding of its influence on convergence behavior. Lastly, we suggest embedding the results within the existing literature to strengthen the context and relevance of their findings.