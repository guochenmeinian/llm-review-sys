ID: HGFcM3UU50
Title: Aligning Language Models with Human Preferences via a Bayesian Approach
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 4, 4, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Bayesian method for modeling human preferences in text annotation, addressing the inconsistency often observed among annotators. The authors propose a contrastive learning method to replace traditional reinforcement learning (RL) in the alignment process, arguing that majority voting methods may not yield accurate labels when multiple annotations are involved. Experiments conducted on two text generation tasks demonstrate some improvements over baseline models, although the results are marginal. The authors claim that their approach allows for a more generalized preference by leveraging information from other texts and provide empirical results supporting the superiority of their method over RL, particularly in the context of smaller datasets.

### Strengths and Weaknesses
Strengths:
1. The paper addresses the important issue of human disagreement in preference modeling, which is often overlooked.
2. The proposed method is technically sound, utilizing Bayesian and variational inference to model uncertainty effectively.
3. Experiments conducted on two datasets show some improvements compared to baseline methods, supported by human evaluations and ablation studies.
4. The clarity of the writing and organization of the results facilitate understanding.
5. The Bayesian approach offers a novel solution to the challenge of inconsistent human annotations.

Weaknesses:
1. The experimental setup lacks robustness, as the backbone models used are outdated, and the performance improvements are marginal, potentially due to regularization effects. The human preference data in the second dataset is not derived from actual human annotations, raising concerns about its validity.
2. The assumption that human preferences can be reduced to a binary classification of acceptable/unacceptable is overly restrictive and undermines the framework's generality.
3. The method relies on point estimates of human preferences during calibration, which does not fully leverage the Bayesian approach.
4. The evaluation metrics used are primarily reference-based and do not adequately capture how well the model respects human preferences.
5. The relationship between loss convergence and reward convergence is not clearly established.
6. The paper may not fully address concerns regarding the standard error calculations for human evaluations, particularly in relation to inter-annotator variability.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their experiments by utilizing more contemporary backbone models, such as RoBERTa and FLAN-T5. Additionally, we suggest that the authors clarify the limitations of their binary classification assumption and consider moving this assumption to the experimental section. It would be beneficial to incorporate a simple baseline for the reward model that treats each human label independently. Furthermore, we encourage the authors to provide a more thorough explanation of the Bayesian aspects of their approach and to include a wider range of datasets to validate the effectiveness of their method. Lastly, we recommend addressing the evaluation metrics to ensure they reflect the model's alignment with human preferences more accurately, and we suggest improving the clarity of the relationship between loss convergence and reward convergence, emphasizing that they are not necessarily monotonically related. Additionally, we suggest that the authors provide a more detailed explanation of the standard error calculations, specifically addressing both inter-example and inter-annotator variability, to strengthen their claims regarding model preferences.