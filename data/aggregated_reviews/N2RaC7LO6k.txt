ID: N2RaC7LO6k
Title: Geometry of naturalistic object representations in recurrent neural network models of working memory
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 6, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study of Working Memory (WM) in Recurrent Neural Network (RNN) models, focusing on the latent space dynamics during WM-related tasks with naturalistic stimuli. The authors propose a more realistic setup by incorporating a perceptual backbone via a CNN and a WM backbone via an RNN. They analyze gated and non-gated RNNs, characterizing how the geometric properties of their latent spaces evolve as new stimuli are integrated into WM. The findings indicate that RNNs can maintain both task-relevant and irrelevant information, with distinct features stored in orthogonal subspaces. Additionally, the paper investigates the transformations within the RNN latent space, focusing on the stability of feature subspace transformations across time steps for the same encoded stimulus. The authors employed orthogonal Procrustes analysis to characterize these transformations, revealing that while the transformation is consistent across different stimuli, it is not stable over time. The model connects a convolutional layer to an RNN, which is biologically plausible due to its inspiration from complex cells and the sequential processing in the visual pathway.

### Strengths and Weaknesses
Strengths:
- The paper utilizes a realistic setup, incorporating naturalistic inputs and complex N-back tasks, providing a more comprehensive understanding of WM.
- The tasks and architectural choices are clearly articulated, supported by informative plots.
- The use of orthogonal Procrustes analysis provides a robust method for characterizing transformations in the RNN latent space.
- The results demonstrate a significant understanding of the temporal dynamics of feature representation in RNNs.
- The model's biological plausibility is well-founded and respected, enhancing the relevance of the research.

Weaknesses:
- The dataset is small, with only 4 categories and limited identities, raising questions about scalability (e.g., how does WM perform for N>3?).
- While stimuli are more realistic, further enhancements (e.g., realistic backgrounds) are needed.
- The discussion in subsection 4.3 lacks clarity, making it difficult to follow the hypotheses presented.
- The initial submission appears incomplete, with methodology and results feeling rushed and several definitions missing.
- The terminology "penultimate layer" may cause confusion and needs clearer definition.

### Suggestions for Improvement
We recommend that the authors improve the dataset size and complexity, potentially including larger N-back windows and more object instances to enhance the robustness of their findings. Additionally, we suggest incorporating more realistic backgrounds for stimuli to further align with naturalistic conditions. The authors should clarify the discussion of the three hypotheses in subsection 4.3 to enhance reader comprehension. Furthermore, addressing the methodological details, such as the size of the hidden layer and training loss, would strengthen the paper. We also recommend improving the completeness of their methodology and ensuring that all definitions are clearly articulated. Clarifying the distinction between their model and those trained on abstract inputs, as well as considering additional controls to enhance the robustness of their findings, would be beneficial. Lastly, we suggest revising the terminology from "penultimate layer" to "convolutional layer" to avoid potential confusion and emphasizing the definitions of the layers used in the model to enhance reader comprehension.