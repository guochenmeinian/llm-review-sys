ID: R0bnWrpIeN
Title: CoSy: Evaluating Textual Explanations of Neurons
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 6, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework for evaluating Neuron Annotation methods by labeling neurons in vision models through textual descriptions. The authors propose generating images using a text-to-image model based on predicted textual descriptions, with the hypothesis that accurately described neurons will activate more than control images. Two scoring functions, AUC and MAD, are employed for quantitative evaluation. The authors validate their framework and assess existing Neuron Annotation methods, revealing that some produce random textual descriptions. Additionally, the paper includes a detailed analysis of neuron activations and proposes fitting a logistic classifier to these activations. The authors acknowledge the suggestion to compare AUROC derived from logistic regression with the non-parametric AUC and express intent to incorporate this approach in future analyses. They also recognize the inherent bias of the CLIP-Dissect method towards explaining CLIP models rather than predefined concepts.

### Strengths and Weaknesses
Strengths:
- The authors introduce a unified and intuitive evaluation framework for neuron annotation, addressing the challenge of disparate evaluation metrics in the field.
- The paper is well-written, clear, and straightforward, making the evaluation framework accessible to researchers.
- A meta-evaluation section demonstrates the validity of the proposed framework, highlighting significant findings regarding the reliability of textual descriptions in neuron annotation.
- The paper benefits from detailed comments and positive feedback, indicating a strong foundation and supportive peer review. The inclusion of additional experiments is noted to enhance the manuscript's quality.

Weaknesses:
- [W1] The relationship between the proposed evaluation framework and existing methods needs clarification, particularly regarding alignment with metrics like BERTScores and accuracy measures in CLIP-Dissect.
- [W2] The model pool is limited; additional models, especially ImageNet ResNet50 and self-supervised models like DINO, should be analyzed.
- [W3] The authors should extend their analysis to related works, such as [R1], to strengthen their findings.
- [W4] The use of AUC in section 3.2 requires clarification, as it typically involves generating a curve, which is not clearly defined in this context.
- There is noted confusion regarding references, specifically concerning Appendix A.6 and Figure 8, which could detract from clarity.

### Suggestions for Improvement
We recommend that the authors improve the discussion on how the proposed evaluation framework relates to existing methods, particularly regarding alignment with BERTScores and accuracy metrics. Additionally, expanding the model pool to include more diverse architectures, especially ImageNet ResNet50 and self-supervised models like DINO, would enhance the analysis. The authors should also consider incorporating related works, such as [R1], into their evaluation to provide a more comprehensive analysis. Furthermore, clarifying the application of AUC in section 3.2 and addressing the time-consuming nature of text-to-image models by reporting the speed of various models would improve the paper's applicability and clarity. Lastly, we suggest improving the clarity of references in the manuscript, particularly in relation to Appendix A.6 and Figure 8.