ID: rzvVm0LsyK
Title: ADOPT: Modified Adam Can Converge with Any $\beta_2$ with the Optimal Rate
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 5, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ADOPT, a new adaptive gradient method designed to address the non-convergence issues of the Adam optimizer, particularly in smooth nonconvex settings. The authors claim that ADOPT achieves optimal convergence rates independent of the second-moment coefficient $\beta_2$, without relying on bounded noise assumptions. The method modifies the calculation of second moment estimates and the order of momentum updates, and is evaluated across various tasks including image classification, language modeling, and reinforcement learning.

### Strengths and Weaknesses
Strengths:
- The paper tackles the significant non-convergence problem of Adam, providing a theoretically robust foundation for ADOPT.
- ADOPT eliminates the need for problem-dependent tuning of the $\beta_2$ parameter, enhancing user-friendliness.
- Comprehensive experiments demonstrate ADOPT's competitive performance across multiple tasks, consistently outperforming Adam in some scenarios.

Weaknesses:
- The practical implications of ADOPT's non-convergence advantages over Adam are unclear, as non-convergence is not a significant issue in many practical applications.
- The analysis assumes that the second moment of the stochastic gradient is uniformly bounded, which may not hold in all practical scenarios.
- The paper lacks thorough comparisons with more recent optimizers beyond Adam, and does not adequately discuss the potential computational overhead introduced by ADOPT.

### Suggestions for Improvement
We recommend that the authors improve the investigation into the sensitivity of ADOPT's parameters in practical scenarios, including a comprehensive study comparing its performance with Adam. Additionally, addressing the computational overhead of ADOPT compared to Adam and other optimizers would strengthen the paper. Clarifying the choice of hyperparameters in Theorem 4.1 and discussing the implications of the $\epsilon$ parameter's size would also enhance the theoretical contributions. Finally, including comparisons with recent optimizers like CO2 could provide a more robust empirical foundation for ADOPT's claims.