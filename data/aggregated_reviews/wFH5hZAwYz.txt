ID: wFH5hZAwYz
Title: Sharp Calibrated Gaussian Processes
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 5, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for calibrating Gaussian process (GP) posterior variances using held-out data, addressing the issue of poor calibration in quantile estimation. The authors propose a method that minimizes the width of centered confidence intervals while ensuring empirical quantile accuracy, supported by theoretical results. The approach is empirically evaluated on synthetic and real-world datasets, demonstrating improved sharpness compared to existing methods. Additionally, the authors propose using different symbols to distinguish between training and calibration data to enhance clarity. They acknowledge that while adding more data generally improves model sharpness, it is challenging to ascertain the effect on calibration. The authors will present results for both vanilla and fully Bayesian GPs using the complete dataset and clarify the distinction between models in the revised paper. They also address the treatment of noise standard deviation in predictions, committing to include it in their revised methodology.

### Strengths and Weaknesses
Strengths:
- The idea is novel and theoretically well-supported, contributing to the field of GP calibration.
- The proposed method shows improved sharpness in predictive performance, as evidenced by real-world experimental results.
- The authors provide code, enhancing reproducibility and potential impact.
- The methodology is well-motivated, providing a coverage guarantee with mild assumptions.
- The authors have made efforts to address reviewer concerns and have conducted additional experiments to clarify results.
- The final algorithm is practical and achieves good results.

Weaknesses:
- The paper lacks clarity in presentation, particularly in the evaluation section, making it difficult to follow.
- The validity of Theorem 5.3 is questionable due to its reliance on assumptions that may not hold in the GP context.
- The use of boldface to denote best results in tables may misrepresent statistical significance, as overlapping estimates are not adequately addressed.
- Some experiments could have been clearer, particularly regarding the fully Bayesian GP results, which are limited due to computational constraints.
- There is a potential for confusion regarding the treatment of noise standard deviation in predictions.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the evaluation section, particularly in Table 2, by clearly indicating overlapping estimates. Additionally, please address the validity of Theorem 5.3 and provide a proof regarding the monotonicity of posterior variance in relation to the inverse length scale. It is also crucial to clarify the interpretation of results in Figure 2 and ensure that the metrics STD and NLL in Table 2.1 are well-defined. We suggest that the authors improve clarity by using different symbols for training and calibration data and clearly stating this in the revision. Furthermore, we recommend ensuring that the noise standard deviation is included in predictions for the vanilla GP to avoid confusion with the purely Bayesian setting. Lastly, it would be beneficial to explicitly state the computational limitations of the fully Bayesian GP approach in the revised paper to prevent misunderstandings.