ID: uuUQraD4XX
Title: Large Language Models Can Self-Improve
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for self-improvement of large language models (LLMs) using unlabelled data, specifically through the language model self-improved (LMSI) approach. The authors utilize a specific LLM, PaLM, to generate high-confidence answers for unlabeled questions, which are then used for fine-tuning. The results indicate significant performance enhancements across various datasets, including GSM8K, DROP, and OpenBookQA. Key contributions include demonstrating that LLMs can improve without ground truth outputs, conducting detailed ablation studies, and achieving high accuracy in low-resource scenarios.

### Strengths and Weaknesses
Strengths:
- The empirical results validate the effectiveness of the proposed method, showing substantial improvements in performance.
- The approach is straightforward and leverages existing techniques like Chain-of-Thought prompting and self-consistency.
- The paper is well-structured and clearly written, with a detailed related work section.

Weaknesses:
- The method's reliance on a single, non-open-source 540B model raises reproducibility concerns and limits generalization to smaller models.
- The assertion that the approach mimics human learning lacks empirical support and clarity.
- The title may mislead regarding the applicability of self-improvement to smaller models, which have not been systematically evaluated.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by testing the LMSI method on other large language models, such as LLaMA, to assess its effectiveness across different architectures. Additionally, we suggest providing more empirical evidence to support the claim that the approach mimics human learning, as well as discussing potential negative impacts of further tuning on non-reasoning tasks. Clarifying the calibration of predicted confidence in Figure 2 and addressing the limitations of using a single model would enhance the paper's robustness. Finally, including comparisons to training on human-labeled data could provide valuable insights into the necessity of such data for achieving performance benchmarks.