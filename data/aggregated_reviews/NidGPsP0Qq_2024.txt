ID: NidGPsP0Qq
Title: Provably Efficient Interactive-Grounded Learning with Personalized Reward
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 5, 5, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on Interactive-Grounded Learning (IGL) with personalized rewards, introducing two algorithms that utilize novel Lipschitz reward estimators to achieve provably efficient performance with sublinear regret. The authors empirically validate their methods on both image classification and conversational datasets, showcasing the effectiveness of their approach. 

### Strengths and Weaknesses
Strengths:  
- This is the first work to provide provably efficient algorithms with regret guarantees for IGL with personalized reward.  
- The exploration of a novel task in the IGL literature, particularly the experiments on the conversational dataset, is noteworthy.  
- The paper introduces a new reward estimator via inverse kinematics, generalizing to randomized binary rewards, which is a significant advancement over prior work.

Weaknesses:  
- The paper lacks a comparison of the proposed algorithms to the IGL-P algorithm from Maghakian et al. (2022).  
- Justifications for the use of Lipschitz reward estimators are provided, but the potential failures of alternative estimators remain unclear.  
- The technical novelty is questioned, as the studied setting may not present greater challenges than traditional contextual bandits, and the reliance on standard bandit algorithms limits the contribution.  
- The performance guarantee of Algorithm 2 is based on more restrictive assumptions, which may be difficult to satisfy.

### Suggestions for Improvement
We recommend that the authors improve the comparison with the IGL-P algorithm to clarify the advantages of their approach. Additionally, providing counter-examples to illustrate the limitations of alternative estimators would enhance the paper's rigor. It would also be beneficial to discuss the implications of the assumptions on parameters $\alpha$ and $\theta$ in Algorithms 1 and 2, as well as the role of parameter $\gamma$ in Algorithm 2. Furthermore, we suggest that the authors explore more advanced exploration strategies to optimize the balance between exploration and exploitation, and comment on the optimality of the provided regret bounds. Lastly, including regret plots for the experiments would provide clearer insights into the performance of the algorithms.