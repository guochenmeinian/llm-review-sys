ID: 3s8V8QP9XV
Title: Nearly Optimal Approximation of Matrix Functions by the Lanczos Method
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 7, 6, 8, 7, -1, -1, -1, -1
Original Confidences: 2, 2, 4, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the convergence of the Lanczos method for computing matrix functions \( f(A)v \) for a matrix \( A \) and a vector \( v \). The authors propose a near instance optimality bound that incorporates the condition numbers of matrices related to \( A \) and empirically validate this bound. The approximation quality of the Lanczos-FA method for rational functions is analyzed, improving known uniform bounds to instance-specific bounds that better explain the method's practical performance. The authors also discuss the optimality of the Lanczos algorithm in terms of approximation error for rational functions.

### Strengths and Weaknesses
Strengths:  
- The paper provides new theoretical insights into the Lanczos method, contributing to a better understanding of Krylov subspace methods.  
- It is well-structured and generally easy to read, with a clear proof sketch in Section 2.1.  
- The numerical experiments convincingly demonstrate the theoretical results and address a practically relevant problem.  

Weaknesses:  
- The analysis is limited to exact arithmetic, which may not reflect practical implementations.  
- The proposed bounds include the constant \( \kappa(A)^q \), which can be large, raising questions about their practical applicability.  
- The connection to machine learning applications could be strengthened by including relevant matrix functions in the experiments.  
- Some minor grammatical issues and formatting deviations from the Neurips style were noted.

### Suggestions for Improvement
We recommend that the authors improve the explanation of how the proposed bound captures the convergence behavior of the Lanczos method compared to existing bounds. Additionally, we suggest conducting experiments with finite precision arithmetic to assess the practical implications of the theoretical bounds. To enhance the connection to machine learning, we encourage the authors to include experiments involving matrix functions relevant to the field, such as Gaussian process regression. Lastly, addressing the minor grammatical issues and ensuring adherence to the Neurips formatting guidelines would improve the manuscript's clarity and presentation.