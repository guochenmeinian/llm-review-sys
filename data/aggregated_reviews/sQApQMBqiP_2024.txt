ID: sQApQMBqiP
Title: Learning Human-like Representations to Enable Learning Human Values
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 6, 5, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of how representational alignment between humans and AI agents influences the ability of AI systems to learn human values effectively and safely. The authors propose that AI systems that learn human-like representations can better generalize human values and ensure safer exploration during learning. They support their hypothesis through theoretical analysis and extensive experiments, including simulations and human value judgments in a reinforcement learning context. Additionally, the authors investigate the morality of large language models (LLMs) and their representational alignment, conducting further experiments such as having LLMs play a bandit game and obtaining morality ratings directly from them. These additional experiments provide solid support for the central hypothesis of the paper.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a novel approach linking representational alignment with value alignment, addressing a significant challenge in AI safety.  
- The experiments are thorough and well-structured, covering various aspects of human values and employing multiple machine learning models.  
- The work focuses on personalizing LLM behavior in a safe manner, demonstrating the benefits of representational alignment across several domains and function approximators.  
- The authors addressed most reviewer concerns and provided additional analyses that strengthen the paper's arguments.  
- The additional experiments conducted based on reviewer suggestions enhance the evidence linking representational analyses to behavior.

Weaknesses:  
- The theoretical analysis relies on strong assumptions, such as specific kernel functions and Gaussian process regression, which may limit the generalizability of the results.  
- The hypothesis that increased representational alignment allows for better learning of human values is not directly tested; the authors should conduct behavioral experiments with LLMs to validate this hypothesis.  
- The results of the simulations are not informative and may be better suited for the appendix.  
- The experimental evaluation lacks depth, and the criteria for evaluating representational alignment could be more comprehensive.  
- The findings may not apply to all machine learning models, and the human value judgments were collected from a homogenous group, limiting generalizability.  
- Some reviewers noted the limited scope of evidence, including ceiling effects, weak correlations, and limited models.  
- There are concerns regarding the appropriateness of providing external links to additional results, which could lead to disqualification.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis by discussing the impact of strong assumptions on real-world applicability and consider additional validations. Conducting comparative experiments with different alignment metrics will enhance the robustness of the findings. We suggest highlighting the best results in tables and clarifying metrics in captions, as well as improving color distinctions in figures for better clarity. Expanding the experimental evaluation and including a wider range of participants in future studies will strengthen the conclusions and ensure broader applicability of the results. Additionally, we recommend that the authors improve the scope of their experiments by including a harder task that can provide a better test in more realistic settings, ideally one that all LLMs agree to respond to. Furthermore, we suggest that the authors adhere strictly to submission guidelines regarding external links to avoid any potential issues with disqualification.