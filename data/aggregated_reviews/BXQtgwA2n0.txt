ID: BXQtgwA2n0
Title: Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 5, 5, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel offline multi-agent reinforcement learning (MARL) algorithm named OMIGA, which converts global entropy regularization into local regularizations under a linear weighted value decomposition assumption. The authors evaluate the method on offline multi-agent MuJoCo and SMAC datasets, demonstrating that OMIGA nearly outperforms current state-of-the-art (SOTA) baselines. The framework integrates concepts from offline reinforcement learning and value decomposition, facilitating decentralized policy learning while retaining the benefits of global regularization. The authors acknowledge that the true Q-function cannot be conditioned solely on joint observations in non-Markovian settings, emphasizing the need for a more robust theoretical foundation. They provide updated experimental results showing OMIGA's performance, achieving the highest scores in 23 out of 24 tasks compared to other baselines, and introduce statistical metrics, such as the Mann-Whitney U-statistic, to evaluate the robustness of OMIGA's improvements over its competitors.

### Strengths and Weaknesses
Strengths:
- The paper introduces a principled framework for decomposing global regularization into local regularizations, which is significant for offline MARL.
- OMIGA demonstrates superior performance in most tasks, significantly outperforming behavioral cloning (BC) and other baselines.
- The theoretical analysis supports the proposed method, and empirical results indicate that OMIGA performs well against existing offline MARL algorithms.
- The authors employ rigorous statistical analysis to validate their results, enhancing the credibility of their findings.
- The writing is clear and the paper is well-organized, making it accessible to readers.

Weaknesses:
- The factorization assumption underlying the method is not adequately justified; more analysis and examples are needed.
- The theoretical underpinnings of OMIGA are questioned, particularly regarding its reliance on dataset actions and the clarity of its optimization process.
- The paper lacks a thorough characterization of the experimental settings and the mechanisms of the components used, such as policies and value functions.
- There are concerns regarding the fairness of comparisons, as not all methods may have access to global information.
- The hyper-parameter choices for the baselines appear inconsistent with their original papers, potentially affecting the results.
- There is a lack of significant improvement on suboptimal datasets, raising concerns about the practical applicability of the method.

### Suggestions for Improvement
We recommend that the authors improve the justification for the factorization assumption by providing more analysis and illustrative examples. Additionally, the authors should clarify whether the local Q-function adheres to the Individual-Global-Maximum (IGM) and ensure that all compared methods have access to global information to maintain fairness in results. We suggest revising the presentation of results in Table 1 to accurately reflect statistical significance and addressing the characterization of the experimental settings and the mechanisms of the components used. Furthermore, we recommend that the authors improve the theoretical clarity of OMIGA by explicitly detailing how it optimizes Equation (2) and addressing the concerns regarding the use of dataset actions. It would also be beneficial to provide a more comprehensive analysis of the performance on suboptimal datasets to demonstrate the practical benefits of OMIGA, and to clarify the derivation of the second term in Equation (12) and its relationship to Equation (11) to resolve existing ambiguities.