ID: MLKLYoXypN
Title: Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the cross-lingual consistency of factual knowledge in multilingual pre-trained models, introducing a new metric, RankC, to quantify this consistency. The authors modify existing datasets to create BMLAMA-17 and BMLAMA-53 for probing models across languages. Key findings indicate that cross-lingual consistency is low in mainstream models, does not correlate with model size, and is influenced by genetic similarity among languages. The paper also emphasizes the need for further exploration of knowledge transfer across languages.

### Strengths and Weaknesses
Strengths:
- The motivation for the study is clear and significant for understanding implicit knowledge in multilingual models.
- The paper is well-organized and well-written, with a comprehensive analysis across various state-of-the-art models.
- The proposed metric could advance research in this area and is supported by reasonable predictions.

Weaknesses:
- The authors may overstate the novelty of their dataset contributions, as they derive from existing datasets with minor adaptations.
- The experiments on BLOOM do not consider unsupported language pairs, potentially skewing results.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the interplay between consistency and accuracy, specifically addressing whether consistency implies accuracy. Additionally, consider conducting experiments on BLOOMZ and LLaMA to analyze the effects of fine-tuning and to validate conclusions with cutting-edge models. Furthermore, clarify the translation processes mentioned in the paper to ensure accuracy and bias mitigation. Lastly, enhance the readability of Figures 3 and 4 by increasing the font size for the numbers.