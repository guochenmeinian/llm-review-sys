ID: AnFUgNC3Yc
Title: Resetting the Optimizer in Deep RL: An Empirical Study
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 6, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a critique of the standard use of Adam-type optimizers in deep reinforcement learning (RL), arguing that the internal parameters of these optimizers can become "contaminated" over iterations. The authors propose resetting these parameters at the start of each iteration to mitigate this issue. Empirical results demonstrate that this approach enhances performance in the Atari domain, particularly with the Rainbow algorithm. Additionally, the paper explores the performance of the Rainbow algorithm with varying values of the hyper-parameter $K$, noting that Rainbow performs equally well with $K=1$ as with $K=8000$. This finding may allow for the removal of the target network without negatively impacting performance. The authors also discuss the implications of optimizer resets and the need for further investigation into their effects across different games.

### Strengths and Weaknesses
Strengths:
- The key idea of resetting optimizer parameters is simple and effective, contributing to improved performance across multiple optimizers.
- The finding that Rainbow performs well with $K=1$ is a significant and exciting result that could influence future research.
- The writing is clear and accessible, facilitating comprehension of the problem's context and significance.
- The authors effectively validate the utility of optimizer resets across various algorithms, including DQN and IQN.
- The experimental protocol is robust, utilizing a comprehensive set of 55 Atari games and multiple seeds, which supports the statistical significance of the findings.

Weaknesses:
- The paper lacks a clear definition and empirical evidence of "contamination," which undermines the claim that it affects performance.
- Many results are based on a single random seed, limiting the validity of the findings; a more rigorous statistical analysis is needed.
- The results show mixed performance across environments, with some experiencing deterioration, and the authors do not adequately explain these negative effects.
- The update equations for Adam are incorrect, and the tuning of the parameter $K$ is insufficient, with no exploration of smaller values.
- The paper lacks a detailed description of hyper-parameters for all algorithms, which could hinder reproducibility.
- There is insufficient explanation regarding the varying performance of games in relation to optimizer resets, particularly for games like DemonAttack and Breakout.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of "contamination" and provide empirical evidence to support its existence. Additionally, conducting a more comprehensive statistical analysis, including confidence intervals and multiple seeds, would enhance the robustness of the findings. The authors should also explore the reasons behind the mixed performance results and provide a rationale for using a single seed in their initial analysis. Furthermore, correcting the update equations for Adam and tuning the parameter $K$ more thoroughly, including testing smaller values, would strengthen the paper. We suggest including a dedicated section on hyper-parameters to enhance self-containment and improve reproducibility. Additionally, we encourage the authors to expand the discussion on the implications of removing the target network from Rainbow and include a few lines in the conclusion to highlight this important finding. Lastly, providing a plot of cosine similarities before target updates would better elucidate the effects of optimizer resets.