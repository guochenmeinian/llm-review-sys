ID: Ncb0MvVqRV
Title: Minimum Description Length and Generalization Guarantees for Representation Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 7, 7, 7, 6, -1, -1, -1
Original Confidences: 4, 2, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents new generalization bounds for conditional mutual information-flavor, focusing on predictions from the supersample. The authors derive bounds in terms of KL divergence with symmetric priors, extending to lossy compression and tail bounds. They establish tighter bounds related to binary entropy, akin to classical PAC-Bayes bounds, and relate these findings to representation learning algorithms, emphasizing that the encoder's structure is crucial while allowing flexibility in the decoder to minimize training loss. The numerical evaluation demonstrates the potential benefits of the proposed methods. Additionally, the study addresses supervised learning problems by deriving two types of generalization gap bounds. The first type establishes that the generalization gap can be upper bounded by approximately $\sqrt{\mathrm{KL}(P || Q) / n}$, where $P$ is the average predicted label distribution over $n$ training and $n$ test examples, and $Q$ is a symmetric prior distribution. The second type of bounds focuses on classifiers with an encoder and classification head, where the bounds similarly depend on representations rather than the classification head. Both types derive in-expectation and in-probability bounds, and tighter bounds are introduced using a convex function $h:[0,1] \times [0,1] \rightarrow [0,2]$. The authors also address the issue of infinite KL divergence in continuous settings by proposing a "quantized" algorithm.

### Strengths and Weaknesses
Strengths:
- The new results significantly advance representation learning.
- The theoretical analysis is comprehensive, addressing various variants and tightenings.
- The exploration of how learned representations affect generalization is significant for the NeurIPS community, with a commendable summary of criticisms regarding information-bottleneck compression ideas.
- The presentation is generally clear and pedagogical, with thorough discussions of related work.
- Numerical evaluations effectively illustrate the proposed methods' benefits.
- The bounds related to representation complexity are novel, suggesting a new type of information bottleneck for representation learning.

Weaknesses:
- Some connections to prior work are missing, and the literature on information-theoretic generalization bounds is inadequately cited, missing key references that could enhance the paper's context and relevance.
- Certain results may be incorrectly stated, and some notation is undefined, leading to clarity issues.
- The significance of the main results in Section 3 is unclear, particularly regarding the estimation of Theorem 5 and the applicability of the KL term as regularization.
- The reliance on a solid understanding of information theory may limit accessibility.
- The main result, Theorem 5, requires further explanation regarding its implications and practical applications, and the paper suffers from cumbersome notation and insufficient detail in some derivations.

### Suggestions for Improvement
We recommend that the authors improve the clarity of connections to frameworks like functional and evaluated CMI, and distinguish between symmetric priors and almost exchangeable priors. Additionally, we suggest providing a more detailed explanation of Theorem 5, particularly its implications for generalization based on mutual information. It would be beneficial to include an analytical evaluation of Theorem 5 in tractable settings, such as a two-layer neural network on Gaussian data, to assess the bounds' effectiveness. A tabular comparison of the proposed bounds against classical mutual information-based bounds would clarify positioning. Furthermore, we urge the authors to cite relevant literature on information-theoretic generalization bounds, particularly works by Russo and Zou, Xu and Raginsky, and others, to contextualize their contributions better. Finally, we encourage the authors to enhance the presentation by addressing the numerous small mistakes, cumbersome notation, and to include a separate Conclusions section summarizing contributions and future work, while providing more intuition for proof techniques throughout the paper.