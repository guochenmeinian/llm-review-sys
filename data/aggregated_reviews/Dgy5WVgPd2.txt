ID: Dgy5WVgPd2
Title: Instruction Tuning Large Language Models to Understand Electronic Health Records
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 9, 9, 7, 7
Original Confidences: 3, 5, 4, 4

Aggregated Review:
### Key Points
This paper presents a 400k large-scale dataset sourced from MIMIC-III aimed at enhancing Large Language Models (LLMs) for information extraction and clinical reasoning on Electronic Health Records (EHR) data. The authors propose the Llemr foundation model, which aligns event embeddings to a frozen LLM to reduce effective context length while preserving the pretrained model's performance. The model is pretrained on Schema Alignment and Clinical Reasoning tasks, demonstrating superior performance compared to various LLM and clinical prediction model baselines. The work is significant as it introduces one of the first high-quality clinical datasets for LLM training and evaluation, potentially transforming clinical prediction tasks. The authors also plan to release the dataset, code, and model weights, although this contribution appears less compelling given existing open-source resources in the field.

### Strengths and Weaknesses
Strengths:
1. The dataset is valuable for general clinical understanding and prediction.
2. The integration of event embeddings into a pretrained LLM is a significant contribution, effectively retaining the benefits of pretraining.
3. The research quality is generally high, with well-executed experiments and strong benchmarks.
4. The potential for improved healthcare outcomes through better EHR model training is notable, with ethical implications acknowledged.
5. The results are outstanding and promising, indicating a broad impact on the community.

Weaknesses:
1. Insufficient description of the dataset, particularly the generation and sampling of the 350k schema alignment data.
2. Lack of clarity on the correctness of Llemr's predictions and absence of code for reproducibility.
3. The necessity of embedding compression is questioned, particularly in a future where context lengths may increase.
4. Concerns regarding the clarity and appropriateness of generated questions, which may not reflect actual clinician inquiries.
5. The paper could benefit from additional clarity in several areas, including methodology and the distinction between Llemr as a framework versus a specific model.
6. No discussion of limitations or future research directions.

### Suggestions for Improvement
We recommend that the authors improve the dataset description by detailing how the schema alignment data is generated and sampled, including demographic and clinical condition breakdowns. Additionally, clarify how Llemr's predictions are validated, particularly for mortality and treatment recommendations, and consider releasing the code to enhance reproducibility. We suggest including ChatGPT-4o and GPT-4 as baselines with proper prompt engineering, and addressing the handling of clinical reasoning questions, especially regarding the realism of answers not present in EHR data. Furthermore, we recommend improving the evaluation of Llemr by benchmarking the best-performing baseline LLM (vicuna-7b-v1.5) on event streams without the event embedding step and evaluating Llemr on the EHRNoteQA dataset to better assess instruction-following capabilities. To enhance clarity, please ensure consistent tense usage and consider copy editing for grammatical accuracy. Address the ambiguity in the description of the dataset and clarify the tokenization process for digits. Lastly, we encourage the authors to explore alternative methods such as RAG or LangChainâ€™s MapReduceChain for multi-step refinement to avoid verbosity in answers and to discuss limitations and provide insights on future research opportunities.