ID: 0LmWBhIYLi
Title: Universal Prompt Tuning for Graph Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a universal prompt-based tuning method called Graph Prompt Feature (GPF) and its variant GPF-plus for pre-trained Graph Neural Network (GNN) models. GPF operates on the input graph's feature space and can achieve results equivalent to any prompting function, outperforming fine-tuning with average improvements of 1.4% in full-shot and 3.2% in few-shot scenarios. The authors provide rigorous theoretical analyses supporting the universality and effectiveness of GPF, alongside extensive experimental validation.

### Strengths and Weaknesses
Strengths:
1. Theoretical Guarantees: The authors offer rigorous derivations demonstrating GPF's universality and effectiveness, proving it can outperform fine-tuning.
2. Experimental Validation - Reproducibility: Extensive experiments across various pre-training strategies show GPF's superiority over fine-tuning and existing specialized methods, with source code availability enhancing reproducibility.

Weaknesses:
1. Lack of Explanation: The paper does not adequately explain why adding the learnable vector \( p \) to input features yields better results than linear probing with a trainable layer, leaving readers questioning the underlying mechanisms.
2. Theoretical Limitations: Theorem 2 is based on a simple linear 1-layer GNN without activations, which may not generalize to more complex architectures commonly used in practice.
3. Limited Backbone Exploration: The exclusive use of Graph Isomorphism Network (GIN) as the backbone GNN for fine-tuning limits the exploration of potentially more powerful architectures that could enhance performance.

### Suggestions for Improvement
We recommend that the authors improve the explanation of the relationship between GPF and linear probing, clarifying why GPF achieves superior performance. Additionally, we suggest discussing the generalizability of Theorem 2 to more complex GNN models with non-linear activations. It would also be beneficial for the authors to explore and experiment with more powerful GNN architectures, such as subgraph GNNs or higher-order GNNs, in the fine-tuning stage to assess performance differences. Furthermore, addressing the limitations of the proposed methods and providing detailed pre-training settings in the main text would enhance the paper's clarity and comprehensibility.