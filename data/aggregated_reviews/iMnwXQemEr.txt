ID: iMnwXQemEr
Title: Discovering Universal Geometry in Embeddings with ICA
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the geometry of embeddings using independent component analysis (ICA) and principal component analysis (PCA), demonstrating that word embeddings can be represented as a composition of intrinsic interpretable axes. The authors claim that this representation remains consistent across different languages, algorithms, and modalities, and that fewer dimensions are needed to capture the essential information of the embeddings.

### Strengths and Weaknesses
Strengths:
- The findings contribute significantly to understanding embeddings, particularly the "universal" claim that embeddings align across languages without explicit alignment.
- The paper is well-written and supported by extensive experimental results, which enhance its credibility and potential utility for NLP model design.
- The results indicate that embeddings can be projected to lower dimensions while preserving performance.

Weaknesses:
- The reliance on linear analyses like PCA and ICA raises questions about their sufficiency for analyzing the nonlinear relationships inherent in pretrained word embeddings.
- The paper lacks detailed explanations of the FastICA approach and its assumptions, which could clarify the analysis.
- There is insufficient focus on popular contrastively trained text embeddings, and the implications of model size/capacity are not addressed.
- Clarity issues remain, particularly regarding the association of axes with specific words and the meaning of "intrinsic interpretable axes."

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing more detailed explanations of the FastICA approach and its assumptions. Additionally, the authors should consider including analyses of popular contrastively trained text embeddings like sentence-bert (SBERT) and discuss the impact of model size/capacity. Furthermore, addressing the clarity of the associations in Fig. 1 and elaborating on the term "intrinsic interpretable axes" in the abstract would enhance the paper's comprehensibility. Finally, providing source code would improve reproducibility.