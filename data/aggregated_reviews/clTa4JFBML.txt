ID: clTa4JFBML
Title: Return of Unconditional Generation: A Self-supervised Representation Generation Method
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 8, 5, 8, 9, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a framework called Representation-Conditioned Generation (RCG) that aims to leverage Conditional Generation techniques within Unconditional Generation settings. The authors propose using a representation generator network, trained to approximate image feature distributions from a pre-trained self-supervised network (MoCo-v3), instead of class labels. The effectiveness of this approach is demonstrated across various generative models, including latent diffusion models and diffusion transformers, showing improvements on the ImageNet Unconditional Generation benchmark.

### Strengths and Weaknesses
Strengths:  
- The proposed method is sound, with clear and thorough technical details.  
- Extensive experiments and ablations confirm robust results across different generative models.  
- The paper provides new empirical evidence indicating that diffusion models can convincingly generate self-supervised representations, which serve as effective conditioning signals for image generation models.  

Weaknesses:  
- The technical novelty is limited, as prior works also generate images using pretrained features for conditioning. The primary differentiator is the choice of pre-trained feature extractor and image generator.  
- The interpretation of ablation results lacks depth; insights on how to make critical choices regarding feature extraction would enhance understanding.  
- The necessity of a self-supervised encoder and its implications for other modalities should be discussed more thoroughly.

### Suggestions for Improvement
We recommend that the authors improve the interpretation of their ablation results by providing intuitions on the importance of their choices in feature extraction. Additionally, including comparisons with conditioning on clustering structures would strengthen the contribution. It would also be beneficial to clarify why MoCo-v3 features outperform DINO or iBOT features for this task. Furthermore, we suggest discussing the limitations regarding the necessity of a self-supervised encoder and its applicability to other data modalities. Lastly, addressing potential misuses of generative models, such as deep fakes, would provide a more comprehensive view of societal impacts.