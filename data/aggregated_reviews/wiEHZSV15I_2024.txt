ID: wiEHZSV15I
Title: Parsimony or Capability? Decomposition Delivers Both in Long-term Time Series Forecasting
Conference: NeurIPS
Year: 2024
Number of Reviews: 23
Original Ratings: 7, 6, 7, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Selective Structured Components-based Neural Network (SSCNN) for long-term time series forecasting, addressing the limitations of existing state-of-the-art (SOTA) methods. The authors propose a decomposition-based model enhanced with a selection mechanism, achieving superior performance with significantly fewer parameters. Comprehensive experiments validate SSCNN's effectiveness across various datasets, demonstrating its capability to capture complex data patterns while maintaining a minimal model size.

### Strengths and Weaknesses
Strengths:
1. The paper offers an original approach by utilizing data decomposition, which has been underexplored in time series forecasting, leading to state-of-the-art performance with a minimal parameter count.
2. The writing is clear and logical, facilitating reader comprehension.
3. Extensive experimental validation, including ablation studies, supports the effectiveness of SSCNN.
4. The code is reproducible and well-documented, allowing for easy replication of results.

Weaknesses:
1. The rationale for not using the ETT dataset in experiments is unclear, despite its common use for ablation studies.
2. Model size should be reported in Table 1 for better comparison with other SOTA methods.
3. The paper lacks a discussion on the advantages of SSCNN over lightweight models that also demonstrate satisfactory performance.
4. Some figures lack captions, and the text size in legends is too small, affecting readability.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology by providing additional explanations for the Polynomial Regression layer and the rationale behind the exclusion of the attention mechanism in the long-term component. Additionally, addressing the lack of captions in figures and enhancing the readability of legends would improve the overall presentation quality. The authors should also clarify the decision to exclude the ETT dataset and consider including model size in Table 1 for clearer comparisons. Lastly, a discussion on the advantages of SSCNN over existing lightweight models would strengthen the paper's argument.