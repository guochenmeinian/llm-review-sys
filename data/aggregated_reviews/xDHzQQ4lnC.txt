ID: xDHzQQ4lnC
Title: Probabilistic inverse optimal control for non-linear partially observable systems disentangles perceptual uncertainty and behavioral costs
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 6, 7, 5, 6, 7, 4, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 1, 4, 3, 3, 2, 2, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an algorithm for inverse optimal control in partially observable Markov decision processes (POMDPs), utilizing only state trajectories. The authors propose a three-step approach: estimating a policy using iterative Linear Quadratic Gaussian (iLQG), estimating belief dynamics with the Extended Kalman Filter (EKF), and performing filtering through linearized belief propagation. The effectiveness of the algorithm is demonstrated through tests on synthetic problems, showing improved parameter estimation compared to a maximum causal entropy baseline. Additionally, the authors introduce a method to infer an agent’s internal model in a POMDP when the agent’s actions are unobservable, utilizing local linearization to construct a closed-form approximation of the likelihood function for state trajectories, enabling maximum likelihood estimates. The method is shown to better disambiguate confounding factors compared to baseline approaches, distinguishing between the control problem from the agent’s viewpoint and the inference problem from the researcher’s perspective.

### Strengths and Weaknesses
Strengths:  
- The problem addressed is relevant, particularly the consideration of scenarios without action information, enhancing practical applicability.  
- The writing is coherent, and the introduction is well-motivated, with informative discussions on related work.  
- The proposed method is novel, and the authors thoroughly discuss its limitations.  
- The paper effectively lays the groundwork for the problem, reviews previous work, and highlights shortcomings of existing methods.  
- The motivating example in Fig. 1 is compelling, and the paper's structure is coherent.  
- The experiments are well-designed, and results clearly establish the technique’s superior performance over the baseline.

Weaknesses:  
- The assumptions underlying the proposed method are unclear, particularly regarding the necessary information for Algorithm 1.  
- The operational details of the algorithm are inadequately explained, especially concerning parameter estimation and the computational graph involved in gradient-based optimization.  
- The contribution regarding the specific challenges posed by missing action information needs clearer articulation.  
- The contribution is perceived as lacking significance for acceptance at the venue, with the problem formulation being only marginally more general than existing work.  
- While the linearization for approximate likelihood is clean, it is not novel.  
- The results are not surprising, as techniques aware of generative assumptions typically yield better posterior estimates, and the disambiguation behavior is expected from improved estimation.  
- The experimental evaluation relies on synthetic datasets, lacking comparisons with recent inverse optimal control or inverse reinforcement learning works and more realistic datasets.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the assumptions made in their method, particularly regarding the required knowledge of dynamics and observation models. Additionally, the authors should provide a more detailed explanation of how the learning process operates, specifically the computational graph used in automatic differentiation. It would be beneficial to include comparisons with recent inverse optimal control or inverse reinforcement learning works and to test the algorithm on more realistic datasets. Furthermore, we suggest that the authors emphasize their contribution in relation to the specific difficulties posed by missing action information. Lastly, we recommend improving the clarity of the presentation, particularly in distinguishing between partial observability and noise to enhance the understanding of the experimental contributions. Consider replacing the EKF with newer forms of probabilistic filtering, such as Dynamic Expectation Maximization, and explore the possibility of learning covariance matrices online. Additionally, including a statement about the assumptions made in the approach would clarify its applicability and limitations.