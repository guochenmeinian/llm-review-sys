ID: hSmn7BQZ2v
Title: What a Scientific Language Model Knows and Doesn't Know about Chemistry
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: 6, 7, -1
Original Confidences: 3, 4, 3

Aggregated Review:
### Key Points
This paper presents a benchmark dataset for molecular property prediction and evaluates Galactica 1.3B on this benchmark. The authors investigate the effects of prompting styles, evaluation data subsets, and task categories on performance, providing insights into in-context learning behavior for molecular property prediction tasks.

### Strengths and Weaknesses
Strengths:
- The paper conducts a comprehensive analysis of molecular property prediction tasks, revealing interesting phenomena, some of which contrast with previous observations.
- Open-sourcing the benchmark dataset is a valuable contribution.

Weaknesses:
- The term "understanding" is used frequently but lacks a clear definition, appearing as a catch-all for the language model's ability to generalize. The authors should clarify that their focus is on generalization.
- The choice to focus solely on Galactica 1.3B limits the study's conclusions; including other LLMs like GPT-4 or Claude-2 would enhance the analysis.
- The paper does not address the variability in outputs from LLMs given the same prompt, which can significantly impact predictions for certain properties.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the term "understanding" by explicitly stating their focus on generalization. Additionally, including relevant references such as [2] in the related work section would enhance the discussion. We suggest labeling the rows and columns in Table 1 for better clarity and providing error bars to ensure more reliable estimates of model performance. In Section 4.1, the authors should explain the differing evaluation performances for "first 100" versus "random" compounds. Furthermore, we encourage the authors to elaborate on why zero-shot performance being better than few-shot implies that the LLM relies on prior memorized knowledge. In Section 4.4, the authors should clarify what "understanding of the correct output" entails and consider relocating the discussion on unrealistic values generated by the model. Finally, we suggest exploring instruction tuning with domain-specific datasets to address issues related to performance variance and memorization.