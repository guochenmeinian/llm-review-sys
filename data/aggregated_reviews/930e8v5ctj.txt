ID: 930e8v5ctj
Title: ReMI: A Dataset for Reasoning with Multiple Images
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 7, 7, 6, 7, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents the ReMI dataset, a benchmark designed to evaluate the reasoning capabilities of Large Language Models (LLMs) with multiple images across various domains, including math, physics, logic, coding, and spatial/temporal reasoning. The dataset allows for the assessment of models on tasks requiring reasoning over 2 to 6 images. The authors conducted extensive evaluations of several state-of-the-art LLMs, revealing a significant performance gap compared to human proficiency. The paper also provides a detailed analysis of model performance, error sources, and the impact of few-shot prompting, and it promotes further research by open-sourcing the dataset.

### Strengths and Weaknesses
Strengths:
- The ReMI dataset is a well-motivated benchmark that addresses the need for evaluating multi-image reasoning in LLMs, highlighting a substantial gap in performance compared to humans.
- The paper is comprehensive in its task descriptions, dataset construction, and analysis of existing models, including qualitative insights.
- The dataset covers a diverse range of tasks, providing transparency in its creation and evaluation.

Weaknesses:
- The dataset's size is limited, with only 200 samples per task, which may undermine the reliability of results and facilitate potential gaming by existing models.
- There is a lack of evaluation involving open-source LLMs, which would provide a broader understanding of model performance relative to human baselines.

### Suggestions for Improvement
We recommend that the authors improve the dataset size to enhance the reliability of results and mitigate the risk of gaming by existing models. Additionally, we suggest evaluating open-source models alongside closed-source ones to provide a more comprehensive analysis of performance disparities. Furthermore, exploring the effects of increasing the number of prompt examples beyond 2 in few-shot learning could yield valuable insights into model capabilities. Finally, incorporating temporal multiple images, such as video frames, would broaden the dataset's applicability.