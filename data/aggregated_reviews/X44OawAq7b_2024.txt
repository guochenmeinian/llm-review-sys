ID: X44OawAq7b
Title: Provable Acceleration of Nesterov's Accelerated Gradient for Asymmetric Matrix Factorization and Linear Neural Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 4, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of the convergence rates of gradient descent and Nesterov's accelerated gradient descent methods for asymmetric matrix factorization and linear neural networks. The authors establish that unbalanced initialization can lead to linear convergence for both methods, with Nesterov's method achieving a faster rate. Theoretical findings are supported by numerical experiments.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and generally easy to follow.  
- The results are novel and relevant to the non-convex optimization field.  
- The analysis improves upon state-of-the-art results using innovative technical approaches.  

Weaknesses:  
- The paper studies a well-explored problem using standard techniques, which may limit its impact.  
- The numerical experiments are conducted on small matrices, lacking insights from larger datasets.  
- There are minor typos and unclear statements that detract from the overall clarity.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related literature, particularly the works by Zhang et al. (2021) and Bi et al. (2022), to contextualize their findings better. Additionally, consider illustrating the effect of unbalanced initialization in the numerical experiments and expanding the analysis to include larger matrices. We suggest clarifying the reasoning behind the convergence rate's independence from $\|A\|_F$ and addressing the potential for extending results to non-linear and non-smooth activation functions. Finally, please ensure thorough proofreading to correct minor errors and clarify ambiguous statements, such as the mention of the preconditioned method and the bounds in Theorems 1 and 2.