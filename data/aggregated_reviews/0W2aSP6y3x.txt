ID: 0W2aSP6y3x
Title: Vision-Enhanced Semantic Entity Recognition in Document Images via Visually-Asymmetric Consistency Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for semantic entity recognition that enhances model training with additional visual input encoding semantic entity types through colors. The authors propose a training scheme utilizing visually asymmetric consistency learning (VANCL) to improve visual encoders in multimodal models. The method demonstrates consistent performance improvements over existing models like LayoutLM on various datasets, while also allowing for a visually-enhanced training flow that can be detached during inference.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and presents a sound model design, with a clear motivation for using color to enhance recognition.  
- Empirical results show significant performance improvements and the method is adaptable to existing multimodal encoder backbones.  
- The authors provide thorough follow-up studies, including comparisons with other training schemes and visualizations of learned representations.

Weaknesses:  
- The marginal performance gain raises questions about the method's overall effectiveness, particularly given the lack of ablation studies on consistency loss.  
- Some claims regarding the limitations of existing visual encoders are not convincingly supported by evidence.  
- The explanation of the color painting step and the overall architecture could be clearer, as some terminology is inconsistently used.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the color painting step and ensure consistent terminology throughout the paper. Additionally, we suggest conducting a comprehensive ablation study on the consistency loss to better evaluate its impact. It would also be beneficial to provide more evidence supporting claims about the limitations of existing visual encoders. Finally, we encourage the authors to clarify the architecture diagram to enhance reader understanding.