ID: o8m4RM5mBk
Title: Attention Temperature Matters in ViT-Based Cross-Domain Few-Shot Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 5, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the application of Vision Transformer (ViT) in Cross-Domain Few-Shot Learning (CDFSL). The authors propose that the attention module in ViT negatively impacts performance in the target domain due to its discriminative nature, leading to a trade-off between discriminability and transferability. They introduce a method to enhance ViT's transferability by adjusting attention mechanisms through temperature scaling, which effectively reduces attention maps to uniform distributions. The proposed approach aims to limit the learning of query-key parameters while encouraging non-query-key parameters, demonstrating improved performance across multiple CDFSL datasets.

### Strengths and Weaknesses
Strengths:
1. The analysis of how attention affects CDFSL results is comprehensive and well-supported by quantitative analyses.
2. The proposed method achieves state-of-the-art performance on multiple datasets.
3. The paper is well-organized and free of typographical errors.

Weaknesses:
1. The conclusions drawn about the attention module's impact on performance are not convincingly supported by the simplistic model training methods used.
2. The distinction between the effects of "few-shot" and "cross-domain" learning is unclear, lacking rigorous analysis to separate these factors.
3. The paper primarily focuses on empirical results without providing sufficient theoretical insights into the observed phenomena.
4. The proposed method may require costly retraining and involves hyper-parameter tuning that could be challenging in practice.

### Suggestions for Improvement
We recommend that the authors improve the theoretical foundation of their claims regarding the attention module's performance impact, possibly by incorporating more sophisticated training methods like MAML or ProtoNet. Additionally, a clearer distinction between the effects of "few-shot" and "cross-domain" learning should be established through rigorous analysis. We suggest addressing the limitations of the proposed method, particularly regarding the need for retraining and hyper-parameter tuning, to enhance its practical applicability. Finally, providing high-resolution vector diagrams and correcting reference formatting would improve the overall presentation of the paper.