ID: lYdjzx3DYu
Title: EMR-Merging: Tuning-Free High-Performance Model Merging
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 5, 5, 6, -1, -1, -1, -1
Original Confidences: 4, 5, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method called EMR-MERGING (ELECT, MASK & RESCALE-MERGING) for merging models fine-tuned on different tasks into a single model with multi-task capabilities without the need for additional tuning or training. The authors identify significant performance degradation and the requirement for additional data in existing model merging methods as key issues. EMR-MERGING addresses these limitations and shows strong empirical performance across various benchmarks.

### Strengths and Weaknesses
Strengths:
- **Theoretical and Empirical Analysis:** The paper provides a solid theoretical foundation, including detailed proofs and empirical analyses, enhancing the robustness of the claims.
- **Tuning-Free:** The method does not require additional data, tuning, or training, making it practical for real-world applications.
- **Extensive Experiments:** The paper includes comprehensive experiments across visual, NLP, and multi-modal models, demonstrating significant performance improvements.

Weaknesses:
- **Additional Computational Overhead:** The method introduces extra computational overhead during inference due to masks and rescalers, which should be discussed regarding its impact on efficiency.
- **Storage Considerations for PEFT Models:** The storage implications of using masks in EMR-MERGING compared to adapter weights in PEFT methods are not adequately addressed.
- **Impact of Model Quantization:** The paper lacks discussion on how model quantization affects the storage overhead of masks, which is increasingly significant.
- **Limited Dataset Size in Experiments:** The small datasets used in experiments undermine the persuasive power of the results, suggesting the need for larger, more diverse datasets.
- **Novelty Concerns:** The method's novelty is questioned as it closely resembles existing techniques like TIES and DARE, warranting more detailed ablation studies to clarify performance improvements.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the additional computational overhead introduced by masks and rescalers during inference, particularly its impact on real-world applications. The authors should address the storage implications of using masks compared to PEFT methods and discuss the effects of model quantization on their method. To enhance the robustness of their findings, we suggest using larger and more diverse datasets in experiments. Additionally, we encourage the authors to provide a more disentangled analysis of the three steps in EMR-MERGING through ablation studies to clarify the performance improvements over existing methods. Lastly, addressing the missing related works and ensuring comprehensive comparisons across all benchmarks would strengthen the paper's contributions.