ID: 7MmYaN93lb
Title: Is Robustness Transferable across Languages in Multilingual Neural Machine Translation?
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the transferability of robustness in multilingual neural machine translation (MNMT) models when subjected to various types of noise. The authors conduct experiments that reveal robustness can indeed transfer across languages, with notable differences in transfer magnitude between one-to-many and many-to-one models. The findings suggest that robustness is transferable, which could inform future improvements in MNMT.

### Strengths and Weaknesses
Strengths:  
- The investigation into robustness transferability is novel and well-motivated.  
- The experiments yield insightful findings that could stimulate further research on multilingual encoders.  
- The paper effectively demonstrates that robustness can be transferred from high-resource to lower-resource languages without explicit attacks.

Weaknesses:  
- The connection to related work is insufficient, lacking context and comparison with earlier findings.  
- Many results are unsurprising, and surprising findings, such as the lack of reliance on source language similarity, are not explored in depth.  
- Important details regarding datasets and experimental setups are missing, leading to potential confusion.  
- Some figures are redundant or unclear, detracting from the paper's conciseness and clarity.  
- There is a noted trade-off between robustness and performance on clean datasets that requires further investigation.

### Suggestions for Improvement
We recommend that the authors improve the connection to related work by contextualizing their findings with earlier studies, clarifying how their results align or differ. Additionally, we suggest that the authors delve deeper into surprising findings, particularly regarding the independence of robustness transfer from source language similarity. It would be beneficial to clarify the datasets used in the experiments and how they are combined. We also advise revising figures for clarity and considering the use of tables where appropriate. Lastly, we encourage the authors to investigate the trade-off between robustness and clean performance more thoroughly and to provide specific examples in sections 4.3 and 4.4 to support their claims.