ID: jhWV5bLeT0
Title: How to Backdoor HyperNetwork in Personalized Federated Learning?
Conference: NeurIPS
Year: 2023
Number of Reviews: 2
Original Ratings: 6, 6
Original Confidences: 4, 3

Aggregated Review:
### Key Points
This paper presents a novel backdoor attack on HyperNetFL, where the server utilizes a hypernetwork to generate personalized models for clients. The authors introduce a learning rate randomization strategy to stealthily replace clean weights with backdoored weights. Theoretical analyses demonstrate the minimal impact of backdoored weights on the global model and their stealthiness, while extensive experiments validate the attack's effectiveness against baseline data poisoning methods.

### Strengths and Weaknesses
Strengths:
1. The paper addresses a new federated learning scenario that has not been previously explored in the literature.
2. The approach of using a single trojaned model to manipulate compromised clients is innovative.

Weaknesses:
1. The abstract claims adaptation of backdoor-resistant FL training algorithms into HyperNetFL, yet these defenses are ineffective. The authors should indicate a promising direction for defense.
2. Some claims are difficult to follow, such as the assertion that "X has a better model utility on legitimate data samples." The rationale behind this statement is unclear.
3. In Algorithm 1, line 10, the choice of a uniform distribution [a, b] is questionable; a Bernoulli distribution may be more appropriate.
4. The paper lacks clarity on how the proposed attack functions in standard FL settings.
5. It is ambiguous why using the gradient from a single model does not degrade utility, especially given the observed accuracy drop in Figure 2. Additionally, the explanation of HyperNet-based personalized federated learning is insufficient, and the abbreviation "HNREPL" is not defined.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their claims, particularly regarding the utility of the model and the rationale behind their assertions. It would be beneficial to provide a clearer explanation of HyperNet-based personalized federated learning and define all abbreviations, including "HNREPL." Additionally, we suggest that the authors explore and articulate a promising defense strategy against their proposed attack. Finally, reconsidering the choice of distribution in Algorithm 1 may enhance the robustness of the methodology.