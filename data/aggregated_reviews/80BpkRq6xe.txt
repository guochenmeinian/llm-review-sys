ID: 80BpkRq6xe
Title: PSSD: Making Large Language Models Self-denial via Human Psyche Structure
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework, PSSD, which enhances the reasoning accuracy of large language models (LLMs) by mimicking the human psyche structure through three roles: intuition-based id, rule-driven superego, and script-centric ego. The authors demonstrate that this multi-agent approach improves LLM performance on various datasets, outperforming current state-of-the-art methods. However, the paper lacks detailed explanations regarding the implementation and practical applications of the proposed method, particularly concerning the experimental roles and the implications of manual annotations.

### Strengths and Weaknesses
Strengths:
1. The paper innovatively applies psychological theories to enhance LLM reasoning, making the model behavior more relatable to human cognition.
2. Comprehensive experiments validate the proposed method's effectiveness and its integration with existing models.
3. The structured approach and two-stage fine-tuning strategy balance computational efficiency with reasoning accuracy.

Weaknesses:
1. The relationship between the challenges outlined in the introduction and the proposed methods is not clearly articulated.
2. The methodology section lacks necessary details, particularly regarding the operational specifics of the superego and ego roles.
3. The margin of improvement over existing methods is not substantial, and the computational costs are not analyzed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology by providing detailed explanations of the operational specifics for the superego and ego roles. Additionally, we suggest including a comprehensive analysis of computational costs and the implications of manual annotations required for the approach. The authors should also address the relationship between the identified challenges and their proposed solutions more explicitly. Furthermore, we encourage the inclusion of more experimental details, such as the rationale behind parameter choices and comparisons with other fine-tuning methods, to enhance the robustness of the findings.