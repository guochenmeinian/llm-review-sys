ID: plH8gW7tPQ
Title: Algorithmic Capabilities of Random Transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 4, 5, 5, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the algorithmic capabilities of randomly initialized transformer models, focusing on tasks such as integer addition, token retrieval, parenthesis balancing, and modular arithmetic. The authors demonstrate that optimizing only the embedding and unembedding mappings is sufficient for solving these tasks, suggesting that untrained transformers possess inherent algorithmic capabilities. The study includes ablation studies indicating the necessity of training both mappings and explores the limitations of untrained transformers in implementing functions within low-dimensional subspaces and sparse sub-networks. Additionally, it assesses the performance of untrained transformers in imitating smaller random transformers, revealing that while they achieve non-trivial performance, they do not match fully trained models. The exploration of the relationship between randomly initialized neural networks and circuit-based interpretability techniques is original and relevant.

### Strengths and Weaknesses
Strengths:  
1. **Insight into Model Initialization:** The research provides novel insights into the intrinsic algorithmic abilities of transformers prior to training.  
2. **Diverse Task Exploration:** The paper addresses a significant question regarding the inherent capabilities of transformers, exploring a diverse range of tasks that relate to existing literature.  
3. **Control Experiments:** Important control experiments clarify the necessity of training both input and output mappings, enhancing the interpretability of the findings.  
4. **Originality and Relevance:** The exploration of the relationship between randomly initialized neural networks and circuit-based interpretability techniques is original and relevant.  

Weaknesses:  
1. **Writing Quality:** The paper's writing style and organization require improvement, with missing references and unclear sections.  
2. **Generalization Concerns:** The findings are primarily based on synthetic tasks, lacking a thorough discussion on their applicability to real-world datasets.  
3. **Misuse of Terminology:** The term "algorithmic capabilities" is misapplied, as the study does not assess these capabilities outside the training distribution.  
4. **Confusing Conclusions:** The conclusions regarding low-dimensional subspaces and the performance of random transformers are mixed and unclear, leading to potential inconsistencies in the results presented.  
5. **Depth vs. Breadth:** The breadth of experiments may compromise depth, leading to preliminary interpretations that require caution, and some caveats and alternative explanations are not adequately discussed.  
6. **Lack of Rigor:** Section 3 lacks rigor in its formalism, and minor references to relevant literature are missing.

### Suggestions for Improvement
We recommend that the authors improve the writing quality to meet professional academic standards, addressing missing references and clarifying ambiguous sections. We suggest enhancing the depth of analysis by including additional control experiments and ablations, such as comparing against a random LSTM with trained input and output mappings to relate findings to reservoir computing literature, replacing the transformer with a random MLP to assess the importance of the attention mechanism, and establishing a naive baseline for task difficulty to evaluate the algorithmic capabilities of Random Transformers more accurately. Additionally, we encourage the authors to provide a more thorough discussion on the applicability of their findings to real-world tasks and reconsider the use of the term "algorithmic capabilities" to avoid overhyping their results. Expanding the limitations section to explicitly address the generality of findings and alternative explanations that cannot be ruled out would strengthen the paper. Clarifying the limitations of PCA in identifying nonlinear subspaces and the implications of neuron basis results, along with including a naive baseline in the KL divergence plots to contextualize performance gaps, would also be beneficial. Finally, we recommend enhancing the rigor of mathematical notation in Section 3 and referencing relevant literature on reservoir computing to improve the paper's academic rigor.