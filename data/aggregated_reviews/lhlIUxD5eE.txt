ID: lhlIUxD5eE
Title: Maximum Entropy Reinforcement Learning via Energy-Based Normalizing Flow
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 6, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new framework for Maximum Entropy Reinforcement Learning (MaxEnt RL) using Energy-Based Normalizing Flows (EBFlow). The authors argue that this framework offers three main advantages: obtaining the soft value function without estimation, combining policy evaluation and improvement in one iteration, and effective interaction with the environment without relying on Monte Carlo methods. The proposed techniques include Learnable Reward Shifting (LRS) for reward adjustment and Shifting-Based Clipped Double Q-Learning (SCDQ) utilizing two reward shifting networks.

### Strengths and Weaknesses
Strengths:
1. The paper is well-structured, with detailed background explanations that facilitate reader comprehension.
2. Diverse experimental domains clearly demonstrate the strengths of the proposed algorithm, supported by well-structured experiments.
3. The algorithm's avoidance of Monte Carlo methods enhances its training efficiency by enabling immediate action selection.
4. The ability to obtain the soft value function without estimation mitigates errors common in existing SAC-based algorithms.

Weaknesses:
1. The novelty of the algorithm may seem weak due to existing RL methods based on flow-based models. Additional considerations or advantages of EBFlow in RL should be highlighted.
2. The paper lacks sufficient mathematical proofs to validate the proposed framework's efficacy, particularly for LRS and SCDQ. Including such proofs would enhance confidence in the method.
3. Comparisons with more recent state-of-the-art algorithms are missing, as the current comparison with SAC feels outdated. Including these comparisons would better contextualize the framework's position in the evolving landscape of RL.

### Suggestions for Improvement
We recommend that the authors improve the novelty discussion by emphasizing any unique advantages of EBFlow in the context of RL. Additionally, we suggest including mathematical proofs to validate the convergence of the proposed framework to the optimal policy. It would also be beneficial to compare the proposed methods with more recent algorithms to provide a clearer understanding of their standing in the current RL landscape. Furthermore, a time-wise comparison of the algorithm's performance relative to SAC would clarify efficiency differences. Lastly, addressing the organization of the paper by moving critical discussions from the appendix to the main text would enhance clarity and accessibility.