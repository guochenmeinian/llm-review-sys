ID: PzYAMXmIT3
Title: Does Visual Pretraining Help End-to-End Reasoning?
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of self-supervised visual pretraining to enhance end-to-end methods for visual reasoning tasks. The authors propose a self-supervised representation learning framework based on slot tokens and evaluate its performance across multiple benchmarks, including CATER, ACRE, and RAVEN. The findings indicate that the proposed model outperforms traditional supervised pretraining methods and achieves competitive results with object-centric approaches.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and addresses an interesting problem, proposing a compelling solution that may interest the neuro-symbolic and end-to-end learning communities.
- The originality of the pre-training scheme and the successful transfer of representations from CATER to RAVEN are notable contributions.
- The technical soundness of the proposed method is supported by a robust number of ablations and visualizations.

Weaknesses:
- A significant weakness is the reliance on synthetic datasets, particularly CLEVR objects, which raises concerns about the applicability of the self-supervised objective to real-world images. The authors should consider adding evaluations on real-world datasets.
- The proposed method shows similarities to prior work, such as "Learning What and Where," which achieves higher accuracy on CATER. A detailed discussion of these similarities and differences is necessary.
- The evaluation protocol lacks clarity, particularly regarding the use of uniformly sampled frames and the omission of results on the moving camera split of CATER.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including experiments on real-world datasets to validate the effectiveness of their approach in practical scenarios. Additionally, a thorough comparison with existing methods, particularly regarding the performance of slot-based inputs versus object-centric representations, should be included. Clarifying the evaluation protocol on CATER and discussing the applicability of the proposed method to moving camera scenarios would enhance the paper's rigor. Finally, a more detailed discussion on the conceptual differences between their approach and existing slot attention methods is warranted to address originality concerns.