ID: 58jpJdPgKi
Title: Representation Projection Invariance Mitigates Representation Collapse
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 6
Original Ratings: -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel regularization method, Representation Projection Invariance (REPINA), aimed at mitigating representation collapse during the fine-tuning of pre-trained language models. The authors evaluate REPINA against five baselines across 13 language understanding tasks, demonstrating consistent outperformance in most cases. The study introduces a new regularization term based on the distance between pre-trained and fine-tuned representations, connects the method to multi-task learning, and provides metrics to quantify representation collapse.

### Strengths and Weaknesses
Strengths:
- The proposed method shows strong performance and low levels of representational collapse compared to baseline approaches.
- The experiments are comprehensive, covering various datasets and robustness scenarios, including out-of-distribution performance and label perturbation.
- The paper is well-written and presents a clear framework for addressing representation collapse.

Weaknesses:
- The evaluation is limited to the BERT-large model, raising questions about the method's applicability to other architectures.
- The MLP formulation is underspecified, lacking clarity on whether it solves an inner optimization problem or uses a first-order approximation.
- Some baselines omit stronger counterparts, such as Low-Rank Adaptation and Sharpness Aware Minimization, which could provide a more robust comparison.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the MLP formulation by specifying the optimization approach used. Additionally, expanding the evaluation to include diverse models beyond BERT, such as T5 or GPT-2, would strengthen the paper's conclusions. The authors should also consider incorporating recent parameter-efficient fine-tuning strategies in their comparisons. Furthermore, providing a more detailed analysis of REPINA's performance metrics and exploring its applicability in prompt tuning settings would enhance the paper's depth. Lastly, we suggest improving the writing in sections discussing regularization loss to clarify the logic presented.