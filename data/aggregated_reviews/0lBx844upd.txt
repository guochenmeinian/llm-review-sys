ID: 0lBx844upd
Title: ALPS: Improved Optimization for Highly Sparse One-Shot Pruning for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 3, 7, 6, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ALPS, an optimization-based framework for one-shot pruning of large language models (LLMs). ALPS formulates the pruning problem as an $\ell_0$-constrained optimization issue, utilizing an ADMM-based algorithm with operator splitting and preconditioned conjugate gradient methods. The authors demonstrate that ALPS achieves substantial reductions in test perplexity and improved performance on downstream tasks, particularly in high-sparsity regimes, while providing theoretical convergence guarantees.

### Strengths and Weaknesses
Strengths:  
1. ALPS achieves significant reductions in test perplexity and enhances zero-shot benchmark performance for highly sparse models.  
2. The framework offers theoretical convergence guarantees for $\ell_0$-constrained optimization problems using the ADMM solver.  
3. Efficient post-processing techniques via conjugate projected gradient are implemented, improving computational performance.  

Weaknesses:  
1. The paper omits important references, such as "Fast and optimal weight update for pruned large language models" by Bo≈æa, which addresses similar problems with an ADMM-based approach.  
2. Limited novelty is observed as ALPS closely resembles existing methods, particularly in its application to LLMs, which may not be sufficiently distinct to establish it as a novel contribution.  
3. At very high sparsity levels, ALPS's perplexity remains significantly higher than that of dense models, indicating potential performance limitations.  
4. The comparison with methods like Wanda and DSnoT is unfair, as it focuses solely on perplexity without considering overall running time per iteration/epoch.  

### Suggestions for Improvement
We recommend that the authors improve the literature review by including relevant references, particularly "Fast and optimal weight update for pruned large language models" and "Progressive weight pruning of deep neural networks using ADMM." Additionally, we suggest expanding the novelty discussion to clarify how ALPS differentiates itself from existing methods. 

To enhance the evaluation, we encourage the authors to report the overall running time per epoch for ALPS in comparison with Wanda and DSnoT, and to include perplexity results for dense models in all tables. Furthermore, we recommend evaluating ALPS on more recent models, such as LLaMA-3, and incorporating knowledge-intensive and reasoning-based tasks like MMLU and GSM8K for a more comprehensive assessment. Finally, we suggest exploring structured pruning to improve inference speed and practical applicability.