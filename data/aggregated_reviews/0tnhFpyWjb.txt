ID: 0tnhFpyWjb
Title: Two-Stage Predict+Optimize for MILPs with Unknown Parameters in Constraints
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 7, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel two-stage framework for Predict+Optimize that addresses uncertain parameters in constraints, focusing on learning unknowns in these constraints. The first stage involves making a soft commitment based on predictions, while the second stage updates this commitment to minimize the objective value plus a penalty for deviation. The authors argue that their approach simplifies the framework proposed by Hu et al. by eliminating the need for separate correction and penalty functions, instead solving a single optimization problem. They acknowledge recent work by Hu et al. and Nandwani et al., noting that while these approaches are technically applicable, they target different problems and may not perform well in their context. The paper includes numerical experiments demonstrating improved performance over previous methods, particularly in terms of post-hoc regret, but the authors emphasize the importance of contextualizing their work within the broader literature to clarify its contributions and applicability.

### Strengths and Weaknesses
Strengths:
- The proposed framework is described as simple and sensible, effectively improving soft commitments without violating constraints.
- The authors effectively situate their contributions within the existing literature, clarifying how their approach differs from prior work.
- The paper is well-written, with clear explanations and useful examples, particularly in real-world contexts.
- Experimental results indicate improved performance over previous methods, validating the proposed framework.

Weaknesses:
- The novelty is questioned due to insufficient comparisons with existing differentiable quadratic program solvers and other relevant literature.
- The literature review could be more comprehensive, particularly regarding the applicability of related works.
- The benchmarks used in the experiments are considered weak, lacking state-of-the-art approaches for a more robust evaluation.
- There is a lack of clarity on suitable penalty functions for their method, which could benefit from further elaboration.
- The computational complexity of the proposed method is not adequately discussed, raising concerns about its practicality.
- The runtime improvements of the proposed approach compared to existing implementations, such as cvxpylayers, remain unclear.
- The paper's reliance on linear penalties is noted as a limitation compared to Hu et al.'s framework.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by including comparisons with various differentiable quadratic program solvers, such as those referenced in the literature. Additionally, we suggest that the authors enhance the literature review by incorporating more related work to explain the applicability of various methods and how they compare to the proposed approach. We also recommend that the authors clarify the terminology around "Two-stage Predict+Optimize" to avoid confusion with existing literature. Furthermore, we encourage the authors to elaborate on the admissible penalty functions in Section 4, providing clearer guidelines on their design and application. Strengthening the experimental section by including state-of-the-art approaches for one-stage predict+optimize would enhance the evaluation of their framework. A detailed discussion on the computational cost and scalability of their approach is necessary, including dimensions of \(x\) and the number of constraints in the experiments. Lastly, clarifying the runtime performance of their approach relative to existing implementations and addressing the limitations of relying solely on linear penalties would provide a more balanced perspective on the framework's applicability.