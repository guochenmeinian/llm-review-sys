ID: dTj5tH94xv
Title: Does a sparse ReLU network training problem always admit an optimum ?
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 6, 7, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the existence of an optimal solution for training neural networks under sparsity constraints. The authors prove that, while it is often assumed that an optimal solution exists, this is not universally true. They derive necessary and sufficient conditions for the existence of an optimal solution, focusing on various sparsity patterns and their implications for neural network training.

### Strengths and Weaknesses
Strengths:
1. The relevance of the work is well defended, addressing an important and understudied problem in deep learning.
2. The results contribute significantly to the literature on the existence of optimal solutions in neural network training, particularly regarding sparsity.
3. The paper is coherent and well-structured, with clear definitions and a good literature review.
4. The inclusion of numerical examples aids in illustrating complex concepts.

Weaknesses:
1. The arguments against regularization in neural network training lack compelling support from existing literature.
2. The main example presented is viewed as artificial and not representative of practical scenarios in deep learning.
3. Many results are limited to 2-layer networks, leaving many cases open and potentially downplaying the limitations of the assumptions made.
4. Terminology in Table 1 may be misleading, and the clarity of the presentation, particularly in tables, could be improved.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding regularization-based methods for compressing neural networks, as this could clarify the implications of their work. Additionally, we suggest providing more relevant examples that are applicable to real-world scenarios, enhancing the practical relevance of the findings. The authors should also consider revising the terminology in Table 1 for clarity and ensuring that the limitations of their assumptions are more transparently communicated in the introduction. Lastly, we encourage the authors to enhance the presentation of results in the introduction and improve the clarity of tables for better reader comprehension.