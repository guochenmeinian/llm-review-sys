ID: jhdVt7rC8k
Title: Large Language Models are Temporal and Causal Reasoners for Video Question Answering
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel framework called Flipped-VQA for video question answering (VideoQA), which involves three tasks: predicting the answer A from video V and question Q, predicting question Q from V and A, and predicting video V from Q and A. The authors demonstrate that these additional objectives enhance performance on the primary VideoQA task while mitigating linguistic bias. Implemented in LLaMA-VQA, the framework outperforms both LLM-based and non-LLM-based models across five VideoQA benchmarks.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow, with a creative approach that expands perspectives on utilizing VideoQA data.
- Extensive experiments validate the proposed framework, showing significant improvements over strong baselines and providing insights into reducing linguistic bias.
- The evaluation is thorough, with a solid ablation study confirming the effectiveness of the auxiliary tasks.

Weaknesses:
- The VideoQA benchmarks utilized are all choice-based; incorporating generation-based datasets like ActivityNet-QA could enhance diversity.
- The framework is only applied to LLM-based models, limiting its verification for non-LLM-based models like HiTeA and InternVideo.

### Suggestions for Improvement
We recommend that the authors improve the diversity of their benchmarks by including generation-based VideoQA datasets such as ActivityNet-QA. Additionally, we suggest further verifying the effectiveness and universality of the Flipped-VQA framework by applying it to non-LLM-based models. Finally, we encourage the authors to incorporate recent studies on Video-LLMs into the related work section to strengthen the paper's context.