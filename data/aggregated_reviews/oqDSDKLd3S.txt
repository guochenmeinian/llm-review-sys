ID: oqDSDKLd3S
Title: Sample-Conditioned Hypothesis Stability Sharpens Information-Theoretic Generalization Bounds
Conference: NeurIPS
Year: 2023
Number of Reviews: 39
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to information-theoretic generalization bounds by introducing several new stability assumptions, specifically the sample-conditioned hypothesis (SCH) stability. The authors propose new IOMI and CMI bounds that address limitations of existing information-theoretic bounds in stochastic convex optimization (SCO) contexts. They clarify the relationship between stability parameters and information-theoretic quantities, emphasizing that the latter is not a valid bound on its own but is essential for understanding generalization in specific contexts. The authors provide examples demonstrating scenarios where traditional stability and information-theoretic bounds are insufficient, yet their proposed bounds achieve optimal rates. They demonstrate the tightness of their bounds through various examples, particularly highlighting Example 3, which shows that their bound is superior when both the stability parameter and MI quantity decay appropriately with sample size.

### Strengths and Weaknesses
Strengths:  
- The paper addresses significant limitations of information-theoretic generalization bounds, particularly their incompatibility with uniform stability frameworks.  
- The introduction of new stability assumptions and their connection to existing bounds is a noteworthy contribution.  
- The authors effectively illustrate the limitations of existing bounds through detailed examples, particularly Example 3, which showcases the optimality of their proposed bound.  
- The paper is generally well-written and presents a clear connection between stability and information-theoretic bounds, enhancing the theoretical understanding of generalization error.  
- The authors provide clear examples that effectively illustrate the advantages of their proposed bounds.

Weaknesses:  
- The presentation requires substantial improvement, particularly in defining additional structures and variables, which are often vague.  
- The intuition behind some results, especially in comparison to CMI bounds, is unclear.  
- Certain results appear redundant, with some theorems previously established in the literature.  
- The proposed bounds primarily rely on a strong notion of stability applicable only to deterministic algorithms, limiting their applicability to randomized algorithms.  
- The proposed bounds may not demonstrate significant improvements over conventional mutual information bounds, as highlighted by some reviewers.  
- There remains a lack of clarity regarding the dimension dependence of the proposed bounds, which could undermine their applicability in certain scenarios.  
- Some reviewers express concerns regarding the clarity of the direct analysis of generalization error, particularly questioning the derivation of the bound \(O(1/n\sqrt{n})\).  
- There is a perceived lack of consideration for the dimensionality in the counter-examples, which may affect the validity of the proposed bounds.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by providing clearer definitions for all variables and structures, particularly those that are currently vague. Additionally, we suggest that the authors clarify the intuition behind their results, especially in relation to CMI bounds. To enhance the novelty of their contributions, we encourage the authors to address redundancies with existing literature and explore the extension of their bounds to randomized algorithms. Furthermore, we recommend including examples that demonstrate the effectiveness of the proposed bounds in practical scenarios, particularly where neither stability nor classical information-theoretic bounds achieve the desired rates. We also suggest that the authors improve the clarity of the relationship between dimension dependence and their proposed bounds, specifically addressing how stability terms can be dimension-independent while still providing meaningful generalization guarantees. Additionally, we recommend that the authors improve the clarity of the direct analysis of generalization error, specifically detailing how they derive \(O(1/n\sqrt{n})\), and address the concerns regarding dimensionality in their counter-examples to strengthen their arguments. Providing a more thorough explanation of the conditions under which their bounds hold, particularly in relation to the stability parameter and MI quantity, would also enhance the manuscript's rigor.