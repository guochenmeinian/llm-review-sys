ID: 3TSpM7X2aY
Title: DRAM-like Architecture with Asynchronous Refreshing for Continual Relation Extraction
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel architecture for continual relation extraction (CRE) inspired by Dynamic Random Access Memory (DRAM) mechanisms, aimed at addressing catastrophic forgetting. The proposed method includes a DRAM-like architecture with three components: a perceptron, a controller, and a refresher, alongside asynchronous refreshing strategies. The authors evaluate their approach against two benchmarks and three baselines, reporting a modest improvement of about 1.5% over existing methods. The paper includes ablation studies that clarify the contributions of each component.

### Strengths and Weaknesses
Strengths:
- The approach offers a relevant solution to the challenge of catastrophic forgetting in relation extraction.
- The architecture is well-motivated, and the intuition behind the proposed method is clear.
- The ablation studies provide valuable insights into the contributions of different components.

Weaknesses:
- The reported performance improvement is not substantial, raising questions about the impact of the proposed method.
- The paper contains minor typos and awkward sentences that detract from overall readability.
- The Conclusions section closely resembles the abstract and lacks depth in discussing the significance of the findings.
- The lack of resources for reproducibility hinders the validation of the experiments.

### Suggestions for Improvement
We recommend that the authors improve the language and clarity throughout the paper to enhance readability. A revision of the Conclusions section is necessary to more effectively articulate the impact of the proposed approach. Additionally, we suggest that the authors provide resources to facilitate reproducibility of their experiments. Clarifying the connection to rehearsal learning and expanding on the rationale for specific components would also strengthen the paper. Finally, consider extending the forgetting performance results in Table 4 to include up to T8 for a more comprehensive evaluation.