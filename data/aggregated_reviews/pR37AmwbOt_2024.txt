ID: pR37AmwbOt
Title: Leveraging Catastrophic Forgetting to Develop Safe Diffusion Models against Malicious Finetuning
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to prevent malicious fine-tuning attacks on text-to-image (T2I) models by leveraging catastrophic forgetting through contrastive learning. The authors propose two techniques to separate safe and harmful distributions in the latent space of T2I models, demonstrating the effectiveness of their approach through quantitative and qualitative results across various datasets and scenarios.

### Strengths and Weaknesses
Strengths:
- The proposed algorithm is well-motivated, clearly illustrated, and easy to follow.
- The integration of contrastive learning with diffusion models is innovative and addresses a significant issue in generative models.
- Experiments show that the proposed methods significantly improve safety and resistance to harmful image generation.

Weaknesses:
- There are concerns regarding the feasibility of achieving a universally safe model, as current experiments only address specific concepts (nudity, violence).
- Numerical results indicate only marginal improvements, particularly in Aesthetic and CLIP Scores, with notable performance degradation when addressing violence.
- The organization of Appendix A is unclear, and the quality of Figure 3 is low.
- The use of the NSFW score as a metric is problematic, and there is a lack of comparison with other methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of mathematical formulations, particularly in Equations 4 and 5, and ensure consistent usage of bold values in tables. Additionally, we suggest including more comprehensive evaluations of the model's performance on normal images, such as FID scores, and exploring the robustness of the model against more complex attacks beyond simple fine-tuning. It would also be beneficial to clarify the definition of "unlearning model" and provide more details about the human evaluation participants. Finally, we encourage the authors to experiment with unlearning additional harmful concepts to assess the maximum number of concepts that can be effectively removed without compromising generation quality.