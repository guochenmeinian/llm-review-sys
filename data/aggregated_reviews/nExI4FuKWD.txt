ID: nExI4FuKWD
Title: FineCLIP: Self-distilled Region-based CLIP for Better Fine-grained Understanding
Conference: NeurIPS
Year: 2024
Number of Reviews: 21
Original Ratings: 5, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FineCLIP, a vision-language model that integrates three techniques for enhancing fine-grained understanding in dense prediction tasks: global contrastive loss, regional contrastive loss, and a real-time self-distillation scheme. The authors also introduce a new dataset, CC2.5M, for further fine-tuning. FineCLIP aims to address the limitations of existing models like CLIP, particularly in their ability to comprehend fine-grained details. The method demonstrates significant performance advantages over competing models, even when utilizing less effective region proposal methods. The authors argue that FineCLIP's training time is comparable to that of CLIPSelf and RegionCLIP, despite its high GPU memory requirements. The focus of the paper is on FineCLIP's effectiveness in common fine-tuning settings, with scalability experiments suggesting potential for pre-training from scratch. The authors address concerns regarding the influence of region proposals and the computational efficiency of FineCLIP.

### Strengths and Weaknesses
Strengths:
- The proposed FineCLIP effectively combines multi-grained contrastive learning and self-distillation, achieving better fine-grained understanding while maintaining image-level performance.
- FineCLIP outperforms other methods in performance metrics, as shown in various tables.
- The paper is well-written, clearly detailing the technical aspects and conducting extensive experiments across multiple datasets.
- The authors provide extensive experiments validating FineCLIP's effectiveness in fine-tuning settings.
- The inclusion of multi-GPU training code in supplementary material enhances usability.

Weaknesses:
- **W1:** Limited practical impact, as FineCLIP's performance on benchmarks like OV-COCO is significantly lower than CLIPSelf, raising questions about input resolution choices.
- **W2:** Insufficient and incomplete experiments, particularly regarding input resolution comparisons and missing metrics like mean mask AP on rare categories for OV-LVIS.
- **W3:** The limitations of CLIPSelf remain unaddressed, particularly regarding the reliance on the teacher's capabilities for self-distillation, which is undermined by performance drops in image-level benchmarks.
- High GPU memory requirements may necessitate multi-GPU implementations, raising concerns about accessibility.
- The potential unfair advantage in using detectors and strong LVLMs for training data generation is questioned, impacting the perceived fairness of the method.

### Suggestions for Improvement
We recommend that the authors improve the experimental comparisons by including higher input resolutions for FineCLIP to provide a fair assessment against CLIPSelf. Additionally, the authors should include the mean mask AP on rare categories for OV-LVIS and clarify the performance discrepancies observed in the ablation study. It would also be beneficial to provide more details on the computational efficiency of FineCLIP, including training time and GPU memory usage, to better demonstrate its usability. Furthermore, we suggest exploring methods to reduce GPU memory consumption, possibly through frameworks like DeepSpeed or FlashAttention. Lastly, we recommend clarifying the implications of using detectors and LVLMs in training to address concerns about direct supervision and its effect on performance.