ID: yCh1z6Dcto
Title: Stepping Forward on the Last Mile
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a quantized approach for on-device fine-tuning of pretrained models, focusing on fixed-point forward gradients to reduce memory usage. The authors propose the Quantized Zeroth-order Forward Gradient (QZO-FF) algorithm, which integrates techniques like sign-m-SPSA, Momentum Guided Sampling, Sharpness-aware Perturbation, Sparse Update, and Kernel-wise Normalization. The algorithm is evaluated across various tasks in vision and audio domains, demonstrating its feasibility for model customization on edge devices while maintaining competitive performance compared to traditional backpropagation methods. Additionally, the paper provides a comprehensive comparison of QZO-FF and backpropagation (BP) regarding memory, computational complexity, and convergence speed, emphasizing that while ZO-FF is more compute-efficient at the same memory cost, it converges approximately 2x slower than BP. The authors propose to include training curves and a time-compute-memory trade-off analysis in their ablation studies.

### Strengths and Weaknesses
Strengths:  
- The problem addressed is highly relevant to on-device training and builds upon recent work.  
- The proposed algorithm is sound, well-explained, and shows effective performance across diverse experimental settings.  
- The paper is well-organized and clearly written, with a thorough exploration of various architectures and modalities.  
- The detailed analysis of memory and computational efficiency between BP and ZO-FF highlights practical benefits for edge devices.  
- The inclusion of empirical measurements and a clear explanation of the time-memory trade-off enhances the paper's technical rigor.  

Weaknesses:  
- The clarity regarding what is kept in full precision in the proposed procedure is lacking.  
- The experiments are not sufficiently detailed, leading to difficulties in understanding the setups; there are also no error bars in tables and figures, complicating result interpretation.  
- The paper does not include ablation studies to assess the individual contributions of the proposed enhancements, raising concerns about the robustness of the findings.  
- Some reviewers remain unconvinced by the rationale for the absence of these studies, which may affect the overall acceptance of the work.  
- The model sizes tested are relatively small, raising questions about the method's scalability to larger models.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of what quantities are maintained in full precision and provide a detailed pseudo-algorithm for cases with multiple perturbations. Additionally, we suggest including error bars in all figures and tables to enhance the statistical significance of the results. The authors should conduct ablation studies for a selected use case in Section 4.1 to clarify the contributions of each enhancement technique and strengthen the paper's findings. If time allows, please consider which use case would be of particular interest to the research community for these studies. Furthermore, we encourage the authors to explore the performance of their method on larger models to validate its applicability in practical scenarios.