ID: Sxu7xlUJGx
Title: Implicit Variational Inference for High-Dimensional Posteriors
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 4, 8, 8, 8, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents LIVI, a variational-inference-based approach for Bayesian neural networks (NNs) that utilizes implicit variational inference (IVI) to sample from arbitrary distributions. The authors address challenges such as the ill-defined KL divergence and intractable entropy by introducing Gaussian noise to the output of the neural sampler, resulting in a Gaussian deep latent variable model (DLVM). They propose a linearization of the entropy term and a lower-bound approximation to the ELBO, which enhances scalability for high-dimensional settings. Initial results indicate that LIVI can scale effectively to modern network sizes, achieving an accuracy of 76.7% on a WideResNet(28,10) architecture with 36.5M parameters on CIFAR-100. The method demonstrates improved performance on various tasks compared to baseline Bayesian methods and aligns with established baselines, allowing for meaningful comparisons within the literature on approximate inference methods.

### Strengths and Weaknesses
Strengths:
* The approach innovatively employs hyper-networks for learning a Bayesian model using variational inference.
* LIVI demonstrates promising scalability to large neural networks, as evidenced by its competitive accuracy and negative log-likelihood results.
* The experimental results show significant improvements on out-of-distribution tasks compared to baseline methods.
* The authors provide a comprehensive experimental evaluation, comparing LIVI with relevant methods in the field, including KIVI and AVB.
* The paper is clearly written, with comprehensive details that support reproducibility.

Weaknesses:
* The method struggles with scalability to larger networks, as evidenced by the largest tested network having only 2.7 million parameters, despite claims of effective scaling.
* The entropy estimation method is questioned, particularly regarding the treatment of the standard deviation of noise.
* The experimental comparisons are limited to somewhat outdated methods, lacking evaluations against more recent Bayesian models and flow-based methods.
* The authors do not include certain relevant references in their initial submission, which may limit the contextual understanding of LIVI's contributions.
* The focus on implicit VI methods may overlook other important aspects of approximate inference that could enrich the discussion.
* The choice of datasets and benchmarks is insufficient to fully demonstrate the advantages of the proposed method.

### Suggestions for Improvement
We recommend that the authors improve the scalability of their method to accommodate larger networks, such as ResNet architectures, and provide a clear strategy for this enhancement. Additionally, we suggest that the authors justify their approach to estimating the entropy term more thoroughly. To strengthen the experimental section, we encourage the authors to include comparisons with more recent Bayesian models and alternative methods, such as flow-based approaches. Expanding the dataset range to include more challenging benchmarks, like CIFAR-100 and fine-grained classification datasets, would also be beneficial. Lastly, we advise including runtime performance metrics in the main paper to provide a clearer understanding of the method's efficiency. Furthermore, we recommend improving the clarity of the experimental setup by explicitly stating the rationale for their choice of baselines and addressing the limitations of the excluded references. Enhancing the related works section by incorporating the suggested references would provide a more comprehensive context for LIVI's contributions.