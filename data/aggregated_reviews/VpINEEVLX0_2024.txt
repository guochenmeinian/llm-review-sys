ID: VpINEEVLX0
Title: A Topology-aware Graph Coarsening Framework for Continual Graph Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TACO, a novel rehearsal-based continual graph learning method that addresses catastrophic forgetting in Graph Neural Networks (GNNs). TACO stores information from previous tasks as a reduced graph, utilizing a graph coarsening algorithm, RePro, which maintains essential topological information while compressing the graph. The authors propose a generalized class-il setting based on time steps rather than classes, and experimental results demonstrate significant performance improvements across three graph datasets.

### Strengths and Weaknesses
Strengths:  
- The introduction of a topology-aware coarsening approach for continual learning in GNNs is innovative and addresses a significant problem.  
- The methodology is well-defined, enhancing reproducibility, and extensive experiments provide strong empirical support for TACO's efficacy.  
- The proposed method effectively preserves topological information while maintaining manageable memory consumption.

Weaknesses:  
- The paper lacks a clear structure, with some confusing statements.  
- Important existing works on continual graph learning are missing from the discussion.  
- The choice of baseline methods may not encompass all recent advances in continual learning and graph coarsening.  
- The clarity of certain sections, particularly section 4.4.3, is inadequate, and the task splitting methodology requires further elaboration.

### Suggestions for Improvement
We recommend that the authors improve the paper's structure and clarity, particularly in section 4.4.3, to enhance understanding. Additionally, a more comprehensive comparison with existing methods, including those related to graph condensation, is necessary to validate TACO's contributions. We suggest including an ablation study to investigate the overall performance of TACO with different reduction rates and to empirically demonstrate that the replay buffer is not the primary performance booster. Furthermore, the authors should clarify the task splitting process and ensure that all references are complete and accurate.