ID: Oq32ylAOu2
Title: MindMerger: Efficiently Boosting LLM Reasoning in non-English Languages
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MergeMinds, a method that integrates LLMs with multilingual models to enhance reasoning capabilities across multiple languages. The authors propose a two-step training scheme: first, embedding the multilingual model into the LLM using translation data, and second, collaboratively utilizing the built-in capabilities of both models. The method shows significant improvements in multilingual reasoning and language understanding tasks, particularly in low-resource languages, as evidenced by accuracy enhancements across various datasets. Additionally, the authors conduct a comparative analysis of training data types for enhancing the performance of LLMs, proposing three types of training data: (a) translation data, (b) English task data, and (c) query translation task data. They demonstrate that MindMerger consistently outperforms fully fine-tuned methods, achieving an average accuracy improvement of 5.5% with translation data.

### Strengths and Weaknesses
Strengths:
1. The MergeMinds method offers a novel solution by merging LLMs with external multilingual capabilities, enhancing model robustness.
2. Extensive experimental results validate the effectiveness of the proposed method across multiple languages, particularly low-resource languages.
3. The experimental design is robust, exploring multiple dataset usage settings, and MindMerger shows significant performance gains over the fully fine-tuned method, highlighting the effectiveness of translation data.

Weaknesses:
1. The motivation for MergeMinds closely resembles that of LangBridge, as both utilize a mapping layer to connect multilingual encoders to existing MLLMs.
2. The paper lacks clarity regarding the use of translation data and English task datasets in the two-stage training process.
3. The method's effectiveness in generation tasks remains ambiguous, as it primarily focuses on understanding tasks.
4. Confusing terminology, such as "mapping stage" and "augmentation stage," may lead to misinterpretations of the methodology.
5. The paper does not explore the potential of using only two-stage training without an external multilingual model, which could provide additional insights.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the two training stages, specifically detailing the utilization of translation and parallel data. Additionally, we suggest including a comparison with baselines regarding the number of parameters trained during the two-stage training process. To enhance the method's credibility, we encourage the authors to demonstrate its effectiveness on generation tasks, such as summarization and translation. Furthermore, we advise refining the terminology used in the paper to avoid confusion and consider adding explicit visual examples or diagrams to clarify the methodology. Finally, we recommend that the authors explore the two-stage training approach for LLMs without including an external multilingual model to provide further insights.