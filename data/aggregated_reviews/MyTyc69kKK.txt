ID: MyTyc69kKK
Title: TSTR: Target Similarity Tuning Meets the Real World
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the deployment of the Target Similarity Tuning (TST) model in production, addressing practical issues such as sensitivity and inference costs of transformer models, dataset curation for training, and evaluation without code generation. The authors propose training a neural network on embeddings to capture code similarity with the TST objective, selecting positive and negative examples for improved results and smaller data sizes. They also introduce a ranking-based evaluation method that correlates with end-to-end code generation performance. 

### Strengths and Weaknesses
Strengths:  
- The paper tackles a practical real-world problem and presents simple techniques to enhance performance.  
- The proposed modifications yield gains, particularly when the language distribution of examples differs from the test set.  
- The dataset filtering improves results and accelerates training.  
- The evaluation method is simpler and aligns well with end-to-end evaluations.  

Weaknesses:  
- The training and evaluation data are not publicly available, which raises reproducibility concerns.  
- The presentation of the paper requires significant improvement.  
- The work is considered incremental, lacking comparison with other similar methods and a clearer explanation of the evaluation metric.  
- Insufficient details on the training dataset curation process are provided.  

### Suggestions for Improvement
We recommend that the authors improve the clarity and presentation of the paper. Additionally, please provide results on public datasets as promised, and ensure that comparisons with more recent related work are included. It is crucial to elaborate on the training dataset curation process and clarify the evaluation metric to enhance its realistic meaning. Addressing these concerns will strengthen the paper's contribution and reproducibility.