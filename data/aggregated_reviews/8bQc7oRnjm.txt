ID: 8bQc7oRnjm
Title: Provably Efficient Offline Reinforcement Learning in Regular Decision Processes
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 8, 6, 6, 8, 6, -1, -1
Original Confidences: 3, 3, 3, 2, 4, -1, -1

Aggregated Review:
### Key Points
This paper presents an offline reinforcement learning (RL) algorithm designed to learn near-optimal policies in episodic Regular Decision Processes (RDPs). The authors propose a two-phase approach: first, learning the transition function of the RDP, and second, transforming the problem into an offline learning scenario for Markov Decision Processes (MDPs) to derive a near-optimal policy. The algorithm includes a novel automata learning technique and introduces a concentrability coefficient, providing theoretical guarantees and sample complexity bounds.

### Strengths and Weaknesses
Strengths:
- The paper introduces the first algorithm for offline RL in RDPs with provable guarantees, significantly contributing to the field.
- The exposition is clear and well-organized, facilitating a coherent reading experience.
- The theoretical results, including sample complexity bounds, are robust and support the claims of novelty.

Weaknesses:
- There is a lack of empirical validation; empirical experiments, even on toy examples, would enhance the paper's value.
- The presentation could benefit from a comparative analysis of RDP algorithms in tabular form for clarity.
- Key theoretical challenges and insights are not sufficiently highlighted in the main text, which could limit the paper's impact.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation of their algorithm by including experiments on toy examples to substantiate their theoretical claims. Additionally, presenting a comparative analysis of RDP algorithms in a tabular format would enhance clarity. The authors should also consider reclaiming space from example 1 to better highlight key theoretical challenges and insights. Finally, a more detailed technical discussion on the implications of their assumptions, particularly Assumption 1, would provide valuable context for their contributions.