ID: v8fRIzqeob
Title: Exploring the Cognitive Knowledge Structure of Large Language Models: An Educational Diagnostic Assessment Approach
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an educational diagnostic assessment approach to evaluate the performance of LLMs across various dimensions. The authors propose a method that utilizes MoocRadar, a human test dataset based on Bloom's Taxonomy, to explore three primary research questions: Performance Analysis, Deficit Assessment, and Error Assessment. The main contributions include introducing the cognitive knowledge structure of LLMs, proposing the Educational Diagnostic Assessment method, and assessing LLMs’ performance, deficits, and errors.

### Strengths and Weaknesses
Strengths:  
- The paper is clearly organized and well-written, facilitating reader comprehension.  
- The motivation is clear, and the research question is valuable, contributing to the understanding of LLMs' knowledge structures and cognitive patterns.  
- The experimental methodology is detailed and presented from multiple perspectives, leading to reasonable conclusions.  
- The results comprehensively illustrate the strengths and weaknesses of current LLMs in answering questions and providing explanations.

Weaknesses:  
- The authors need to analyze the effect of selected instructions on the experimental results, as different instructions significantly impact LLM performance.  
- The approach may not effectively extend to other datasets, limiting the exploration of knowledge structures.  
- The description of the MoocRadar dataset should be more detailed, particularly regarding Knowledge-Types, to avoid barriers for readers.  
- Essential details about experiment settings and reproducibility are missing, such as annotator training, inter-rater scores, and the prompts used for model explanations.

### Suggestions for Improvement
We recommend that the authors improve the description of the diagnostic assessment method to provide more clarity. Additionally, the authors should evaluate their work against more models for a more comprehensive analysis. It is crucial to add details about the dataset annotations and the models' answer/explanation accuracies to enhance credibility. Clarifying the apparent contradictions regarding the zero-shot scenario and providing an extended version of Table 4 that includes ±context, ordering effect, and topic subjects would also be beneficial. Finally, we suggest that the authors include insights in the Discussion section on why LLMs perform worse on intermediate-level Bloom's Taxonomy questions and propose strategies for improving model performance.