ID: wOmtZ5FgMH
Title: RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 7, 7, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the RWKU benchmark, aimed at evaluating the knowledge unlearning capabilities of large language models (LLMs) by targeting sensitive, copyrighted, or harmful knowledge. The authors propose a novel forgetting method and validate its effectiveness through extensive experiments, including membership inference attacks and adversarial probes. The benchmark focuses on unlearning knowledge about 200 famous individuals, providing insights into the challenges of unlearning in real-world scenarios. Additionally, the paper includes a manual qualitative analysis of probe diversity, incorporating a clustering analysis of 200 randomly sampled probes, which identified 48 distinct cluster centers. The authors demonstrate that the questions cover a wide range of topics, with GPT-4 showing a preference for certain categories such as 'Received awards' and 'Acted films'. The analysis reveals diverse questioning styles within most categories, although some, like 'Place of birth' and 'Date of birth', exhibit limited template usage. The authors aim to enhance diversity in these specific categories through paraphrasing strategies.

### Strengths and Weaknesses
Strengths:
- The introduction of the RWKU benchmark is comprehensive, addressing a critical issue of unlearning in LLMs, which is essential for privacy and regulatory compliance.
- The experimental design is thorough, covering various unlearning scenarios, models, and baseline methods, yielding detailed insights into the efficacy and challenges of different approaches.
- The paper is well-written, with a clear structure that guides the reader through the motivation, methodology, and results.
- The manual clustering analysis effectively demonstrates a wide range of topics and diverse questioning styles across most categories, providing valuable insights into probe diversity.

Weaknesses:
- The reliance on famous individuals as unlearning targets may introduce bias, limiting the benchmark's generalizability to other knowledge types.
- The absence of a dedicated forget corpus raises concerns about the clarity and feasibility of evaluating unlearning effectiveness.
- Some unlearning methods may not be feasible for all models due to computational constraints, and the effectiveness of adversarial probes may not comprehensively represent all potential extraction methods.
- The impact of unlearning on overall model performance is not thoroughly discussed, particularly regarding potential degradation in unrelated areas.
- Certain categories, specifically 'Place of birth' and 'Date of birth', show a tendency towards uniform phrasing, indicating a lack of diversity in question generation.

### Suggestions for Improvement
We recommend that the authors improve the diversity of unlearning targets beyond celebrities to encompass a broader range of knowledge types, including scientific concepts and historical events. Additionally, we suggest providing specific API endpoints for reproducibility and conducting human evaluations to ensure the quality and diversity of generated probes. It would also be beneficial to discuss the practicality of the benchmark in real-world scenarios and assess the impact of unlearning on long-tail entities. Furthermore, we recommend that the authors actively employ paraphrasing strategies to enhance the diversity of questions in categories with limited templates. Finally, clarifying the selection process for knowledge points in fill-in-the-blank style probes and the motivation for the new task setting and knowledge sources would enhance the overall understanding of the study.