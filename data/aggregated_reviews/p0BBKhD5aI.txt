ID: p0BBKhD5aI
Title: Infinite Limits of Multi-head Transformer Dynamics
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 6, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an analysis of scaling limits of transformer models concerning key-query dimension \(N\), head count \(H\), and depth \(L\) using dynamical mean-field theory (DMFT). It demonstrates that for the \(N \to \infty\) limit, \(1/N\) scaling of pre-attention scores is necessary for stable learning, leading to degenerate heads. In the \(H \to \infty\) limit, heads behave independently, while for \(L \to \infty\), a branch scaling of \(1/L\) is required for feature evolution. The theoretical findings are supported by experiments on natural language datasets.

### Strengths and Weaknesses
Strengths:
- The paper provides a solid analysis of hyperparameter scaling in large-scale transformer models, extending previous work to the transformer architecture.
- The theoretical results establish appropriate scaling regimes that ensure diverse kernels across attention heads, supported by heuristics and detailed experiments.
- The analysis includes concrete recommendations regarding scaling regimes, MLP layers, learning rate scaling, and LayerNorm.

Weaknesses:
- The paper's complexity and convoluted notation may hinder understanding, especially for those unfamiliar with DMFT.
- There is insufficient discussion on the effectiveness of each scaling concerning computational cost, particularly regarding optimal configurations in terms of flops.
- The relevance of the \(H\) limit is questionable, as practical applications typically use fewer than 100 heads.
- The paper lacks a thorough exploration of previous results or approaches analyzing limiting dynamics of neural networks under Adam.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing more background on DMFT in the main text and simplifying the notation. Additionally, a discussion on the computational efficiency of different scaling configurations should be included. The authors should also explore the implications of interpolating between the scaling exponents \(\alpha_\mathcal{A} = 1\) and \(\alpha_\mathcal{A} = 1/2\). Furthermore, addressing the relevance of the \(H\) limit in practical applications and incorporating related work on non-asymptotic approaches to scaling would strengthen the paper. Lastly, we suggest adding a section explaining the merits of the path integral approach compared to other methods to enhance the paper's accessibility.