ID: ss9UXxbSys
Title: Bridging the Gap: Teacher-Assisted Wasserstein Knowledge Distillation for Efficient Multi-Modal Recommendation
Conference: ACM
Year: 2024
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TARec, a framework designed to compress complex multi-modal recommender systems into efficient ID-based Multi-Layer Perceptron-based systems using a teacher-assisted Wasserstein Knowledge Distillation approach. The framework employs a two-stage distillation process, incorporating logit-level KD with Wasserstein Distance and embedding-level contrastive KD. Extensive experiments demonstrate that TARec enhances both efficiency and effectiveness compared to traditional methods.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to understand.
2. It provides an anonymous code link, facilitating reproducibility for other researchers.
3. The proposed two-stage distillation approach is reasonable given the significant gap between MMRec and MLPRec.

Weaknesses:
1. Key information is missing, including the dataset used and the model that produced the results, raising doubts about the findings.
2. The structure of the Teacher model and the Student model is not explained, limiting understanding.
3. The experiment section lacks results for the Teacher and TA models, making it difficult to assess method effectiveness.
4. Comparisons with recent baselines, particularly lightweight multi-modal recommender systems, are insufficient.
5. The novelty of the approach may be perceived as incremental, as it combines existing technologies without significant innovation.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the dataset and model details in the Introduction to bolster confidence in the results. Additionally, the authors should provide a detailed explanation of the Teacher and Student model structures. In the experiment section, it would be beneficial to include results for the Teacher and TA models and verify the performance of MLPRec without pre-training. We also suggest incorporating comparisons with more recent baselines and discussing the necessity of the second distillation stage from the TA to the student. Finally, enhancing the figures by removing shadows could improve clarity.