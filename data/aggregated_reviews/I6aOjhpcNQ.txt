ID: I6aOjhpcNQ
Title: Robust Concept Erasure via Kernelized Rate-Distortion Maximization
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 5, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Kernelized Rate-Distortion Maximizer (KRaM) framework for the concept erasure task, which aims to remove specific attributes from distributed representations while preserving other information. The authors provide a theoretical analysis and empirical results demonstrating KRaM's effectiveness across various representation forms, including categorical and continuous variables. The paper also introduces a measurement for evaluating representation alignment before and after concept erasure.

### Strengths and Weaknesses
Strengths:
- The topic is significant and timely, addressing AI safety and interpretability.
- The paper is well-structured, with a clear introduction and comprehensive empirical and theoretical analyses.
- The proposed method offers a novel approach to measuring information preservation in representations.

Weaknesses:
- The contributions and novelty of KRaM are unclear, primarily as it appears to be a straightforward application of kernels to the rate-distortion maximizer framework.
- The theoretical results are limited to the RBF kernel, lacking generalization to other kernel types.
- The experiments do not sufficiently demonstrate KRaM's effectiveness, as they overlook competitive methods and show limited improvements over existing approaches.
- The necessity of training an additional concept function f is questioned, particularly regarding its impact on performance if only pre-trained model outputs are used.

### Suggestions for Improvement
We recommend that the authors improve the clarity of KRaM's contributions and its distinctions from existing concept erasure methods. Additionally, the authors should expand the theoretical analysis to include results for other kernel types. To strengthen the empirical validation, we suggest incorporating more competitive methods in the experiments and addressing the performance gap observed in Table 2. Furthermore, the authors should clarify the rationale behind training the extra concept function f and explore the implications of using outputs from a pre-trained model instead.