ID: XbVnNXaIQY
Title: Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 3, 7, 5, 7, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the "holistic transfer" (HT) task, which addresses the challenges of using incomplete target data that covers only a subset of class labels to fine-tune a pre-trained model. The authors propose several techniques for fine-tuning, including leave-out local SGD (LOLSGD), freezing a linear classifier, selective distillation, and feature rank regularization, aimed at enhancing the model's generalization to unseen-class data in the target domain. They argue that HT differs significantly from source-free domain adaptation (SFDA) and test-time adaptation (TTA), which primarily focus on recovering true labels for unlabeled target data. Experimental results across multiple benchmark datasets demonstrate the effectiveness of the proposed method, although there are concerns regarding its performance on unseen classes.

### Strengths and Weaknesses
Strengths:
- The problem setting of holistic transfer is relevant and significant for practical machine learning applications.
- The proposed method effectively boosts overall accuracy across various datasets.
- The authors provide a clear distinction between HT and related paradigms like SFDA and TTA, highlighting the unique challenges HT addresses.
- They acknowledge the limitations of standard fine-tuning in scenarios with missing classes, positioning HT as a more flexible solution for partial data situations.
- The authors express a willingness to incorporate SFDA and TTA methods into their final version, indicating openness to feedback.

Weaknesses:
- The paper lacks a thorough discussion comparing the proposed method with related work, particularly with partially zero-shot domain adaptation (PZDA) and other existing methods like test-time adaptation.
- The performance of the proposed method on unseen classes is often comparable to or worse than the source model, raising doubts about its generalizability.
- The manuscript is poorly structured, with insufficient discussion on related work and limitations, while the description of the problem setting is redundant.
- There is skepticism regarding the novelty and practical value of HT compared to standard fine-tuning.
- The evaluation lacks a unified approach for different pre-trained models, which may affect the robustness of the findings.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the comparison of their method with existing techniques such as test-time adaptation, online domain adaptation, and PZDA in the experiments. Additionally, clarifying the effectiveness of the model in adapting to unseen classes, even when accuracy does not improve, is essential. The authors should also enhance the manuscript's structure, particularly the introduction and related work sections, to provide a clearer rationale for the proposed problem setting. Furthermore, we suggest unifying the evaluation framework for different pre-trained models to enhance the consistency of results. Finally, consider renaming "holistic transfer" to a more specific term that accurately reflects the nature of the task, such as "partial target data setting," and further articulate the practical implications and potential applications of HT to strengthen its perceived value in the field.