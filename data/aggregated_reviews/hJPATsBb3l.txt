ID: hJPATsBb3l
Title: M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 8, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents M3Exam, a benchmark for evaluating large language models (LLMs) that incorporates multilingual, multimodal, and multilevel aspects. It consists of 12,317 questions in nine languages, including 2,816 questions requiring image comprehension. The authors benchmark several state-of-the-art LLMs, revealing significant limitations in their performance, particularly with low-resource languages and complex multimodal inputs. The findings suggest that LLMs do not consistently outperform on easier questions, contrasting with human learning patterns. The authors propose a focus on zero-shot and few-shot inference to assess the models' understanding of world knowledge and reasoning abilities. They acknowledge the potential for future research comparing translated versus organically sourced questions and the benefits of supervised fine-tuning, although preliminary experiments did not yield significant improvements. The authors clarify their evaluation logic and the representative nature of the multiple-choice questions (MCQs) included in their dataset.

### Strengths and Weaknesses
**Strengths:**
1. The benchmark addresses a crucial research question regarding the evaluation of LLMs.
2. The data collection process is well-documented, and the dataset is open source.
3. The analysis of LLMs provides valuable insights into their reasoning and understanding capabilities.
4. The comprehensive approach to evaluating LLMs and thoughtful consideration of future research directions are notable strengths.

**Weaknesses:**
1. Human performance on the benchmark is not provided for direct comparison, limiting the evaluation's robustness.
2. The evaluation relies solely on accuracy as a metric, which does not adequately capture model behaviors, especially in multimodal contexts.
3. The range of LLMs evaluated is narrow, and the highest question difficulty only extends to high school level, which may limit the benchmark's challenge.
4. There is a lack of direct comparisons with human performance and limited analysis dimensions beyond accuracy, which could enhance the understanding of model capabilities.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by including human performance data for direct comparison to enhance evaluation accuracy. Additionally, consider incorporating a wider variety of LLMs, including instruction-tuned models like mT0, to broaden the analysis. It would be beneficial to employ more granular evaluation metrics beyond accuracy to better understand model performance on multimodal questions. Furthermore, we suggest providing a deeper analysis of experimental results, particularly regarding the performance discrepancies observed across different educational levels and subjects. We also recommend utilizing the rich meta-information available in the M3Exam dataset to incorporate additional dimensions beyond accuracy. Considering the hiring of human annotators for a more equitable comparison with model performance would be advantageous. Lastly, we encourage the authors to explore the effectiveness of fine-tuning on larger models and relevant datasets in future work, as well as to clarify their evaluation logic further to ensure transparency. Addressing potential copyright concerns related to the use of exam questions is essential for ethical compliance.