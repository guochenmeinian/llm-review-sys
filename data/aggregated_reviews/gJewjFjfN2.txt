ID: gJewjFjfN2
Title: Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 5, 7, 5, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a federated long-tailed learning (Fed-LT) approach, introducing the Fed-GraB algorithm, which integrates a Self-adjusting Gradient Balancer (SGB) module to manage gradient re-balancing based on feedback from a Direct Prior Analyzer (DPA) module. The authors conduct experiments on several long-tailed datasets, demonstrating the method's effectiveness in addressing data heterogeneity and privacy concerns in federated learning.

### Strengths and Weaknesses
Strengths:
1. The investigation of federated long-tailed learning by combining global and local perspectives is intriguing.
2. The proposed Fed-GraB method aligns well with the paper's motivation and appears technically feasible.
3. Comprehensive experiments across multiple long-tailed benchmarks validate the proposed method's effectiveness.
4. The framework's potential impact on fairness and rare-event detection is significant, and it incurs minimal computational and memory costs.

Weaknesses:
1. The presentation lacks clarity, with several confusing elements that need addressing.
2. The uniqueness of the Fed-GraB model in tackling data heterogeneity and privacy concerns is insufficiently justified.
3. The paper assumes global class priors are available for re-balancing, which may not be realistic in practice.
4. The interplay between the SGB and DPA modules requires a more comprehensive description.
5. The claims regarding privacy issues and computational costs are not convincingly substantiated.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manuscript by addressing the confusing elements noted in the reviews. Additionally, the authors should provide a clearer justification of the uniqueness and effectiveness of the Fed-GraB model in addressing the challenges of federated learning. A more detailed theoretical analysis or insights regarding the DPA module's function and operation would enhance the paper's rigor. Furthermore, the authors should clarify how the proposed method would handle scenarios where local distributions are not long-tailed. Lastly, we suggest including a more comprehensive comparison with advanced FL algorithms that utilize gradient distortion techniques for privacy protection.