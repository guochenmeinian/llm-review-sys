ID: N6sXsHuWDE
Title: ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond Visual Common Sense
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the ROME dataset, designed to evaluate Vision-Language Models (VLMs) on their ability to interpret counter-intuitive content related to color, shape, material, size, and positional relations. The authors employ an algorithmic approach to generate counter-intuitive descriptions and utilize generative models like DALLE-2 to create corresponding images. The evaluation of various state-of-the-art vision-language models reveals significant limitations in their ability to handle counter-intuitive scenarios.

### Strengths and Weaknesses
Strengths:
- The ROME dataset serves as a valuable testbed for assessing VLMs' capabilities in counter-intuitive reasoning.
- The integration of ViComTe and image generative models is innovative and potentially extensible.
- The experiments conducted are comprehensive, covering multiple models and aspects of systematic probing.

Weaknesses:
- The inter-annotator agreements on human annotations are not reported, raising concerns about the reliability of the dataset.
- The methodology for retaining generated samples lacks clarity, particularly regarding the voting process used.
- The assessment focuses solely on multimodal LLMs, which may not fully represent the capabilities of general VLMs.
- The dataset is relatively small (1k images) and may overlap with larger datasets like VQA2.0 or GQA.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the annotation process by reporting inter-annotator agreements and specifying the voting method used for sample retention. Additionally, consider exploring alternative prompt designs for both commonsense and counter-intuitive questions to enhance the evaluation. It would be beneficial to include a comparative analysis of model performance in typical environments to better contextualize the impact of counter-intuitive content. Lastly, we suggest expanding the dataset size or demonstrating how it uniquely contributes to the field beyond existing larger datasets.