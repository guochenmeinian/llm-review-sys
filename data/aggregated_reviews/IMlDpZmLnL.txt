ID: IMlDpZmLnL
Title: A Comprehensive Analysis on the Learning Curve in Kernel Ridge Regression
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 6, 7, 7, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive study of the learning curves of kernel ridge regression (KRR) under minimal assumptions, analyzing key properties of the kernel such as spectral eigen-decay and smoothness. The authors investigate the validity of the Gaussian Equivalent Property (GEP) and derive new bounds across various settings. The main result indicates that while error rates derived under Gaussian design are correct in the strong regularization regime, they can be faster in the weak regularization regime.

### Strengths and Weaknesses
Strengths:  
- The authors provide a thorough analysis of learning curves in diverse scenarios, enhancing understanding of KRR behavior.  
- Improved bounds for bias under weak ridge assumptions are presented, validating the GEP.  
- The manuscript is well-written, with a clear context and smooth reading experience, making it significant for the theoretical community.

Weaknesses:  
- The paper primarily combines existing technical results, which may limit its originality.  
- The presentation style is challenging, lacking explicit theorems for verification, which may confuse readers.  
- Some results are only upper bounds, and the assumptions used in comparisons with existing literature are unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity and readability of the presentation by explicitly stating their results and assumptions, possibly using a formal structure such as "Theorem: Under [specific assumptions], our results can be summarized in the following table..." Additionally, we suggest providing a discussion on the intuition behind the (GF) assumption and clarifying how it differs from previous concentration assumptions. It would be beneficial to include more general kernels in numerical studies and to define terms like "over-parameterized" and "under-parameterized" clearly. Finally, addressing the missing lower bounds for bias and variance under generic features would strengthen the demonstration of the GEP.