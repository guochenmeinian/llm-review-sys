ID: B5vQ7IQW7d
Title: Model Sensitivity Aware Continual Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 6, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to continual learning (CL) that addresses the trade-off between retaining previously acquired knowledge and excelling in new task performance. The authors propose minimizing model parameter sensitivity to updates, thereby mitigating catastrophic forgetting and overfitting. This is achieved through a min-max optimization framework that operates within a parameter distribution neighborhood, utilizing a Gaussian model for parameter distributions. The method is compatible with existing CL methodologies and shows substantial improvements in both knowledge retention and new task performance, supported by theoretical and empirical evidence. Additionally, the authors clarify that the inner maximization is treated as a sensitivity penalty, while the outer minimization optimizes the mean of the parameter distribution. They discuss the relationship between their method and Sharpness-Aware Minimization (SAM), highlighting differences in deterministic versus probabilistic strategies and parameter-specific updates. The authors also propose formulations that incorporate concerns regarding data imbalance and absence in the generalization bounds.

### Strengths and Weaknesses
Strengths:
- The originality of reducing model parameter sensitivity to enhance both retention and new task performance is commendable.
- The methodological rigor and comprehensive theoretical analysis are well-executed.
- The paper is well-structured, with clear explanations of the proposed method and its advantages.
- The ability to improve CL models' performance without sacrificing old knowledge retention is a significant advancement.
- The authors provide a clear distinction between their method and existing techniques like SAM, emphasizing the advantages of a probabilistic approach.
- The introduction of the novel concept of "Model Sensitivity" is an original contribution to the field.

Weaknesses:
- The computational complexity, particularly in calculating the Fisher Information Matrix, may hinder large-scale applications.
- The method's effectiveness in online CL scenarios is not explored.
- Hyperparameter sensitivity, especially regarding the learning rate Î·, necessitates careful tuning, potentially limiting usability across different settings.
- The datasets used are outdated, and more recent datasets could enhance the relevance of the findings.
- The theoretical framework remains unconvincing to some reviewers, particularly regarding the handling of data imbalance and absence.
- The lack of references to existing literature in the introduction may weaken the contextual grounding of the proposed methodology.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational overhead introduced by the proposed method, particularly in large-scale datasets or real-time applications. Additionally, exploring strategies for optimizing hyperparameters in a more automated fashion would enhance the method's usability across various datasets and tasks. The authors should also consider extending their approach to online continual learning scenarios and address the anticipated challenges. Furthermore, we suggest that the authors include the ResNet-32 results for continual learning in the revised manuscript to enhance empirical validation. Finally, incorporating more recent datasets into the evaluation would strengthen the paper's relevance and applicability, and refining the generalization bounds to encompass both data imbalance and absence simultaneously would ensure that the original theorem is specific and robust.