ID: FCfY1wYkn9
Title: MU-Bench: A Multitask Multimodal Benchmark for Machine Unlearning
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 8, 6, 9
Original Confidences: 3, 3, 3

Aggregated Review:
### Key Points
This paper presents MU-Bench, a comprehensive machine unlearning benchmark that evaluates various models, modalities, and tasks. The authors conduct a detailed analysis of different machine unlearning methods, providing extensive experimental results that facilitate fair comparisons among these algorithms. The paper addresses significant questions in the field, such as the scaling law of machine unlearning and the effects of curriculum learning, offering new insights into the advantages and disadvantages of the methods evaluated.

### Strengths and Weaknesses
Strengths:  
- The paper provides comprehensive and detailed experimental results, contributing valuable insights to the community.  
- The benchmark includes a diverse range of tasks and data modalities, establishing a standardized framework for evaluation.  
- The introduction is well-structured, and the summary of results is particularly strong.  

Weaknesses:  
- Many models used are not the most recent, raising questions about the inclusion of newer open-sourced large language models.  
- Important discussions on scaling laws and curriculum learning lack references to relevant figures, which could enhance clarity.  
- The selective inclusion of existing machine unlearning methods may limit the benchmark's ability to capture the full landscape of current algorithms.  
- The taxonomy of techniques is somewhat difficult to follow for those unfamiliar with the methods.  
- Error bars or standard deviations are not plotted in the results, and the main results figures are located in the appendix, complicating interpretation.

### Suggestions for Improvement
We recommend that the authors improve the benchmark by including the newest open-sourced large language models to enhance its relevance. Additionally, linking discussions on scaling laws and curriculum learning to the relevant figures would clarify the analysis. To address the selective inclusion of methods, consider expanding the range of unlearning algorithms evaluated. We suggest simplifying the taxonomy of techniques by providing a detailed walkthrough of one method as an example. Furthermore, we recommend plotting error bars or standard deviations in the results and relocating the main figures from the appendix to the experiment section for easier interpretation. Lastly, if possible, provide insights or hypotheses regarding the effects of Negrad on bias and the scaling limitations of SCRUB, as well as exploring the implications of low scores on D_f.