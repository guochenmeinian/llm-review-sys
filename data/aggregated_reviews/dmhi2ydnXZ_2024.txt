ID: dmhi2ydnXZ
Title: Scalable DBSCAN with Random Projections
Conference: NeurIPS
Year: 2024
Number of Reviews: 19
Original Ratings: 4, 5, 5, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a significant advancement in density-based clustering algorithms through the introduction of sDBSCAN, which improves scalability and speed, making it suitable for large datasets. The authors propose a scalable DBSCAN algorithm that utilizes random projections to approximate the ε-neighborhood, achieving substantial speed improvements over conventional DBSCAN algorithms. The theoretical foundations and empirical results support the algorithm's effectiveness, demonstrating comparable performance at reduced computational costs. Additionally, the authors analyze the performance of sDBSCAN compared to other sampling DBSCAN variants, using the correlated coefficient (CC) as a new measurement alongside normalized mutual information (NMI) and adjusted mutual information (AMI). The results indicate that sDBSCAN outperforms other methods across various datasets, and the authors clarify the asymptotic theoretical analysis of Lemma 1, emphasizing the weak limit and the conditions under which it holds.

### Strengths and Weaknesses
Strengths:
1. sDBSCAN significantly enhances the scalability of density-based clustering, efficiently handling million-point datasets.
2. The algorithm effectively manages high-dimensional data by utilizing cosine distance and random projections.
3. The extension of sDBSCAN to other distance metrics (L2, L1, χ², and Jensen-Shannon) via random kernel features broadens its applicability.
4. The authors provide updated results showing the superiority of sDBSCAN over other methods.
5. The theoretical analysis of Lemma 1 is clarified, enhancing the understanding of its implications.

Weaknesses:
1. The algorithm's performance is sensitive to parameter selection, yet this sensitivity is not thoroughly explored.
2. Implementation complexity due to random projections and kernel features may hinder practical application.
3. The performance of the algorithm across different distance metrics is not comprehensively evaluated.
4. The paper lacks a broader comparative analysis with various clustering algorithms beyond DBSCAN variants.
5. The organization of the paper is problematic, particularly with the description of sOPTICS relegated to supplementary material.
6. The notation in Lemma 1 is currently imprecise, particularly regarding the use of the '$\sim$' notation in limits.
7. The formulation of Lemma 1 could be improved by explicitly defining $r_*$ as the closest vector to $q$ among $D$ random vectors.

### Suggestions for Improvement
We recommend that the authors improve the exploration of the algorithm's parameter sensitivity and provide guidelines for selecting parameters such as the number of random vectors and the radius for core points. A more detailed analysis of memory usage and the effectiveness of sDBSCAN when extended to L2, L1, χ², and Jensen-Shannon distances should be included. Additionally, we suggest incorporating a broader comparative analysis with a wider range of clustering algorithms and ensuring that the description of sOPTICS is integrated into the main text for clarity. We also recommend improving the formulation of Lemma 1 by defining $r_*$ clearly and correcting the notation to avoid ambiguity in the limit notation. Furthermore, we suggest reporting both AMI and CC for the Mnist, Pamap, and Mnist8m datasets, while retaining NMI for comparisons with other papers. Lastly, addressing the clarity of language and organization throughout the paper will enhance its readability.