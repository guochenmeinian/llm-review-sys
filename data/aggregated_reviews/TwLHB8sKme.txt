ID: TwLHB8sKme
Title: Tools for Verifying Neural Models' Training Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 5, 6, 5, -1, -1, -1, -1
Original Confidences: 2, 2, 3, 2, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a protocol called Proof-of-Training-Data (PoTD), designed for verifying that a model trainer submits accurate data and learned weights, with a verifier checking the correctness of these weights. The authors argue that PoTD addresses limitations in existing Proof-of-Learning (PoL) methods by focusing on training set attacks. The paper provides a formal definition of the PoTD protocol and outlines necessary conditions for its practical implementation. However, it lacks a clear motivation for the need for PoTD and its advantages over PoL.

### Strengths and Weaknesses
Strengths:  
- The motivation behind the PoTD protocol is interesting, and the proposed heuristics, particularly the memorization-based tests, are appealing and can efficiently detect spoofed checkpoints.  
- The paper is structured adequately, and solid experiments demonstrate the effectiveness of the proposed protocol against known attacks from the PoL literature.  

Weaknesses:  
- The paper does not sufficiently explain the motivation for PoTD or its impact, making it unclear why PoTD is necessary compared to existing PoL methods.  
- The only attack that PoL cannot handle but PoTD can is a data subtraction attack, yet the significance of addressing this attack is not adequately justified.  
- Experimental results lack comparisons with baseline methods, limiting the demonstration of the proposed method's effectiveness.  
- The presentation is fragmented and difficult to follow, with unclear relationships between sections and definitions that are not referenced later in the text.  

### Suggestions for Improvement
We recommend that the authors improve the introduction by clearly articulating the motivation for PoTD and its advantages over PoL. Additionally, the authors should discuss the limitations of existing PoL methods and how PoTD addresses these issues. We suggest including a more comprehensive comparison of experimental results with baseline methods to validate the effectiveness of PoTD. Furthermore, the authors should enhance the clarity of the paper's presentation by ensuring that all definitions are referenced appropriately and that the relationships between sections are clearly articulated. Lastly, we encourage the authors to simplify the memorization heuristic introduced in Section 3.2 to improve comprehensibility.