ID: BEFiYM5Vtx
Title: Multi-Task Knowledge Distillation with Embedding Constraints for Scholarly Keyphrase Boundary Classification
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for keyphrase boundary classification in scholarly documents, addressing the challenge of limited labeled data by proposing multi-task knowledge distillation with embedding constraints. The authors argue that training a student model to imitate multiple teacher models enhances performance on three datasets of scientific documents. The evaluation demonstrates that the proposed approach achieves superior results compared to existing baselines.

### Strengths and Weaknesses
Strengths:  
- The paper is well-written and provides sufficient background on the problem of limited training data in keyphrase boundary classification, which is significant for applications like indexing and searching.
- The approach effectively utilizes a basic cosine adjustment to improve performance, and the comprehensive experimentation opens up new research avenues regarding auxiliary tasks and dataset relationships.

Weaknesses:  
- The novelty is limited, as a similar teacher-student model approach has been previously proposed, and there is a lack of comparison against other such models.
- The rationale for multi-task knowledge distillation needs empirical demonstration, particularly through an ablation study to justify its necessity over single-task learning.

### Suggestions for Improvement
We recommend that the authors improve the historical perspective by acknowledging prior work, such as the QasemiZadeh and Schumann 2014 paper. Additionally, the authors should explore alternatives to softmax and clarify the rationale for using cross-entropy loss combined with cosine values. We suggest simplifying the language in lines 006-012 and introducing numerical results in line 021 to enhance reader engagement. Furthermore, the authors should provide quantitative results to support the benefits of the cosine embedding constraint and consider including references to related works on uncertainty mitigation and multi-objective learning. Lastly, we recommend a more thorough comparison with alternative MTL + distillation methods to validate the effectiveness of their approach.