ID: G5lMFOtFHa
Title: Where Do Large Learning Rates Lead Us?
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical study on the effects of large initial learning rates (LRs) in neural network training, focusing on two main questions: how large an initial LR is optimal and the differences between models trained with varying LRs. The authors identify a narrow range of initial LRs slightly above the convergence threshold that leads to optimal results after fine-tuning. This range allows optimization to locate high-quality minima and results in sparse, relevant features for the task.

### Strengths and Weaknesses
Strengths:  
1. The authors conduct a detailed empirical analysis in a controlled setting, providing novel insights into the geometry of the loss landscape in neural network training.  
2. The study's findings contribute to understanding feature learning, revealing that initial LRs lead to a sparse set of learned features.  
3. The paper effectively captures practical considerations of LR scheduling through its pretraining-finetuning setup, identifying nuanced training subregimes with distinct impacts on model generalization.

Weaknesses:  
1. The experiments are limited to specific datasets (CIFAR or synthetic) and architectures (ResNet), which may hinder generalizability to more complex scenarios.  
2. The conclusion regarding the optimal range of initial LRs is challenging to apply in practice, as determining the convergence threshold is problem-specific and difficult.  
3. Some results echo prior work, and the practical application of the identified training regimes remains unclear, particularly regarding how to choose the best LR.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by conducting experiments on a wider variety of datasets and neural network architectures, such as VGG and ImageNet. Additionally, we suggest that the authors clarify how practitioners can determine the convergence threshold without traversing all possible LRs. It would also be beneficial to discuss how different training regimes can be identified from the training loss alone, as access to oracle test accuracy is often impractical.