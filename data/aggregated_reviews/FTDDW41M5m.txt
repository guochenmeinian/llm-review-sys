ID: FTDDW41M5m
Title: Robust Multi-fidelity Bayesian Optimization with Deep Kernel and Partition
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 5, 6
Original Confidences: 3, 3

Aggregated Review:
### Key Points
This paper presents a new method for multi-fidelity Bayesian optimization that utilizes a hierarchical deep kernel and random exploration of a search space constrained by UCB, LCB, and expected excess risk. While the analysis of simple regret is included, the implications are not thoroughly discussed. The experimental results appear promising, but it is unclear how much performance improvement stems from the deep kernel versus the new excess risk term. The authors propose a method that explicitly accounts for misspecification at each fidelity level, which is an original contribution to Bayesian optimization.

### Strengths and Weaknesses
Strengths:
- Generally well written with a good introduction to the problem.
- Promising experimental results.
- Strong underlying idea and theoretical backing for the approach.
- The method is a reasonable extension of recent theoretical results.

Weaknesses:
- The term "Partition" is not explained.
- The cost-aware nature of the algorithm is not clarified.
- The impact of the hierarchical kernel compared to the new search scheme is not discussed.
- Model architecture is insufficiently explained.
- Analysis and discussion of the method are lacking, with limited information on baselines and relevant details relegated to the appendix.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the term "Partition" and provide a detailed explanation of the cost-aware nature of the algorithm. Additionally, the authors should discuss the impact of the hierarchical kernel versus the new search scheme more thoroughly. We suggest enhancing the explanation of the model architecture to aid understanding. Furthermore, we encourage the authors to include deeper experimental evidence and a more transparent link to the algorithm RFBO in the main text. Lastly, addressing the misprints and presentation issues, such as defining k_{t - 1} and correcting convergence rates, will enhance the overall quality of the paper.