ID: 5tOVh81aze
Title: Language models scale reliably with over-training and on downstream tasks
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 5, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into scaling laws for neural language models, particularly focusing on the "Chinchilla over-trained" regime. The authors propose a scaling law that relates pre-training compute and over-training to validation loss, demonstrating its predictive capability through empirical experiments on models of varying sizes. They also explore the relationship between perplexity and downstream benchmark error, finding that average downstream performance is predictable.

### Strengths and Weaknesses
Strengths:  
The paper addresses a highly relevant topic, as many recent models fall within the "Chinchilla over-trained" regime. It introduces a well-motivated scaling law that is empirically validated, contributing valuable insights for researchers and practitioners in the field. The clarity of presentation and detail in the experimental setup are also commendable.

Weaknesses:  
The primary weakness is the lack of novelty, as the main findings regarding predictability in overtraining and average downstream performance have been previously discussed in other works, such as Owen 2024. Additionally, the authors do not compare their proposed scaling law with standard scaling laws from Kaplan et al., raising concerns about the robustness of their claims. The choice of models for fitting the scaling laws is also unclear, and there are potential issues with data leakage in downstream tasks.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their work by providing a more thorough comparison with standard scaling laws, particularly those from Kaplan et al. This would clarify the value of their proposed reparameterization. Additionally, the authors should address the selection criteria for models used in their analysis to ensure transparency. Expanding on the implications of the observation that individual task performance is less predictable would enhance the paper's depth. Lastly, we suggest that the authors investigate potential data leakage issues in their downstream tasks and clarify their approach to model selection in the experimental setup.