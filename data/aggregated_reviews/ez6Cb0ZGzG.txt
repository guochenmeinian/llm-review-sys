ID: ez6Cb0ZGzG
Title: Continual Learning for Instruction Following from Realtime Feedback
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 7, 7, 8, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 3, 3, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for training a continually learning, instruction-following agent using real-time user feedback in the CerealBAR environment. The authors formulate the learning scenario as a contextual bandit problem, addressing challenges such as irregularly timed rewards and credit assignment through heuristics. The effectiveness of the method is demonstrated through extensive experiments, showing significant improvements in instruction execution accuracy over time. Additionally, the authors propose a novel approach to adaptive systems that improve through user interaction, acknowledging the inherent challenges of conducting experiments in this domain, particularly when utilizing platforms like MTurk. They emphasize the importance of their findings as a foundation for future research, despite the limitations of their current results.

### Strengths and Weaknesses
Strengths:
- The paper is straightforward and relevant, with thorough experiments that convincingly highlight the continual learning capabilities of the agent.
- The evaluation of the agent's performance through human interactions is comprehensive, providing valuable insights into the learning process.
- The writing is well-organized and clear, effectively communicating the framework and results.
- The authors tackle a challenging and relevant problem in adaptive systems, demonstrating commitment to rigorous research despite the difficulties involved.

Weaknesses:
- The assumption that feedback is closely matched with actions is restrictive, and transitions without rewards are underutilized, wasting resources.
- The framework's design lacks clarity, particularly regarding the policy model's transformation of human instructions into actions.
- The reliance on binary feedback limits the representation of human satisfaction and may not generalize well to more complex tasks.
- The need for more rigorous studies is emphasized, particularly regarding sample efficiency analysis, which could prevent costly mistakes for future researchers.
- The results presented are described as rudimentary, limiting the authority of the findings.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the framework design, particularly in detailing the policy model and its transformation processes. Additionally, consider exploring more complex feedback mechanisms beyond binary signals to enhance generalizability. We encourage the authors to improve the rigor of their studies, particularly in the sample efficiency analysis, to ensure that future researchers do not incur significant costs based on their findings. Conducting experiments with alternative off-policy learning algorithms could strengthen the robustness of the framework. Finally, we suggest providing a more detailed analysis of the reward mapping process, as well as comparing against baseline models to better illustrate the effectiveness of the proposed approach.