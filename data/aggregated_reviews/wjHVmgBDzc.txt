ID: wjHVmgBDzc
Title: $\texttt{ConflictBank}$: A Benchmark for Evaluating the Influence of Knowledge Conflicts in LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 5, 7, 4, 8, -1, -1, -1
Original Confidences: 4, 3, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark called ConflictBank to study Large Language Model (LLM) behavior under three types of knowledge conflicts: conflicts in retrieved knowledge, conflicts within models' internal knowledge, and multiple sources of conflict. The authors propose to construct a synthetic dataset to understand LLM behavior in knowledge conflicts, revealing insights through experiments on various settings. The dataset is large, constructed from Wikidata, and aims to evaluate how LLMs handle contradictory information.

### Strengths and Weaknesses
Strengths:
- The dataset is large-scale and rigorously constructed, ensuring high data quality and reliability.
- The paper provides valuable insights into LLM behavior under diverse conflict scenarios, contributing to an important research question.

Weaknesses:
- There is a lack of clarity regarding the definition of knowledge conflicts, as some examples do not represent true conflicts.
- The methodology requires further refinement, particularly in exploring the exact relations between entities and the nature of the conflicts introduced.
- The validation process for the dataset is limited, relying on a small number of annotators for a limited sample size.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of knowledge conflicts, ensuring that examples accurately reflect true conflicts. Additionally, we suggest exploring strategies for mitigating knowledge conflicts, such as addressing misinformation, temporal, and semantic conflicts. Providing concrete examples of conflicts and LLM-generated texts in the main paper would enhance understanding. Furthermore, expanding the validation process to include a larger sample size and more annotators would significantly improve data quality assurance. Lastly, consider adapting the ConflictBank dataset to assess closed-source models like GPT-4 or Gemini.