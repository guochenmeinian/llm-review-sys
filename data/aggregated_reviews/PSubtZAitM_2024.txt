ID: PSubtZAitM
Title: Efficient Policy Evaluation Across Multiple Different Experimental Datasets
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 5, 6, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 1, 3, 3, 1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the evaluation of policies under distribution shifts between source and target sites, introducing identification criteria for policy effectiveness and developing doubly robust estimators that achieve fast convergence. The authors generalize their results to multiple source datasets and provide simulation results to demonstrate the method's effectiveness.

### Strengths and Weaknesses
Strengths:
1. The paper is well written, with clear setups and accurately stated assumptions and methodologies.
2. The proposed framework is general, accommodating both two-domain and multiple-domain scenarios.
3. Empirical results align well with theoretical expectations for both synthetic simulations and real-world datasets.

Weaknesses:
1. The paper is difficult to follow due to grammatical mistakes and confused notations, such as inconsistent use of symbols.
2. The empirical evaluations are limited, with non-synthetic experiments conducted only on the ACTG 175 clinical trial dataset, which could be enhanced by including additional datasets.
3. The assumptions regarding the number of studies and matching source populations may be overly optimistic and unrealistic in practical scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by addressing grammatical mistakes and unifying notations. Additionally, please provide detailed descriptions of the simulated and empirical experiments, possibly moving lengthy descriptions to an appendix. We suggest that the authors consider using D4RL benchmark datasets or OpenAI Gym environments for multi-stage evaluations and include competing methods in the experiments. Furthermore, it would be beneficial to discuss the implications of the results on real-world problems, particularly regarding the ACTG 175 dataset and the findings presented in Figure 3.c. Lastly, we encourage the authors to explore adaptive learning methods for identifying matching source populations and to address the handling of distributional shifts in their methodology.