ID: ewedHtUI5X
Title: ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ASSERT, a suite of methods designed to automatically generate test prompts for evaluating the robustness of large language models (LLMs) in the context of AI safety. ASSERT includes three methods: semantically aligned augmentation for generating semantically equivalent samples, targeted bootstrapping for synthesizing closely related but non-equivalent samples, and adversarial knowledge injection for creating adversarial examples. The authors empirically demonstrate that LLMs exhibit less robustness to semantically similar unsafe prompts compared to safe ones and are vulnerable to adversarial attacks. The evaluations encompass mainstream LLMs, yielding significant findings regarding their robustness.

### Strengths and Weaknesses
Strengths:
- The paper addresses a critical issue in AI safety and proposes a novel automated evaluation strategy.
- It provides empirical evidence of LLM vulnerabilities, contributing valuable insights to the field.
- The overall presentation is clear, and the experiments yield useful findings.

Weaknesses:
- The experimental design lacks thoroughness, particularly regarding the robustness of different models and the implications of findings for practitioners.
- The targeted bootstrapping method is not clearly explained, raising questions about its objectives and the validity of conclusions drawn from its results.
- The need for human validation of generated synthetic examples is not adequately discussed, potentially affecting the quality and reproducibility of the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the targeted bootstrapping method, specifically detailing the robustness issues it aims to uncover. Additionally, we suggest that the authors discuss the implications of their experimental findings for practitioners, providing guidance on mitigating the negative effects of instability in LLMs. Furthermore, it would be beneficial to elaborate on the human validation process for the generated synthetic examples to ensure quality and reproducibility.