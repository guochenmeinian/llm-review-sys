ID: Du5geIL4rM
Title: Training Machine Learning Models with Ising Machines
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 8, 9
Original Confidences: 3, 4

Aggregated Review:
### Key Points
This paper presents a modified Ising machine designed for natively solving sub-optimization problems within trust-region-based optimization. The authors provide theoretical guarantees on the convergence rate of the Ising machine and introduce the iTrust (Ising trust region optimization) scheme for training machine learning models. They conduct numerical experiments comparing iTrust with traditional first-order and second-order methods in the context of training quantum machine learning models, along with a preliminary analysis of resource consumption. The work is positioned as a new direction for exploring Ising machines as optimizers in machine learning.

### Strengths and Weaknesses
Strengths:
1. The paper offers a thorough theoretical analysis of the convergence rate of the proposed scheme, supported by numerical experiments.
2. The topic and proposed ideas align well with the workshop's scope.
3. Despite the omission of detailed proofs due to length constraints, the results appear sound.

Weaknesses:
1. The writing requires polishing, as there are numerous typos, including line 24 (Ising), line 74 (Polyak-Lojasiewicz), line 161 (repeated), and line 196 (trust-region).
2. The authors should discuss the implementation of the gradient and Hessian on real Ising machines.
3. The authors mention that traditional second-order methods require $O(n^3)$ time for matrix manipulation but should explicitly state how the complexity of using Ising machines scales with $n$. If it is constant, this should be clearly articulated.
4. The execution overhead in gradient and Hessian estimation is specific to quantum machine learning. The authors may want to compare training classical neural networks, where the advantages of iTrust may be more evident.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their writing by addressing the identified typos. Additionally, we suggest that the authors elaborate on how the gradient and Hessian can be implemented on real Ising machines. It would also be beneficial for the authors to explicitly state the scaling of the complexity of Ising machines with respect to $n$. Finally, we encourage the authors to include comparisons with classical neural networks to highlight the advantages of iTrust more effectively.