ID: uh5euNmL7t
Title: Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the cross-lingual interpretability of token embeddings in multilingual language models (LMs), focusing on the distribution of writing systems and the characteristics of different model families, particularly XLM-R and mT5. The authors find that XLM-R encodes languages into isolated clusters, while mT5 aggregates similar words regardless of language. The study reveals that the XLM family achieves higher accuracy in Unicode prediction and shows greater language unification among token neighbors.

### Strengths and Weaknesses
Strengths:  
- The paper is well-organized, well-written, and provides interesting insights into the differences in encoding styles among multilingual LMs.  
- It includes both large-scale experiments and detailed analyses, addressing specific questions effectively within its limited scope.  

Weaknesses:  
- The findings are somewhat weak and descriptive, lacking deeper explanations of the observed characteristics.  
- There is a limited scope, with reviewers noting the absence of concrete takeaways regarding the implications of the results for model selection or leveraging insights from embedding similarities.  

### Suggestions for Improvement
We recommend that the authors improve the depth of their analysis by explaining the implications of their findings on model selection and usage. Specifically, they should clarify how understanding the differences in encoding styles can inform decisions about which model to choose or how to leverage them effectively. Additionally, consider incorporating comparisons of models based on their architecture types, such as including multilingual BERT. Lastly, we suggest relocating some figures to the main text for better accessibility and ensuring that references adhere to the official ACL anthology bibkeys.