ID: dw6xO1Nbk5
Title: Generalization in Neural Operator: Irregular Domains, Orthogonal Basis, and Super-Resolution
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 4, 7, 5, 5, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 3, 2, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of neural operators (NOs), enhancing the understanding of generalization bounds, discretization errors, and super-resolution capabilities. The authors propose a more precise convergence rate for super-resolution errors and introduce a novel formulation of NOs in an infinite-dimensional space, which facilitates the application of advanced learning theory tools. They validate their theoretical insights through numerical experiments and acknowledge the gap between theory and practice, particularly regarding the model parameter matrix norm and its implications for generalization performance. The work includes upper bounds for the excess risk of NOs, an extension to irregular domains, and an examination of super-resolution errors.

### Strengths and Weaknesses
Strengths:
1. The paper provides a tighter and more general generalization bound compared to prior works.
2. Significant improvements in understanding discretization and super-resolution errors in NOs are presented.
3. The proposal section is clear and well-structured, with Theorems 3.2 and 3.3 being properly defined.
4. The introduction of neural operators in an infinite-dimensional space represents a significant innovation, facilitating the use of established learning theory tools.
5. The paper effectively clarifies the distinctions between their work and existing literature, particularly the JMLR paper.
6. The insights gained from the theoretical analysis are validated by numerical experiments.

Weaknesses:
1. The techniques used are standard, and the authors should emphasize their novelties more clearly.
2. Some theorems, particularly Theorem 3.1, lack clarity, especially regarding the dependence on the infinite-dimensional covering number and network parameters.
3. The discussion on applying NOs to arbitrary irregular domains is insufficient, with only a brief example provided.
4. The overall presentation could be improved, as some sections are vague, particularly in the experimental results.
5. Limitations of the work are not adequately addressed.
6. There may be a gap between theoretical predictions and practical applications, particularly concerning the model parameter matrix norm.
7. The assumption that similar training procedures yield comparable generalization performance may not hold in all scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity of theorems, particularly Theorem 3.1, by discussing how to estimate and bound the covering number. Additionally, the authors should analyze the relationship between training loss and the upper bounds presented, providing insights into how to bound the training loss. More detailed discussions on the application of NOs to arbitrary irregular domains should be included, along with a clearer explanation of the integration schemes and basis selections used in experiments. We also suggest that the authors improve clarity regarding the implications of the model parameter matrix norm on generalization performance in practical settings. Providing more concrete examples or case studies demonstrating the application of their theoretical insights in real-world scenarios could enhance the paper's practical relevance. Finally, we suggest that the authors explicitly address the limitations of their work in the revised manuscript.