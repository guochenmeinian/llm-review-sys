ID: Luc1bZLeMY
Title: CHAMMI: A benchmark for channel-adaptive models in microscopy imaging
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 9, 7, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a benchmark dataset and evaluation framework for channel-adaptive microscopy models, addressing the challenge of varying channel configurations in microscopy images. The authors curate three publicly available datasets and propose nine scientifically meaningful tasks for model evaluation, focusing on the evaluation of various strategies and their performance on validation and generalization tasks. They acknowledge the limitations of their dataset's scale and diversity, which may impact generalization performance on out-of-distribution (OOD) evaluations. The work highlights the need for channel-invariant models, aiming to improve generalization and reusability across different microscopy settings. Additionally, the authors clarify the evaluation setup for the 1-NN approach and relate their work to existing literature on generalist models in microscopy.

### Strengths and Weaknesses
Strengths:
- The paper tackles a crucial and underexplored problem in microscopy imaging, providing a unique benchmark dataset and tasks.
- The datasets are well-curated, ensuring consistency and compatibility, with a systematic approach to evaluating channel-adaptive models.
- The authors have made significant improvements in the presentation of experimental results, enhancing clarity and interpretability.
- The inclusion of various augmentation techniques, such as thin-plate-spline transformations, demonstrates a proactive approach to addressing dataset limitations.
- The addition of transfer learning experiments shows the practical applicability of the proposed methods.

Weaknesses:
- The evaluation setup lacks clarity, particularly regarding the 1-NN approach and generalization to unseen labels, raising concerns about batch effects and the use of test data points in the training set.
- The dataset's scale and diversity are limited, potentially affecting the model's generalization performance on out-of-domain evaluations and hindering the exploration of model capacity.
- Some experimental results are difficult to interpret due to complex visualizations and small font sizes.
- The study does not sufficiently explore self-supervised learning or the implications of varying channel sizes in practical applications, nor does it address the generalization of models to other microscopy modalities beyond single-cell imaging.

### Suggestions for Improvement
We recommend that the authors improve clarity in the evaluation setup by explicitly stating when test data points are included in the training set for the 1-NN approach and addressing concerns about batch effects. Additionally, we suggest simplifying visualizations and using larger font sizes to enhance the interpretability of experimental results. Expanding the dataset to include more diverse microscopy modalities and larger scales would also be beneficial for enhancing generalization capabilities. Furthermore, we encourage the authors to explore self-supervised learning techniques further, as they may offer significant contributions to the performance of channel-adaptive models. Finally, providing more background on the relationship between their work and existing generalist models in microscopy, such as CytoImageNet and Microsnoop, would help contextualize their contributions. Including a sample notebook for evaluating pretrained models would also be advantageous for users.