ID: OWmu3QOa0O
Title: Sparse maximal update parameterization: A holistic approach to sparse training dynamics
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 6, 5, 3, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Sparse Maximal Update Parameterization (SµPar), a method aimed at stabilizing training dynamics in sparse neural networks by ensuring that activations, gradients, and weight updates are scale-invariant with respect to sparsity. The authors propose that SµPar allows for consistent optimal hyperparameters across varying model widths and sparsity levels, thereby reducing tuning costs. The empirical results indicate that SµPar outperforms standard parameterization (SP) and maximal update parameterization (µP) in maintaining stable optimal hyperparameters and achieving better loss metrics across different sparsity levels.

### Strengths and Weaknesses
Strengths:
- The proposed approach is theoretically sound and intuitive, with clear motivation and well-structured writing.
- SµPar demonstrates stable optimal learning rates across a wide range of sparsities, outperforming tuned SP and µP configurations with reduced tuning costs.
- The experimental results are well-supported and the paper is easy to follow.

Weaknesses:
- The importance of individual parameterizations within SµPar is not thoroughly studied, and the paper lacks an evaluation of its performance against other sparse weight initialization methods.
- The method is only evaluated with random unstructured sparsity patterns, which are not commonly used in practice.
- The theoretical contribution is perceived as a simple extension of existing methods without significant technical improvement, and the paper does not convincingly demonstrate SµPar as a holistic solution applicable to various sparse training algorithms.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of individual parameterizations within SµPar by conducting ablation studies. Additionally, including accuracy comparisons between SµPar and other sparse weight initialization methods would strengthen the findings. The authors should also explore the applicability of SµPar to structured sparsity patterns and dynamic sparse training methods, as this would enhance the generalizability of their claims. Finally, providing more comprehensive performance metrics beyond loss values and improving the paper's structure and clarity would benefit the overall presentation.