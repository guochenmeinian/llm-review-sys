ID: FdtdjQpAwJ
Title: Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 7, 7, 7, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Conditioned Constrained Policy Optimization (CCPO) framework, which includes two modules: Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and Conditioned Policy Inference (CPI) for encoding arbitrary constraint thresholds during policy optimization. The authors propose a versatile Q function that adapts to varying safety constraints without retraining, demonstrating its effectiveness through extensive experiments in robotic tasks.

### Strengths and Weaknesses
Strengths:
- The method exhibits training efficiency and zero-shot adaptation capability, allowing the trained policy to adjust to varying safety constraints during deployment.
- The theoretical analysis of the q-function estimation and constraint violations is robust.
- The extensive experiments validate the method's effectiveness, showing good safety performance in comparison to baselines.

Weaknesses:
- The writing lacks clarity, particularly in the pretraining phase, making it difficult to connect components of the method. An algorithm summary in the main text would enhance understanding.
- Experimental results show that while safety criteria are met, reward performance is often lower than that of baselines, raising concerns about the trade-off between safety and reward.
- The method's scalability with multiple constraints and its performance in higher-dimensional domains remain unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing, especially in the pretraining phase, by including an algorithm summary in the main text. Additionally, we suggest conducting more extensive experiments across a wider range of tasks to confirm the consistency of performance and to address the trade-off between safety and reward. It would also be beneficial to provide a comprehensive comparison with other safe RL methods beyond basic Lagrangian approaches. Furthermore, we encourage the authors to clarify the assumptions made regarding latent vectors and to elaborate on the challenges of extending CVPO to constraint-conditional scenarios. Lastly, we recommend providing the complete code in a more accessible format for better examination of the algorithm.