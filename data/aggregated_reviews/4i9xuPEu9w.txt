ID: 4i9xuPEu9w
Title: BECAUSE: Bilinear Causal Representation for Generalizable Offline Model-based Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 27
Original Ratings: 2, 7, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to address the objective mismatch problem in model-based offline reinforcement learning (MBRL) by utilizing causal discovery methods. The authors propose a framework that incorporates causal graph learning through bilinear Markov Decision Processes (MDPs), focusing on confounders affecting state dynamics and behavior policies. Additionally, the paper formulates world models within the context of MDPs, emphasizing the generality of MDPs for various tasks and introducing bilinear causal representation learning as a key contribution. The authors acknowledge concerns regarding the term 'world model' and express willingness to replace it with 'dynamics model' to mitigate confusion, while maintaining that this change does not affect their core contribution. The framework aims to enhance learning by re-weighting and uncertainty quantification, with empirical results demonstrating its effectiveness in causal discovery and policy learning.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and presents a clear problem statement.
2. The proposed methodology is innovative, particularly the formulation of a latent bilinear MDP for causal discovery.
3. The theoretical properties of the approach are well-studied, and the empirical results are satisfactory.
4. The writing is clear, making complex material accessible.
5. The paper provides a clear formulation of world models in MDPs, highlighting their applicability to real-world tasks.
6. The authors demonstrate openness to feedback by offering to modify terminology to enhance clarity.

Weaknesses:
1. The conditions in bilinear MDPs may be challenging to achieve in real-world scenarios.
2. Some comparisons in the experiments could be strengthened by including more state-of-the-art model-based offline reinforcement learning approaches.
3. The reliance on assumptions, such as the invariance of causal graphs, may limit the applicability of the method in real-world contexts.
4. The use of the term 'world model' has caused confusion among reviewers, indicating a potential lack of clarity in the paper's definitions.
5. Some reviewers feel that the historical information's role in MDPs is not adequately addressed, which may impact the understanding of the proposed model.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including comparisons against stronger baselines, such as those proposed by Sun et al. (2023). Additionally, providing real-world examples for the confounders $\mu_{\pi}$ and $\mu_{c}$ would enhance the paper's relevance. Clarifying the inherent difficulties in fully learning the causal process through bilinear MDP optimization, particularly regarding feature space entanglement and the optimization search space, would address significant concerns raised in the reviews. We also recommend that the authors improve the clarity of their model definitions to avoid ambiguity, particularly regarding the transition from 'world model' to 'dynamics model.' Furthermore, explicitly addressing the role of historical information in MDPs would strengthen their argument and enhance the understanding of their bilinear causal representation. Lastly, discussing the potential societal impacts of the proposed work would strengthen the paper's ethical considerations.