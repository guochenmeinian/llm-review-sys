ID: WH5blx5tZ1
Title: Large Scale Transfer Learning for Tabular Data  via Language Modeling
Conference: NeurIPS
Year: 2024
Number of Reviews: 26
Original Ratings: 7, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TABULA-8B, a specialized large language model for tabular data prediction, trained on a high-quality dataset (T4) comprising 1.5 million unique tables and over 800 million rows. The authors detail a novel packing and attention scheme for fine-tuning the Llama 3-8B model, achieving significant improvements in zero-shot and few-shot learning scenarios, outperforming existing models like XGBoost and TabPFN. The evaluation demonstrates a zero-shot accuracy exceeding random guessing by over 15 percentage points and superior performance in few-shot settings. The authors emphasize the importance of few-shot learning in domains with limited data, such as healthcare and finance, and provide a comprehensive evaluation of TABULA-8B against strong commercial LLMs, specifically Claude 3 Sonnet and Claude Instant, showing that TABULA-8B consistently outperforms these models across all evaluated shot values.

### Strengths and Weaknesses
Strengths:
- The T4 dataset provides a robust foundation for training and evaluation, enabling TABULA-8B to achieve high accuracy in zero-shot and few-shot learning.
- The methodology demonstrates significant performance improvements in low-data regimes compared to traditional models like XGBoost.
- The paper includes robustness and ablation studies that clarify the contributions of various components to model performance.
- The approach allows for direct inference on rows with missing values without preprocessing, enhancing its applicability.
- The experimental design is thorough, utilizing a large and representative benchmark suite, and the authors effectively address reviewer concerns, enhancing the clarity of their results.

Weaknesses:
- The paper overstates its novelty in applying transfer learning to tabular prediction tasks and neglects to discuss several critical prior studies.
- The evaluation lacks comparisons with existing cross-table transfer learning baselines, which would enhance the assessment of the proposed method's efficacy.
- The model does not provide confidence scores or probabilities for predictions, complicating its application in real-world scenarios.
- There is a potential data leakage issue, as the evaluation data may overlap with the training dataset.
- The paper lacks a novel architecture or optimization algorithm, which may limit its perceived novelty.
- Some comparisons to existing models, such as UniPredict, are constrained by the lack of open-source implementations, which could affect the robustness of the evaluation.
- The paper does not sufficiently demonstrate that treating numbers as text allows LLMs to handle complex numerical reasoning tasks, and the approach to regression tasks is deemed impractical due to high computational demands.

### Suggestions for Improvement
We recommend that the authors improve the discussion of related work by including references to critical prior studies on transfer learning in tabular prediction tasks. Additionally, we suggest conducting comparisons with existing cross-table transfer learning baselines to provide a more comprehensive evaluation of TABULA-8B's performance. The authors should also consider incorporating confidence scores or probabilities for predictions to enhance trustworthiness in real-world applications. Furthermore, we advise addressing potential data leakage concerns by ensuring that the evaluation dataset is distinct from the training dataset. Improving the demonstration of TABULA-8B's capabilities in handling complex numerical reasoning tasks, particularly on datasets that challenge LLMs, would also be beneficial. Lastly, including comparisons with recent works leveraging LLMs and selecting representative datasets for a fair evaluation will enhance the comprehensiveness of the paper's contributions.