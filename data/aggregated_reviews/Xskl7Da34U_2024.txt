ID: Xskl7Da34U
Title: MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 5, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a mixture of multimodal experts (MoME) approach aimed at reducing task interference in multimodal instruction tuning. The authors propose a mixture of vision experts (MoVE) and a mixture of language experts (MoLE), utilizing a dynamic routing module for visual features and a multi-adapter structure for language experts. The paper claims to show improvements over baselines through comprehensive experiments across various downstream tasks.

### Strengths and Weaknesses
Strengths:
- The paper addresses the important issue of task interference in multimodal settings, providing a clear analysis and motivation for the proposed MoME approach.
- The experiments are well-conducted, showcasing both quantitative and qualitative results that demonstrate the effectiveness of the proposed method.

Weaknesses:
1. The improvements shown in Table 3 lag behind methods without MoE, such as LLaVA, despite the proposed approach having significantly more parameters and pretraining.
2. The claim that language experts in the MoLE module specialize in distinct task domains is not supported by the visualizations, particularly in Fig. 5.
3. The evaluation lacks common and recent multimodal benchmarks like SEED, MME, and VQAv2, which are considered in other methods.
4. The design of MoE differs significantly between LLM and visual encoders; it is unclear if typical MoE was tested in a single visual encoder.
5. The use of deformable cross-attention lacks experimental support compared to simple cross-attention.
6. The paper states that load balancing did not help, but does not provide insights into this outcome or discuss potential training instabilities, which are critical in MoE training.
7. The novelty of applying MoE to vision encoders is limited, as similar explorations have been conducted in previous works.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including results from common multimodal benchmarks such as MME and VQAv2 to enhance the comprehensiveness of their experiments. Additionally, we suggest providing clearer explanations of the internal mechanisms behind the significant improvements observed with ADT and Router. It would also be beneficial to conduct experiments comparing deformable cross-attention with simple cross-attention to justify the design choice. Finally, we encourage the authors to discuss the reasons behind the lack of effectiveness of load balancing and any instabilities encountered during training, as these are crucial aspects of MoE methodologies.