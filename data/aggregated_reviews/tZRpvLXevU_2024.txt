ID: tZRpvLXevU
Title: Latent Representation Matters: Human-like Sketches in One-shot Drawing Tasks
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 4, 4, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the impact of various regularization techniques on the performance of Latent Diffusion Models (LDMs) in one-shot drawing tasks. The authors explore six regularization methods: KL divergence, vector quantization, classification, prototype-based, SimCLR, and Barlow Twins. They evaluate these methods using a framework that contrasts originality with recognizability and introduce a novel approach for generating feature importance maps. The findings indicate that LDMs utilizing prototype-based and Barlow Twins regularizations yield sketches that closely resemble human drawings, significantly narrowing the gap in performance.

### Strengths and Weaknesses
Strengths:
- The paper provides a thorough examination of six regularization techniques, offering valuable insights into their effectiveness for one-shot drawing tasks.
- It introduces a novel method for generating feature importance maps in LDMs, facilitating direct comparisons with human perceptual strategies.
- The interdisciplinary approach integrates computer science, cognitive science, and neuroscience, potentially enhancing understanding of human visual cognition.
- The paper is clearly written, with detailed information on experimental setup, hyperparameters, and code availability, promoting reproducibility.

Weaknesses:
- The analysis of generalizability from the simple datasets used to more complex creative processes is limited, particularly as the study primarily focuses on the QuickDraw-FS dataset with minimal exploration of the Omniglot dataset.
- The claim that the gap between human and machine performance is "almost closed" is overstated, as qualitative results reveal issues like blurriness and distortion in generated sketches.
- The paper lacks a detailed analysis of how different components of the regularizers contribute to overall performance and does not include a human evaluation of the generated samples.

### Suggestions for Improvement
We recommend that the authors improve the generalizability discussion by addressing how findings may apply to more complex drawing tasks beyond the analyzed datasets. Additionally, we suggest including a comparative analysis with DDPM used in previous studies to better understand the effectiveness of pushing denoising into latent space. Furthermore, a more systematic exploration of the contributions of individual regularizer components and a human evaluation of generated sketches would enhance the depth of the analysis. Lastly, clarifying the construction of the sketch codebook used for VQ-VAE would provide valuable insights into the methodology.