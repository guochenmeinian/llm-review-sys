ID: f5FDfChZRS
Title: LoCal: Logical and Causal Fact-Checking with LLM-Based Multi-Agents
Conference: ACM
Year: 2024
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the LoCal framework, a multi-agent fact-checking system utilizing LLM-based agents to address logical and causal issues in verifying claims. The framework decomposes claims into subtasks, which are processed by reasoning agents that retrieve evidence and perform evaluations. Two evaluating agents—one for logical assessment and another for counterfactual reasoning—determine the validity of the claims. The effectiveness of LoCal is demonstrated through experiments on two datasets, showing improvements over baseline models.

### Strengths and Weaknesses
Strengths:
- The introduction of counterfactual labels for evaluation is a novel approach to fact-checking.
- The experiments are well-designed, featuring a robust selection of datasets and baseline models.
- The framework is clearly presented, and the visuals effectively convey the methodology.

Weaknesses:
- The minor performance decrease when removing evaluation agents raises questions about their importance.
- The abstract and introduction lack clarity, making it difficult to grasp key concepts such as claim decomposition and evaluation types.
- The paper does not adequately explain how iterations differ when the input statement and agents remain unchanged.
- There is insufficient discussion on handling partially true information and the computational cost for real-time application.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the abstract and introduction by providing precise definitions and examples of key concepts. Additionally, the authors should elaborate on how the proposed method addresses the errors listed in Section 5.3 and provide a working example to illustrate the framework's functionality. Including hyper-parameter details would enhance reproducibility, and a clearer discussion on the framework's applicability to real-world fact-checking challenges, particularly in closed-book scenarios, would strengthen the manuscript. Finally, we suggest improving figure captions to be more self-sufficient and informative.