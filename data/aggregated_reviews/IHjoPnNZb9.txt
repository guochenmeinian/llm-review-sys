ID: IHjoPnNZb9
Title: Rethinking Decoders for Transformer-based Semantic Segmentation: A Compression Perspective
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel perspective by conceptualizing semantic segmentation as data compression, similar to PCA, and introduces DEPICT, white-box decoders that clarify the functions of learnable class embeddings, self-attention, and dot-product operations. The authors demonstrate that DEPICT achieves comparable or superior performance with fewer parameters on the ADE20K dataset compared to traditional methods. The work enhances the understanding of decoder mechanisms in semantic segmentation.

### Strengths and Weaknesses
Strengths:
1. The figures and tables are informative and support the findings effectively.
2. Experiments validate that the proposed methods reduce model parameters while maintaining adequate performance.
3. The paper includes thorough mathematical derivations of related theories.

Weaknesses:
1. Experimental validation is limited to a single dataset, ADE20K; broader testing across various datasets and a detailed analysis of FLOPs would strengthen evidence of the methods' efficiency and adaptability.
2. Some mathematical proofs are confusing or inadequately explained, particularly regarding the implications of transposing elements and the equivalence claims made.
3. Certain figures, such as Figure 2's subfigures (b) and (c), appear identical without sufficient explanation, raising questions about their purpose.
4. The claim that semantic segmentation can be viewed as PCA is overly broad and not specific to the novel contributions of this paper.

### Suggestions for Improvement
We recommend that the authors improve the experimental validation by including a variety of datasets, such as Cityscapes, and comparing DEPICT against prominent Transformer decoder-based methods like SegFormer, Segmenter, Mask2Former, and FeedFormer. Additionally, we suggest including GFLOPs in the comparisons, as they are crucial for evaluating efficiency in Transformer decoder structures. The authors should also refine the presentation of mathematical proofs to enhance clarity and rigor, ensuring that readers can effectively assess their accuracy. Lastly, addressing the potential misunderstanding of recent advances in segmentation will strengthen the paper's claims and relevance.