ID: k1Xy5zCNOJ
Title: Lookaround Optimizer: $k$ steps around, 1 step average
Conference: NeurIPS
Year: 2023
Number of Reviews: 32
Original Ratings: 8, 6, 5, 5, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Lookaround, a novel optimizer for weight average ensembling (WA) that operates through a two-step process during training. The "around" step trains multiple networks on transformed data with different augmentations, while the "average" step combines these networks to create an averaged network for subsequent iterations. Lookaround shows clear superiority over state-of-the-art methods, including SGDM, Lookahead, Model Soups, and DART, in extensive experiments on CIFAR and ImageNet datasets, utilizing both traditional CNNs and Vision Transformers. The authors provide theoretical justification for their approach and emphasize the importance of consistent data augmentation techniques across methods to ensure fair comparisons. They also commit to open science by making the code publicly available. However, the results indicate significant discrepancies compared to previously reported values, raising concerns about the reliability of the findings.

### Strengths and Weaknesses
Strengths:
1. **Innovative Approach**: Lookaround introduces a straightforward and effective optimizer that enhances network diversity and preserves weight locality.
2. **Theoretical Justification**: Strong theoretical support is provided through convergence analysis, enhancing the credibility of Lookaround.
3. **Extensive Experimental Validation**: The method demonstrates effectiveness across popular benchmarks, outperforming existing techniques.
4. **Commitment to Open Science**: The authors' intention to make the code publicly available fosters reproducibility.
5. **Revised Experimental Results**: Supplementary experiments improved the reliability of results, particularly for ViT-B/16 and ResNet50.

Weaknesses:
1. **Computational Complexity and Comparison**: The paper lacks a comprehensive discussion on the computational complexity introduced by Lookaround, particularly regarding runtime and memory usage.
2. **Limited Experiments on Diverse Datasets**: The focus on image classification limits the evaluation of Lookaround's versatility; experiments on other tasks are necessary.
3. **Minor Experimental Issues**: Some results appear inconsistent, and the absence of error bars raises questions about the reliability of the reported values.
4. **Concerns with ImageNet Results**: The experimental results for ImageNet raise concerns about the reliability of the findings, as the performance is significantly lower than expected.
5. **Inadequate Comparison with Other Methods**: There is a lack of adequate comparison with methods like model soups and DART, which may affect the perceived novelty and effectiveness of Lookaround.
6. **Batch Normalization Parameters**: The paper does not sufficiently address the updating of batch normalization parameters, which could impact the results.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational complexity of Lookaround, including a detailed analysis of runtime and memory usage compared to existing methods. Additionally, conducting experiments on datasets beyond image classification, such as language modeling or segmentation tasks, would demonstrate the generalizability of Lookaround. We also suggest including error bars in the experimental results to provide a clearer representation of variability and conducting a fair comparison with other optimization methods, particularly addressing the discrepancies noted in accuracy results. Furthermore, we recommend improving the clarity of the experimental setup, particularly regarding the discrepancies in ImageNet results, and ensuring that batch normalization statistics are updated appropriately in both the proposed method and the baselines. Finally, clarifying the augmentation policy used during baseline training and ensuring that the theoretical foundations are accessible without requiring prior knowledge of related works would enhance the paper's clarity.