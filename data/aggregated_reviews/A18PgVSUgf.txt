ID: A18PgVSUgf
Title: Accelerating Molecular Graph Neural Networks via Knowledge Distillation
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 7, 4, 7, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 3, 2, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper explores knowledge distillation (KD) to enhance the performance and scalability of graph neural networks (GNNs) for molecular simulations, specifically focusing on regression tasks for energy and force predictions. The authors propose various KD strategies, including node-to-node, edge-to-node, and vector-to-vector distillation, and conduct empirical benchmarks across different teacher-student model combinations. They utilize the OC20 and COLL datasets, noted for their chemical diversity and ability to test generalization across various system sizes and compositions. Results indicate that KD can improve the performance of smaller molecular GNNs, closing significant gaps in energy and force predictions. The authors emphasize the trade-off between speed and accuracy in model performance, asserting that smaller GNN models can be beneficial for specific applications, even if they exhibit higher mean absolute errors (MAE).

### Strengths and Weaknesses
Strengths:
- The paper addresses a critical and underexplored area of applying KD to GNNs in 3D molecular simulations.
- It presents a clear and well-structured analysis of various KD techniques, demonstrating their potential to enhance smaller models.
- The use of diverse datasets (OC20 and COLL) supports the generalization of the proposed method.
- The paper effectively addresses the trade-off between speed and accuracy, positioning KD as a means to optimize this balance.
- The authors provide a clear rationale for their choice of established error metrics, aligning with common practices in the field.

Weaknesses:
- The improvements in performance, while notable, remain marginal, particularly in force metrics, with the best student model still underperforming compared to larger models.
- The technical novelty is limited, as the proposed methods do not significantly deviate from existing KD techniques applicable to other domains.
- The study lacks comprehensive contextualization within the broader GNN distillation literature, and comparisons with existing methodologies are insufficient.
- Stability analysis, while acknowledged as significant, is not included in the current work, which may limit the robustness of the findings.
- The accuracy of smaller models remains a concern, as high force/energy MAE could render them less usable in practical applications.

### Suggestions for Improvement
We recommend that the authors improve the contextualization of their work by comparing their techniques with existing GNN distillation methods, particularly those focused on regression tasks. Additionally, including a broader range of benchmark datasets would strengthen the empirical findings and demonstrate the generalizability of the proposed methods. We suggest that the authors analyze the stability of their models in molecular simulations and provide a clearer evaluation of error metrics. Furthermore, exploring additional teacher-student combinations, particularly those involving state-of-the-art architectures, could yield more substantial insights into the effectiveness of their KD strategies. Lastly, we recommend that the authors address the usability of smaller models by exploring methods to reduce the force/energy MAE, thereby enhancing their practical applicability in downstream tasks.