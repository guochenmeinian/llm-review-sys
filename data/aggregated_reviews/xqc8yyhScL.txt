ID: xqc8yyhScL
Title: Is Programming by Example Solved by LLMs?
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 6, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the effectiveness of Large Language Models (LLMs) in solving Programming-by-Example (PBE) tasks across three domains: lists, strings, and LOGO/Turtle graphics. The authors propose that while pretrained LLMs are not inherently effective for PBE, fine-tuning significantly enhances their performance on in-distribution tasks. The study evaluates LLMs, particularly 7B and 33B models, and finds that fine-tuning improves accuracy, while a semi-supervised approach enhances out-of-distribution (OOD) generalization.

### Strengths and Weaknesses
Strengths:
- The paper presents a thorough evaluation and detailed analysis of LLMs in PBE tasks.
- It effectively connects modern LLMs with classical PBE problems and includes clear examples.
- The insights derived from experiments are valuable, especially regarding the adaptation of LLMs to OOD tasks.

Weaknesses:
- The title poses an undefined question regarding whether LLMs "solve" PBE, lacking clarity on what "solved" means.
- The novelty is limited, primarily presenting an evaluation without introducing new techniques.
- The adaptation method may expose models to test set content prematurely, raising fairness concerns.
- Comparisons with classic symbolic methods may be misleading, as LLMs' advantages stem from extensive training data rather than language expressiveness alone.
- The paper does not evaluate simple prompting methods like CoT, and the examples used are not representative of real-world programming tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the title and define what "solved" means in the context of PBE. Additionally, we suggest that the authors tone down claims of novelty regarding the wake-sleep algorithm and present the paper more as an evaluation study. To enhance the robustness of the findings, consider including comparisons with more recent LLM competitors in the graphics domain and addressing the fairness of the adaptation method. Lastly, we encourage the authors to evaluate the impact of different prompting techniques and ensure that examples used in experiments closely resemble real-world programming tasks.