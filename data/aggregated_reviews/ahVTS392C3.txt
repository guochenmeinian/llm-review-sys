ID: ahVTS392C3
Title: JASMINE: Arabic GPT Models for Few-Shot Learning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents JASMINE, a collection of Arabic GPT models trained on a diverse dataset that includes both standard and dialectic Arabic. The authors provide a thorough description of the training procedures, datasets, and evaluation tasks, including human evaluations. The JASMINE models generally outperform existing models like AraGPT2 and mGPT, with only a few exceptions. The authors also address inherent biases and shortcomings of the models, contributing significantly to the field of Arabic generative models.

### Strengths and Weaknesses
Strengths:
- The paper is well written and executed, providing comprehensive details about training procedures and evaluation benchmarks.
- The JASMINE models demonstrate superior performance compared to previous models, enhancing research on Arabic generative models.
- The authors candidly discuss biases and shortcomings, which adds depth to the evaluation.

Weaknesses:
- There are missing details regarding data creation and testing, including the distribution of dialectal data and perplexity calculations.
- Some aspects of the methodology, such as the generation of false answers in commonsense inference and the selection criteria for occupations, lack clarity.
- Reproducibility is hindered by underspecified parameters and the availability of training data.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the methodology by providing detailed explanations for perplexity calculations and the generation of false answers in commonsense inference. Additionally, addressing the distribution of dialectal data and its potential biases is crucial. We suggest including state-of-the-art results in the tables for better context and ensuring that all evaluation benchmarks are made available to the community. Finally, we encourage the authors to proofread the paper to simplify complex language and clarify sections that are currently ambiguous.