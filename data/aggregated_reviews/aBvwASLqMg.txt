ID: aBvwASLqMg
Title: On the Representational Capacity of Recurrent Neural Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical exploration of the computational power of recurrent neural networks (RNNs), extending the Turing completeness result to the probabilistic case. The authors demonstrate that RNNs with rational weights and unbounded computation time can simulate any probabilistic Turing machine (PTM). Additionally, the paper establishes a lower bound by showing that these models can simulate deterministic real-time rational PTMs under real-time computation constraints. The work is accessible to NLP researchers, even those less familiar with formal language theory.

### Strengths and Weaknesses
Strengths:  
- The paper provides a strong theoretical result, showing that RNN language models can simulate probabilistic Turing machines.  
- It is exceptionally well-written and easy to follow, with sound and convincing proofs.  
- The first half offers a clear exposition of classical results in Turing machine theory.  

Weaknesses:  
- The unique contributions of the paper are not well-defined, making it difficult to discern its novelty.  
- The analysis feels abstract and lacks adequate motivation for the significance of focusing on probability distributions over unweighted languages.  
- The paper does not introduce new proving techniques or tighter bounds, and the extension from Turing completeness to the probabilistic case is seen as incremental rather than fundamental.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of their unique contributions and provide a stronger motivation for the significance of their focus on probability distributions. Additionally, addressing Open Question 5.1 regarding the precise computational power of rationally weighted recurrent language models would enhance the paper's impact. To captivate readers further, the authors should explore the computational power of real-time recurrent or self-attention-based language models with finite precision and rational hidden states. Lastly, we suggest revisiting the purpose and significance of figures and symbols used in the paper for better clarity.