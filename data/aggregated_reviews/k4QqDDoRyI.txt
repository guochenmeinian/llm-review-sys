ID: k4QqDDoRyI
Title: ATFormer: A Learned Performance Model with Transfer Learning Across Devices for Deep Learning Tensor Programs
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents ATFormer, a cost modeling and tensor program optimization framework aimed at enhancing deep neural network efficiency. The authors propose framing the tuning problem as an optimization task within a hierarchical search space, utilizing attention mechanisms to capture global and long-range dependencies in features. The model demonstrates improved latencies and search times across various GPU platforms, showcasing its applicability in both traditional and transfer learning contexts.

### Strengths and Weaknesses
Strengths:
- The paper is well-written, with extensive experiments demonstrating the soundness of the authors' design.
- The approach effectively addresses critical challenges in neural network compilation, including large search spaces and cross-device transferability.
- The evaluation includes a wide range of DNNs and adequate baselines, with insightful ablations.

Weaknesses:
- The problem addressed is relatively simple, focusing only on the innermost non-loop statements for tuning.
- Some performance gains appear incremental, with LSTM outperforming ATF in certain cases.
- The code generation templates are overly simplistic, lacking optimizations like loop tiling, which are important for DNN workload optimization.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by refactoring it to emphasize the general applicability of their approach beyond innermost loops. Additionally, providing more interesting examples upfront, as discussed in the rebuttal, would help illustrate the optimization capabilities of ATFormer. The authors should also address the questions raised regarding the concatenation of outputs in Figure 3, the latency values for BERT-large and GPT in Figure 6, and the performance differences observed in Figure 7. Finally, we suggest including more details on experimental settings for online datasets to enhance reproducibility.