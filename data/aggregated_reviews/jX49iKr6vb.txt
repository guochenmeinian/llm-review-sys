ID: jX49iKr6vb
Title: Beyond Deep Ensembles: A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift
Conference: NeurIPS
Year: 2023
Number of Reviews: 14
Original Ratings: 4, 6, 8, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical evaluation of various Bayesian Deep Learning (BDL) algorithms on real-world datasets, focusing on generalization performance and uncertainty estimation under distribution shifts. The authors propose a signed version of calibration error to identify overconfidence and underconfidence, enhancing the understanding of BDL algorithms in safety-critical applications. The study benchmarks methods from the WILDS dataset collection, including both single-mode and ensemble approaches, revealing that ensemble methods often improve generalization and calibration. The authors have made efforts to address concerns raised in previous reviews, incorporating additional experiments and concise takeaway messages in the conclusion. However, doubts remain regarding the overall contribution and interest level of the findings.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant need for benchmarking BDL techniques, contributing to the community's understanding of model performance on out-of-distribution (OOD) data.
- The choice of methods and evaluation metrics is well-justified, reflecting typical Bayesian approaches and capturing both uncertainty quantification and generalization capabilities.
- The introduction of signed calibration metrics is a valuable addition for distinguishing model confidence levels.
- The experiments are reproducible, with detailed methodology and code provided.
- The authors addressed some reviewer concerns effectively, leading to increased scores from multiple reviewers.
- The inclusion of SNGP as an additional baseline strengthens the paper.
- The addition of concise takeaway messages enhances clarity in the conclusion.

Weaknesses:
- The contribution to the community is perceived as limited, with a lack of clear takeaway recommendations from experimental results.
- Some reviewers still express doubts about the paper's overall contribution and the interest of the findings.
- The paper lacks comprehensive baseline comparisons, missing methods like DUQ and frequentist approaches, which diminishes its thoroughness.
- The writing in the experimental section is unclear, lacking insights and conclusions, making it difficult to extract key messages.
- There is a noted need for a more thorough analysis regarding the preference for certain methods, which remains insufficiently addressed.
- Some claims, such as BDL's competitiveness with OOD-specific algorithms, are unsupported by adequate comparisons.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental section by explicitly stating key takeaways and conclusions from each experiment. Additionally, including a discussion on the computational complexity of each method would enhance the evaluation. To strengthen the paper, please incorporate comparisons with established baselines like Deep Kernel Learning and address the absence of certain algorithms in the experiments. We also suggest providing evidence to support claims about the realism of distribution shifts and clarifying the methodology for hyperparameter selection and prior tuning. Furthermore, we recommend that the authors improve the analysis of why and when specific methods are preferred, as this was a concern raised by multiple reviewers. Finally, consider revising the writing for better readability and ensuring that all figures and tables are adequately referenced and formatted.