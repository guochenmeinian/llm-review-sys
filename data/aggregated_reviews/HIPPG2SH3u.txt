ID: HIPPG2SH3u
Title: Unified Representation for Non-compositional and Compositional Expressions
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents PIER and PIER+, models designed to enhance representations for potentially idiomatic expressions (PIEs). The authors propose two main components: attention fusion layers and a similarity learning objective, alongside prompt infilling. Experimental results from intrinsic and extrinsic evaluations indicate the potential of PIER and PIER+ in effectively representing PIEs.

### Strengths and Weaknesses
Strengths:
- PIER is a well-crafted model that consistently outperforms GIEA in generating contextually appropriate embeddings for idiomatic and literal PIEs.
- The paper provides a thorough analysis through various intrinsic and extrinsic evaluations, demonstrating the model's effectiveness and potential interest to the ACL community focused on idiomatic language.

Weaknesses:
- The current baselines do not adequately demonstrate the effectiveness of the attention fusion layers, as all models utilize these layers, making it difficult to isolate their impact.
- The results for SpanDET are inconsistent, with certain configurations underperforming compared to GIEA, raising questions about the model's reliability.
- The model's reliance on multiple components limits its applicability outside figurative language processing tasks, and the contributions of individual components remain unclear.

### Suggestions for Improvement
We recommend that the authors improve the clarity of their model's contributions by designing additional baselines that exclude the attention fusion layers to assess their necessity. Additionally, we suggest conducting ablation studies for type classification prompts and definition generation prompts to better understand their impact. Furthermore, addressing the inconsistencies observed in the SpanDET results with detailed explanations would enhance the paper's rigor. Lastly, exploring the model's performance on idiom paraphrase identification tasks could provide valuable insights into its capabilities.