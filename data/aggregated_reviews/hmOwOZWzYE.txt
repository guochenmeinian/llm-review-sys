ID: hmOwOZWzYE
Title: GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MQA (Multi Query Attention) and GQA (Grouped-query Attention) to address memory bandwidth issues in the self-attention mechanism of language models. MQA utilizes a single key and value for faster computation but sacrifices model capacity, while GQA employs grouped keys and values, achieving speed comparable to MQA and quality similar to the Multi-head approach. The authors propose an uptraining method to convert existing multi-head model checkpoints to multi-query models, demonstrating that uptrained GQA closely matches Multi-head Attention (MHA) in quality and MQA in speed.

### Strengths and Weaknesses
Strengths:
1. The methodology effectively reduces computational overhead while maintaining performance.
2. The paper is well-structured and easy to read, with reasonable experimental design and results supporting the proposed methods.
3. The uptraining method significantly lowers the cost of model conversion, requiring only 5% of the computing power of the original pre-training.

Weaknesses:
1. The experimental design lacks detail, particularly regarding the optimizer, total epochs, and training duration.
2. Insufficient experimental results to substantiate claims, particularly regarding the stability of training and performance metrics.
3. The evaluation method (ROUGE score) may not fully reflect model performance, necessitating further exploration.

### Suggestions for Improvement
We recommend that the authors improve the detail regarding the experimental design, including specifics about the optimizer, total epochs, and training duration. Additionally, we suggest providing more comprehensive experimental results to support claims, particularly concerning the performance decline at higher alpha ratios and the correlation between group size and training stability. Finally, we encourage the authors to explore the impact of applying MQA and GQA to the encoder and to clarify the minimum number of groups required to stabilize training with GQA.