ID: LzLeAscHnj
Title: Bridging The Gap between Low-rank and Orthogonal Adaptation via Householder Reflection Adaptation
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 7, 5, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel adaptation method called Householder Reflection Adaptation (HRA), which fine-tunes pretrained models by multiplying frozen weight matrices with orthogonal matrices derived from Householder reflections. The authors establish a connection between HRA and low-rank adaptation, proposing a unified framework for fine-tuning that retains pre-training knowledge while reducing the number of learnable parameters. Experimental results across various pretrained models demonstrate that HRA achieves superior performance with fewer parameters compared to existing methods.

### Strengths and Weaknesses
Strengths:  
- The relationship between low-rank and orthogonal adaptation is insightful, contributing to a unified fine-tuning framework.  
- HRA effectively reduces the number of learnable parameters while maintaining strong experimental results.  
- The paper is well-motivated and presents extensive experiments across diverse tasks, showcasing the method's applicability.

Weaknesses:  
- The paper's structure lacks clarity, particularly in the abstract and introduction, where the motivation for a unified framework is not well articulated.  
- The novelty of HRA is somewhat limited as it is a special case of Orthogonal Fine-Tuning (OFT).  
- There is insufficient discussion on the limitations of HRA and its societal impacts, particularly regarding its application in large language models (LLMs).  
- The authors do not adequately explain why HRA outperforms baselines like LoRA and OFT.

### Suggestions for Improvement
We recommend that the authors improve the structure of the paper by clarifying the motivation for the unified framework in the abstract and introduction. Additionally, please provide a comprehensive discussion on the limitations of HRA, including its societal impacts. We suggest including empirical evidence to support claims regarding the retention of pre-training knowledge and addressing the computational costs of HRA compared to LoRA. Furthermore, we advise the authors to explore the impact of varying the regularization parameter Î» and to clarify which parts of the model utilize HRA, including whether each layer has a similar r.