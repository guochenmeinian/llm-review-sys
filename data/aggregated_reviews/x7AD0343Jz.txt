ID: x7AD0343Jz
Title: Limits of Transformer Language Models on Learning to Compose Algorithms
Conference: NeurIPS
Year: 2024
Number of Reviews: 13
Original Ratings: 4, 6, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 2, 4, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper investigates the ability of transformer-based language models to learn compositional discrete tasks, introducing two new tasks: pointer execution neighbor (PEN) and pointer execution reverse multicount (PERM), alongside established tasks such as multiplication and highest subsequence sum. The authors examine the sample efficiency of models trained from scratch and the performance of prompted models like GPT-4 and Gemini. The findings indicate that compositional learning requires more samples than the sum needed for individual subtasks, challenging the assumption that models can learn compositions efficiently. Theoretical analysis supports these results, highlighting the inefficiencies of gradient descent in memorizing feedforward models. Additionally, the authors present a focused investigation using a minimal set of synthetic tasks to avoid learning spurious correlations and inefficiencies associated with larger sets of subtasks. They argue that a model's performance on simple synthetic tasks is indicative of its potential in more complex settings and acknowledge concerns regarding tokenization and the synthetic nature of their experiments.

### Strengths and Weaknesses
Strengths:
- The paper poses an intriguing question regarding the compositional learning capabilities of language models, contributing to the understanding of their limitations.
- It includes a solid theoretical framework that demonstrates the challenges of learning compositions when models rely on memorization.
- The empirical study is extensive, utilizing various models and prompting techniques, and provides a thorough literature review.
- The authors provide a clear rationale for their choice of a minimal set of synthetic tasks, emphasizing the importance of avoiding spurious correlations.
- The additional results on tokenization were well-received, indicating a positive response to reviewer feedback.
- The authors express openness to further discussion and clarification, demonstrating a collaborative approach to improving their work.

Weaknesses:
- Hypothesis H1 lacks empirical disprovability due to vague definitions, and the notion of "sub-task" is not clearly defined, leading to ambiguity in H2 and H3.
- The necessity and contribution of the new tasks over simpler ones are not well justified, and the presentation of these tasks is unclear, hindering comprehension.
- The expectation that an untrained small model can learn to compose subtasks without prior training is questionable, and the potential negative impact of tokenization on the API experiments is not adequately addressed.
- Concerns remain regarding the synthetic nature of the experiments, particularly the use of English words as tokens, which may not accurately represent in-distribution data.
- The limited scope of additional experiments on Natural-PEN may restrict the evaluation of pre-training effects on compositionality.
- While the controlled synthetic setting is valuable, it may not fully capture the complexities of real-world applications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of hypotheses, particularly H1, by explicitly defining the "constant" used in experiments. Additionally, the authors should clarify the atomicity and uniqueness of subtasks to strengthen the definitions of H2 and H3. Justifying the introduction of new tasks and their advantages over simpler alternatives is essential, as is enhancing the presentation of these tasks to improve visual clarity. Furthermore, we suggest that the authors analyze the tokenization of input strings in the API experiments to assess its impact on task performance. We also recommend that the authors improve the evaluation of their model by designing a testbed that simulates an in-distribution task, as this would provide more relevant insights into the model's compositional abilities. Lastly, addressing the concerns regarding the synthetic nature of their experiments more thoroughly in the revised manuscript would strengthen their findings.