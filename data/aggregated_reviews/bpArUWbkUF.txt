ID: bpArUWbkUF
Title: Argue with Me Tersely: Towards Sentence-Level Counter-Argument Generation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new benchmark annotated dataset (ArgTersely) for sentence-level counter-argument generation derived from ChangeMyView (CMV), comprising 31,197 pairs of arguments and counter-arguments. The authors propose the Arg-LLaMA framework, which integrates a language model and a filter for selecting high-quality counter-arguments, along with a novel evaluator (Arg-Judge) for the counter-argument generation task. The authors also utilize a dataset of claims and counters from CMV, fine-tuning an LLM on this dataset with specific instructions, and employ chain-of-thought (CoT) prompting to generate and rank counter-arguments.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- It introduces a novel dataset and framework for generating high-quality counter-arguments.
- The resources, including instructions and prompts, are valuable for the research community.
- Extensive experiments validate the claims made in the paper.

Weaknesses:
- The annotation process lacks sufficient detail, including the training and instructions given to annotators, as well as the evaluation of annotation quality.
- The related work section is underdeveloped, providing little elaboration on previous approaches to counter-argument generation.
- The claim regarding Arg-Judge being trained with human preference is misleading, as the data appears to be automatically generated.
- The dataset creation process could benefit from a clearer explanation, particularly regarding the seed instructions and their expansion.

### Suggestions for Improvement
We recommend that the authors improve the description of the annotation process, including details on the training of annotators, the instructions provided, and the time and effort required to complete the task. Additionally, clarifying how the quality of annotations was evaluated would strengthen the paper. We also suggest that the authors address the ethical concerns related to annotator privacy and consider providing a more comprehensive analysis of the dataset, including human agreement metrics. Furthermore, comparing their model against more powerful systems, such as Project Debater from IBM, would enhance the evaluation of their framework. Lastly, we encourage the authors to elaborate on the relationship between the created instructions and traditional argumentation schemes.