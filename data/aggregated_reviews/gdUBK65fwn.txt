ID: gdUBK65fwn
Title: LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive empirical study of parameter-efficient fine-tuning (PEFT) for large language models (LLMs), focusing on various adapters such as Prefix-Tuning, Series Adapter, LoRA, and Parallel Adapter. The authors explore optimal adapter placements and evaluate hyperparameters like LoRA rank and bottleneck size across two reasoning tasks: Arithmetic and Commonsense. They also introduce two new datasets, Math10K and Commonsense170K, and develop the "LLM-Adapter" framework for fine-tuning LLMs.

### Strengths and Weaknesses
Strengths:
- The paper provides a solid technical implementation and extensive experimental analysis, contributing valuable insights into adapter configurations for LLMs.
- The authors address an important question in fine-tuning LLMs and offer a framework and datasets that enhance empirical understanding.

Weaknesses:
- Generalization of findings is limited, as experiments are conducted on a single model and task, raising concerns about applicability to other datasets and architectures.
- The paper lacks clarity on the utility of the datasets and the distinctive features of the LLM-Adapter framework compared to existing solutions.
- Writing issues are present, with some sections being overly verbose and lacking succinctness.

### Suggestions for Improvement
We recommend that the authors improve reproducibility by providing a repository for downloading experiments. Additionally, consider using different evaluation measures to enhance the analysis. Clarifying the definition of accuracy for the studied tasks would be beneficial. To strengthen generalization claims, we suggest including experiments on a more diverse set of tasks and models. Furthermore, providing more details about the LLM-Adapter framework and its unique contributions compared to existing methods would enhance the paper's clarity and impact. Lastly, we advise simplifying verbose sections to improve readability.