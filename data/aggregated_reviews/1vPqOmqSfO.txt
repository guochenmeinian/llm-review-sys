ID: 1vPqOmqSfO
Title: Sketched Lanczos uncertainty score: a low-memory summary of the Fisher information
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 6, 8, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a memory-efficient algorithm for computing uncertainty scores in neural networks, specifically through the Sketched Lanczos Uncertainty (SLU) method. The authors combine the Lanczos method with sketching to achieve a low-rank approximation of the Generalized Gauss-Newton (GGN) matrix, crucial for calculating uncertainty scores. They demonstrate that orthogonalization can be performed post-hoc, allowing for significant memory savings. Empirical results indicate that SLU outperforms existing metrics in out-of-distribution detection tasks, particularly under low-memory constraints.

### Strengths and Weaknesses
Strengths:
- Originality: The authors are the first to integrate sketching with approximate matrix eigendecompositions to reduce memory usage.
- Quality: A comprehensive analysis of the SLU method is provided, including visualizations of the GGN matrix's low-rankness and the effectiveness of low-rank approximations.
- Clarity: The paper is well-organized, facilitating reader understanding of SLU's features and benefits.
- Significance: SLU compensates for sketching errors with higher-rank approximations, yielding improved detection results under low-memory budgets.

Weaknesses:
- The experimental validation is limited; a simple experiment with synthetic data is needed to support the claim regarding noise from sketching being outweighed by higher-rank approximations.
- The experimental settings are simplistic, lacking tests on larger-scale models like Vision Transformers.
- The authors only consider the $M_\text{noeig}$ case, limiting SLU's flexibility for other covariance matrices.

### Suggestions for Improvement
We recommend that the authors improve the experimental validation by including a synthetic data experiment to substantiate the claim about sketching noise. Additionally, we suggest testing SLU on larger-scale models, such as Vision Transformers, to assess its applicability in more complex scenarios. Furthermore, we encourage the authors to explore alternative methods for reducing memory bottlenecks in computing uncertainty scores, as well as providing justifications for the fixed memory budget of $3p$ in their experiments. Lastly, including a limitations section that discusses the dependence of SLU's performance on the sketching algorithm would enhance the paper's comprehensiveness.