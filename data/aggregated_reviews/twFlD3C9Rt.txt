ID: twFlD3C9Rt
Title: Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 6, 9, 7, -1
Original Confidences: 4, 4, 4, -1

Aggregated Review:
### Key Points
This paper presents a dynamic benchmarking system for conversational agents, evaluating their performance through a single, extended userâ†”agent interaction that incorporates multiple tasks and context switching. The authors propose a new benchmark that assesses the Long-Term Memory (LTM) and Continual Learning (CL) capabilities of agents, revealing that while LLMs perform well on single-task interactions, they struggle significantly when tasks are interleaved. The benchmark demonstrates that agents equipped with an LTM system show less performance decline compared to those without, suggesting a potential "focusing" effect. The task-interleaving approach increases benchmark difficulty, with scores varying by up to 1.5 points between interleaved and isolated tasks.

### Strengths and Weaknesses
Strengths:
- The introduction of the LTM Benchmark provides a comprehensive evaluation of conversational agents in realistic scenarios.
- The automated system effectively assesses LTM and CL capabilities through prolonged conversations with multiple tasks.
- The paper includes extensive evaluations of advanced language models, highlighting the inadequacies of contemporary benchmarks.

Weaknesses:
- The paper lacks detailed information on the generation processes for the synthetic benchmark, which could enhance its self-containment.
- It assesses agent robustness based on only three repetitions, which may not be reliable.
- Significant time and financial costs limit the feasibility of multiple repetitions for the benchmark.

### Suggestions for Improvement
We recommend that the authors improve the paper by including detailed descriptions of the synthetic benchmark generation processes to enhance clarity. Additionally, consider expanding the benchmark to include over 100 test cases to reduce evaluation bias and noise. It would also be beneficial to discuss significant works on Long-Term Memory, such as Memorizing Transformer and MemoryBank, to contextualize the contributions of this benchmark within the existing literature. Lastly, please ensure correct bibliographic formatting and enable hyperlinks for cited papers, while omitting acknowledgments in the draft submission.