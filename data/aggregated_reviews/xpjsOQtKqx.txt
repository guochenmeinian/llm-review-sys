ID: xpjsOQtKqx
Title: StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners
Conference: NeurIPS
Year: 2023
Number of Reviews: 23
Original Ratings: 8, 7, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into using synthetic images generated by text-to-image models, specifically Stable Diffusion, for training self-supervised image embedding models. The authors propose a novel multi-positive contrastive learning method called StableRep, which treats multiple images generated from the same text prompt as positive examples for each other. The findings indicate that self-supervised methods trained on synthetic images can achieve performance comparable to or better than those trained on real images, particularly when language supervision is included. StableRep addresses performance issues on CIFAR-10/100 and consistently outperforms existing methods. The authors demonstrate significant improvements in fairness of representations, evaluated on the FairFace benchmark, where their method achieves a mean accuracy of 37.2% compared to CLIP's 30.4% with synthetic data. Additionally, StableRep enhances understanding of compositionality, showing competitive results on the ARO benchmark. The method excels in dense prediction tasks, outperforming the official MAE checkpoint, and exhibits strong generalization to ImageNet. The authors also suggest that combining synthetic and real data can further enhance performance.

### Strengths and Weaknesses
Strengths:
1. **Interesting topic:** The exploration of synthetic images as effective visual representation learners addresses a timely and significant question in the field.
2. **New contrastive learning method:** The introduction of StableRep is a novel approach tailored for synthetic data.
3. **Comprehensive evaluation:** The paper includes extensive evaluations of the representations learned by StableRep on large-scale datasets, demonstrating promising results.
4. **Performance across benchmarks:** StableRep consistently outperforms other methods across various benchmarks, including CIFAR-10/100, FairFace, and dense prediction tasks.
5. **Fairness and compositionality:** The method significantly improves fairness and understanding of compositionality in representations.
6. **Strong generalization ability:** Demonstrated in fine-tuning setups, StableRep even outperforms MAE trained on real data.

Weaknesses:
1. **Potentially entangled comparison in terms of data amount:** The claim regarding the equivalence of synthetic and real image data amounts lacks clarity, as the authors do not adequately address the implications of using a generative model trained on a much larger dataset.
2. **Limited motivation for using synthetic images:** The rationale for employing synthetic images in training is not sufficiently justified, particularly in relation to their performance against state-of-the-art models.
3. **Lack of exploration of diverse downstream tasks:** The evaluation is primarily focused on classification tasks, leaving questions about the performance of synthetic data in dense prediction tasks like object detection or semantic segmentation unaddressed.
4. **Resolution fix concerns:** Some results indicate that the resolution fix may lead to performance drops on certain datasets, suggesting a need for selective application.
5. **Reliance on synthetic data:** The method's reliance on synthetic data raises questions about its applicability in real-world scenarios.

### Suggestions for Improvement
We recommend that the authors improve the discussion surrounding the comparison of data amounts, specifically addressing the implications of using a generative model trained on a larger dataset. Additionally, we suggest that the authors provide a clearer motivation for the use of synthetic images, including comparisons with real-world unlabelled data. To enhance the evaluation, we encourage the authors to explore the performance of synthetic data on diverse downstream tasks beyond classification. Furthermore, we recommend that the authors improve the clarity of their findings regarding the resolution fix, particularly how it affects performance across different datasets. Lastly, we suggest exploring the selective application of the resolution fix strategy to real images based on their resolution to potentially enhance the performance of the "Syn + Real" setup.