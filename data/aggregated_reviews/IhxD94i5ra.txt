ID: IhxD94i5ra
Title: Calibration by Distribution Matching: Trainable Kernel Calibration Metrics
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified framework for model calibration through distribution matching, utilizing Maximum Mean Discrepancy (MMD) as a trainable regularizer. The authors empirically demonstrate that appropriate kernel selection can enhance calibration while maintaining prediction sharpness. The proposed kernel-based calibration metrics unify various calibration forms and are shown to improve decision-making in regression tasks.

### Strengths and Weaknesses
Strengths:
- The introduction of a unified framework for calibration metrics as distribution-matching problems offers an innovative perspective on uncertainty calibration.
- The empirical results validate the effectiveness of the proposed kernel-based calibration method, showing improvements in calibration and sharpness.
- The paper is mostly well-written, providing clear support for its claims.

Weaknesses:
- The paper lacks experiments on multi-class classification, despite claiming applicability to such tasks.
- There is insufficient comparison with similar kernel methods, particularly regarding their differences.
- Implementation details for the MMD approach are vague, and the paper does not adequately contextualize its contributions relative to existing methods.
- The experiments do not compare the proposed method against other training-time calibration approaches, limiting the assessment of its relative significance.

### Suggestions for Improvement
We recommend that the authors improve the discussion on how to apply their method to multi-class classification, including relevant experiments. Additionally, we suggest providing a detailed comparison with similar kernel methods to clarify the distinctions and advantages of their approach. The authors should enhance the implementation details in Section 4 to guide practitioners on practical applications, including the necessity of specific model parameterizations. Furthermore, we encourage the authors to include comparisons with other training-time calibration methods in their experiments to better contextualize their contributions. Lastly, clarifying the evaluation metrics used in experiments, particularly the choice of quantile calibration, would strengthen the paper.