ID: Se71ks7Mfz
Title: DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation
Conference: NeurIPS
Year: 2023
Number of Reviews: 20
Original Ratings: 4, 5, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DiT-3D, an adaptation of the Diffusion Transformer for 3D point cloud generation, which includes modifications such as 3D positional and patch embeddings, and 3D window attention to enhance computational efficiency. The model demonstrates improved performance on the ShapeNet dataset compared to existing methods. However, the novelty of DiT-3D is questioned, as it closely resembles previous works, particularly in its approach to window attention. Additionally, the authors propose a novel approach to using a plain diffusion transformer on voxelized point clouds, addressing challenges specific to indoor object point clouds and emphasizing the significance of 3D positional encoding for achieving high-quality results. Despite these contributions, the novelty of the techniques employed has been questioned, particularly in relation to existing methods like PVD and SWFormer.

### Strengths and Weaknesses
Strengths:
- DiT-3D achieves state-of-the-art performance in single-category point cloud generation, showcasing its effectiveness.
- The paper is well-written and easy to follow, with thorough experiments that support its claims.
- The utilization of pre-trained DiT-2D checkpoints from ImageNet enhances the model's performance in 3D generation.
- The authors provide detailed technical insights into their Transformer design.
- The importance of 3D positional encoding is acknowledged and supported by empirical results.
- The paper addresses specific challenges related to indoor object point clouds.

Weaknesses:
- The novelty of DiT-3D is limited, as it closely follows existing designs, particularly in window attention, which has been previously explored.
- The paper only addresses unconditional generation tasks, lacking experiments on conditional generation, which limits its applicability.
- Comparisons to other relevant methods, such as Point-E, are insufficient, and qualitative results are primarily limited to the main paper without adequate visualization of competing methods.
- The technical differences presented lack sufficient motivation and insights, making them appear similar to hyperparameter tuning.
- The claim of being the first to use plain diffusion on voxelized point clouds is unclear and requires clarification.
- The process of voxelization and patchification is confusing, particularly regarding the resolution and method of reshaping.

### Suggestions for Improvement
We recommend that the authors improve the novelty aspect by providing more insights into the unique contributions of DiT-3D compared to existing works. Additionally, we suggest including experiments on conditional generation tasks, such as point cloud completion or 2D image-to-3D point cloud generation, to broaden the scope of the study. Furthermore, we advise the authors to enhance qualitative comparisons by including visualizations of methods like LION and MeshDiffusion in the main paper, as well as addressing the computation resources used in the experiments. We also recommend improving the motivation behind their technical choices and providing clearer insights into the novelty of their work, particularly clarifying the claim regarding being the first to use plain diffusion on voxelized point clouds. Additionally, enhance the explanation of the voxelization and patchification process in Section 3.2 to address the confusion about resolution and the rationale for the chosen approach. Finally, ensure that the significance of 3D positional encoding is presented without overstating its novelty.