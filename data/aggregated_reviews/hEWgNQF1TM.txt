ID: hEWgNQF1TM
Title: Parameter-Efficient Language Model Tuning with Active Learning in Low-Resource Settings
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an empirical study on the integration of active learning (AL) and parameter-efficient fine-tuning (PEFT) for text classification tasks in low-resource settings. The authors demonstrate that PEFT outperforms full fine-tuning (FFT) and analyze forgetting dynamics and instance-level representations, highlighting the potential synergy between AL and PEFT. The study includes experiments on four text classification datasets and implements various active learning algorithms combined with PEFT methods.

### Strengths and Weaknesses
Strengths:
- The paper explores the interplay between AL and PEFT, providing empirical evidence of their effectiveness in low-resource settings.
- The analysis of forgetting dynamics offers valuable insights into the behavior of PEFT and FFT.
- The extensive experimental setup includes multiple PEFT methods and AL strategies, supporting the authors' claims.

Weaknesses:
- The novelty of combining AL with PEFT appears incremental, lacking significant innovation compared to prior work.
- The validation is limited to four datasets and a single model architecture (BERT), which may lead to overgeneralization of conclusions.
- The paper does not sufficiently address the challenges of integrating PEFT and task-adaptive pre-training (TAPT) in the active learning framework.

### Suggestions for Improvement
We recommend that the authors improve the analysis of the challenges and specifics of combining PEFT and TAPT in the active learning framework to enhance the perceived novelty. Additionally, including experiments on prompt tuning and expanding the dataset selection to include benchmark datasets with more categories would strengthen the validation of the proposed method. Providing more training details, such as batch size and hyper-parameters, and conducting a hyper-parameter search for the learning rate would also improve reproducibility. Lastly, we suggest presenting absolute performance metrics for PEFT and FFT in AL settings to make the results more intuitive and impactful.