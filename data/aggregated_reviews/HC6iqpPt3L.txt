ID: HC6iqpPt3L
Title: Adaptive Exploration for Data-Efficient General Value Function Evaluations
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 7, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GVFExplorer, a novel method for evaluating multiple Generalized Value Functions (GVFs) in parallel using off-policy methods. It adaptively learns a single behavior policy that minimizes total variance in returns across GVFs, thereby reducing environmental interactions. The method employs a temporal-difference-style variance estimator and demonstrates that each behavior policy update decreases the overall mean squared error in GVF predictions. Empirical validation is conducted across various settings, including both tabular and non-linear function approximations, with stationary and non-stationary reward signals.

### Strengths and Weaknesses
Strengths:
- The concept of selecting a behavior policy for simultaneous learning of multiple policies is innovative and applicable to various scenarios.
- The systematic derivation of the algorithm is novel and compelling.
- The incorporation of temporal difference learning to minimize return variance is an interesting approach.
- The paper is well-written and presents its findings clearly.

Weaknesses:
- More extensive experiments are needed to fully understand the algorithm's behavior.
- The choice of a uniform policy as the best baseline is questionable; more competitive baselines, such as ablations, would enhance the study.
- The experimental environments are overly synthetic; results from more typical and challenging RL environments, like Mujoco, would be beneficial.
- The paper does not convincingly justify the necessity of minimizing return variance, lacking robust proofs or empirical investigations.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including more diverse and challenging environments to strengthen the empirical findings. Additionally, we suggest incorporating more competitive baselines to provide a clearer comparison of performance. Clarifying the justification for minimizing return variance through stronger theoretical or empirical evidence would enhance the paper's contribution. Finally, we advise rephrasing the claims regarding Theorem 4.2 to accurately reflect that aggregated variance "does not increase" rather than "decreases."