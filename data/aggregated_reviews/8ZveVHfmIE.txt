ID: 8ZveVHfmIE
Title: On the Convergence of Encoder-only Shallow Transformers
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 5, 6, 5, 7, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a convergence proof for shallow transformer networks, specifically those with a single self-attention layer followed by a ReLU and a linear layer. The authors demonstrate that global convergence can be achieved under various weight initialization schemes (LeCun, He, NTK) and scaling regimes. They establish that quadratic over-parameterization is necessary for convergence with the $d_m^{-1/2}$ scaling, while linear over-parameterization suffices for the $d_m^{-1}$ scaling, albeit with slower convergence rates. Additionally, the paper offers a theoretical contribution regarding the stability of Neural Tangent Kernels (NTK) during training and its connection to kernel regression predictors. The authors propose an analysis of NTK in the context of Transformers, addressing various scalings and initializations. Empirical validation is included through experiments on synthetic and real-world datasets, although the formulation and presentation of results require significant clarity improvements.

### Strengths and Weaknesses
Strengths:
- This work is among the first to provide theoretical insights into the convergence of transformers, potentially guiding future research.
- The authors effectively explore different initialization schemes and their implications for convergence.
- The paper offers valuable theoretical insights into NTK and its implications for Transformer training.
- The authors demonstrate a willingness to address feedback and improve the clarity of their writing and presentation.
- The paper is well-organized, with clear assumptions and a logical flow of ideas, making it relatively easy to follow.

Weaknesses:
- The model's formulation is somewhat simplistic, lacking essential components like residual connections, which are crucial in practical transformer architectures.
- The clarity of presentation is compromised by grammatical errors, typos, and convoluted explanations, making it difficult for readers to grasp the core concepts.
- The analysis does not adequately address the implications of the results for practical applications, and the experiments do not fully explore the impact of different scaling regimes.
- The presentation of results lacks pedagogical clarity, with insufficient explanations of key propositions and terms.
- The learning algorithm is not adequately introduced, leading to confusion regarding the methodology.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by revising it for better readability, addressing grammatical errors and typos, and ensuring that explanations are straightforward. Specifically, they should revise sentences such as those found in lines 228 and 245 to enhance readability. Additionally, including a more detailed discussion on the practical implications of their findings would enhance the paper's relevance. 

We suggest that the authors provide more detailed commentary on Proposition 1 and clarify the NTK analysis in Sections 4.2, 4.3, and 4.4. It is crucial to specify the scaling used in Theorems 3 and 4 clearly. Furthermore, the authors should introduce the learning algorithm properly, including the gradient descent updates for Transformer parameters, to ensure that readers can follow the methodology without confusion. Lastly, it would be beneficial to incorporate a comparison of theoretical and empirical convergence in a plot to substantiate their claims, and to expand their analysis to include more realistic transformer architectures, such as those with residual connections. A comprehensive rewrite focusing on clarity and pedagogical presentation will significantly benefit the paper.