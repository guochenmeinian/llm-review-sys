ID: b7ZJcAkjC3
Title: Enhancing Structured Evidence Extraction for Fact Verification
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for open-domain fact verification that retrieves evidence from a knowledge base containing both text and tables. The authors propose a two-step pipeline for extracting tables and associated cells: (i) coarse-grained extraction using a pre-trained tabular model to score encodings of table rows and columns, and (ii) fine-grained cell selection through an evidence graph built from selected tables and extracted evidence sentences. The authors evaluate their system on the FEVEROUS dataset, achieving state-of-the-art performance.

### Strengths and Weaknesses
Strengths:
- The proposed method is simple yet effective, significantly improving semi-structured evidence retrieval for fact verification.
- The approach is well-motivated with clear examples, and the error analysis identifies gaps in evidence retrieval, suggesting future research directions.
- The paper is generally well-written and easy to follow, with comprehensive experimentation that enhances understanding of the method.

Weaknesses:
- The paper is verbose and at times difficult to read, lacking concise descriptions of key concepts.
- Some parts of the error analysis appear less relevant to the main narrative, and the authors do not sufficiently explain certain observations, such as the performance of the verdict classifier.
- The motivation behind the proposed method is not fully articulated, and the authors do not provide explanations for underperformance in specific metrics.

### Suggestions for Improvement
We recommend that the authors improve the clarity and conciseness of the paper, particularly in sections where complex ideas are presented. The authors should provide more insights into the motivation for their method in the introduction, addressing limitations of existing methods and the advantages of their approach. Additionally, we suggest including explanations for the underperformance observed in Table 3 and clarifying the relevance of the error analysis to the overall findings. The authors should also consider releasing their source code for reproducibility and report mean and standard error in their results to confirm robustness. Lastly, we advise revising figures for clarity and ensuring consistency between text and visual data.