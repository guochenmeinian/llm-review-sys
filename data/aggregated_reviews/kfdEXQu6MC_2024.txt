ID: kfdEXQu6MC
Title: A generalized neural tangent kernel for surrogate gradient learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 8, 8, 8, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extension of neural tangent kernel (NTK) methods for analyzing neural networks with non-differentiable activation functions through surrogate gradient learning. The authors propose a generalized NTK, termed the surrogate gradient NTK (SG-NTK), which utilizes a quasi-Jacobian matrix based on surrogate gradients. This construct is shown to be deterministic in the infinite width limit, with derivations provided for both the NTK and SG-NTK for the sign activation function. The paper includes empirical results demonstrating that the distribution of networks trained with SG-NTK aligns with real-world observations.

### Strengths and Weaknesses
Strengths:
- The paper offers a novel extension of NTK to non-differentiable activation functions, supported by strong empirical validation.
- The presentation is clear, with compelling motivation and aims.
- The derivations appear correct and fill an important gap in the literature.

Weaknesses:
- The complexity of the theoretical framework may hinder accessibility for practitioners.
- Additional experiments on real datasets (e.g., MNIST, CIFAR10, ImageNet) could enhance validation.
- Certain notational clarifications are needed, particularly regarding definitions and divergence in the context of the paper.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding notation, specifically in lines 144, 215, 244, and 289, to ensure that terms are well-defined and easily understood. Additionally, we suggest including average error values between the analytic and empirical kernels to better illustrate divergence, as the current graphical representations may not effectively convey this information. Finally, we encourage the authors to conduct further experiments on real datasets to strengthen the empirical validation of their approach.