ID: 0jZH883i34
Title: Model Sparsity Can Simplify Machine Unlearning
Conference: NeurIPS
Year: 2023
Number of Reviews: 19
Original Ratings: 8, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of leveraging model sparsification to enhance machine unlearning (MU). The authors propose two types of sparsification methods: data-independent and data-dependent, and empirically validate their effectiveness across various datasets and architectures. They introduce a membership inference attack-based metric to evaluate unlearning success, demonstrating that sparsity can improve unlearning performance. Additionally, the authors propose a "Retrain" approach, which involves training the model from scratch without the forgetting dataset, serving as a benchmark for assessing unlearning. They discuss the implications of using the $\ell_1$ norm for sparsity-aware unlearning, noting that while it encourages weight reduction, it does not guarantee strict sparsity. The authors introduce two methods: (M1) "Prune first, then unlearn," which preserves hard sparsity, and (M2) "$\ell_1$-sparse unlearning," which may not achieve strict sparsity. They also explore the impact of a post-unlearning hard thresholding step to enhance M2's performance.

### Strengths and Weaknesses
Strengths:
1. The paper provides a thorough recap of unlearning literature and metrics, aiding readers unfamiliar with the topic.
2. The intuition behind the proposed methods is elegant, particularly the insights into different pruning methodologies.
3. The empirical results are promising, showcasing the benefits of sparsity in unlearning.
4. The authors provide thorough responses and additional experiments that clarify their methodology.
5. They effectively differentiate between their proposed methods and existing approaches, particularly regarding sparsity and unlearning performance.
6. The integration of a post-unlearning hard thresholding step shows a proactive approach to addressing reviewer concerns.

Weaknesses:
1. There is inconsistency in empirical results, with instances where sparsity negatively impacts unlearning without clear patterns.
2. The paper lacks a theoretical analysis, which would strengthen its contributions.
3. The focus on convolution-based architectures limits the generalizability of findings; other architectures, such as transformers, are not explored.
4. The $\ell_1$-sparse unlearning method does not guarantee strict sparsity, which may limit its effectiveness.
5. The authors acknowledge that the performance of their methods may be sensitive to the chosen sparsity levels, which could impact generalization.

### Suggestions for Improvement
We recommend that the authors improve the analysis of empirical results to identify patterns where sparsity may hinder unlearning. Conducting experiments to assess how sparsity's effect on unlearning scales with model size, particularly with varying ResNet sizes, would be beneficial. Additionally, we suggest including a theoretical analysis to complement the empirical findings and addressing the limitations regarding the lack of comparisons with sparsity-aware unlearning in all experiments. We recommend improving the clarity of claims regarding the impact of $\ell_1$-sparse unlearning on model sparsity. Furthermore, we suggest incorporating a more detailed discussion on the limitations of their methods, particularly concerning the tradeoff between unlearning efficacy and generalization. It would be beneficial to explore the implications of shared information in the context of model performance when certain data points are removed. Finally, we encourage the authors to refine their phrasing around the results of their weight distributions to avoid any potential misinterpretation.