ID: hNQJI0KS3T
Title: RDR: The Recap, Deliberate, and Respond Method for Enhanced Language Understanding
Conference: AAAI
Year: 2023
Number of Reviews: 2
Original Ratings: 6, 5
Original Confidences: 4, 3

Aggregated Review:
### Key Points
This paper presents a novel approach to enhance Natural Language Understanding (NLU) models using knowledge graphs, termed Recap Deliberate and Respond (RDR). The authors propose modifying the traditional method by incorporating two additional losses—paraphrase loss on text embedding and graph embedding loss on graph embedding—aiming to improve performance without manipulating NLU benchmarks. Experimental results indicate slight improvements on GLUE benchmarks compared to BERT, RoBERTa, and ALBERT.

### Strengths and Weaknesses
Strengths:  
1. The integration of paraphrasing loss and graph embedding loss for NLU tasks is an interesting contribution.  
2. The paper is well-organized and maintains clarity throughout.

Weaknesses:  
1. The experimental results are limited, showing improvements of less than 1%, and comparisons with other state-of-the-art methods for GLUE are lacking.  
2. The paper does not clearly articulate how RDR addresses the limitations of existing works discussed in the Introduction.

### Suggestions for Improvement
We recommend that the authors improve the alignment between the research question and methodology, ensuring that the proposed approach is evaluated against alternative problems that emphasize its ability to avoid benchmark gaming. A more thorough literature review is encouraged to clarify the differentiation of their approach from previous methods. Additionally, we suggest providing clearer explanations of the differences between the language model in the traditional method and the paraphrasing model in RDR, as well as elaborating on the process of extracting the ground truth subgraph. Finally, including more substantial evidence to support claims of avoiding benchmark manipulation would strengthen the paper.