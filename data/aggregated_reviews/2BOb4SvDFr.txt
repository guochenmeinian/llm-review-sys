ID: 2BOb4SvDFr
Title: Two applications of Min-Max-Jump distance
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 3, 4, 4, 4, -1, -1, -1
Original Confidences: 4, 3, 4, 5, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Min-Max-Jump (MMJ) distance, a new metric for clustering that aims to identify non-spherical clusters by optimizing path distances in a complete graph. The authors propose two algorithms for calculating MMJ, demonstrating its application in k-means clustering and as an internal clustering evaluation index. The paper claims that MMJ improves clustering performance compared to standard k-means and serves as a better evaluation metric.

### Strengths and Weaknesses
Strengths:
- The MMJ metric has intuitive properties and strong connections to minimum spanning trees, enhancing its theoretical foundation.
- The paper includes notable theoretical contributions regarding the properties of MMJ and its efficient calculation methods.
- Experimental results indicate that MMJ-based k-means clustering outperforms standard k-means clustering.

Weaknesses:
- The writing quality is subpar, lacking clarity and coherence, with insufficient exposition and detail in proofs and algorithms.
- The novelty of the work is questionable, as it appears to reiterate existing MST-based methods without adequately contextualizing its contributions.
- The paper's structure is disorganized, particularly in the introduction of distance metrics, leading to a lack of logical flow.

### Suggestions for Improvement
We recommend that the authors improve the writing quality to meet academic standards, providing clearer explanations and justifications for their methods and results. The introduction should be expanded to define the problem, its significance, and relevant literature. We suggest that the authors analyze the relationship of MMJ to existing methods, particularly single linkage clustering, to better contextualize their contributions. Additionally, we encourage the authors to conduct extensive experiments on larger datasets to substantiate their claims of improved performance.