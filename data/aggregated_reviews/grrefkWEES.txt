ID: grrefkWEES
Title: Diffusion4D: Fast Spatial-temporal Consistent 4D generation via Video Diffusion Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 5, 6, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Diffusion4D, a novel framework for generating 4D content that integrates spatial-temporal consistency within a single video diffusion model. The authors propose a method that enhances generation efficiency and 4D geometry consistency, utilizing a curated dataset of high-quality dynamic 3D objects from Objaverse. The framework includes innovations such as motion magnitude reconstruction loss and 3D-aware classifier-free guidance to improve dynamics learning and generation.

### Strengths and Weaknesses
Strengths:
1. This work is the first to directly generate 4D videos using a video diffusion model, addressing spatial-temporal inconsistencies seen in prior methods.
2. The authors curated a large-scale, high-quality dynamic 3D dataset to support future research.
3. The paper is well-written, with reasonable evaluations against state-of-the-art methods and thorough ablation studies.

Weaknesses:
1. The method relies on two models (VideoMV and Diffusion4D), and despite claims of similarity, supplementary videos reveal inconsistencies that affect overall coherence.
2. The current resolution and temporal sequence length limit the realism of generated 4D content, making motion difficult to discern in many samples.
3. Qualitative comparisons lack convincing results, as generated motions appear small and simplistic, necessitating more experimental results to validate claims.

### Suggestions for Improvement
We recommend that the authors improve the consistency of the outputs from the two models by addressing the discrepancies observed in supplementary videos. Additionally, we suggest providing more diverse results to demonstrate the capability of the 4D Gaussian to render high-quality images from various viewpoints, such as fixing the camera at a frontal view or using opposing viewpoints. Furthermore, we encourage the authors to explore the sensitivity of the model's performance to the tuning of the motion magnitude metric and reconstruction loss parameters, as well as to clarify the effects of the motion magnitude guidance term on the overall results. Lastly, a clearer comparison with related work in Section 2, including a table outlining the pros and cons of the proposed method versus existing approaches, would enhance the paper's readability and impact.