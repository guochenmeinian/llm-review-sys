ID: RIfgKCknTu
Title: Online Adaptation of Language Models with a Memory of Amortized Contexts
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Memory of Amortized Contexts (MAC), an online adaptation framework designed to efficiently update large language models (LLMs) using memory augmentation. The authors propose a parameter-efficient approach that encodes new documents into a memory bank, allowing for effective retrieval during question answering (QA). The method is evaluated against existing fine-tuning baselines across multiple QA tasks, demonstrating superior performance in online adaptation scenarios. The paper includes additional analyses and ablative studies to provide insights into the model's design and behavior.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and provides sufficient background for readers to understand the proposed method.
2. The method is efficient and shows strong performance across various datasets and architectures, supporting its generalization ability.
3. The experimental design is reasonable, with appropriate baselines and multiple datasets, indicating the proposed method's effectiveness.

Weaknesses:
1. The proposed method is a natural extension of memory-augmented LLMs and lacks comparisons with context compression methods, such as text token compression with RAG or long-context models.
2. The experimental settings raise concerns regarding the relevance of the evaluated datasets, as they are commonly used in pretraining LMs, making it difficult to assess the benefits of online adaptation.
3. The paper does not adequately address the performance of MAC compared to advanced retrieval methods like BM25 and RAG, which raises questions about its competitive effectiveness.

### Suggestions for Improvement
We recommend that the authors improve their comparisons by including context compression methods and recent retrieval techniques such as DPR, RAPTOR, and IRCoT. Additionally, it is essential to report zero-shot and few-shot results with the base model to provide a clearer understanding of the benefits of online adaptation. Exploring the performance of MAC on larger models and investigating the impact of instruction tuning would also be valuable. Furthermore, addressing the growth of the memory bank and its management during online adaptation should be prioritized, along with providing a theoretical analysis of key design choices.