ID: H3at5y8VFW
Title: Self-Retrieval: End-to-End Information Retrieval with One Large Language Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 6, 6, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Self-Retrieval, an end-to-end information retrieval (IR) system driven by a single large language model (LLM). The model integrates indexing, retrieval, and reranking functions, transforming retrieval into passage generation tasks through self-supervised learning. Experimental results indicate that Self-Retrieval outperforms traditional retrieval methods on benchmark datasets like NQ and TriviaQA. The authors propose that the model's architecture consolidates the multi-step retrieval-augmented generation (RAG) pipeline, achieving superior performance in both retrieval and answer generation tasks.

### Strengths and Weaknesses
Strengths:
1. The integration of all IR functions into a single LLM is a novel contribution, leveraging LLM capabilities for a streamlined approach.
2. The concept of Self-Retrieval is clearly articulated, enhancing accessibility for readers.
3. The paper presents strong experimental results, demonstrating significant improvements over existing retrieval methods.

Weaknesses:
1. The experimental settings differ from existing work, particularly with NQ@40k datasets instead of the standard NQ@320k, which raises concerns about the validity of results. The authors need to clarify this choice and justify the selection of models for different datasets.
2. The "1+2" learning strategy in Section 3.4 requires further explanation, along with experimental evaluations of alternative strategies, such as joint training.
3. Consistency in terminology, particularly the use of "LLMs," should be maintained throughout the paper to enhance readability and professionalism.
4. The reliance on Wikipedia-based datasets raises questions about performance on less structured corpora, and the absence of comparisons with standard two-stage retrieval pipelines is a notable gap.

### Suggestions for Improvement
We recommend that the authors improve the justification for their experimental settings, particularly the choice of NQ@40k datasets, and provide a rationale for model selection across different experiments. Additionally, we suggest that the authors elaborate on the "1+2" learning strategy and include experimental evaluations of joint training. Ensuring consistent terminology throughout the paper will enhance clarity. Finally, we encourage the authors to conduct evaluations on diverse datasets beyond Wikipedia and include comparisons with established two-stage retrieval methods to strengthen their claims.