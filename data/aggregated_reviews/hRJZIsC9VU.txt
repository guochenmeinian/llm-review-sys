ID: hRJZIsC9VU
Title: Introducing Rhetorical Parallelism Detection: A New Task with Datasets, Metrics, and Baselines
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents rhetorical parallelism detection (RPD) as a novel NLP task aimed at identifying textual spans that exhibit parallelism as defined by rhetoreticians. The authors formalize the task, propose several evaluation metrics, and provide a novel expert-annotated dataset of parallelisms in Latin and Chinese texts. They also identify technical challenges, such as tagging schemes, and report performance across various baseline models, indicating both substantial learning and significant room for improvement.

### Strengths and Weaknesses
Strengths:
- Clear problem statement, motivation, methods, results, and discussion.
- Broad interest, particularly in applications of NLP to writing pedagogy.
- Comprehensive scope, including a novel task, formalization, and extensive modeling.
- Detailed methods and appendices that support replication and address diverse technical challenges.

Weaknesses:
- Arbitrary division between "hyperparameters" and "variants," leading to excessive model evaluations on the test set.
- Low inter-annotator agreement scores for the Latin dataset raise concerns about dataset quality and construct validity.
- Lack of clarity regarding the different performance metrics used for early stopping versus critical evaluations.
- The focus on Latin, a less commonly used language, may limit practical applications and intrinsic motivation for the NLP community.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the expected practical utility of the Latin dataset, particularly regarding its relevance to writing pedagogy for classics students. Additionally, we suggest treating architecture and tagging scheme choices as hyperparameters and reporting differences in validation set performance rather than exhaustive test set evaluations. An error analysis would also be beneficial to provide insights into the reported F1 score and enhance understanding of the task. Finally, we encourage the authors to include more examples of different types of parallelism and their prevalence in the datasets to give readers a better feel for the task.