ID: 9cF6RUwMe7
Title: Learning Space-Time Continuous Latent Neural PDEs from Partially Observed States
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 6, 5, 5, 6, 6, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a grid-independent generative model for learning partial differential equations (PDEs) from noisy, partially observed data on irregular grids. The authors employ amortized variational inference for posterior approximation and adapt multiple shooting, a method for efficiently training neural ordinary differential equations (ODEs), to the PDE context. The framework incorporates a spatio-temporal encoder and utilizes a variational formulation to tackle the Bayesian problem, validated on datasets including Shallow Water, Navier-Stokes, and Scalar Flow. Additionally, the authors construct latent state dynamics by integrating classical numerical PDE analysis methods, such as the collocation method and the method of lines, and apply their model to three use cases, including two synthetic analyses and one real-world dataset.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a clear motivation and model formalism.
- The experimental results show solid performance, outperforming DINo and MAgNet on the tested datasets.
- The approach of using a generative model for PDEs is innovative, and the architecture is well-explained despite some notation challenges.
- The concept of multiple shooting analysis for training datasets from dynamical systems with long time simulations is innovative, facilitating cost-effective learning and reducing instabilities.
- The analysis in Appendix D regarding the impact of radius and multiple shooting on model performance adds depth to the evaluation.
- The examples provided demonstrate the framework's applicability to realistic scenarios.

Weaknesses:
- The paper is difficult to read, with brief sections on introduction and related work that lack necessary background.
- The contribution is limited as it primarily improves upon existing methods rather than introducing a novel approach.
- Important experimental details are missing, such as time specifications and the generative aspect of the model, which is not adequately demonstrated in the results.
- The paper contains several heuristic choices and assumptions lacking concrete proofs, such as the arbitrary selection of a linear interpolant and the reliance on neighborhood locations for function derivatives, which hinder a rigorous convergence analysis.
- The novelty of the multiple shooting method is questionable, as similar techniques have been previously explored in literature.
- The paper does not adequately position itself within the existing body of work on model discovery in the PDE community, necessitating a more detailed comparison with methods like SINDy, Neural ODE, and auto-encoder-based approaches.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by expanding the introduction and related work sections to include more background on PDE deep learning and relevant methodologies like Neural Operators. Additionally, the authors should conduct more extensive experiments comparing their method against a broader range of state-of-the-art methods, including evaluations on finer and coarser grids, and testing prediction capabilities over longer horizons. 

It would be beneficial to clarify the time resolution in figures and provide details on the generative aspect of the model, including probabilistic outputs. We suggest moving some formalism to the appendix to make space for these enhancements. Furthermore, we recommend that the authors improve the theoretical foundation of their work by providing concrete proofs for their heuristic choices, particularly regarding the convergence of solutions as the radius approaches zero and the order of error reduction. 

A more comprehensive comparison with existing methods in the model discovery literature is essential for contextualizing their contributions. We suggest exploring alternative interpolants beyond linear interpolation and providing insights into their performance. Clarifying the implications of assuming a Gaussian continuity-inducing prior and addressing potential limitations if the actual process is non-Gaussian would strengthen the analysis. Lastly, enhancing the paper's structure and flow will improve readability and overall contribution to the conference.