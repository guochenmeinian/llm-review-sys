ID: rQYyWGYuzK
Title: Monomial Matrix Group Equivariant Neural Functional Networks
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 5, 7, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an exploration of learning over weight spaces, specifically focusing on neural networks that process other neural networks. The authors propose a novel architecture that extends previous permutation equivariant networks to account for activation-based symmetries, including weight scaling symmetries of ReLU networks and weight sign-flipping symmetries of sine or Tanh networks. The paper formalizes the relevant group of symmetries using monomial matrices and introduces a weight space network architecture that is equivariant to these groups, demonstrating improved parameter efficiency compared to baseline models.

### Strengths and Weaknesses
Strengths:
1. The paper is well-structured and clearly written.
2. It addresses a significant problem in deep weight space learning, presenting a novel architecture that incorporates additional symmetries.
3. The proposed architecture is more parameter-efficient than existing weight space networks.

Weaknesses:
1. The empirical evaluation is limited, lacking diverse and challenging baselines, which diminishes the perceived contribution.
2. The construction of G-equivariant layers is not sufficiently detailed, and the expressivity of the proposed layers may be constrained by the chosen activation functions.
3. There is a lack of discussion regarding the extension of the method to other architectures and activation functions.

### Suggestions for Improvement
We recommend that the authors improve the empirical evaluation by including a broader range of baselines, particularly GNN-based models and DWSNets, to provide a more comprehensive assessment of performance. Additionally, we suggest that the authors include a concrete example of G-equivariant layer construction to clarify their methodology. Expanding the discussion on the limitations of expressivity and the potential for extending the architecture to other activation functions would enhance the paper's depth. Finally, including runtime and memory consumption comparisons with baselines, as well as conducting ablation studies on design choices, would strengthen the overall contribution.