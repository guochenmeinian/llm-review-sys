ID: qjqJL2lfkH
Title: Rank-1 Matrix Completion with Gradient Descent and Small Random Initialization
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 8, 5, 6, 4, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 3, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on the global convergence of the gradient descent (GD) algorithm for the rank-1 matrix completion problem, demonstrating that GD, initiated from a small random initialization, converges to the ground truth matrix in polynomial time with near-optimal sample complexity. The authors show that the incoherence regularizer may be unnecessary for this problem and provide sufficient conditions for convergence, including a coherence assumption on the initialization scale.

### Strengths and Weaknesses
Strengths:  
- The results are novel and relevant to the optimization and machine learning communities, particularly regarding the convergence properties of GD without explicit regularization.  
- The paper is well-structured, with clear derivations and supportive simulations that enhance understanding.  
- The analysis is primarily based on dynamical systems, which could be applicable to related problems.

Weaknesses:  
- A more detailed comparison with existing literature, particularly with Chen et al. (2020), is needed to contextualize the findings.  
- The assumptions made, particularly regarding the rank-1 case, may be overly restrictive, limiting the paper's broader applicability.  
- The presentation of proof ideas could be improved for clarity, and some sections are considered too lengthy.

### Suggestions for Improvement
We recommend that the authors improve the comparison with Chen et al. (2020) to highlight differences and contributions more explicitly. Additionally, a more detailed discussion on the significance of analyzing the rank-1 case should be included. We suggest formalizing the global convergence result as a theorem rather than a remark and clarifying the parameters related to convergence time in line 55. Consistency in terminology, such as using "GD" throughout, should be ensured. Furthermore, we advise reducing the length of the proof discussions in Sections 4-5 by approximately two pages for better readability. Lastly, addressing the noise conditions and providing a clearer explanation of the implications of coherence assumptions would enhance the paper's rigor.