ID: uOEeui0rL7
Title: Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 5, 7, 7, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Human Guided Exploration (HUGE), a system that integrates human feedback within the Goal-Conditioned Reinforcement Learning (GCRL) framework to enhance learning in robotic tasks. The authors aim to balance exploration by utilizing human feedback to identify states needing further exploration, building on traditional GCRL algorithms and the GO Explore framework. A target distance estimation function is trained using binary feedback, which aids in generating precise subgoals. The Goal-Conditioned Supervised Learning (GCSL) paradigm is then employed to establish interaction strategies with the environment. Experimental results in Mujoco and Pybullet indicate that HUGE surpasses previous GCRL and Human-in-the-loop algorithms.

### Strengths and Weaknesses
Strengths:
- The integration of human feedback into the GCRL setting is well-conceived, providing a clear motivation and logical methodology.
- The technique offers a straightforward interface for human labelers to provide binary evaluations, effectively guiding exploration.

Weaknesses:
- The innovation presented is somewhat limited, primarily combining existing methodologies (GCRL, Go Explore, and Human Preference) without significant novelty.
- The paper's results may not be surprising, as human feedback typically enhances performance by introducing prior knowledge, and the tasks evaluated are not sufficiently challenging.
- The evaluation tasks lack complexity, and the main results do not include comparisons for Sim2Real tasks or PickNPlace, which could provide a more comprehensive assessment.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their approach by exploring more original methodologies rather than relying on existing frameworks. Additionally, consider evaluating more complex real-world tasks to better demonstrate the effectiveness of HUGE. We suggest including clearer numeric results in the main text for task performance comparisons and addressing the limitations of the PPO baseline by training it to convergence with a fine-tuned reward function. Lastly, clarify the definition of "closer" in the context of human intuition versus mathematical distance in the model $f_\theta$.