ID: KCe98ynJl3
Title: Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel evaluation metric, FFLM, aimed at zero-shot faithfulness evaluation for summarization, utilizing the probability output of foundation models. FFLM combines changes with prior and conditional probabilities and demonstrates effectiveness through extensive experiments and ablation studies across various benchmarks, even with a comparatively small backbone model and no fine-tuning. The authors clarify concepts like extrinsic hallucination, which is often ambiguous in recent work, and provide a well-structured comparison of prompting versus instruction-tuning.

### Strengths and Weaknesses
Strengths:
- The metric's guidelines are clearly motivated and independent of other tasks and dataset-specific expertise.
- The paper offers a comprehensive comparison with numerous models across various datasets, showing strong correlation with human ratings.
- The formulation of FFLM is intuitive, and the extensive experiments support its claims.

Weaknesses:
- The proposed metric closely resembles HaRiM, with only slight modifications, and HaRiM_LLaMa outperforms FFLM in human correlation.
- Advances over state-of-the-art methods are minimal, and the work is described as incremental.

### Suggestions for Improvement
We recommend that the authors improve the distinction of their approach from HaRiM, particularly in conceptual terms, to clarify the novelty of their contribution. Additionally, we suggest exploring the use of smaller models, such as Flan-T5, for experiments and providing results based on tuning combination weights on the development set. Finally, addressing the correlation of metric components and the adjustment of score or weighting parameters to account for different generation prefix lengths would enhance the paper's rigor.