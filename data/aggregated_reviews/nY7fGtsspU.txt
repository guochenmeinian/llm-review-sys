ID: nY7fGtsspU
Title: Graph Neural Networks Do Not Always Oversmooth
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 7, 5, -1, -1, -1
Original Confidences: 3, 3, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the over-smoothing effect in Graph Convolution Networks (GCNs) in the infinite width limit, leveraging their Gaussian processes equivalence. The authors propose a generalization of deep information propagation to GCNs, linking ordered and chaotic phases in deep neural networks to over-smoothing and non-over-smoothing regimes in GCNs. They demonstrate that initializing weights with large variance can effectively escape over-smoothing in GCNs.

### Strengths and Weaknesses
Strengths:  
1. The theory of applying deep information propagation to graphs is novel and yields interesting findings, providing a fresh perspective on over-smoothing in GNNs.  
2. The paper is well-structured and articulates the implications and limitations of the work in detail.  
3. The investigation addresses a fundamental problem in GNNs, with a clear motivation and insightful experiments presented well.  

Weaknesses:  
1. The analysis employs a column shift stochastic shift operator, which may not translate well to practical GCN constructions using degree-normalized adjacencies. Empirical evidence using CSBM could strengthen this aspect.  
2. The theoretical analysis is confined to the infinite width limit, which may not hold in finite width settings, although this is not a critical weakness.  
3. The transfer of the developed theory to practical networks is minimal, as real-world data is not utilized in Sec. 4.3.  
4. The derivation of Eq. (17) lacks clarity without further explanation, and no implementation is provided, with no rationale given for this omission.  

### Suggestions for Improvement
We recommend that the authors improve the paper by evaluating their findings on at least one real graph dataset, such as Cora or Citeseer, to provide empirical validation. Additionally, exploring whether the results extend to other graph neural networks like GAT would enhance the work's applicability. Clarifying the significance of considering $\epsilon$ noise in $Y$ (eq. 2) and providing a more detailed explanation of the derivation of Eq. (17) would also be beneficial. Finally, including a comment on the computational costs of the method in practical terms, as suggested, would add valuable context for readers.