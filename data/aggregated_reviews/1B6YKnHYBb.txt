ID: 1B6YKnHYBb
Title: De novo Drug Design using Reinforcement Learning with Multiple GPT Agents
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 6, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method named MolRL-MGPT for drug molecular generation, utilizing GPT-based agents to iteratively generate candidate compounds while employing a special reward signal to encourage exploration in diverse directions. The authors evaluate their method on the GuacaMol benchmark, demonstrating its superiority over previous state-of-the-art methods. They also provide detailed hyper-parameter settings and resource consumption for their experiments, emphasizing the efficiency of their method on the ChEMBL dataset. Furthermore, the authors clarify the distinctions between their work and previous methods, particularly regarding the applicability of diffusion-based and multi-agent reinforcement learning (MARL) techniques to *de novo* drug design.

### Strengths and Weaknesses
Strengths:  
1. An effective method for de novo drug design is proposed, showcasing significant results on public benchmarks.  
2. The introduction of a special reward signal promotes agent diversity.  
3. The use of multiple agents in small molecule optimization is original and beneficial.  
4. The authors demonstrate the effectiveness of their approach through comprehensive benchmarking across multiple tasks, showing consistent advantages.  
5. Detailed hyper-parameter settings and resource consumption are provided, enhancing reproducibility.  
6. Clear distinctions are made between their method and other existing approaches, addressing potential concerns about applicability.  

Weaknesses:  
1. The term "RL-based" is misleading; the method lacks a true RL objective function.  
2. The approach does not adequately address the varying difficulties of different generation timesteps.  
3. The agents operate independently without sharing experiences, neglecting potential correlations.  
4. The performance of the code is slow, and the results are based on a single seed, raising concerns about reproducibility.  
5. Missing comparisons with relevant baseline methods such as MolGPT, GFlowNets, and diffusion models limit the demonstration of the method's superiority.  
6. Concerns are raised regarding the effectiveness of certain equations in the model, particularly Eq. 3 and Eq. 4, which may lack theoretical guarantees and clarity on how they promote diversity.  
7. The relationship between agent rewards and the generation of diverse molecules is questioned, suggesting that the current design may not adequately encourage exploration of the chemical space.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the algorithm's classification by avoiding the term "RL-based" and instead using a more accurate description. Additionally, we suggest incorporating comparisons with existing methods like MolGPT and GFlowNets to substantiate claims of improved diversity. The authors should also conduct experiments with multiple seeds to enhance reproducibility and provide detailed hyperparameter tuning methods and values. Furthermore, we recommend improving the theoretical justification for Eq. 3 and Eq. 4, particularly in demonstrating how these equations effectively encourage the exploration of diverse chemical structures. We encourage the authors to provide clearer evidence that different high-scoring molecules contribute to diversity, as the current framework may not adequately support this claim. Finally, clarifying the significance of the GuacaMol tasks and addressing the potential for agents to share experiences could strengthen the paper's contributions.