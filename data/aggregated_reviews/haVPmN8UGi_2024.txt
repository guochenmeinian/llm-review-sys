ID: haVPmN8UGi
Title: GraphVis: Boosting LLMs with Visual Knowledge Graph Integration
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 6, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents GraphVis, a novel method for enhancing large vision language models (LVLMs) by integrating visual knowledge graphs (KGs) to improve performance on question-answering (QA) tasks. The authors propose a curriculum fine-tuning strategy that begins with basic graph features and progresses to complex reasoning tasks, demonstrating significant performance gains in both textual QA and zero-shot visual question answering (VQA).

### Strengths and Weaknesses
Strengths:
- The introduction of GraphVis effectively integrates structured knowledge from KGs into LVLMs using visual modalities, addressing limitations of traditional text-based representations.
- The sequential curriculum fine-tuning approach is technically sound, enhancing model training on graph features.
- Extensive evaluations across commonsense reasoning QA and VQA benchmarks showcase substantial performance improvements.

Weaknesses:
- The experiments primarily utilize LLaVA-v1.6-Mistral, limiting the demonstration of GraphVis's effectiveness across diverse LVLMs.
- The evaluation of visual graph understanding is insufficient, as only final VQA performance is reported without separate metrics for visual graph comprehension.
- The paper lacks clarity on how the properties of generated graph images affect model performance and does not adequately compare GraphVis with existing methods.

### Suggestions for Improvement
We recommend that the authors improve the diversity of LVLMs tested to validate the effectiveness of GraphVis across various models. Additionally, the authors should evaluate the visual graph comprehension ability using suitable metrics to provide a clearer understanding of its impact. It would be beneficial to include examples demonstrating the effectiveness of Visual Graph Comprehension Fine-tuning. Furthermore, the authors should address the properties of generated graph images and their influence on performance, and provide a more equitable comparison with existing zero-shot approaches. Lastly, including qualitative results of GraphVis versus baseline models would strengthen the paper's claims.