ID: hh6azymUaE
Title: Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 2, 3, 3, 2, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a second-order version of Nesterov's accelerated gradient (NAG) descent and Adagrad for logistic regression by introducing a "quadratic gradient" that incorporates a diagonal approximation to the Hessian. The authors demonstrate the method under both non-private and fully Homomorphic Encryption scenarios, claiming empirical advantages over traditional methods. However, the novelty of the quadratic gradient is questioned, as it appears to be a minor extension of existing work. 

### Strengths and Weaknesses
Strengths:
1. The paper is clearly written, with a well-structured introduction and motivation.
2. The experiments utilize real-world datasets, which have strong practical implications.

Weaknesses:
1. Limited novelty: The quadratic gradient method is largely based on previous work, particularly that of Bonte and Vercauteren, and lacks significant innovation.
2. Lack of clarity: The presentation of the enhanced NAG and Adagrad methods is confusing, with unclear definitions and inconsistencies in notation.
3. Experimental results are unconvincing, often showing lower accuracy and AUC compared to baseline methods, raising questions about the claimed advantages.
4. The datasets used for experiments are not standard benchmarks, and the lack of test set statistics diminishes the credibility of the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the enhanced NAG and Adagrad methods by providing clearer definitions and consistent notation throughout the paper. Additionally, we suggest including a more comprehensive discussion of the novelty of the quadratic gradient method, particularly in relation to existing literature. The authors should also consider using standard benchmarking datasets for their experiments and provide detailed statistics to support their claims. Lastly, we advise addressing the limitations of the proposed methods more thoroughly, including a discussion on the applicability of the quadratic gradient beyond logistic regression.