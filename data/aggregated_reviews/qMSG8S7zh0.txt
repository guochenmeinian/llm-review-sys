ID: qMSG8S7zh0
Title: On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into the calibration of language models, particularly for abstractive text summarization tasks. The authors analyze various calibration methods, including Monte Carlo Dropout, Deep Ensembles, Batch Ensemble, and Spectral-normalized Neural Gaussian Process (SNGP), assessing their ROUGE performance across three public datasets and examining the sequence-level Expected Calibration Error (ECE). The study also identifies failure patterns in probabilistic methods within the NLP community, emphasizing the importance of method selection based on task settings.

### Strengths and Weaknesses
Strengths:
- The paper provides a comprehensive comparison of calibration techniques, adapting them to the LLM setup for summarization tasks, which is valuable for future NLP research.
- The presentation is clear, with logical flow and well-drawn figures and tables.

Weaknesses:
- Some sections lack necessary details, such as formal definitions in Section 3 and the calculation of sequence-level ECE in Section 4.2, which may hinder reader comprehension.
- The experimental setup is limited to a single language model (T5-base), and the novelty of the methods is questioned, given advancements in probabilistic methods for classification tasks.

### Suggestions for Improvement
We recommend that the authors improve the clarity of formal definitions in Section 3 before presenting equations, ensuring that terms like x, y, t, and T are defined. Additionally, provide concrete examples in Section 4.2 to clarify how sequence-level ECE is calculated. In Section 4.3, clarify the meaning of "quality" on the y-axis of Figure 1, and consider revising the experimental design to include multiple language models and long document summarization datasets for a more robust evaluation. Lastly, address the numerous typos and ensure that the paper's drafting is more polished.