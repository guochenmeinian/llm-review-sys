ID: Oo7dlLgqQX
Title: Questioning the Survey Responses of Large Language Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 6, 8, 7, -1, -1, -1, -1, -1
Original Confidences: 2, 5, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an evaluation of 42 different language models (LLMs) using the American Community Survey (ACS) to investigate how model responses are influenced by ordering and label biases. The authors challenge the methodology of using LLMs as proxies for human responses in surveys, revealing that biases lead to responses that trend towards uniform distributions, particularly for subgroups with aggregate statistics closest to uniform. The study highlights the importance of understanding the limitations of LLMs in representing demographic characteristics.

### Strengths and Weaknesses
Strengths:
- The study is well-designed, incorporating a diverse range of LLMs, including both base and instruction-tuned models, yielding relevant insights for researchers using LLMs to study human sub-groups.
- The paper effectively challenges previous methodologies, demonstrating that LLMs should not be used out-of-the-box for census data.
- The writing is clear and the results are presented concisely, making the findings accessible.

Weaknesses:
- The expectations of model behavior based on training data are not sufficiently discussed, leaving unclear what responses should be anticipated.
- The exploration of LLM impersonation to assess subgroup-specific responses is lacking, which could clarify the models' abilities to generate relevant answers.
- The reliance on the ACS dataset, primarily focused on demographic information, raises questions about the applicability of findings to opinion-based questions.
- The use of first-token probabilities as a measure may introduce bias, and more advanced prompting techniques could enhance response coherence.

### Suggestions for Improvement
We recommend that the authors improve the discussion on expected model behavior based on their training data, particularly regarding the Common Crawl corpus. Additionally, we suggest exploring LLM impersonation as a method to assess whether models can produce subgroup-relevant responses. To strengthen the analysis, consider including a wider variety of datasets beyond the ACS and addressing the potential biases introduced by first-token probabilities. Lastly, employing advanced prompting techniques, such as Chain-of-Thought, could enhance the coherence of responses.