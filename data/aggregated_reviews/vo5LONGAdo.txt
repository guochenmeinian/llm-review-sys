ID: vo5LONGAdo
Title: Remix-DiT: Mixing Diffusion Transformers for Multi-Expert Denoising
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 7, 7, 5, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Remix-DiT, a modification to the diffusion transformer architecture that utilizes a multi-expert denoiser framework during training and inference. The authors propose using K base models combined with N mixing coefficients to dynamically compute time-specific experts, enhancing efficiency and leveraging task similarities between adjacent intervals. Experimental results on ImageNet indicate that Remix-DiT improves performance across various model sizes while maintaining inference speed comparable to standard DiT models.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and easy to understand.
- The proposed mixing basis strategy is innovative, achieving better performance with fewer parameters than existing multi-expert methods.
- The experiments are well-designed, with comprehensive ablation studies illustrating the impact of various aspects of Remix-DiT.
- The method is simple and effective, making it suitable for practical applications.

Weaknesses:
- There is a lack of experiments validating Remix-DiT's performance against previous methodologies on datasets like FFHQ or MS-COCO, which would strengthen the manuscript.
- The authors do not compare Remix-DiT with DTR and Switch-DiT, which are more parameter-efficient methods addressing multi-task learning in diffusion training.
- The rationale for adopting a multi-expert training approach is insufficiently motivated, particularly regarding quantitative comparisons.
- The performance gain diminishes as the base model size increases, necessitating a more detailed discussion on this issue.

### Suggestions for Improvement
We recommend that the authors improve the manuscript by including comparisons with previous methodologies on additional datasets to validate the performance of Remix-DiT. Additionally, the authors should analyze DTR and Switch-DiT in relation to their method. It would also be beneficial to provide a thorough analysis of the computational resources, time, and energy required for training, along with potential ways to mitigate these costs. Furthermore, we suggest that the authors conduct experiments training all components from scratch to gain insights into the effects of the remixing scheme from the beginning of training. Lastly, including visual examples of generated outputs would enhance qualitative evaluation and strengthen the paper's arguments.