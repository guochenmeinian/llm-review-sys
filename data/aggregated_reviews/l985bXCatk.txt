ID: l985bXCatk
Title: LRVS-Fashion: Extending Visual Search with Referring Instructions
Conference: NeurIPS
Year: 2024
Number of Reviews: 6
Original Ratings: 6, 5, 7, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Referred Visual Search (RVS), a task aimed at retrieving fashion images based on user instructions. The authors constructed a dataset derived from LAION 5B, enriched with metadata using BLIP2, and demonstrated a weakly-supervised training method that outperformed a strong detection-based baseline. The paper introduces a novel task definition, a comprehensive dataset, and a method that leverages category/textual references without requiring object localization or segmentation masks.

### Strengths and Weaknesses
Strengths:
1. The introduction of the LRVS-Fashion dataset, specifically designed for RVS, provides a valuable resource for future research and benchmarking.
2. The authors offer a detailed description of the dataset's construction and methodology, facilitating reproducibility.
3. Comprehensive code is provided, allowing readers to replicate the experiments.

Weaknesses:
1. The latent information \( c_t \) is inadequately explained in the method description, and the asymmetrical setup is not sufficiently detailed in the text.
2. The evaluation appears to use only one reference instruction per complex image in the test set, which may not accurately reflect the model's robustness.
3. The dataset relies heavily on synthetic labels, raising concerns about the quality of generated metadata and its implications for retrieval performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the latent information \( c_t \) in the method description and ensure that the asymmetrical setup is adequately reflected in the text. Additionally, we suggest that the authors address the potential limitations of their dataset, particularly regarding the lack of complex backgrounds and the implications for real-world applications. It would be beneficial to include a quality check on the generated metadata and provide more comprehensive benchmarks by comparing with additional existing models. Finally, we encourage the authors to explore the transferability of their methodology to other domains and consider a more diverse representation of fashion images in their dataset.