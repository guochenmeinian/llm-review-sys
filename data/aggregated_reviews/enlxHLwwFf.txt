ID: enlxHLwwFf
Title: Functional Bilevel Optimization for Machine Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 7, 6, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a functional view of bilevel optimization in machine learning, focusing on inner objectives defined over function spaces rather than traditional parametric settings. The authors prove the convergence of their optimization algorithm and benchmark it for regression and model-based reinforcement learning tasks. They develop functional implicit differentiation and functional adjoint sensitivity, allowing for gradient-based algorithms in functional spaces.

### Strengths and Weaknesses
Strengths:  
- The authors address an important problem with broad applications, introducing a functional perspective that extends the applicability of bilevel optimization beyond strong convexity assumptions.  
- The balance of theoretical contributions and experimental validation is commendable, with a solid writing style that enhances comprehension of complex theories.  
- The proposed method shows potential for practical applications, particularly in causal representation learning and model-based reinforcement learning.

Weaknesses:  
- The transition from function space to parameter space is unclear, as the theory assumes operation in function space while experiments focus on parameterizations.  
- The assumptions in Theorem 3.1 are strong, particularly regarding the optimality of inner and adjoint problems, which may not hold in practice.  
- The experimental results lack robustness, with indications that the proposed method underperforms compared to existing baselines in certain scenarios.  
- The paper does not provide a comprehensive comparison with a broad spectrum of existing algorithms, limiting the evaluation of its performance enhancements.  
- There is a lack of a detailed limitations section, which should address practical experimental considerations.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the transition from function space to parameter space, ensuring that the theoretical framework aligns with experimental setups. Additionally, we suggest that the authors provide a comprehensive comparison with state-of-the-art methods, including those capable of handling nonconvex lower-level problems. It would be beneficial to clarify the dependencies on the sub-optimality constants $\epsilon_{in}$ and $\epsilon_{adj}$ in the convergence analysis. We also recommend including a detailed limitations section that discusses practical considerations and potential scenarios where the methods may underperform. Finally, simplifying notation in the main text and enhancing the readability of figures would improve the overall presentation.