ID: DDkl9vaJyE
Title: Brant: Foundation Model for Intracranial Neural Signal
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 7, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a foundation model called Brain Pre-trained Transformer (BPT) for modeling intracranial brain signals. BPT employs a Transformer architecture with separate encoders for temporal and spatial encoding, pre-trained on a large dataset (1.1TB) for reconstructing masked input signals and fine-tuned for various downstream tasks, including forecasting and seizure detection. Experimental results indicate that BPT achieves state-of-the-art performance compared to other models. However, the representativeness of the dataset, which includes recordings from only 9 subjects, raises concerns about the model's generalizability.

### Strengths and Weaknesses
Strengths:
- The originality of BPT lies in its large-scale pre-training on intracranial signals, making it the largest model of its kind to date.
- The methods are technically sound, with well-designed experiments that support the authors' claims.
- The paper is clearly written and organized, making it accessible to readers.

Weaknesses:
- The choice of baselines appears selective, omitting relevant models such as GNN-based methods.
- The limited number of subjects in the pre-training dataset may restrict the learned representations' generalizability.
- The Method section suffers from an overload of notations, and inconsistencies in terminology are present.
- The model's performance evaluation lacks external validation and does not adequately address potential privacy concerns related to sensitive neural data.

### Suggestions for Improvement
We recommend that the authors improve the representativeness of the dataset by including more diverse subjects and conducting additional downstream experiments on larger datasets. We suggest including GNN-based approaches listed in Figure 1 as baselines. We also recommend providing a more in-depth analysis of the model's interpretability and discussing how different frequency bands contribute to performance. Additionally, we encourage the authors to justify the choice of patch length and address the temporal dependency within patches, possibly by incorporating a small CNN layer. Finally, we recommend including ablation studies to demonstrate the effectiveness of each component in BPT, focusing on long-range dependency, spatial correlation, and time and frequency domains.