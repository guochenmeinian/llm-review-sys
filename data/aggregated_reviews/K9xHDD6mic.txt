ID: K9xHDD6mic
Title: Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 6, 4, 4, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Graph Mixture of Expert (GMoE) model, which enhances Graph Neural Networks' (GNNs) ability to dynamically select optimal information aggregation experts based on diverse graph structures. The authors argue that increasing structural diversity is crucial for improving GNN generalization. GMoE incorporates multiple independent aggregation functions at each layer, allowing for broader information capture. The authors conjecture that real-world graphs exhibit a degree of heterogeneity, which influences expert selection and is reflected in GMoE's learning outcomes. They address the inference efficiency trade-off and note that training time is comparable to non-MoE baselines, with stability attributed to weight-balancing regularization. Extensive experiments validate GMoE's effectiveness against traditional GCN models and demonstrate improvements on various benchmark datasets.

### Strengths and Weaknesses
Strengths:
- The authors provide a thorough review of existing graph models and propose a novel GMoE architecture that effectively utilizes structural diversity.
- The paper is well-written, making it accessible and easy to follow.
- Experimental results indicate that GMoE outperforms traditional GCN models, showcasing its potential in enhancing GNN performance.
- The authors provide a clear explanation of the heterogeneity in real-world graphs and its impact on expert selection.
- The stability of training due to weight-balancing regularization is a notable strength.

Weaknesses:
- The improvement over GCN appears limited, with only marginal gains reported in several instances.
- Comparisons are primarily made against GCN, lacking evaluations against other state-of-the-art models and MoE-based methods.
- The paper does not address the computational cost of the proposed method, nor does it clarify the selection mechanism for aggregation experts.
- The motivation behind the model and its theoretical underpinnings are inadequately discussed, raising questions about its generalization capabilities.
- There is a lack of empirical evidence demonstrating GMoE's performance across various graph types.
- The authors' claims regarding training time could benefit from more detailed comparisons with specific non-MoE baselines.

### Suggestions for Improvement
We recommend that the authors improve the comparison by including evaluations against other state-of-the-art GNN models, particularly those based on MoE. Additionally, providing a detailed analysis of the computational costs associated with GMoE would enhance the practical understanding of its applicability. The authors should clarify the selection mechanism for aggregation experts and discuss the theoretical foundations of their approach to strengthen the paper's contributions. Furthermore, we suggest restructuring the paper to include a separate preliminary section to clarify the motivation and context for the proposed method. Finally, ensuring that all in-text citations are complete and accurate will improve the paper's overall quality. To substantiate their claims, we also recommend that the authors improve the empirical validation of GMoE by including experiments across a wider variety of graph types and providing more detailed comparisons of training times with specific non-MoE baselines.