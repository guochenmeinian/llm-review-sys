ID: XkMrWOJhNd
Title: Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2
Conference: EMNLP/2024/Workshop/BlackBoxNLP
Year: 2024
Number of Reviews: 2
Original Ratings: -1, -1
Original Confidences: 3, 4

Aggregated Review:
### Key Points
This paper presents an effort to accelerate interpretability research using sparse autoencoders (SAE) by releasing a comprehensive suite of SAEs for various layers of the Gemma 2 models, including 9B, 2B, and selected layers of 27B. The authors detail the training procedures and engineering challenges encountered, providing evaluation results to support the potential benefits for future interpretability research.

### Strengths and Weaknesses
Strengths:
- The paper showcases a massive engineering and computational effort, producing an impressive number of SAEs.
- It has the potential to accelerate SAE research and make it accessible to groups lacking the computational resources to train SAEs at this scale.
- The technical details are dense yet well-organized and readable.

Weaknesses:
- The massive scale of the project raises environmental concerns, and there is uncertainty regarding the longevity and relevance of the specific SAE flavor used. The authors mention utilizing "over 20% of the training compute of GPT-3," but the applications of SAEs are narrower than those of LLMs.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the environmental impact of the resources used in this project and provide a clearer justification for the chosen SAE flavor to ensure its relevance in future research. Additionally, addressing the potential obsolescence of the SAEs could strengthen the paper's contributions.