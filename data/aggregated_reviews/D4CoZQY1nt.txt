ID: D4CoZQY1nt
Title: The Less the Merrier? Investigating Language Representation in Multilingual Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper analyzes multilingual language models (MLMs), focusing on their training, representation, and generation capabilities across various languages. The authors investigate large-scale models like GPT-3 and XLM-R, as well as community-based models such as AfroLM. The study includes a literature review of language representation in these models and conducts three sets of experiments: clustering sentences in embedding space, classifying languages from embedding spaces, and evaluating multilingual generation capabilities. The findings highlight the importance of dialect, script, and language similarity in understanding multilingual models.

### Strengths and Weaknesses
Strengths:
- The paper addresses a timely and significant issue regarding language representation in multilingual models.
- It provides insightful analyses of community-based models and their performance on low-resource languages.
- The consideration of linguistic families, dialects, and scripts enhances the depth of the analysis.

Weaknesses:
- The experimental methodologies, particularly in visualization and clustering, lack robustness, especially the reliance on t-SNE for dimensionality reduction.
- The paper does not adequately discuss the implications of embedding-based results for multilinguality.
- Section 4 appears unnecessary, as it primarily aggregates existing data without providing new insights.
- The paper is undercited, missing key references that could strengthen its claims.

### Suggestions for Improvement
We recommend that the authors improve the robustness of their experimental methodologies, particularly by exploring alternative dimensionality reduction techniques beyond t-SNE. Additionally, we suggest providing more in-depth analysis and discussion regarding the implications of embedding spaces for multilingual models. It would be beneficial to include a comparison with the community-centered model BLOOM among autoregressive models and to reference it in related work. We also encourage the authors to consider including more complex evaluation tasks, such as Named Entity Recognition (NER), to assess deeper language understanding capabilities. Finally, we advise revisiting Section 4 to ensure it adds substantial value to the paper and to enhance the citation of relevant literature in the field.