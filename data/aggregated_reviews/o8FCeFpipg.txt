ID: o8FCeFpipg
Title: Towards the Universal Learning Principle for Graph Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 9
Original Ratings: 3, 6, 3, 3, 5, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 4, 4, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an Adaptive Power GNN (APGNN) that utilizes exponentially decaying weights and a multiple P-hop message-passing strategy to enhance the aggregation of graph information. The authors conduct a theoretical analysis of Lipschitz smoothness and generalization bounds, while experimental results indicate that APGNN performs comparably to existing models on benchmark datasets. However, the paper lacks a comprehensive comparison with recent spectral GNN literature and fails to adequately justify the proposed designs of $\alpha$ and $P$-hop.

### Strengths and Weaknesses
Strengths:
1. The paper effectively combines theoretical analysis with practical experimentation, demonstrating clear writing and logical flow.
2. The generalization analysis on non-GCN graph neural networks is novel and covers a wide range of existing models.
3. The proposed method shows advantages in experiments, particularly in capturing higher-order neighborhood information.

Weaknesses:
1. The literature review is insufficient, lacking comparisons with recent spectral GNN advancements that also utilize polynomial graph filters.
2. The motivations for the designs of $\alpha$ and $P$-hop are not convincingly articulated, raising questions about their practical benefits.
3. The experimental section does not include spectral GNN baselines, limiting the evaluation of the proposed method's effectiveness.
4. The sensitivity of hyperparameters $\alpha$ and $\beta$ is not adequately addressed, and the choice of fixed data splits may lead to overfitting.

### Suggestions for Improvement
We recommend that the authors improve the literature review by including a detailed comparison with recent spectral GNN methods, such as those utilizing Bernstein and Jacobi polynomial bases. Additionally, the authors should clarify the motivations for using $\alpha$ and $P$-hop, particularly in finite $K$ scenarios, and provide experimental results on heterophilic datasets to demonstrate the model's versatility. We also suggest conducting experiments to investigate the learning of different graph filters and addressing the hyperparameter sensitivity more thoroughly. Lastly, a detailed analysis of computational complexity and the interpretability of learned graph filters would enhance the paper's contributions.