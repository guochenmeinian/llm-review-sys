ID: pE3yaP0Eqg
Title: FlatMatch: Bridging Labeled Data and Unlabeled Data with Cross-Sharpness for Semi-Supervised Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 24
Original Ratings: 5, 5, 4, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FlatMatch, a novel semi-supervised learning (SSL) method inspired by sharpness-aware minimization (SAM). The authors propose that FlatMatch optimizes both current weight parameters and nearby parameters to align with unlabeled datasets, enhancing model performance. An efficient variant, FlatMatch-e, is also introduced, which offers higher performance with lower computational costs. Experimental results indicate that FlatMatch, particularly with the fix label trick, surpasses existing methods. However, concerns are raised regarding the fairness of comparisons due to the substantial increase in labeled data through pseudo-labeling. The authors address issues related to the optimization sign in their formulations and assert that these do not undermine their contributions.

### Strengths and Weaknesses
Strengths:
- The idea of applying SAM techniques to SSL is innovative and presents a clear motivation.
- The paper is well-structured, clearly formulated, and easy to follow, with a comprehensive experimental section.
- The originality of using SAM in this context is noteworthy, as it demonstrates extendable effectiveness on large-scale datasets.
- The authors provide a reasonable and effective design, supported by extended experiments and comparisons.

Weaknesses:
- The experimental setup, particularly the fix label trick, is problematic as it significantly increases the labeled dataset size, potentially skewing results.
- Some reviewers questioned the evaluation scale and the efficiency of FlatMatch, particularly regarding the implementation of sharpness using SAM.
- The analysis of loss landscapes is unclear and contains contradictions, particularly regarding convergence speeds and generalization capabilities.
- There is a lack of experiments on large-scale datasets, which limits the generalizability of the findings.
- Concerns about the optimization sign in formulations remain, despite the authors' claims that this does not affect their methodology or validation.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by providing a fair comparison of FlatMatch with existing methods using the fix label trick. Additionally, we suggest clarifying the analysis of the loss landscape and addressing the contradictions in the text to enhance the paper's rigor. It is essential to include experiments on larger datasets, such as ImageNet, to validate the method's effectiveness in real-world scenarios. We also recommend improving the clarity of formulations by ensuring that all optimization signs are explicitly stated. Furthermore, we encourage the authors to explore additional optimization techniques related to SAM to enhance the efficiency of FlatMatch and provide more detailed explanations on how to stabilize gradient estimation when labeled data is scarce, to further clarify the limitations and potential of their method in such scenarios.