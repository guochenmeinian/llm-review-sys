ID: 4XIKfvNYvx
Title: Iterative Reasoning Preference Optimization
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 6, 5, 5, 8, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an iterative training algorithm aimed at enhancing a model's Chain-of-Thought (COT) capabilities in reasoning tasks through preference optimization. The authors propose a method that generates multiple reasoning steps, evaluates them against ground truth labels, and incorporates successful outputs into subsequent training iterations. Experiments on the GSM8K, MATH, and ARC benchmarks demonstrate significant improvements over baseline models, with continuous enhancements noted across iterations.

### Strengths and Weaknesses
Strengths:  
- The paper introduces preference optimization into self-improvement for reasoning tasks, yielding promising results.  
- The iterative approach shows significant performance boosts on various tasks, outperforming existing methods like DPO and SFT.  
- The evaluation is comprehensive, and the improvements are convincing and significant.

Weaknesses:  
1. The experiments only evaluate models on held-in data, with no performance results provided for held-out tasks.  
2. The novelty of the proposed algorithm compared to self-rewarding language models appears marginal.  
3. There is a lack of detailed ablation studies to clarify the sources of improvement, particularly regarding the impact of additional data and the role of the NLL term in model performance.  
4. Hyperparameter optimization details for DPO and SFT are not provided, which raises concerns about reproducibility and comparison.

### Suggestions for Improvement
We recommend that the authors improve the experimental design by including performance results on held-out tasks to provide a more comprehensive evaluation. Additionally, conducting more ablation studies would clarify the contributions of data quantity and preference optimization to the observed improvements. It would also be beneficial to include hyperparameter details for DPO and SFT to enhance reproducibility. Finally, we suggest a more thorough analysis of the importance of the NLL term, potentially by comparing model mistakes with and without this loss.