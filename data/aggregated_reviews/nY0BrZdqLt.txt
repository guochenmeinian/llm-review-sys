ID: nY0BrZdqLt
Title: Time-Reversal Provides Unsupervised Feedback to LLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 8, 7, 7, -1, -1, -1, -1
Original Confidences: 3, 1, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Time Reversed Language Models (TRLMs), which are pretrained on an unlabeled corpus in reverse order and finetuned on instruction-tuned datasets. The authors demonstrate that TRLMs can effectively provide unsupervised feedback to enhance language model performance across various tasks, including reranking, citation attribution, document retrieval, and augmenting safety filters against toxic queries. The paper also offers theoretical insights into the benefits of reverse scoring and empirically validates TRLMs' effectiveness on multiple applications.

### Strengths and Weaknesses
Strengths:
- The introduction of TRLMs is a novel approach that leverages unsupervised feedback for scoring and generation.
- The authors provide a solid theoretical foundation, including formal results and a bipartite graph model to explain TRLM alignment.
- Comprehensive experiments show significant improvements over baselines, such as a 44% increase in citation attribution accuracy and notable gains on the AlpacaEval leaderboard.
- The versatility of TRLMs is highlighted, demonstrating their applicability across different model families and tasks.

Weaknesses:
- The assumptions made for theoretical results in Section 4 are stylized and may not hold in practice, with the hallucination model being overly simplistic.
- The paper lacks a thorough discussion of the computational costs associated with using TRLM for scoring and reranking.
- Clarity could be improved in certain sections, particularly regarding the purpose of components in Table 1 and the overall structure of the paper.

### Suggestions for Improvement
We recommend that the authors improve clarity by adding a "Problem Statement" section to introduce the various use cases before presenting the model. Additionally, we suggest that Section 6 be considered as a subsection of Section 5 for better organization. The authors should also clarify the presentation of results, ensuring that improvements are expressed in absolute points rather than percentages. Furthermore, a deeper literature search is advised to address missed citations related to backward-trained models. Lastly, a more extensive discussion on potential biases, fairness concerns, and privacy implications of using reverse query generation for safety filtering would enhance the paper's depth.