ID: VFqzxhINFU
Title: StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 7, 6, 8, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 5, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents StoryDiffusion, a framework utilizing diffusion models to generate consistent images and videos based on story texts. It consists of two main components: a novel self-attention mechanism called Consistent Self-Attention, which enhances image consistency and can augment pre-trained diffusion models without additional training, and a Semantic Motion Predictor module that predicts motion conditions in semantic space for smooth video transitions. The combination of these components provides a lightweight, training-free solution to content consistency while maintaining high controllability.

### Strengths and Weaknesses
Strengths:
1. The article is logically structured and easy to understand.
2. Clear code examples facilitate reproducibility of the proposed methods.
3. The Consistent Self-Attention mechanism enhances pre-trained models without additional training.
4. The Semantic Motion Predictor addresses video consistency, ensuring smooth transitions and stable subjects.
5. Extensive experimental results demonstrate the effectiveness of StoryDiffusion.
6. The framework offers an efficient approach to generating consistent visual content.

Weaknesses:
1. The paper lacks comparisons with similar story generation works, despite mentioning methods like IP-Adapter.
2. There are no ablation studies provided to substantiate the claim of plug-and-play modules.
3. A detailed analysis of time and space overhead is missing, despite claims of being lightweight.
4. Quantitative metrics are limited, primarily relying on the CLIP score, and the user study has a small participant pool with insufficient setup details.

### Suggestions for Improvement
We recommend that the authors improve the paper by providing a qualitative comparison with newer story generation tasks such as [1], [2], [3], which are more relevant to their method. Additionally, the authors should include relevant experiments in the main text to substantiate the claim of plug-and-play modules. A clear comparison demonstrating the superiority of StoryDiffusion in terms of inference time and space overhead would also be beneficial. For objective metrics, consider including FVD, and increasing the number of participants in the user study would help mitigate cluster bias in the collected samples. Furthermore, the authors should clarify the process of splitting story text into prompts and provide necessary training details in the main text rather than the appendix.