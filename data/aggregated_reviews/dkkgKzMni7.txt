ID: dkkgKzMni7
Title: Hardness of Learning Neural Networks under the Manifold Hypothesis
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 6, 7, 6, 7, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a hardness result for learning data on manifolds using shallow neural networks within the statistical query model. The authors construct space-filling manifolds that cover exponentially many quadrants of the Boolean cube, establishing bounds on learning hardness for low-dimensional manifolds and extending existing results from Boolean and Gaussian input models to more general geometries. The theoretical findings are supported by synthetic experiments, and the paper proposes a framework for studying data manifold geometry.

### Strengths and Weaknesses
Strengths:  
- The construction of manifolds that are hard to learn is novel, and the paper is clearly written, enhancing readability.  
- The theoretical contributions extend existing hardness results to a broader geometric setting, providing valuable insights into neural network learnability.  
- Empirical evidence supports the theoretical results, and the paper opens new research perspectives in manifold learning.

Weaknesses:  
- The novelty of the hardness results is questioned, as constructing hard-to-learn manifolds is not particularly surprising.  
- The empirical results lack clarity in corroborating the theoretical findings, particularly regarding the scaling of sample complexities.  
- The hardness results apply only to shallow networks, raising concerns about their applicability to deeper networks, as evidenced by existing literature.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation of their theoretical results by exploring various geometries beyond the hypersphere to strengthen their claims. Additionally, the authors should address whether their results hold for deeper networks, considering the implications of existing findings on network depth and curvature. Clarifying the necessity of statistical query access for their hardness results would also enhance the paper's robustness. Finally, a more detailed discussion on the limitations imposed by the chosen class of feedforward neural networks would provide a clearer understanding of the work's applicability.