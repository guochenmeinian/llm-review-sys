ID: L8ifDX5XNq
Title: LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a parameter-efficient fine-tuning (PEFT) method, LISA, which finetunes selected layers while freezing others for specific iterations, later resampling the finetuned layers. The authors benchmark LISA against various tasks, including instruction-following and medical QA, demonstrating that it achieves similar GPU memory consumption as LoRA, with a 1.5x speedup in training and comparable or superior performance across tasks. The main contributions include the simplicity and practicality of LISA, along with thorough experiments supporting its effectiveness.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and easy to follow.
2. The proposed method, LISA, is simple and practical.
3. The experiments are thorough, with results outperforming baseline methods.

Weaknesses:
1. Lack of novelty, as LISA closely resembles the method in reference [46], which pretrains models layer-by-layer. The authors do not leverage their finding of skewed weight-norm distribution to design a better sampling method, opting for uniform sampling instead.
2. Insufficient experimental details regarding optimizer states and an unfair comparison with LoRA, as LISA finetunes the entire model, potentially leading to misleading training time comparisons.
3. Absence of a critical ablation study, as the benefits of LISA could also apply to LoRA, suggesting that a larger LoRA with sampled finetuning might yield similar results.
4. Unintuitive results lacking further analysis, particularly regarding LISA's performance exceeding full finetuning, raising questions about hyperparameter selection and error bars for main results.

### Suggestions for Improvement
We recommend that the authors improve the novelty of LISA by exploring alternative sampling methods that leverage the skewed weight-norm distribution. Additionally, please provide clearer details on optimizer states and ensure fair comparisons with LoRA by accounting for offloaded states in training time assessments. We also suggest conducting a comprehensive ablation study to clarify LISA's advantages and consider including more analysis on hyperparameter selection to enhance the interpretability of results. Lastly, we encourage the authors to refine the presentation, particularly in terms of layout and capitalization consistency.