ID: thbXgJ8gNK
Title: No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 6, 6, 7, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive evaluation of three efficient training algorithms for transformer language models: layer stacking, layer dropping, and selective backpropagation. The authors emphasize the necessity of defining a training budget for performance comparisons and introduce a metric called "reference system time" (RST) to facilitate comparisons across different hardware configurations. The study reveals that these algorithms do not consistently outperform standard training methods, with layer stacking generally yielding better results, while layer dropping and selective backpropagation often show diminished performance. The authors also note that pre-training performance does not necessarily correlate with generalization ability in downstream tasks, urging caution in the application of these methods due to potential overheads. Additionally, the authors clarify the advantages of RST amidst software optimization fluctuations and define "significant improvement," detailing the independent tuning of hyperparameters for each method.

### Strengths and Weaknesses
Strengths:
- The paper's originality lies in its focus on evaluating efficient training algorithms under a defined training budget, introducing RST for cross-hardware comparisons.
- High-quality methodology and thorough experimental setup, with careful analysis of various parameters.
- Clarity in writing and logical organization, making the motivations and findings easily understandable.
- Significant contributions to the field by addressing computational challenges in training language models and providing practical insights.
- The authors satisfactorily addressed previous concerns and broadened the evaluation scope with additional algorithms.

Weaknesses:
- Limited scope, focusing on only three specific algorithms, which may not represent the broader landscape of efficient training methods.
- The novelty is somewhat constrained as the paper revisits existing methods without introducing groundbreaking discoveries.
- Insufficient justification for the superiority of RST over other computational measures.
- An overemphasis on computational efficiency at the expense of model performance quality.

### Suggestions for Improvement
We recommend that the authors improve the justification for the RST metric, clarifying its advantages over traditional measures like wall-clock time. Additionally, the authors should consider expanding their evaluation to include a broader range of efficient training algorithms to enhance the paper's significance. It would be beneficial to unify the experimental design across all methods and datasets to provide clearer results. Furthermore, we suggest that the authors elaborate on the ecological implications of their findings, particularly regarding the potential negative societal impacts of inefficient training algorithms. Lastly, enhancing the clarity of figures and results presentation would significantly improve the paper's educational value, and we recommend further improving the clarity of their definitions and explanations to ensure that all technical terms are accessible to a broader audience.