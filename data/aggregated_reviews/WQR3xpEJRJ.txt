ID: WQR3xpEJRJ
Title: CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a learnable scorer, the CTQ Scorer, designed to select examples for in-context learning in machine translation (MT). The model employs a shallow MLP regressor with COMET targets on 11 tabular features derived from triples of (in-context input \(x_p\), target \(y_p\), and an example's input \(x\)). The method achieves significant improvements in translation quality across low and high-resource languages on two datasets. A notable finding is that using COMET\((y_p, x)\) for selection can yield nearly all gains from tuning the in-context prompt, suggesting a simpler approach than the proposed full MLP model.

### Strengths and Weaknesses
Strengths:
- Significant gains in translation quality.
- Comprehensive analysis and ablation studies.
- Clear and understandable presentation of the methodology.

Weaknesses:
- The proposed model is more complex than necessary, as using one of the 11 features can yield similar results.
- Lack of analysis regarding feature salience.
- Unclear experimental setup for generating training data for the CTQ scorer.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental setup, particularly how training data for the CTQ scorer is generated. It should be explicitly stated which features are new in Section 3.2. Additionally, we suggest addressing the unexpected results regarding BM25's performance compared to random selection. To enhance the paper, consider including comparisons with efficient fine-tuning methods like LoRA or soft prompting. Lastly, please clarify the specific sentence-level MT metric used for training the scorer as early as possible in the paper.