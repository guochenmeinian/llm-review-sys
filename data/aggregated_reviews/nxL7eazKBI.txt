ID: nxL7eazKBI
Title: Model LEGO: Creating Models Like Disassembling and Assembling Building Blocks
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 7, 7, 2, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel Model Disassembling and Assembling (MDA) framework, inspired by biological vision systems, to extract and reassemble task-aware components from CNNs and other deep learning architectures, including graph neural networks. The authors introduce techniques such as contribution aggregation and allocation for disassembling models and propose a parameter scaling strategy for assembling them without retraining. The methodology extends beyond classification to include image detection, segmentation, generation, and factual knowledge subtasks, emphasizing the use of self-supervised learning for feature extraction, which allows for model compression related to defined subtasks. Extensive experiments validate the effectiveness of MDA across various architectures, demonstrating its potential for applications like model compression and decision route analysis. However, the authors acknowledge potential interference issues that may arise during assembly, particularly in Transformers.

### Strengths and Weaknesses
Strengths:  
- The MDA concept is innovative, providing a new perspective on model architecture and enabling flexible reuse of components across multiple deep learning architectures.  
- The methodology and experimental design are clearly articulated, with comprehensive results that validate the approach.  
- The paper is well-written, with clear illustrations and accessible language, enhancing its readability.  
- Comprehensive experimental setups using publicly available datasets, with detailed reporting on model parameters and FLOPs.  
- Effective demonstration of disassembled sub-model performance across various subtasks.

Weaknesses:  
- The rationale behind the proposed metric in section 3.1.1 is unclear, raising questions about its effectiveness in measuring feature contributions.  
- The performance of disassembling diminishes as the number of categories increases, necessitating separate decomposition for optimal results.  
- The claim that assembled models do not require retraining is questionable, as some assembled models perform worse than baselines.  
- The method's applicability to transformer models is not adequately demonstrated, and the absence of systematic comparisons with existing methods limits the evaluation of its technical soundness.  
- Lack of established experimental settings for systematic comparisons with existing methods.  
- Potential performance interference between subtasks when assembling sub-models, particularly in Transformers, without further fine-tuning.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the rationale behind the proposed metric in section 3.1.1 to enhance understanding. Additionally, addressing the performance drop in disassembling with increasing categories could strengthen the methodology. It would be beneficial to provide a systematic comparison with existing methods, such as deep model reassembly, to substantiate the claims of MDA's advantages. Furthermore, addressing the interference issues in Transformer sub-model assembly through more robust fine-tuning strategies could enhance performance outcomes. Lastly, including more detailed experiments involving transformer models and further exploration of the implications of self-supervised learning on downstream tasks would enhance the generalizability and contributions of the approach.