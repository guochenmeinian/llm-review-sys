ID: iDZQG9aUGH
Title: Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Bayesian approach to multi-task prompt tuning, termed BMTPT, which aims to enhance the performance of pre-trained language models on target tasks by leveraging correlations among source tasks. The authors propose using Stein Variational Gradient Descent (SVGD) to obtain representative source prompts, which are then utilized for initializing target task prompts. Extensive experimental results indicate that BMTPT outperforms existing parameter-efficient fine-tuning methods on various benchmark NLP tasks.

### Strengths and Weaknesses
Strengths:
- Clear writing and well-structured presentation of the paper.
- Thorough experimental analysis demonstrating the method's effectiveness and parameter efficiency.
- The approach shows commendable innovation and captures task correlations effectively.

Weaknesses:
- The proposed method does not demonstrate significant improvements over existing methods, particularly in the lower part of Table 1.
- Lack of computational complexity analysis raises concerns about practical significance.
- Insufficient comparative analysis with important baselines, such as P-tuning v2, and the exclusive use of T5-base limits generalizability.
- The theoretical justification for the proposed method is inadequately detailed, and empirical results show only marginal improvements.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the proposed method and its expected benefits compared to existing approaches. Additionally, including a computational complexity analysis would enhance the paper's practical relevance. We suggest that the authors conduct comparisons with other methods when scaling to larger models and provide precise parameter/task numbers in their tables for better comparison. Furthermore, exploring the use of regular variational inference instead of SVGD and conducting experiments in a few-shot scenario could provide valuable insights. Lastly, incorporating visual or interpretable analyses with concrete natural language statements would strengthen the paper's contributions.