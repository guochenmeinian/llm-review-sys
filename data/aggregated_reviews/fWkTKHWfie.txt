ID: fWkTKHWfie
Title: Fine-Tuning Language Models Using Formal Methods Feedback
Conference: AAAI
Year: 2023
Number of Reviews: 3
Original Ratings: 7, 7, 8
Original Confidences: 4, 5, 3

Aggregated Review:
### Key Points
This paper presents a novel method for planning called direct preference optimization via automated feedback (DPO-AF), which utilizes formal verification to generate preference data. The authors apply this method within an autonomous driving system using the Carla simulator, demonstrating performance improvements. The paper is well-rounded, featuring strong motivation, technical contributions, and a clear evaluation section.

### Strengths and Weaknesses
Strengths:  
- The innovative application of DPO-AF for fine-tuning LLM-based controllers is a significant contribution.  
- The experimental results, particularly in controller construction and the formulation of steps, are commendable.  
- The paper is clearly written, with well-positioned contributions relative to existing research.  

Weaknesses:  
- The sections on interfacing with simulators and formal methods are somewhat dry and could be shortened.  
- The experiments are limited to a single environment, which may restrict the generalizability of the findings.  

### Suggestions for Improvement
We recommend that the authors improve the sections on interfacing with simulators and formal methods by reducing their length and focusing more on limitations, adaptations to new domains, and next steps. Additionally, we suggest expanding the experimental scope to include multiple environments to enhance the robustness of the findings.