ID: DEiNSfh1k7
Title: DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data
Conference: NeurIPS
Year: 2023
Number of Reviews: 6
Original Ratings: 8, 7, 8, 7, -1, -1
Original Confidences: 5, 3, 4, 3, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel dataset of images generated through prompting Stable Diffusion models, aimed at studying mid-level image similarities. The authors collect human perceptual judgments for image triplets using 2AFC and JND tests, and develop a perceptual metric based on large pretrained vision models, which shows high consistency with human judgments. The authors also introduce a new metric, DreamSim, designed to assess image similarity holistically and demonstrate its effectiveness in image retrieval tasks.

### Strengths and Weaknesses
Strengths:
- The paper makes a significant contribution by being one of the first to evaluate AIGC images and provides a valuable dataset (NIGHTS) with human similarity judgments of 20k synthetic image triplets.
- The experiments are well-organized, highlighting the limitations of existing perceptual metrics and validating the proposed metric's effectiveness.
- The feature inversion aspect aligns well with the Analysis-by-Synthesis model evaluation methodology and is an interesting addition.
- The research angle on visual similarity is timely and has practical applications in computer vision.

Weaknesses:
- The definition of distortion is somewhat confusing, as it refers to mid-level changes rather than degradation of image quality.
- The methodology for the JND experiment lacks clarity, particularly regarding participant responses to images with minimal pixel changes.
- The paper does not evaluate all metrics in the image retrieval task, raising questions about the generalizability of the proposed metric and dataset.
- The avoidance of human faces in the dataset limits the exploration of a common subject in visual scenes, and the retrieved images in Figure 8 raise concerns about the model's focus.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the definition of distortion to align it more closely with established literature. Additionally, providing more details on the JND experiment's methodology would enhance understanding. We suggest including a comprehensive evaluation of the proposed metric on classical image quality assessment datasets and testing it on other benchmark datasets of real images. Furthermore, addressing the concerns regarding the retrieval of images with human faces and clarifying the user ratings in the image retrieval task would strengthen the paper. Lastly, we encourage the authors to explore practical use cases for the proposed metric, such as its application as a plug-n-play module in diverse image retrieval tasks.