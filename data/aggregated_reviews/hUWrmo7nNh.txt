ID: hUWrmo7nNh
Title: Bi-Drop: Enhancing Fine-tuning Generalization via Synchronous sub-net Estimation and Optimization
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a batch-wise subnetwork detection and parameter update method aimed at mitigating overfitting during the fine-tuning of pretrained language models. The authors propose leveraging gradient-related information to identify important parameters by repeatedly feeding the current mini-batch data. Experimental results indicate that the proposed method demonstrates superiority over previous approaches, despite modest improvements.

### Strengths and Weaknesses
Strengths:
- The paper includes sufficient experiments, particularly regarding out-of-distribution (OoD) generalization and task generalization, which are compelling in the context of continual pre-training.
- The proposed Perturbation Factor and Scaling Factor are effective and straightforward.

Weaknesses:
- The motivation for using stepwise sub-network detection and parameter updates lacks empirical or theoretical support, raising questions about its advantages over static sub-networks.
- The ambiguity in the rationale for reusing training data to mitigate overfitting requires further clarification and additional references or experiments.
- There are structural concerns regarding the identification of subnetworks, particularly whether attention weights and weights from other layers are treated equally.
- The sensitivity of the proposed method to factors such as the number of feedforward processes (K), batch size, and the highest value of p percentile necessitates detailed analysis.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the motivation behind the stepwise sub-network detection and parameter update, providing empirical or theoretical evidence to support its advantages over static methods. Additionally, please clarify the rationale for reusing training data and consider including more references or experiments to substantiate this claim. Address the structural issues in subnetwork identification by explicitly discussing the treatment of attention weights versus other layer weights. Finally, we suggest conducting ablation studies regarding the impact of K, batch size, and p percentile on the method's performance to better understand its sensitivity to these factors.