ID: yWpY5I3XyX
Title: FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation
Conference: NeurIPS
Year: 2023
Number of Reviews: 17
Original Ratings: 6, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
We note that the paper introduces the FETV benchmark for evaluating text-to-video (T2V) generation, comprising around 600 prompts across three aspects: major content, attribute control, and prompt complexity. The authors highlight the limitations of automatic metrics, advocating for manual evaluation. The work also includes a video edit dataset with 619 prompts and videos, alongside substantial experimental analysis, revealing weak correlations between automatic metrics and human evaluations. Additionally, the proposed benchmark offers unique contributions in evaluating temporal information and multi-aspect categorization compared to existing datasets like Make-a-Video-Eval and MSR-VTT. The emphasis on temporal categories allows for a more comprehensive assessment of T2V models, identifying challenging areas for future development. While the Likert-scale human grading scores provide valuable insights, concerns were raised about the generalizability of human evaluations and the analysis of automatic metrics, particularly regarding CLIPScore.

### Strengths and Weaknesses
Strengths:
- The FETV benchmark offers a comprehensive evaluation framework that includes multiple aspects, enhancing the depth of analysis compared to existing benchmarks.
- The introduction of a temporal-aware and multi-aspect evaluation framework enhances the understanding of T2V model capabilities.
- The authors effectively demonstrate the inadequacies of current automatic evaluation metrics, emphasizing the need for manual assessments.
- The use of advanced metrics, such as UMT for video-text alignment, aligns well with human evaluations.

Weaknesses:
- The paper lacks a robust automatic performance evaluation metric, which is crucial for T2V tasks.
- Insufficient analysis of automatic metrics, particularly regarding CLIPScore, raises concerns about the reliability of evaluations.
- Temporal consistency is inadequately addressed, with discontinuities in textures and actions noted as detrimental to viewer experience.
- The human evaluation methodology is simplistic and may not yield significant insights for future research.
- A limited number of evaluated T2V generation methods may affect the comprehensiveness of the benchmark.

### Suggestions for Improvement
We suggest that the authors develop a solid automatic evaluation metric to enhance the benchmark's reliability. Enhancing the analysis of automatic metrics, particularly by elaborating on the potential of UMT and exploring additional metrics like Vision Question Answering or Video Caption metrics, is essential. Incorporating temporal consistency into the evaluation framework is crucial. Additionally, expanding the categories within the three aspects of the benchmark could provide richer insights. We also recommend evaluating a broader range of T2V generation models, aiming for 6-8 models to ensure a comprehensive benchmark. Providing case studies in supplementary materials to illustrate the dataset's quality and the weaknesses of existing methods would be beneficial. Finally, clarifying the definitions of "unusual prompts" and addressing the occlusion issue in figures will improve the overall clarity of the paper.