ID: biLgaNKYdB
Title: Transforming to Yoked Neural Networks to Improve ANN Structure
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 3, 3, 3, 3, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method called Yoked Neural Networks (YNN) that transforms traditional artificial neural network (ANN) structures into yoked neural networks, enhancing information transfer and performance. The authors analyze the structural bias inherent in ANNs and propose YNN to eliminate it efficiently. In their model, nodes aggregate and transform features while edges dictate information flow. They implement auxiliary sparsity constraints to focus on critical connections and design a small neural module structure based on the minimum cut technique to reduce computational burden. Additionally, the paper critically reimagines traditional neural networks by addressing the limitations of asynchronous tensor flow and hierarchical structures that hinder effective node communication. The authors propose a method enabling synchronous communication among nodes at the same level, enhancing information transformation and overall NN capacity. Their approach draws inspiration from biological neural systems, promoting collaboration among neural units to achieve superior performance and bridging the gap between artificial and biological networks. Experimental results indicate that the learned connectivity surpasses traditional NN structures.

### Strengths and Weaknesses
Strengths:  
1. YNN significantly promotes information transfer, improving method performance.  
2. The model effectively eliminates structural bias in ANN.  
3. The design of a small neural module structure based on the minimum cut technique reduces computational burden.  
4. The proposed method facilitates synchronous communication, significantly improving NN capabilities.  
5. The approach is inspired by biological neural systems, which may enhance the efficiency of artificial networks.  

Weaknesses:  
1. The absence of an ablation study comparing model performance with varying clique sizes or numbers of cuts.  
2. The equations in Section 3.3 are overly complex and difficult to understand, with excessive use of "W" and "w" leading to confusion.  
3. The paper lacks a clear distinction between YNN and Graph Neural Networks (GNN), raising questions about their fundamental differences.  
4. The experimental evaluation is limited to small-scale datasets, and the clarity of the experimental setup is insufficient.  
5. The lack of experimental results raises concerns about the practical applicability and validation of the proposed method.  

### Suggestions for Improvement
We recommend that the authors improve the clarity of the equations in Section 3.3 and reduce the complexity of the notation to avoid confusion. Additionally, conducting an ablation study to compare model performance with different clique sizes or numbers of cuts would strengthen the analysis. A comprehensive discussion contrasting YNN with GNN and self-attention mechanisms should be included to clarify the novel contributions of the proposed method. Furthermore, the authors should enhance the description of the experimental setup, including details on the datasets, metrics used for comparison, and the architectures of the models involved. Finally, providing experimental results to address the concerns raised regarding the effectiveness of their proposed method and explicitly addressing the limitations of the work would provide a more balanced perspective.