ID: e397soEZh8
Title: Learning Structure-Aware Representations of Dependent Types
Conference: NeurIPS
Year: 2024
Number of Reviews: 26
Original Ratings: 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for capturing and utilizing internal compilation states of Agda, a dependently typed programming language, through JSON files that reflect various coding stages. Key contributions include:

- An explanation of how JSON files mirror Agda's dynamic internal state during compilation, capturing the evolution of file content.
- A detailed approach that iterates through each definition, focusing on all sub-terms, enhancing data resource utilization.
- The treatment of each file as a data resource, maximizing content utilization through a type-safe data augmentation routine.
- Recording of typing contexts, including types, definitions, and local variables, while thoroughly explaining structural aspects of terms in Agda.
- Inclusion of pretty-printed displays of each abstract syntax tree (AST) for better readability.

The paper also introduces a novel neural architecture for representing dependently typed programs, demonstrating its effectiveness compared to vanilla transformers.

### Strengths and Weaknesses
Strengths:
- The dataset extracted from Agda is the first of its kind, significantly benefiting the neural theorem proving community.
- The proposed neural representation structure shows strong performance and ablation results, offering better inductive bias and completeness.

Weaknesses:
- The paper assumes familiarity with Agda, making it difficult for non-experts to understand certain concepts and results.
- The experimental setup lacks clarity, particularly regarding the configuration of the Transformer baseline and its training process.
- The importance of the new dataset could be discussed more thoroughly, especially in relation to existing datasets for automated theorem proving.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing a brief introduction to Agda and its dependent types to assist non-expert readers. Additionally, the authors should clarify the experimental setup, particularly how the Transformer is configured and trained. We suggest including illustrative figures to explain complex concepts such as dynamic compilation, typing contexts, and term structures. Furthermore, we encourage the authors to elaborate on the unique contributions of the new dataset in the introduction, clearly connecting these contributions to the details in the remaining sections.