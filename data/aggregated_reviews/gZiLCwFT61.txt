ID: gZiLCwFT61
Title: Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 6, 7, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Skilled Population Curriculum (SPC), an automatic curriculum learning framework for multi-agent reinforcement learning (MARL). The SPC employs a contextual bandit approach to select training tasks for agents, guided by the concept of "Learning Progress," which utilizes metrics such as testing returns to evaluate curriculum effectiveness. The method addresses the forgetting and relearning problem by continuously adapting to the agents' current behaviors. Experiments conducted in the Multi-agent Particle Environment (MPE) and Google Research Football (GRF) demonstrate the method's effectiveness, particularly in GRF, where it accelerates training compared to MARL baselines. The authors also explain that the SPC can rapidly switch to the largest population in a multi-agent environment to maximize rewards.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and clear, effectively balancing intuition and detail.
- The proposed method shows clear benefits in the GRF environment.
- The use of suitable baseline approaches and environments enhances the study's credibility.
- The authors effectively clarify the rationale behind using testing returns as rewards for the teacher, linking it to task selection efficacy.
- The integration of an RNN to capture student learning status as context demonstrates a thoughtful approach to managing non-stationarity in the environment.
- The teacher-student structure is well-defined, utilizing IPPO, HRL, and communication among agents.

Weaknesses:
- The paper lacks adequate baseline comparisons for GRF, making it difficult to evaluate the significance of the proposed method.
- SPC introduces significant computational complexity compared to simpler baselines like IPPO, yet does not report wall-clock time or resource requirements.
- The hierarchical RL component's impact on performance is unclear, and the necessity of its inclusion warrants further discussion.
- The limitations section is brief and could benefit from more thorough exploration.
- The explanation of the forgetting and relearning problem could be more detailed to enhance understanding.
- The paper may benefit from a clearer depiction of how the teacher balances exploration and exploitation in task selection.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of their method by including relevant baseline comparisons for GRF, such as population-based approaches in MARL or other automatic curriculum learning methods. Additionally, the authors should provide information on the computational resources required for training their models to contextualize the complexity of SPC. Clarifying the necessity of the hierarchical RL component and its impact on performance would strengthen the paper. Furthermore, we suggest enhancing the limitations section to address the complexity and applicability of the proposed method in simpler environments. We also recommend improving the explanation of the forgetting and relearning problem to provide greater clarity, as well as elaborating on the mechanisms by which the teacher balances exploration and exploitation during curriculum selection to enhance the comprehensiveness of their approach. Lastly, minor writing fixes and clearer explanations of figures and terms would improve overall presentation.