ID: CNWdWn47IE
Title: DataComp-LM: In search of the next generation of training sets for language models
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 7, 8, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DCLM, a framework for evaluating data curation processes relevant to training Generative Language Models (GLMs). DCLM establishes a standardized environment that encompasses raw datasets, model architectures, hyperparameters, and evaluation tasks, facilitating performance measurement and comparison in data curation. The authors also introduce DCLM-Baseline, a new training dataset for LLMs, which demonstrates superior performance on natural language understanding tasks. The extensive experiments validate the components of DCLM and DCLM-Baseline, providing valuable references for future research.

### Strengths and Weaknesses
Strengths:  
- DCLM enables fair comparisons of data curation methods, and DCLM-Baseline is valuable for language model training.  
- The experimental results are significant and can serve as references for future studies.  
- The paper is well-structured, clearly written, and includes comprehensive documentation.

Weaknesses:  
- DCLM does not introduce new methods for measuring data curation performance.  
- The paper lacks experiments demonstrating the performance gain of DCLM-Baseline over existing datasets and across various model architectures.  
- The requirement for a form to access data may not fully comply with accessibility guidelines.

### Suggestions for Improvement
We recommend that the authors improve the justification for using the Open-LM architecture, as its superiority over alternatives like Llama or Mistral has not been established. Additionally, it would be beneficial to include experiments that demonstrate how data curation performance varies with different model architectures. We also suggest addressing the implications of ethics and toxicity related to the dataset, as well as providing a dedicated page explaining data preprocessing, usage, and statistics. Lastly, clarifying the flexibility in data source, model, and evaluation options in the framework would enhance usability.