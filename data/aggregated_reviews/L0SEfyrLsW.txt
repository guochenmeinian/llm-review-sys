ID: L0SEfyrLsW
Title: SELFOOD: Self-Supervised Out-Of-Distribution Detection via Learning to Rank
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 6
Original Ratings: -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SELFOOD, a self-supervised out-of-distribution (OOD) detection method that utilizes only in-distribution samples for supervision. The authors formulate the task as an inter-document intra-label (IDIL) ranking problem and introduce the IDIL loss for optimization. SELFOOD demonstrates significant performance improvements over state-of-the-art methods across various datasets, specifically in text classification tasks. However, the evaluation is limited to this domain, raising questions about its generalizability.

### Strengths and Weaknesses
Strengths:
- The proposed method is well-motivated, simple, and effective for OOD detection.
- Extensive experiments show significant improvements on benchmark datasets.
- The paper is well-structured and easy to follow.

Weaknesses:
- The contribution may not be sufficient for a long paper, as it focuses solely on text classification without exploring other common tasks like intent classification or sentiment analysis.
- There is a lack of empirical support for claims regarding model calibration and the effectiveness of the IDIL loss compared to traditional methods.
- The paper does not report certain metrics, such as ACC@FPRn, which are crucial for evaluating detection performance.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of SELFOOD by evaluating it on additional NLP tasks beyond text classification, such as intent classification and sentiment analysis. Additionally, we suggest providing empirical results or theoretical analysis to support claims about the calibration of the model and the effectiveness of the IDIL loss. It is also advised to include the ACC@FPRn metric in the main text to clarify the trade-off between detection and classification performance. Lastly, a discussion on how previous self-supervised methods perform with backward propagation would enhance the paper's depth.