ID: jcJVgIFY2r
Title: Generator Born from Classifier
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 8, 3, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for reconstructing an image generator from a classifier without relying on extensive training data. The authors propose a learning paradigm that utilizes a pre-trained classifier's parameters to guide the generator's training, ensuring optimality through the theory of Maximum-Margin Bias (MMB) of gradient descent. The method includes a novel stationarity loss and duality loss, and it is empirically validated across various image generation tasks. While the authors argue that their method may not show significant differences in FID/ISC scores compared to Haim et al. (2022), it operates with a smaller parameter space and less complexity, achieving comparable results at a reduced cost. The authors emphasize that their generator can produce images with clearer outlines and more complete shapes, supporting conditional sampling and attribution editing, which are capabilities not present in Haim et al.'s work.

### Strengths and Weaknesses
Strengths:
- The paper addresses a significant and challenging problem in image generation.
- The proposed method is novel and shows promise, with a clear motivation and logical structure.
- The approach effectively utilizes MMB theory to train a generator without direct access to training data.
- The method demonstrates the ability to generate semantically structured images not present in the original dataset.
- The authors provide empirical validation through convincing proof-of-concept experiments.

Weaknesses:
- The evaluation lacks quantitative metrics such as precision, recall, and FID, relying solely on qualitative assessments.
- The qualitative results of the proposed method are not particularly impressive, which is acknowledged by the authors.
- The experimental setup is simplistic, with basic architectures for both the generator and classifier.
- The current performance metrics, such as FID, indicate that the generator underperforms compared to existing methods.
- The writing quality is poor, making it difficult to follow key components and understand the methodology.
- Important related works on data-free knowledge distillation are not adequately discussed.

### Suggestions for Improvement
We recommend that the authors improve the evaluation section by incorporating quantitative metrics like precision, recall, and FID to strengthen their claims. Additionally, the authors should provide a more comprehensive discussion of related works, particularly in data-free knowledge distillation. Enhancing the clarity and organization of the writing will also help make the paper more accessible to a broader audience. We suggest that the authors explore using more advanced architectures for both the generator and classifier to improve the experimental results. Furthermore, we recommend enhancing the qualitative results of the generator to better demonstrate its capabilities and including further comparisons with other relevant works to strengthen the paper's position in the field. Lastly, we encourage the authors to address any remaining concerns from reviewers to ensure all feedback is adequately incorporated.