ID: DKSI3bULiZ
Title: Multiple Physics Pretraining for Spatiotemporal Surrogate Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 7, 5, 10, 4, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Multiple Physics Pretraining (MPP) model, a pretraining approach for physical surrogate modeling of spatiotemporal systems using transformers. MPP employs a backbone model to predict the dynamics of various heterogeneous physical systems simultaneously, utilizing a shared embedding and normalization strategy for effective learning and transfer across different scales. The accuracy of MPP is validated on fluid mechanics benchmarks, demonstrating superior performance compared to baselines without finetuning. MPP-trained models yield more accurate predictions for downstream tasks than models trained from scratch or through finetuning.

### Strengths and Weaknesses
Strengths:
* The model is robust, outperforming task-specific baselines without finetuning.
* The authors provide open-source code and pretrained models, enhancing reproducibility.
* The method exhibits good transfer capabilities to systems with limited training data.

Weaknesses:
* The network architecture is primarily focused on 2D fluid mechanics, raising questions about its applicability to other domains.
* Validation is conducted only with high-quality data, which may not reflect real-world scenarios.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of the MPP model by addressing its applicability to diverse physics systems beyond 2D fluid mechanics. Specifically, the authors should explore the implications of the compositionality assumption and its validity in complex phenomena like fluid-structure interactions. Additionally, we suggest considering a mixture of experts architecture to potentially enhance performance compared to a single common backbone network. Finally, clarifying the nature and scope of "new physics" in the abstract would strengthen the paper's contribution.