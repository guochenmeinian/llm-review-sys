ID: QGJSXMhVaL
Title: WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 4, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents WorldCoder, a model-based agent that learns world models through program synthesis using a large language model (LLM). The authors define logical constraints to guide interactions with the environment, achieving optimism despite uncertainties in the world model. They prove polynomial sample complexity and conduct empirical studies in environments such as Sokoban, Minigrid, and AlfWorld, demonstrating the potential of their approach. The authors clarify that in the large-compute limit, their method converges to a program that perfectly fits the training examples, although it is only asymptotically complete and relies on the soundness of inherently imperfect LLMs. The paper includes additional experiments and explanations regarding AlfWorld, contributing to its overall value.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel method for learning world models via program synthesis with an LLM, diverging from previous reliance on domain-specific languages.
- It is well-written and easy to follow, with strong empirical results that outperform traditional deep reinforcement learning methods.
- The comparisons between optimistic and non-optimistic models provide valuable insights into the effectiveness of the approach.
- The authors provide a clear explanation of the convergence of their method in the large-compute limit.
- The inclusion of additional experiments and the discussion of AlfWorld enhance the paper's contributions.
- The potential for the method to scale with advancements in LLMs is noted positively.

Weaknesses:
- The reliance on an imperfect LLM and program synthesis method raises concerns about the correctness of the learned world model.
- The evaluation may not be fair, as it is unclear if the compared algorithms are the best or if they are tested against problems where deep reinforcement learning excels.
- The paper lacks solid baselines and does not adequately address scalability or the impact of data contamination on results.
- The absence of a verification mechanism, as seen in AlphaGeometry, is a significant flaw.
- AlfWorld's introduction and state encoding/tokenization are insufficiently detailed in the appendix.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the conditions \(\phi_1\) and \(\phi_2\) by explaining them in plain English and clarifying all notations. Additionally, the authors should address how to handle errors stemming from the use of an imperfect LLM and explore the feasibility of achieving perfect program synthesis. It would be beneficial to include a comparison of the learned model's quality against a true model, rather than solely relying on performance metrics. We also suggest incorporating more robust baselines for comparison, particularly in the contexts of Sokoban, Minigrid, and AlfWorld, and to consider the implications of scalability for larger state spaces. Furthermore, we urge the authors to address the reliance on imperfect LLMs by incorporating a verification mechanism to ensure the correctness of generated solutions. Finally, we recommend improving the introduction of AlfWorld to provide clearer context for readers unfamiliar with it and including the AlfWorld state encoding/tokenization in the appendix to enhance clarity.