ID: a17biETKyI
Title: Improving self-training under distribution shifts via anchored confidence with theoretical guarantees
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Anchored Confidence (AnCon), a self-training algorithm designed to enhance test-time accuracy under distribution shifts. AnCon introduces a temporal ensemble regularization that leverages the consistency of ensemble predictions weighted by their predictive confidences, effectively mitigating the "early-learning phenomenon." The authors provide a rigorous theoretical analysis of the upper bounds on test-time error and demonstrate the algorithm's efficacy through extensive experiments across various distribution shifts. Additionally, the authors clarify notation issues, specifically the change of $m$ to $o$ in Lemma A.4, which represents the number of random Bernoulli samples, asserting that this change does not affect the semantics of other parts of the paper. Revised equations and bounds are provided to support their claims.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and clearly articulates the algorithm's key aspects.
- AnCon offers a novel approach to uncertainty estimation in neural network self-training, showing computational efficiency compared to non-parametric techniques.
- The theoretical contributions, particularly Theorems 3.1 and 3.2, provide solid justification for the algorithm's effectiveness under mild assumptions.
- Empirical results indicate significant improvements in self-training performance across different distribution shifts.
- The authors effectively clarify notation issues, enhancing the paper's clarity.
- The theoretical analysis and empirical results are sound, contributing significantly to the field of self-training and test-time adaptation.
- Positive evaluations from multiple reviewers indicate a strong potential impact on the ML community.

Weaknesses:
- AnCon requires pre-defined regularizer $\lambda$ and discount factor $\beta$, which may limit its flexibility.
- The assumption of at least 50% average accuracy for temporal ensembles in Theorem 3.1 is quite strong.
- Some reviewers expressed concerns regarding the assumption of $\bar{p}(x;c_{0:m}) \geq 1/2$.
- Experimental results lack standard deviations and significant tests, and the calibration improvement lacks theoretical backing.
- The paper does not compare AnCon with relevant state-of-the-art baselines for unsupervised domain adaptation.
- There are still minor issues related to notation and typos that need careful revision.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the connection between AnCon and knowledge distillation, particularly regarding the treatment of pseudo labels. Additionally, we suggest including standard deviations and significant tests in the experimental results, as well as providing a reliability diagram to assess the correlation between accuracy and uncertainty. It would also be beneficial to compare AnCon with more state-of-the-art baselines in Appendix C. Finally, addressing the assumptions made in the theoretical analysis, clarifying mathematical notations, and revising minor notation issues and typos would enhance the paper's rigor and readability.