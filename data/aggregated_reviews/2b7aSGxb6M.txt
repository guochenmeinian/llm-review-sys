ID: 2b7aSGxb6M
Title: MSCFFN: A New FFN with Multi-Space Cross to Accelerate Transformer
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new structure called MSCFFN to replace the feed-forward network (FFN) in Transformers, aiming to reduce computational complexity while maintaining or improving performance on the LRA benchmark. The authors propose splitting the d-dimension vector into n subspaces, applying elementwise products to enhance representation power. 

### Strengths and Weaknesses
Strengths:  
- The MSCFFN theoretically saves over 60% of computations while achieving comparable or superior performance.  
- The proposed method is innovative, leveraging the cross method of subspaces to enhance representation learning.  
- The experimental results show improvements in performance and speed.

Weaknesses:  
- The contribution appears limited, as the complexity remains quadratic, and practical speedup may be less significant with GPU acceleration.  
- The rationale behind the cross method of subspaces is unclear, particularly regarding the selection of pairs and the application of ReLU.  
- The paper lacks sufficient comparison with related work, particularly in the context of existing methods like sparse-MoEs, and does not provide adequate benchmarks beyond LRA.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the cross method of subspaces by explaining the rationale for pairing and the application of ReLU. Additionally, the authors should include comparisons with related work, specifically addressing the references mentioned, and provide more benchmarks to validate the performance claims of MSCFFN. Finally, we suggest conducting a thorough proofreading to correct minor typographical errors and enhance overall presentation quality.