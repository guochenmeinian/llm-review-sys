ID: VkhvDfY2dB
Title: Efficient Exploration in Continuous-time Model-based Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 5, 7, 6, 5, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a continuous-time model-based reinforcement learning method, OCoRL, aimed at controlling fully observed dynamical systems while managing the cost of sampling states. The authors propose a Gaussian process dynamics model and an adaptive measurement selection strategy to optimize sampling times, ensuring convergence to the optimal policy over infinite trials. The method is theoretically analyzed, demonstrating a general regret bound applicable to any measurement selection strategy, and is empirically validated across various dynamical environments.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel approach to continuous-time reinforcement learning, utilizing epistemic uncertainty for measurement selection, a technique not commonly explored in this domain.
- The writing is clear, and the proposed method is well-structured, showcasing both theoretical and empirical analyses.
- The algorithm demonstrates sublinear regret with fewer samples, indicating its potential for practical applications.

Weaknesses:
- The necessity of continuous-time modeling is questioned, particularly regarding its practical implementation and comparison with discrete-time models.
- The first step in the OCoRL algorithm, solving for an optimistic policy, may be time-consuming, raising concerns about its feasibility in practice.
- The experimental results primarily focus on measurement selection strategies without sufficient comparison to existing methods, limiting the evaluation of OCoRL's performance.

### Suggestions for Improvement
We recommend that the authors improve clarity by reorganizing the paper to better connect the various techniques employed and their relationships to the overall concept. Additionally, we suggest including a broader range of comparisons with existing reinforcement learning methods to provide a more comprehensive evaluation of OCoRL's capabilities. It would also be beneficial to address the practical implications of continuous-time modeling versus discrete-time approaches, particularly regarding the necessity and advantages of the former. Furthermore, we encourage the authors to clarify the conditions under which measurements are considered costly and to explore the implications of varying the number of measurements taken in their experiments.