ID: AiMs8GPP5q
Title: Contrastive-Equivariant Self-Supervised Learning Improves Alignment with Primate Visual Area IT
Conference: NeurIPS
Year: 2024
Number of Reviews: 7
Original Ratings: 7, 6, 8, 8, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel contrastive-equivariant self-supervised learning (CE-SSL) framework that enhances traditional self-supervised learning methods by integrating structured variability in response to input transformations. The authors demonstrate that this approach improves the model's alignment with neural activity in the inferotemporal cortex and enhances transfer learning performance. The framework converts standard invariant SSL losses into contrastive-equivariant versions, allowing for effective representation learning without requiring supervised access to transformation parameters.

### Strengths and Weaknesses
Strengths:
1. The CE-SSL framework effectively addresses the excess invariance in traditional SSL methods, aligning better with biological visual systems.
2. The method shows improved alignment with neural responses and enhanced structured variability in representations, validated through comprehensive analyses including the BrainScore pipeline.
3. The paper is well-articulated, presenting a clear discussion of the relationship between the proposed work and prior art in equivariant self-supervised learning.

Weaknesses:
1. The improvements in transfer learning performance are marginal, raising questions about the broader impact of the method.
2. The writing quality, particularly in Section 2, could be enhanced for better readability, with several minor typographical and citation issues noted.
3. The model's performance is sensitive to the hyperparameter Î», and the generalization to out-of-distribution tasks remains limited.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 2 to enhance readability and address the identified typographical errors, such as the incorrect figure references and citation issues. Additionally, we suggest providing a more detailed comparison of the method's performance relative to other techniques in the top rankings to contextualize its effectiveness. It would also be beneficial to explore the robustness of the method with different backbone architectures and augmentation strategies, as well as consider integrating additional forms of self-supervision to further enhance learned representations.