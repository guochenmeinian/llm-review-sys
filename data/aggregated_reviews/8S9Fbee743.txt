ID: 8S9Fbee743
Title: Data-driven Optimal Filtering for Linear Systems with Unknown Noise Covariances
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 8, 6, 4, 7, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an online data-driven method for MSE-optimal filtering of linear systems with linear observations, addressing the duality between estimation and control. The authors propose using stochastic gradient descent (SGD) in the space of steady-state stabilizing gains, claiming asymptotic convergence to the optimal gain and providing an asymptotic probabilistic bound on the deviation from optimal error. The learning problem is framed as a stochastic policy optimization problem, minimizing expected output prediction error while considering unknown covariance matrices.

### Strengths and Weaknesses
Strengths:  
- The exploitation of the duality between control and filtering problems is novel and interesting.  
- The concentration and error bounds provided are non-trivial and useful, particularly the non-asymptotic concentration bound.  
- The proof of SGD convergence and error bounds is original, despite being derived under strong assumptions.  
- The paper is well-organized, with clear formulations and a logical flow from background to analysis.

Weaknesses:  
- The clarity of assumptions and constant definitions is lacking, making it difficult to follow some conclusions.  
- The proof of Theorem 2 is unclear, raising concerns about its soundness, particularly regarding initial policy conditions.  
- The practical setup assumes perfect knowledge of system matrices, which is unrealistic; both system and covariance matrices should be treated as unknown.  
- The paper lacks empirical experiments to substantiate theoretical claims, with insufficient clarity on convergence rates and hyperparameter sensitivity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the assumptions and constant definitions to enhance readability. A more detailed proof for Theorem 2 should be provided to address concerns about soundness. Additionally, discussing the implications of using the surrogate loss in non-observable systems would strengthen the paper. Including empirical results in the main text, plotting figures in log scale to illustrate linear convergence, and comparing results with related works would provide further validation of the proposed method. Finally, addressing the organization of sections, particularly the order of discussing bias in estimated gradients and convergence guarantees, would improve the overall coherence of the paper.