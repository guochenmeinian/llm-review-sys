ID: islVqaCzfa
Title: InstructCoder: Empowering Language Models to Edit Code
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, 5

Aggregated Review:
### Key Points
This paper presents CodeInstruct, a new dataset comprising 114K triplets of natural language instructions, Python code inputs, and outputs for diverse code-editing tasks. The authors construct this dataset by gathering 768 seed tasks from GitHub commits, rewriting commit messages into natural language instructions using Codex, and generating additional examples through ChatGPT, incorporating scenarios for diversity. The authors demonstrate that fine-tuning open-source LLMs (LLaMa and BLOOM) on CodeInstruct yields performance comparable to ChatGPT.

### Strengths and Weaknesses
Strengths:
- CodeInstruct addresses the underexplored area of code editing in the GPT-era, providing a valuable dataset for further research.
- The dataset's innovative incorporation of "scenarios" enhances its diversity and applicability.
- The findings suggest that fine-tuning on this dataset can significantly improve the performance of open-source LLMs.

Weaknesses:
- The comparison of data generation techniques lacks empirical validation against human-written examples from GitHub, particularly regarding the noise in GitHub commits.
- The evaluation methodology is limited, relying on a small test set of 134 examples and primarily on GPT-4 for correctness assessment, which raises concerns about its validity.
- The dataset is exclusively focused on Python, limiting its applicability to other programming languages.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation of their claims regarding the noise in GitHub data and the effectiveness of their data generation techniques compared to human-written examples. Additionally, please clarify the evaluation method, addressing concerns about the small test set and the reliance on GPT-4 for correctness judgments. It would be beneficial to explore output examples in a "diff" format to highlight changes, and ensure that only license-permissive repositories are used for seed data to clarify the dataset's usage rights. Lastly, consider expanding the dataset to include other programming languages to enhance its generalizability.