ID: qp5VbGTaM0
Title: On Softmax Direct Preference Optimization for Recommendation
Conference: NeurIPS
Year: 2024
Number of Reviews: 17
Original Ratings: 5, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an extension of Direct Preference Optimization (DPO) from pairwise to multi-way comparisons using the Plackett-Luce model, particularly in recommendation systems. The authors propose a softmax DPO (S-DPO) that incorporates multiple negative samples to enhance the training of language model-based recommenders. While the paper shows promising experimental results, it is primarily viewed as an application paper rather than a significant theoretical contribution.

### Strengths and Weaknesses
Strengths:
- The paper is technically sound and mostly clearly written.
- The experimental results are promising and well-validated.
- The mathematical formulations are clear and appreciated.

Weaknesses:
- The technical contribution appears somewhat incremental, primarily adapting existing DPO concepts to the recommendation domain.
- The novelty of using multiple negatives in recommendation systems lacks sufficient citation and context.
- Several points require clarification, including the choice of negative samples and the performance metrics used.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding how negative samples are chosen in experiments, as this is not well-explained. Additionally, we suggest including a comparison of S-DPO with DPO using pairs of positive and negative examples, as this would provide valuable insights into performance and runtime. It would also be beneficial to run S-DPO for more iterations to observe performance saturation and to clarify the statistical significance of results presented in figures. Furthermore, we advise restructuring the related work section to provide better context and detail on existing methods. Lastly, addressing the computational complexity of S-DPO in relation to large-scale datasets would enhance the paper's robustness.