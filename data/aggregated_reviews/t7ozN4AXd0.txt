ID: t7ozN4AXd0
Title: Rewiring Neurons in Non-Stationary Environments
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 8, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a bio-inspired rewiring technique to enhance deep reinforcement learning (DRL) in continual learning within non-stationary environments. The rewiring employs permutation matrices for all hidden layers in a multi-layer perceptron (MLP), promoting diverse policy execution and addressing the stability-plasticity dilemma through a novel regularization trick. The method demonstrates state-of-the-art (SOTA) performance with fewer network parameters, supported by ablation studies and robotic control experiments.

### Strengths and Weaknesses
Strengths:
- The writing is clear and accessible.
- The idea is innovative and well-motivated.
- The proposed method is thoroughly evaluated across diverse tasks with comprehensive ablation studies.
- Connections to neuroscience are effectively discussed.

Weaknesses:
- Claims regarding exploration in Section 3.3 lack sufficient empirical support; comparisons with other exploration methods are necessary.
- The performance advantage over CSP is not statistically significant, and the small network size diminishes the impact of reduced parameters.
- Uncertainty exists regarding the rewiring technique's applicability to other architectures like Transformers, CNNs, or RNNs.

### Suggestions for Improvement
We recommend that the authors improve the empirical evidence supporting claims about exploration, particularly by comparing their method with alternatives like pink noise. Additionally, demonstrating that rewiring significantly outperforms CSP with similar model sizes or lower energy consumption would enhance the paper's impact. Clarifying the computational cost and scalability of the rewiring technique, as well as its compatibility with evolutionary algorithms, is essential. We also suggest revising the presentation of ablation studies for clarity and ensuring that the definitions of key terms in figures are explicitly stated. Finally, addressing the potential memory overhead from caching weights and permutations in large networks would strengthen the discussion.