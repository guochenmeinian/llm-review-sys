ID: ZVrrPNqHFw
Title: A Simple Remedy for Dataset Bias via Self-Influence: A Mislabeled Sample Perspective
Conference: NeurIPS
Year: 2024
Number of Reviews: 25
Original Ratings: 6, 5, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 5, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for debiasing biased models by fine-tuning on a subset of bias-conflicting samples identified through self-influence during early training epochs. The authors propose using Bias-Conditioned Self-Influence (BCSI) to detect these samples, allowing for model correction without a clean validation set. The empirical analysis across multiple tasks demonstrates the effectiveness of this approach. Additionally, the authors address spurious correlations in training datasets and evaluate their method using fairness metrics, specifically demographic parity (DP) and equal opportunity (EOP), on the Waterbird dataset, showing significant performance improvements. They also discuss the performance of their method in relation to bias severity in the CIFAR-10C dataset, noting that increasing the pivotal set size can enhance effectiveness, particularly in low-bias scenarios.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and presents a clear hypothesis grounded in thorough analysis.
- The originality of using self-influence for detecting bias-conflicting samples is significant, and the experimental results are promising.
- The authors provide a comprehensive evaluation of their method using established fairness metrics, showing clear performance gains.
- The discussion on the impact of bias severity and the pivotal set size adds depth to the analysis.

Weaknesses:
- The method lacks theoretical justification, raising questions about its failure modes.
- The terminology of "bias-aligned" and "bias-conflicting" is unclear, and the paper contains several grammatical errors and formatting issues.
- The evaluation primarily focuses on accuracy and BCSI score distribution, without adequately demonstrating bias reduction.
- There is confusion regarding the pivotal subset's role in correcting the biased model, necessitating clearer phrasing.
- The response to the performance of the proposed method compared to ERM in various settings lacks clarity.
- There is a need for further exploration of the method's behavior under mislabeling, which is a significant concern in real-world applications.

### Suggestions for Improvement
We recommend that the authors improve the clarity of definitions for "mislabeled samples," "bias-conflicting samples," and "bias-aligned samples" in Section 2.1. Additionally, the authors should address the theoretical underpinnings of their method and explore its performance with various loss functions beyond Generalized Cross Entropy. To enhance the evaluation, we suggest including fairness metrics such as demographic parity and equalized odds, as well as conducting experiments on more datasets from the spurious correlation domain, including CelebA and MultiNLI. Furthermore, we encourage the authors to provide a more insightful analysis of why ERM outperforms their method in certain settings and to discuss potential failure modes, particularly in the presence of mislabeled samples. Lastly, please ensure that the correct metric, Equal Opportunity (EOP), is consistently used throughout the manuscript, and addressing the formatting and grammatical issues throughout the paper will improve readability.