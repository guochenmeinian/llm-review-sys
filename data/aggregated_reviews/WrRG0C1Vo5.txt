ID: WrRG0C1Vo5
Title: Distributionally Robust Ensemble of Lottery Tickets Towards Calibrated  Sparse Network Training
Conference: NeurIPS
Year: 2023
Number of Reviews: 13
Original Ratings: 5, 3, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel Distributionally Robust Optimization (DRO) framework aimed at achieving an ensemble of lottery tickets for improved network calibration. The authors propose that their DRO ensemble learns diverse and complementary sparse sub-networks guided by uncertainty sets, which help capture varying data distributions. The theoretical justification for the calibration performance is provided, demonstrating that the DRO approach effectively lowers the confidence of incorrect predictions. Extensive experiments across multiple benchmarks indicate significant calibration improvements without compromising accuracy or increasing inference costs. Furthermore, the authors have empirically evaluated their proposed technique against existing sparse training methods, incorporating multiple suggested methods to showcase performance. Additional experimental results provided in the rebuttal highlight the performance gap between the proposed technique and existing methods.

### Strengths and Weaknesses
Strengths:
- The authors introduce a unique sparse ensemble framework (DRO) that enhances calibration performance through the learning of complementary sub-networks.
- The robust training process effectively reduces the confidence of incorrect predictions, leading to strong calibration outcomes.
- Comprehensive empirical results validate the effectiveness of the proposed lottery ticket ensemble in competitive classification and open-set detection scenarios.
- The incorporation of suggested sparse training methods enhances the demonstration of the proposed technique's effectiveness.
- Additional experimental results strengthen the empirical evaluation of the paper.

Weaknesses:
- The absence of an ablation study on the feature representations of the subnetworks limits understanding of their ensemble dynamics.
- There is a lack of clarity regarding the evidence for the ERM model's memorization of noisy features introduced by spurious correlations.
- The paper does not adequately address the limitations of existing sparse training methods, overlooking popular static/dynamic approaches that do not require pretraining.
- The theoretical analysis is criticized for lacking rigor and being more motivational than substantive.
- The clarity of Figure 1 requires improvement for better understanding.
- The use of FLOPs in Table 10 may not be as informative as using inference speed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the theoretical analysis to ensure it meets the standards of a formal theoretical contribution. Additionally, conducting an ablation study on the feature representations of the subnetworks would enhance understanding of their ensemble effectiveness. It would also be beneficial to include comparisons with established static/dynamic sparse training methods and to clarify the implications of the findings on the TinyImageNet dataset. Furthermore, we suggest improving the clarity of Figure 1 to enhance comprehension and replacing FLOPs with inference speed in Table 10 to provide more informative data. Finally, we recommend revising the presentation for conciseness and clarity, particularly in explaining the concept of "network calibration" early in the introduction.