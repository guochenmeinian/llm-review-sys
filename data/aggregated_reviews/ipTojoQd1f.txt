ID: ipTojoQd1f
Title: InstaTune: Instantaneous Neural Architecture Search During Fine-Tuning
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: -1, -1, -1
Original Confidences: 4, 5, 3

Aggregated Review:
### Key Points
This paper presents a novel approach to fine-tuning large pre-trained models using one-shot Neural Architecture Search (NAS). The authors propose a framework that leverages the elasticity of existing models, distinguishing their work from traditional fine-tuning methods that focus on tuning subnetworks. They introduce a distillation-based framework that utilizes both a teacher model and randomly drawn subnetworks, optimizing efficiency by limiting the search space. Comprehensive experiments validate their design choices, and the authors claim to achieve Pareto-optimality with their method, suggesting ease of integration into existing training pipelines.

### Strengths and Weaknesses
Strengths:
- The writing is clear and easy to follow, with core concepts well-explained.
- The evaluation encompasses both unimodal and multi-modal Transformer architectures.
- The work represents a novel application of NAS beyond resource-constrained pre-training.

Weaknesses:
- The paper lacks relevant literature comparisons, particularly with efficient fine-tuning methods like LoRA and Adapters, making it difficult to assess its impact on the state-of-the-art.
- There is no limitations or risks section, and the absence of a clear analysis of accuracy drop and sampled networks during fine-tuning raises concerns.
- Reproducibility is compromised due to missing links to code, hardware setup details, and baseline comparisons.
- The practical value of the work is unclear, with insufficient recommendations on how to utilize the proposed metrics.
- Visual elements, such as legends and color distinctions in charts, are not adequately clear.

### Suggestions for Improvement
We recommend that the authors improve the paper by including a limitations and risks section to clarify potential drawbacks. Additionally, a comparison with PEFT baselines such as LoRA and IA3 would strengthen the evaluation. The authors should conduct ablation studies to analyze the impact of the number of sampled networks during fine-tuning. Providing a link to the code and details on the hardware setup would enhance reproducibility. Furthermore, clearer recommendations on the practical applications of their method and improvements to the visual elements in the figures would benefit the overall presentation.