ID: dCYBAGQXLo
Title: Supervised Pretraining Can Learn In-Context Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 8, 7, 6, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel pretraining objective for decision-making tasks, leveraging transformers to predict optimal actions based on the current state and historical interactions. The authors empirically demonstrate that the pretrained model can learn various algorithms implicitly and transfer to unseen tasks. The approach, termed Decision Pretrained Transformer (DPT), is evaluated in both offline and online settings, showing promising results in toy bandit problems and simple MDPs. The theoretical analysis suggests that the pretraining objective aligns with posterior sampling.

### Strengths and Weaknesses
Strengths:
1. The work introduces a pioneering pretraining framework for in-context decision-making tasks, supported by adequate empirical results.
2. The theoretical analysis of the algorithm's generalization capabilities is novel and insightful.
3. The paper is well-written and presents experiments clearly, illustrating the proposed approach effectively.

Weaknesses:
1. The assumption of requiring optimal actions during pretraining may limit scalability to more complex tasks.
2. The unclear benefit of in-context datasets versus optimal actions necessitates an ablation study on context size, which is currently missing.
3. The modification of history-dependent pretraining in Section 6 lacks sufficient clarification.
4. A more comprehensive comparison with previous research, such as retrieval-augmented RL, is needed.

### Suggestions for Improvement
We recommend that the authors improve the scalability of the pretraining objective by addressing the assumption of optimal actions. Conducting an ablation study on context size would clarify the contributions of in-context datasets versus optimal actions. Additionally, we suggest providing clearer explanations regarding the modification of history-dependent pretraining in Section 6. Finally, a thorough comparison with related works in the field should be included to better contextualize the contributions of this paper.