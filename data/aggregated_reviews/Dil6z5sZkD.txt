ID: Dil6z5sZkD
Title: Adversarial Robustness for Large Language NER models using Disentanglement and Word Attributions
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel adversarial attack for Named Entity Recognition (NER) models, utilizing disentanglement and word attribution techniques to enhance robustness against resource-constrained attacks. The authors propose a method that mitigates bias between entity and non-entity words and incorporates a knowledge base and LLM for word substitution. Extensive experiments demonstrate the effectiveness of their approach across multiple datasets.

### Strengths and Weaknesses
Strengths:
- The paper is well-written and easy to follow.
- The proposed method is novel and shows improved performance over previous baselines.
- The research problem is interesting and practical, with clear motivation for the mechanism design.

Weaknesses:
- The threat model is unclear and lacks a dedicated subsection detailing adversary and defender capabilities.
- The motivation for the disentanglement technique is insufficiently analyzed and requires more quantitative and qualitative support.
- Some technical details, such as the context word substitution process and the error in Formula (5), are missing or unclear.
- The paper's novelty is limited, and the analyses are often high-level without sufficient depth.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the threat model by explicitly detailing the adversary's goals, capabilities, and knowledge, as well as those of the defender. Additionally, the authors should provide a clearer explanation of the context word substitution process, including the corresponding prompt or instruction. We suggest enhancing the analysis of the disentanglement mechanism to better justify its effectiveness and exploring the possibility of generating adversarial examples solely through entity word substitution. Furthermore, the authors should revise the title to better reflect the contributions of the paper and ensure that all abbreviations are defined upon first use. Lastly, we encourage the authors to address the concerns about the applicability of their findings to large language models, particularly in light of their experimental results on models like Llama.