ID: UlHueVjAKr
Title: Textually Pretrained Speech Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 6, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents TWIST (Textually Warm-Initialized Speech Transformer-based LMs), a method for initializing SpeechLMs with pretrained textual LMs. The authors demonstrate that this warm start enhances performance compared to random initialization across various metrics and datasets, including a large-scale audio dataset of approximately 150K hours. The work also introduces new evaluation benchmarks for speech-based LMs.

### Strengths and Weaknesses
Strengths:
- TWIST effectively utilizes pretrained textual LMs, showing benefits across different speech LM sizes.
- The paper provides valuable empirical findings and insights into the design of SpeechLMs.
- It introduces new benchmarks for evaluating SpeechLMs.

Weaknesses:
- The lack of theoretical analysis limits the contribution, as the authors do not sufficiently explain the empirical improvements observed.
- The experiments are not comprehensive; only one size of BLOOM/Pythia is considered, and perplexity over speech transcription is not reported.
- Missing automatic evaluation results, such as WER for ASR, complicate comparisons with related work.
- The training costs for warm-start versus cold-start methods are not reported, which would aid in estimating the training costs of large-scale SpeechLMs.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis to substantiate their empirical findings and clarify the mechanisms behind the observed performance gains. Additionally, including perplexity over speech transcription results and automatic evaluation metrics would enhance the robustness of the evaluation. It would also be beneficial to report the training costs associated with both initialization methods to provide a clearer understanding of their implications. Finally, a more detailed analysis of the learned representations with and without TWIST would enrich the paper's contribution.