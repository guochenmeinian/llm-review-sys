ID: VjSxQNhdKs
Title: MCLF: A Multi-grained Contrastive Learning Framework for ASR-robust Spoken Language Understanding
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MCLF, a two-stage multi-grained contrastive learning framework designed to enhance the robustness of spoken language understanding (SLU) systems against automatic speech recognition (ASR) errors. The authors utilize multi-grained contrastive learning alignment at both sequence and token levels as a pre-training objective, alongside various data augmentation strategies to generate transcripts with specific word error rates (WER). The approach demonstrates improved performance across four publicly available SLU datasets.

### Strengths and Weaknesses
Strengths:
- Clear methodology with mathematical formulations and comprehensive experiments on multiple benchmarks.
- Effective use of fine-grained contrastive learning, showing performance improvements, especially with smaller models like TinyBERT.
- Sufficient ablation studies and visual analyses that illustrate the method's effectiveness.

Weaknesses:
- Lack of comparison with end-to-end (E2E) SLU systems, which could provide context for the proposed approach's advantages.
- The novelty of multi-grained alignment is questioned, as similar techniques have been previously employed in SLU systems.
- Insufficient error analysis and exploration of the contributions of different components, such as data augmentation and loss combinations.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the advantages of their robustness approach over E2E SLU systems, even in the absence of empirical comparisons. Additionally, including a thorough error analysis would enhance understanding of the performance improvements attributed to fine-grained contrastive learning. Clarifying the data augmentation techniques and their engineering requirements, as well as exploring the potential benefits of curriculum learning, would also strengthen the paper. Lastly, addressing the underspecified parameter settings and ensuring the reproducibility of results would be beneficial.