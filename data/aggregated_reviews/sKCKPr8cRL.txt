ID: sKCKPr8cRL
Title: Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 2, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents empirical scaling laws for vocabulary size in large language models (LLMs), revealing that the vocabulary size minimizing loss increases with higher FLOPs and decreases with larger embedding sizes. The authors propose two methods for determining optimal vocabulary size: an empirical IsoFLOPs approach and a fast derivative-based approach. The findings indicate that a vocabulary size of 43k outperforms 32k on academic benchmarks like Boolq, emphasizing the critical yet often overlooked role of vocabulary size in model performance.

### Strengths and Weaknesses
Strengths:
- The investigation into vocabulary scaling laws is a novel and interesting research direction.
- The paper is well-written and presents a holistic view of vocabulary size's impact on LLM scaling.
- The introduction of two innovative methods for determining optimal vocabulary size showcases practical contributions.

Weaknesses:
- The results may primarily apply to a limited number of well-funded labs, raising concerns about generalizability.
- The experiments conducted on models up to 3 billion parameters may not adequately represent larger models, and the IsoFLOPs method appears sensitive, indicating insufficient experimental validation.

### Suggestions for Improvement
We recommend that the authors improve the experimental validation by testing models exceeding 7 billion parameters to assess the generalizability of their findings. Additionally, we suggest addressing the uneven distribution of model sizes in Figure 3 (left) to enhance the accuracy of the fitted curve. Clarifying the assumptions regarding the independence of vocabulary size from non-vocabulary parameters and training data in the scaling law context would also strengthen the paper.