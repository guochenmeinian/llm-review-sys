ID: zw3EXf6FIP
Title: IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion
Conference: ACM
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for modeling temporal knowledge graphs (TKGs) using embeddings in multi-curvature spaces, focusing on space-shared and space-specific properties. The authors propose an adjustable multi-curvature pooling module (AMP) to enhance the retention of important information, achieving competitive performance against state-of-the-art (SOTA) temporal knowledge graph completion (TKGC) methods. The experimental evaluation demonstrates significant improvements, particularly on the ICEWS14 dataset.

### Strengths and Weaknesses
Strengths:
- The proposed model outperforms SOTA baselines across three datasets.
- The AMP approach effectively strengthens the retention of critical information.
- The paper is well-structured and easy to follow, with intuitive figures and tables enhancing readability.

Weaknesses:
- The paper lacks source code, which limits reproducibility.
- There is no theoretical analysis of the method's capabilities or complexities.
- The novelty of the pooling method is not sufficiently analyzed, and the rationale for certain methodological choices is unclear.

### Suggestions for Improvement
We recommend that the authors improve the theoretical analysis of the method's capabilities and complexities to provide a clearer understanding of its performance. Additionally, the authors should include a more comprehensive evaluation of the pooling method against established techniques such as average pooling (AP) and max pooling (MP), as well as well-known attention mechanisms like self-attention. Clarifying the reasons behind the sorting of dimensions in the AMP and ensuring consistent terminology throughout the paper would also enhance clarity. Finally, including references to relevant works on hyperbolic and hyperspherical vector operations would make the paper more self-contained.