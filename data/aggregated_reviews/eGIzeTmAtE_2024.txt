ID: eGIzeTmAtE
Title: ColJailBreak: Collaborative Generation and Editing for Jailbreaking Text-to-Image Deep Generation
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 4, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 3, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a jailbreaking framework designed to bypass safety filters in commercial text-to-image (T2I) models, specifically through a multi-step process that combines image generation and editing. The authors propose an attack strategy that transforms safe images into unsafe counterparts, revealing vulnerabilities in existing safety mechanisms. The method is validated on three datasets, demonstrating its effectiveness in producing harmful content.

### Strengths and Weaknesses
Strengths:
1. The method combines generation and editing to bypass safety filters, which is innovative compared to traditional adversarial techniques.
2. The paper is well-written and easy to follow, with extensive evaluation results.
3. The proposed attack is effective against proprietary text-to-image systems in a black-box setting.

Weaknesses:
1. The attack pipeline appears costly and impractical, requiring the inferencing of additional models (SAM and Inpainting). More results regarding the time and computational cost of the attack are needed.
2. The "unsafe components" of the images are edited using open-source models, raising questions about the necessity of generating images through proprietary models.
3. The safety types considered are limited, focusing only on violence, harassment, and self-harm, without addressing other categories like nudity and graphic violence.
4. The test datasets are relatively small, and the cases presented are simple, potentially missing more complex unsafe examples.
5. The evaluation lacks human assessment and relies solely on CLIPScore, which may not adequately measure the alignment between the generated content and the input prompt.

### Suggestions for Improvement
We recommend that the authors improve the practicality of the attack pipeline by providing detailed results on the computational cost and time involved. Additionally, consider discussing the rationale behind using proprietary models for image generation when open-source alternatives exist. Expand the scope of safety types evaluated to include categories such as nudity and graphic violence, and provide more experimental results in these areas. Increase the size and complexity of the test datasets to cover a broader range of unsafe examples. Finally, include human evaluations alongside CLIPScore to better assess the effectiveness of the attack in generating content that aligns with the input prompts.