ID: npJQ6qS4bg
Title: Understanding and Minimising Outlier Features in Transformer Training
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 10, 7, 5, 3, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into Outlier Feature Emergence (OFE) in large language models (LLMs) and proposes potential solutions, including the development of an Outlier Protected (OP) transformer block that empirically reduces OFE while maintaining performance. The authors argue that understanding OFE is crucial for both practical applications, such as improving quantization, and theoretical insights into training dynamics. They explore the relationship between signal propagation and OFE, providing quantitative metrics for analysis. Additionally, the paper addresses quantization failures and offers hyperparameter and architecture tuning guidance that effectively mitigates these issues, demonstrating applicability to larger model sizes (7B). The rebuttal clarifies and strengthens the original submission, indicating that the authors have successfully delivered on their title and abstract.

### Strengths and Weaknesses
Strengths:
- The work clarifies inconsistencies in prior hypotheses regarding OFE and offers novel insights by integrating concepts from signal propagation and entropy collapse.
- The paper is well-written and contextualizes its findings effectively within existing literature, making it accessible to various research communities.
- Empirical results support the proposed metrics and strategies, demonstrating the effectiveness of the OP block in reducing OFE.
- The findings are robust, applicable to larger models, and provide valuable insights into quantization failures.
- The addition of experiments in Table 1 effectively ties together the paper's motivation and key findings.

Weaknesses:
- Many experimental details are relegated to the appendix, which could be integrated into the main text for clarity.
- The experimental scope is limited; while larger-scale experiments with other LLMs are not necessary, they would enhance the paper's significance.
- Writing quality suffers from structural issues and lacks coherence in some sections, making it difficult to follow.
- There may have been an initial perception of underselling the paper's contributions, which could be addressed in the final version.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the manuscript by integrating more experimental details into the main text and ensuring a coherent narrative structure throughout. Additionally, consider providing a formal mathematical definition of the OP block to aid understanding. While large-scale experiments may be challenging, we suggest including experiments with Vision Transformers (ViTs) to broaden the applicability of the findings. We also recommend improving the clarity of the contributions in the title and abstract to ensure they are fully recognized. Finally, incorporating the suggested experiments in Table 1 will enhance the coherence of the paper and further validate the findings.