ID: W4Vk1ufh7l
Title: TRIP: Accelerating Document-level Multilingual Pre-training via Triangular Document-level Pre-training on Parallel Data Triplets
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel pretraining technique that utilizes document-level alignment across three languages, introducing a dataset named MTDD, which includes trilingual document pairs. The authors propose a method that combines document segments from different languages through "grafting," followed by permutation and span corruption, demonstrating improvements over a strong document-level baseline in translation and cross-lingual abstractive summarization. The experiments indicate enhanced performance on various tasks, although the authors acknowledge the need for further analysis on performance heterogeneity across language pairs.

### Strengths and Weaknesses
Strengths:
- The MTDD dataset provides a valuable resource for multilingual document-level pretraining.
- The proposed pretraining technique has minimal requirements, making it adaptable to other datasets.
- The paper is well-written and presents state-of-the-art results.

Weaknesses:
- Claims regarding robustness to uncleanly aligned documents lack sufficient evidence.
- There is a lack of detailed analysis on performance variations across different language pairs.
- The description of the MTDD dataset is vague, particularly regarding statistics and distribution for language pairs.

### Suggestions for Improvement
We recommend that the authors improve the evidence supporting claims of robustness to noisy alignments by including results in the main text rather than just the appendix. Additionally, a more thorough analysis of performance across different language pairs is necessary to substantiate the motivation behind grafting. We suggest providing clearer details about the MTDD dataset, including a comprehensive table of statistics for each triplet language. Furthermore, the authors should clarify their experimental settings, including baseline model training, hyperparameters, and the specifics of pretraining and finetuning processes. Addressing these concerns will enhance the paper's clarity and impact.