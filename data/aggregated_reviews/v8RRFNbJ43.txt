ID: v8RRFNbJ43
Title: Measuring Dejavu Memorization Efficiently
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 5, 7, 6, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method to measure memorization in representation learning models without training multiple models, addressing the computational challenges of existing déjà vu methods. The authors propose a simplified approach using a single model and alternative methods to estimate dataset-level correlations, validated across various image representation learning and vision-language models, yielding results comparable to traditional methods.

### Strengths and Weaknesses
Strengths:
- The proposed method achieves similar aggregate results in measuring memorization as the traditional two-model setup.
- It is validated across multiple datasets and models, including ImageNet-trained and CLIP models, demonstrating consistent results.
- The method can be applied to both image-only and vision-language representation learning models.
- The writing is clear and the paper is well-organized.

Weaknesses:
- The reliance on simpler models like Naive Bayes may limit accuracy in complex scenarios and risk overfitting, potentially skewing results.
- The novelty is reduced as the method builds on existing ideas of dataset-level correlation estimation and déjà vu memorization.
- The results hinge on the smaller classifier not memorizing training data; empirical evidence to support this would strengthen the argument.
- The lack of sample-level memorization scores limits insights, as aggregate metrics do not provide detailed information on specific memorized samples.

### Suggestions for Improvement
We recommend that the authors improve the clarity in Section 4.1.2 regarding the relationship between pre-trained models, memorization, and generalization. Providing empirical results to support the claim that smaller classifiers do not memorize training data would enhance the argument. Additionally, including sample-level memorization scores would offer more granular insights into memorization behavior. Finally, a concluding section with actionable advice or a ranking of models to minimize memorization would better motivate the paper's findings.