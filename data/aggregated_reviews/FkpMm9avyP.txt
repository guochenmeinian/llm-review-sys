ID: FkpMm9avyP
Title: Entropy-dissipation Informed Neural Network for McKean-Vlasov Type PDEs
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 7, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel method for solving McKean-Vlasov equations (MVEs) using an Entropy-dissipation Informed Neural Network (EINN). The authors derive a uniform-in-time bound for the KL divergence between the neural network-derived solution $\rho_t^f$ and the true solution $\bar{\rho}_t$. The method is shown to outperform state-of-the-art approaches on MVEs with singular interaction kernels, specifically Coulomb and Biot-Savart interactions. The theoretical analysis indicates that the error bound does not increase exponentially over time, which is a significant contribution to the field.

### Strengths and Weaknesses
Strengths:
1. The uniform-in-time bound for KL divergence is a strong theoretical result.
2. The method effectively addresses singular interaction kernels, demonstrating improved performance over existing methods.
3. The authors provide solid mathematical proofs and theoretical guarantees for the EINN framework.

Weaknesses:
1. The practical applications of McKean-Vlasov type PDEs are not clearly articulated, which may limit interest from the machine learning community.
2. The computational cost of the proposed approach, which relies on the adjoint and Monte Carlo methods, may not be superior to classical numerical methods like finite difference.
3. The paper lacks a discussion on the neural operator framework for solving PDEs, which is a significant oversight.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the practical applications of McKean-Vlasov type PDEs to engage a broader audience. Additionally, we suggest providing a more detailed comparison of the computational costs between EINN and classical methods, particularly addressing the efficiency of finite difference methods. It would be beneficial to include a discussion on the neural operator framework and its relevance to the proposed method. Finally, enhancing the accessibility of the paper through intuitive explanations or graphical illustrations could help readers without a strong mathematical background better understand the methodology.