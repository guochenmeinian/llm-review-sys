ID: 7EQx56YSB2
Title: Group and Shuffle: Efficient Structured Orthogonal Parametrization
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 4, 4, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel parameterization for orthogonal matrices, termed GS-matrices, aimed at enhancing orthogonal fine-tuning of foundation models. The authors propose structured orthogonal parametrization and demonstrate its application across various tasks, including language modeling and text-to-image generation. The method claims to improve parameter efficiency and training performance compared to existing approaches like BOFT and LoRA.

### Strengths and Weaknesses
Strengths:
- The writing is clear and the overall presentation is well-organized.
- The proposed GSOFT method shows promise in reducing parameters and computation compared to previous methods.
- The GS parameterization is novel and potentially beneficial for orthogonal fine-tuning.

Weaknesses:
- The empirical evaluations are limited, with small-scale experiments and missing training time comparisons in Table 2.
- There is insufficient discussion on the performance of single versus double GSOFT, and the results in different tables are not clearly justified.
- The paper lacks comprehensive comparisons with other advanced methods like DoRA and does not adequately address existing literature on orthogonal matrix parametrization.

### Suggestions for Improvement
We recommend that the authors improve the empirical validation by conducting larger-scale experiments and including wall-clock time comparisons in Tables 1 and 2. Additionally, we suggest providing a detailed discussion on the differences between single and double GSOFT, as well as including ablation studies to clarify their contributions. It would also strengthen the paper to incorporate a review of existing orthogonal matrix methods and to compare the GS parameterization with the Monarch matrices in the experiments. Finally, we urge the authors to release the implementation prior to acceptance to facilitate reproducibility of results.