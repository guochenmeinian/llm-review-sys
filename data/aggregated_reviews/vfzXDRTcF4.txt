ID: vfzXDRTcF4
Title: JourneyDB: A Benchmark for Generative Image Understanding
Conference: NeurIPS
Year: 2023
Number of Reviews: 34
Original Ratings: 6, 5, 8, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
The paper evaluates the image understanding capabilities of generated images by collecting 4 million images from MidJourney and proposing four tasks: prompt inversion, style retrieval, image captioning, and visual question answering. Annotations for these tasks are generated using ChatGPT in a zero-shot manner. The dataset aims to establish benchmarks for image understanding of generated images, particularly from MidJourney, and includes a significant number of annotations from both GPT and human annotators, enhancing its utility for instruction tuning. The authors have extended the dataset to include 22 additional text-to-image generative models, improving its diversity and comprehensiveness. The dataset has undergone thorough filtering to eliminate objectionable content, and the authors have actively engaged with reviewers to address concerns about dataset integrity and ethical considerations.

### Strengths and Weaknesses
**Strengths:**
- The dataset is extensive and represents a significant contribution to benchmarks in image generation and understanding.
- The data collection process is clearly explained, and the benchmarks are comprehensive, with a meticulously curated test set ensuring high consistency and quality.
- JourneyDB achieves a Clip Score of 84.67%, significantly outperforming Coco Caption, indicating high consistency between prompts and images.
- The inclusion of multiple text-to-image generative models enhances the dataset's diversity.

**Weaknesses:**
- The paper lacks clarity regarding its primary objective, making it difficult to ascertain its contributions to the field.
- There is no comparison of different generative models or versions, limiting the conclusions that can be drawn about the dataset's utility.
- The reliance on ChatGPT for annotations raises concerns about the quality and reliability of the labels, and there is a lack of clarity regarding the correctness of these annotations and the specifics of the curation process for the test set.
- Ethical issues related to data scraping from Discord and potential copyright infringement are not adequately addressed, raising questions about the legality of scraping data and compliance with terms of use.

### Suggestions for Improvement
The authors should clarify the primary objective of the paper to enhance its impact. They should detail the curation process for the test set, including instructions given to annotators and their demographics, to facilitate reproducibility. Additionally, the authors need to compare different sets of generated images from various versions of MidJourney and other generative models to provide a more comprehensive analysis. It is crucial for the authors to evaluate the quality of ChatGPT's annotations to ensure meaningful benchmarks and address the ethical implications of data scraping from Discord, potentially by consulting legal professionals and explicitly addressing the terms of use in the main paper. Finally, the authors should adopt a more formal writing style to align with academic standards and continue to emphasize the diversity of their dataset and the human effort involved in annotation processes.