ID: bHP9hX4SvI
Title: Stability and Generalization of Asynchronous SGD: Sharper Bounds Beyond Lipschitz and Smoothness
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 5, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a generalization analysis of Asynchronous Stochastic Gradient Descent (ASGD) focusing on both smooth and non-smooth (HÃ¶lder continuous) losses. The authors establish sharper stability and generalization bounds under weaker assumptions, removing the need for Lipschitz continuity for generalization error while still requiring it for excess generalization error. The paper also investigates the impact of asynchronous delays, model initialization, and sample size on generalization performance, supported by extensive experiments.

### Strengths and Weaknesses
Strengths:
- The manuscript develops stability-based generalization analysis for ASGD under weaker conditions, enhancing the understanding of asynchronous training.
- The theoretical results are validated through numerical experiments across various optimization problems, demonstrating robustness.
- The paper is theoretically solid, with clear presentation of results and detailed proofs.

Weaknesses:
- The generalization error formulation in Theorem 2 is deemed unreasonable as it depends on variables like \( w_1 \) and \( w_k \) rather than fixed constants, limiting applicability.
- The requirement for larger \( \tau_k \) to achieve smaller generalization bounds contradicts intuitive expectations regarding latency.
- The novelty of the work is questioned, as it closely follows previous studies, particularly Lei (2021), without sufficiently distinguishing its contributions.
- The paper lacks necessary comparisons with existing works, hindering readers' understanding of its advantages.

### Suggestions for Improvement
We recommend that the authors improve the generalization error formulation in Theorem 2 to depend on constants such as \( \eta, K, n \) to enhance applicability. Additionally, clarify the counterintuitive finding regarding larger \( \tau_k \) leading to better performance. To strengthen the paper's novelty, we suggest providing a more explicit comparison with Lei (2021) and other relevant works. Including a summary table of theoretical results alongside their implications and comparisons would also clarify the contributions. Lastly, consider addressing the necessity of the Lipschitz condition and explore the potential for relaxing this assumption.