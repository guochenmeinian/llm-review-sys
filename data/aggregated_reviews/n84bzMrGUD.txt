ID: n84bzMrGUD
Title: Clifford Group Equivariant Neural Networks
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 8, 8, 8, 7, 7, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 1, 4, 4, 2, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for creating E(n)-equivariant models based on Clifford (geometric) algebras. The authors identify the Clifford group and its action, extending it to the entire Clifford algebra, which enables the construction of equivariant maps and operations, including grade projection. They propose an equivariant Clifford group neural network that operates on vector fields, offering a more accurate interpretation of underlying structures compared to scalar-field networks. The method demonstrates versatility across various tasks and dimensionalities. Additionally, the authors explore translation and permutation equivariance in the context of $n$-body experiments, enhancing clarity regarding the addition of mean-subtracted particle positions in their methodology. They clarify their approach to permutation invariance, particularly through the use of graph neural networks (GNNs), and express gratitude for the feedback received, which has contributed to improving the manuscript.

### Strengths and Weaknesses
Strengths:
- The high quality of writing and presentation balances technical detail with clarity.
- The paper introduces novel theoretical results based on Clifford algebras, contributing significantly to the field of equivariant neural networks.
- The authors effectively acknowledge and address reviewer feedback, demonstrating a commitment to clarity and accessibility.
- The explanations regarding translation equivariance and the use of GNNs for permutation invariance are insightful and enhance the understanding of the methodology.
- The authors provide a clear rationale for parameter choices in their experiments, ensuring fair comparisons.
- Experimental validation is versatile and largely convincing.

Weaknesses:
- Missing details in the presentation of the proposed method, including translation equivariance, invariant prediction computation, and network architecture in experiments.
- Incomplete comparisons with other methods, particularly regarding the claim that scalar-feature methods fail to extract essential geometric properties and the complexity of the proposed method versus baselines.
- Some details regarding the implementation of GNNs and permutation invariance were initially omitted due to space constraints, which may hinder full comprehension.
- The manuscript may still require further refinement to balance clarity with the constraints of space.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the presentation by addressing the missing details regarding:
- How translation equivariance is achieved, particularly in the context of the n-body experiment, including explicitly stating how the mean-subtracted point is added back to the prediction.
- The computation of O(n)-invariant predictions and the architecture of the proposed networks.
- The experimental setup, including sample sizes and success rates for predictions.
- More detailed explanations of the GNNs used in experiments 5.3 and 5.4 to enhance understanding.

Additionally, we suggest that the authors elaborate on the comparison with scalar-feature methods, particularly regarding the extraction of geometric properties and the complexity of their method relative to baselines. Providing a step-by-step walkthrough of the data preparation and linking the output to use cases in experiments would enhance reader understanding.