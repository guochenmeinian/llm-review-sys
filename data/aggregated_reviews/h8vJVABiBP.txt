ID: h8vJVABiBP
Title: Learning Modulated Transformation in GANs
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 7, 4, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 5, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach to generative adversarial networks (GANs) by introducing a Modulation and Transformation Module (MTM) that predicts spatial offsets based on latent codes. This allows convolution operations to be applied at variable locations, enhancing the model's ability to handle geometric deformations. The authors clarify that their MTM incorporates two noise modulations: one for style modulation from StyleGAN and another for transformation modulation. They emphasize that the learnable offsets in MTM are stochasticity-aware, allowing for varying receptive fields across instances, which is crucial for handling geometric variations. The authors demonstrate that this method can be integrated into various GAN architectures, such as StyleGANv2 and EG3D, without requiring hyper-parameter tuning, and shows improvements in tasks like image, video, and 3D-aware generation. They also address misunderstandings regarding their experimental comparisons and the implications of self-attention in relation to their work.

### Strengths and Weaknesses
Strengths:
- The proposed method is simple, effective, and shows strong performance across various tasks.
- The experimental results are robust and contribute significantly to the field.
- The paper is well-written and easy to follow, with extensive experiments supporting the claims.
- The integration of deformable convolution into GANs is a unique contribution that has not been extensively explored in prior literature.
- The authors clarify key concepts that enhance understanding of their novel approach.

Weaknesses:
- The paper lacks clarity on the technical novelty, as learnable offsets and similar concepts have been previously explored in prior works.
- Comparisons with more recent methods are insufficient, particularly in Tables 1 and 2, which include outdated models.
- The novelty of the insights presented may not be perceived as high, as some concepts are seen as intuitive within the GAN literature.
- The explanation regarding self-attention and its comparison to MTM lacks convincing depth, particularly concerning optimization challenges.
- Visualizations of the learned offsets and qualitative results are missing, hindering understanding of the module's impact.
- The motivation regarding the limitations of AdaIN is unclear, and the performance gains in complex datasets like ImageNet appear marginal.
- Failure cases and limitations of the proposed method are not adequately discussed.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by providing more intuition about the new capabilities of the models, particularly regarding smooth spatial movements and latent-space interpolation. Additionally, we suggest including visual comparisons to baseline models to enhance understanding of the proposed method's properties. The authors should also address the limitations and failure cases in more detail, including datasets where the method may not perform well. Furthermore, we encourage the authors to compare their method with more recent architectures and provide comprehensive explanations for the observed results, particularly regarding the performance gains in FID. Lastly, we recommend that the authors provide a more robust analysis of the self-attention mechanism to address concerns about its relevance and optimization difficulties, and further emphasize the novelty of their contributions to enhance the perceived impact of their work.