ID: FN02v4nD8y
Title: Few-shot Algorithms for Consistent Neural Decoding (FALCON) Benchmark
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 5, 8, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents FALCON, a benchmark and open platform for evaluating adaptation algorithms for intracortical brain-computer interfaces (iBCIs). It includes five datasets of neural and behavioral data from both humans and animals, hosted on DANDI, and aims to standardize the evaluation of iBCI systems by providing a unified interface for testing algorithms across various tasks. The utility of FALCON is demonstrated through several baseline models that support diverse learning strategies such as zero-shot and few-shot learning. The data is provided in an open format (NWB), promoting reusability and enabling longitudinal analysis. Key contributions include a standardized evaluation platform, detailed experimental results, and accessible datasets and code.

### Strengths and Weaknesses
Strengths:
- FALCON addresses a critical gap in iBCI literature by offering standardized datasets and benchmarks, enhancing the reliability of method development.
- The benchmark includes diverse datasets from multiple species, enriching the scope of research.
- The platform supports various decoding tasks with data from both human and non-human subjects.
- Models can be submitted via EvalAI, enhancing evaluation capabilities.
- Open format data files facilitate reusability and longitudinal studies.
- A clear and comprehensive presentation of data collection processes and baseline models supports reproducibility.

Weaknesses:
- The datasets are limited to single participants, which may hinder generalizability.
- Limited sample size, with only one subject per dataset, raises concerns about the robustness of findings.
- Some aspects of the methodology and dataset selection lack clarity, particularly regarding the rationale behind certain standards and metrics.
- Insufficient documentation on the dataset repository (DANDI), which should adhere to FAIR data principles.
- Uncertainty regarding the inclusion of additional datasets or open file formats, which could mitigate sample size issues.
- Lack of long-term maintenance clarity for the website.
- Documentation and example code require further refinement to ensure usability and accessibility.

### Suggestions for Improvement
We recommend that the authors improve clarity by providing detailed explanations for the selection of standards, such as the NWB standard, and participant conditions. Expanding the datasets to include multiple participants for each task would enhance generalizability. Additionally, addressing the usability of the example code and ensuring all figures and results are clearly presented would strengthen the benchmark's impact. We suggest enhancing the documentation on the DANDI repository to provide a comprehensive overview of dataset structure and metadata. It is important to explicitly mention the limitations regarding the one-subject per dataset issue in the paper. Finally, we encourage the authors to release raw recordings for M2 and provide more details on the threshold-setting methodology to address concerns about arbitrary thresholds.