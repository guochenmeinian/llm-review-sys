ID: e9F4fB23o0
Title: Analyzing And Editing Inner Mechanisms of Backdoored Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 2
Original Ratings: 7, 5
Original Confidences: 4, 3

Aggregated Review:
### Key Points
This paper presents a method for understanding backdoor mechanisms in large language models (LLMs) and provides a tool for studying changes in LLM behavior. The authors propose a novel attack that effectively demonstrates backdooring, albeit with a decrease in overall model performance. The paper is technically sound and well-written, though it suffers from readability issues due to jargon that obscures key details. The inclusion of figures in the Appendices detracts from clarity, and short explanations of the three backdoor approaches (mean ablation, logit lens, causal patching) would enhance reader comprehension. The paper's explanations of models, datasets, evaluation metrics, and experimental setup are clear, and the results contribute novel insights into backdooring techniques.

### Strengths and Weaknesses
Strengths:
- Intensive empirical experiments analyze backdoor effects in language models.
- The proposed PCP ablation complements localization techniques by reinserting backdoor, verifying the analysis.
- The paper suggests freezing MLP and embedding projection blocks to avoid the attack based on the results.

Weaknesses:
- The writing is vague and unclear, lacking motivation for the PCP ablation and an explanation of why this technique can reinsert backdoor; the acronym “PCP” is not defined.
- The “Trigger Hidden State” paragraph in Section 4 lacks clarity, needing more details on the experiment, PCA execution, and the meanings of dim1 and dim2.
- The threat model is not presented, as the freezing parameters defense cannot be applied if the attacker fine-tunes and provides the model.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the writing by reducing jargon and providing simpler explanations for complex concepts. It would be beneficial to include the figures in the main body of the text for better readability. Additionally, we suggest that the authors provide short explanations of the three backdoor approaches to set a clearer context for readers. The authors should also clarify the “Trigger Hidden State” paragraph by detailing the PCA process and defining the terms dim1 and dim2. Finally, we advise the authors to present a comprehensive threat model to address the limitations of the proposed defense strategy.