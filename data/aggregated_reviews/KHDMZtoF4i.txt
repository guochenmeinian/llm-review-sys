ID: KHDMZtoF4i
Title: xVal: A Continuous Number Encoding for Large Language Models
Conference: NeurIPS
Year: 2023
Number of Reviews: 3
Original Ratings: 4, 7, -1
Original Confidences: 4, 4, 3

Aggregated Review:
### Key Points
This paper presents xVal, an embedding method for numeric values that multiplies a number with a single "`[NUM]`" token's learned vector embedding. The authors conduct experiments using synthetic datasets for arithmetic operations, global temperature data, and planetary orbit simulations, testing the method in a Masked Language Model (MLM) setting, though it could also be applied in an Auto-Regressive (AR) context. The simplicity of the approach is acknowledged, but concerns are raised regarding its real-world applicability and the clarity of its presentation.

### Strengths and Weaknesses
Strengths:
- The approach is innovative, addressing tokenization challenges in LLMs by using a continuous embedding space for numbers, potentially improving efficiency.
- The theoretical framework is clearly presented, detailing the number inference scheme and implicit normalization via layer-norm.
- The proposed inductive bias for scientific applications could enhance the analysis of scientific datasets with LLMs.

Weaknesses:
- The normalization process is limited to the range [âˆ’5, 5], raising concerns about the method's utility for pre-training on diverse datasets.
- A lack of comparison with models like LLaMa, which effectively handle numerical operations, leaves a gap in the evaluation.
- The experiments appear more theoretical than applicable to complex real-world tasks.
- The method may not adequately differentiate between numbers that are close in value but imply different precision.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the envisioned use case for their model in real-world settings, addressing what specific problems it aims to solve. Additionally, we suggest expanding the evaluation to include larger language models and providing a direct comparison with existing models like LLaMa. Furthermore, the authors should clarify how xVal handles nuances in value interpretation and precision, as these aspects are crucial for various applications.