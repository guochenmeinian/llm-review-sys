ID: esMnmm2VJh
Title: DQA: An Efficient Method for Deep Quantization of Deep Neural Network Activations
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 7, 7, 3
Original Confidences: 4, 5, 4

Aggregated Review:
### Key Points
This paper presents a novel quantization approach for activations, termed DQA, which identifies important channels and quantizes them with higher precision before adjusting to a target compression level. The methodology is well-explained and supported by experiments demonstrating improved performance over NoisyQuant. The authors leverage sub-6-bit quantization across three computer vision tasks, achieving better accuracy than standard methods.

### Strengths and Weaknesses
Strengths:
- The topic is engaging and relevant, addressing the challenging problem of quantizing activations.
- The method appears generalizable across various contexts and shows preliminary effectiveness.

Weaknesses:
- The paper lacks a deep evaluation of the memory/accuracy tradeoff.
- There is insufficient discussion on channel selection and its impact on memory overheads and accuracy.
- Execution time overheads for runtime quantization and de-quantization are not analyzed.
- The paper contains repetitive content, making it hard to parse and lacking depth.

### Suggestions for Improvement
We recommend that the authors improve the clarity and depth of the paper by addressing the following points:
- Provide a more detailed evaluation of the memory/accuracy tradeoff, including an ablation study on channel selection.
- Include memory overheads and accuracy comparisons with respect to the FP32 model in Table 1.
- Analyze the additional operations required for quantization and de-quantization on realistic devices.
- Enhance the problem statement and motivation, clearly articulating the impact of the proposed work in light of existing literature.
- Address the efficiency claims made throughout the paper, providing quantification and evidence to support assertions of improved efficiency.
- Revise the structure of the paper to keep the abstract and introduction high-level, reserving detailed algorithmic descriptions for later sections to improve readability.