ID: inN4TdboJX
Title: Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an approach to assess the robustness of large language models (LLMs) in multi-hop arithmetic reasoning tasks using domain-agnostic perturbations. The authors systematically apply perturbations at various levels of abstraction, including lexical and semantic changes, to analyze LLM behavior. They explore few-shot prompting techniques and demonstrate that increasing the proportion of perturbed examples enhances the robustness of these methods. The paper also compares different prompting methods, providing insights into their strengths and weaknesses.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a systematic approach to test LLM robustness in multi-hop reasoning tasks, utilizing domain-agnostic perturbations.  
- It provides valuable comparisons of various prompting methods, such as chain-of-thought prompting and least-to-most prompting, under different perturbations.  
- The authors demonstrate that increasing perturbed exemplars in prompts improves robustness, contributing to the understanding of LLM performance.

Weaknesses:  
- The scope is limited, as experiments are conducted solely with GPT-3 and the GSM8K dataset, raising concerns about generalizability.  
- The conclusions drawn from a single model and dataset may lack significant evidence, as performance variations appear modest.  
- The paper requires more detailed manual inspection of perturbations, as automated evaluations may not capture all issues.

### Suggestions for Improvement
We recommend that the authors improve the generalizability of their findings by conducting experiments on multiple datasets and models. Expanding the range of perturbations to include written style and order variations would enhance the robustness analysis. Additionally, incorporating human evaluations alongside automated assessments could provide a more comprehensive understanding of the model's performance. Lastly, we encourage the authors to elaborate on empirical approaches to improve robustness, as this aspect requires further discussion.