ID: 2dfBpyqh0A
Title: Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images
Conference: NeurIPS
Year: 2024
Number of Reviews: 18
Original Ratings: 5, 7, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Gaussian Graph Network (GGN), a novel approach for generalizable 3D Gaussian Splatting (3GDS) reconstruction. The authors address a significant issue in prior works that regress pixel-aligned Gaussians and directly combine them from multiple views, leading to excessive Gaussian counts. GGN utilizes a graph network to identify and merge similar Gaussians, enhancing efficiency and reconstruction quality. Experimental results demonstrate that GGN outperforms baseline methods in both efficiency and quality, particularly with an increasing number of input views.

### Strengths and Weaknesses
Strengths:
1. The concept of merging redundant Gaussians is both novel and practical, effectively addressing redundancy in generalizable 3DGS.
2. Experimental results are satisfactory, showing that GGN achieves superior reconstruction with fewer Gaussians and scales well with more input views.
3. The paper is well-written, and the experiments are comprehensive.

Weaknesses:
1. The innovation of GGN is questioned, as it closely resembles existing GNN frameworks and methods that combine GNN and Gaussian Splatting (GS), such as SAGS and Hyper-3DG.
2. The inference efficiency of GGN is not adequately demonstrated, raising concerns about its performance with large Gaussian counts.
3. Some implementation details, such as view selection and the handling of large-scale scenes, are unclear, and the paper lacks a thorough discussion on training efficiency.

### Suggestions for Improvement
We recommend that the authors improve the clarity regarding the differences between GGN and existing methods that combine GNN and GS. Additionally, we suggest including a detailed analysis of inference efficiency, particularly concerning the number of input views. It would be beneficial to clarify the view selection process and how the proposed method can handle large-scale scenes effectively. Lastly, we encourage the authors to benchmark training and inference latency and consider integrating advancements from point cloud processing, such as Point Transformer V3, to enhance performance.