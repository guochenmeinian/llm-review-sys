ID: RnQdRY1h5v
Title: B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 5, 6, 7, 3, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 4, 4, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents B'MOJO, a novel architecture that integrates sliding window attention with state-space models (SSMs) to balance eidetic and fading memory. The authors propose an error function inspired by Stochastic Realization Theory to manage the long-term sliding key-value (KV) cache. The model is evaluated on multi-query associative recall (MQAR) and language modeling tasks, showing promising results, particularly in out-of-distribution (OOD) length generalization.

### Strengths and Weaknesses
Strengths:
- The paper addresses the critical issue of memory efficiency in sequence models.
- It offers a comprehensive overview of stochastic realization theory and its relevance to modern sequence models.
- The innovative use of an error function for storage/eviction policy in sliding window attention is noteworthy.
- The improved OOD generalization results are impressive, and the architecture is well-written.

Weaknesses:
- Several relevant works are inadequately cited, including Block state transformers and adaptive KV caches.
- The justification for insights gained from the stochastic realization framework is lacking, particularly regarding the Innovation Selection process.
- Experimental results are weak, with insufficient details on task sequence lengths and datasets used.
- The distinction between B'MOJO and B'MOJO-F is unclear, and the aggregation of memory sources in the attention mechanism is not well explained.
- The performance of B'MOJO compared to Mistral and Transformers raises questions, particularly in zero-shot evaluations.

### Suggestions for Improvement
We recommend that the authors improve the citation of related works, particularly Block state transformers and adaptive KV caches, to better position their contributions. Clarifying the distinction between B'MOJO and B'MOJO-F, as well as providing a detailed explanation of the Innovation Selection mechanism, would enhance the paper's clarity. Additionally, including more comprehensive experimental details, such as task sequence lengths and datasets, is essential for assessing the model's performance. We suggest that the authors address the aggregation of memory sources in the sliding window attention mechanism and provide clearer definitions of the dimensions of matrices and vectors used. Finally, discussing the implications of performance gaps with respect to context lengths and training conditions would strengthen the paper's conclusions.