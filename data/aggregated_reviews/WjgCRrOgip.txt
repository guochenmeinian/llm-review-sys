ID: WjgCRrOgip
Title: Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 7, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a study on neural text degeneration, specifically the tendency of language models to generate repetitive loops. The authors demonstrate a correlation between text degeneration and the presence of repetitive text in training data. They propose a method called repetitive dropout, which applies dropout to attention weights over repetitive contexts, and show through experiments on three datasets that this method significantly reduces repetition. The authors also re-evaluate previous hypotheses regarding degeneration, concluding that penalizing repetitions in training data is crucial and that their method effectively breaks self-reinforcement loops caused by repetitive words.

### Strengths and Weaknesses
Strengths:
- The paper is well-structured and easy to follow.
- It addresses a significant issue in the field, with findings that may interest researchers.
- The experimental design is clever, and the proposed method effectively reduces repetition.
- The analysis of three types of repetitions in training data is insightful.

Weaknesses:
- The authors only compare their method with one baseline, lacking comparisons with other mitigations for text degeneration.
- Section 6.2 is difficult to follow, with vague statements and unclear connections between arguments.
- Some claims, such as the effectiveness of repetition dropout in breaking self-reinforcement loops, lack supporting evidence.
- The focus on GPT-2-sized models may limit the paper's relevance to the broader audience.

### Suggestions for Improvement
We recommend that the authors improve the comparison by including additional baselines for text degeneration mitigation. Clarifying Section 6.2 with a brief introduction to high-inflow words and clearer topic sentences would enhance readability. Providing evidence for claims regarding the effectiveness of repetition dropout and elaborating on the explanation of exposure bias would strengthen the paper. Additionally, exploring the impact of larger language models and instruction-tuning on degeneration could broaden the paper's relevance and insights.