ID: mQPNcBWjGc
Title: Scaling Open-Vocabulary Object Detection
Conference: NeurIPS
Year: 2023
Number of Reviews: 7
Original Ratings: 6, 7, 7, 7, -1, -1, -1
Original Confidences: 5, 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the OWLv2 model and the OWL-ST self-training recipe for open-vocabulary object detection, leveraging a dataset of approximately 10B images and associated alt-text strings for weak supervision. The authors propose various self-training strategies, including token dropping, instance selection, and mosaics augmentation, to enhance training efficiency. The proposed method demonstrates significant performance improvements on open-vocabulary benchmarks, particularly on the LVIS dataset.

### Strengths and Weaknesses
Strengths:
- The authors provide essential implementation details for scaling the open-vocabulary detector using a self-training method on massive web data.
- The performance on the LVIS benchmark is notably impressive, indicating the effectiveness of the proposed approach.
- The paper is well-written, rigorous, and discusses the challenges of extending OWL-ViT to large-scale datasets.

Weaknesses:
- GLIP2 outperforms OWL-ST on the ODinW dataset; the authors should explain the reasons for this discrepancy.
- The benefits of mosaic augmentation are unclear; ablation studies comparing with and without this technique are recommended.
- The significant computational requirements may hinder reproducibility for general research institutions, raising questions about the potential for open-sourcing the model as a foundational resource.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the inference process in the next version. Additionally, the authors should discuss how to handle noise in self-training and its effects if left unaddressed. Addressing typographical errors and enhancing the discussion on the filtering strategy would also strengthen the paper. Lastly, we encourage the authors to consider the implications of their model's reliance on large-scale data and computation for broader accessibility in the research community.