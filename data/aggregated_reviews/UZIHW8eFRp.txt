ID: UZIHW8eFRp
Title: A Tractable Inference Perspective of Offline RL
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 6, 6, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Trifle (Tractable Inference for Offline RL), which leverages Tractable Probabilistic Models (TPMs) to enhance offline reinforcement learning (RL) performance. The authors argue that tractability, or the ability to efficiently answer probabilistic queries, is crucial for accurate evaluation in stochastic environments. Trifle employs a mixture of single-step and multi-step value estimates to condition action generation, improving the likelihood of sampling high-return actions through an enhanced rejection sampling method. Comprehensive empirical evaluations demonstrate that Trifle outperforms state-of-the-art baselines across various benchmarks, highlighting its robustness in challenging environments. Additionally, the paper integrates the probabilistic circuits framework into RL and proposes enhancements to the experimental setup, although it notes the need for a more thorough description of the TPM model.

### Strengths and Weaknesses
Strengths:
- The paper introduces a novel perspective on the importance of tractability in offline RL, addressing a gap in existing literature.
- The methodology is well-supported by theoretical insights and extensive empirical evaluations, showing significant performance improvements over strong baselines.
- The paper is well-written and organized, making complex concepts accessible, with detailed experimental setups aiding comprehension.
- The authors are responsive to feedback, demonstrating a willingness to improve the paper based on reviewer suggestions.

Weaknesses:
- The novelty and significance of the approach may be limited, as some key ideas appear to be based on classical rejection sampling methods with minor modifications.
- The description of the TPM model in Appendix B is insufficiently detailed, potentially hindering accessibility for the average RL practitioner.
- There is a lack of empirical comparisons against viable alternatives, such as sequence models augmented by Q-function learning.
- Some aspects of the method, including the implications of theorems and computational complexity, require clearer exposition.
- The paper does not provide a thorough analysis of Trifle's computational complexity and scalability, particularly in high-dimensional action spaces.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the proposed algorithm by merging and rephrasing Sections 4 and 5, and consider adding a concise algorithm box in the main paper. Additionally, expanding the description of the TPM model to ensure it is comprehensible to RL practitioners would enhance understanding. We suggest including more extensive empirical comparisons against alternative methods, particularly those that utilize similar computational efficiencies. Furthermore, conducting additional ablation studies to isolate the contributions of various components, such as the adaptive thresholding mechanism and beam search strategy, would provide deeper insights into their impact on performance. Lastly, we encourage the authors to include more details on the implications of theorems, beam search parameters, computational complexity, and the adaptive thresholding mechanism in the final version of the paper, as well as defining the RvS abbreviation early and elaborating on the formulation and training of the TPM model.