ID: ADLaALtdoG
Title: SciCode: A Research Coding Benchmark Curated by Scientists
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 8, 5, 8
Original Confidences: 4, 4, 4

Aggregated Review:
### Key Points
This paper presents SciCode, a benchmark comprising 73 scientific programming problems across various fields, decomposed into 305 subproblems. The problems, authored by scientists, require advanced scientific knowledge and coding skills. The evaluation of eight models on SciCode reveals their reasoning capabilities, with current models struggling to achieve satisfactory performance.

### Strengths and Weaknesses
Strengths:
- The dataset is relevant and useful, reflecting real-world scientific challenges, including problems from Nobel Prize-winning studies.
- Thoughtful problem setup allows for high-resolution progress tracking through the decomposition of main problems into subproblems, supported by gold-standard solutions and diverse test cases.
- High annotation quality ensures no data contamination, with all problems verified by experienced researchers.

Weaknesses:
- The evaluation lacks diversity in prompting techniques and does not explore models that may reason better than standard LLMs.
- Insufficient detail on dataset construction, including the selection process for problems and the involvement of scientists in quality control.
- The paper does not adequately analyze model failures or discuss the implications of context length on performance.

### Suggestions for Improvement
We recommend that the authors improve the analysis of model failures to understand the reasons behind poor performance, such as domain knowledge gaps or coding errors. Additionally, we suggest providing clarity on the abrupt performance decline noted in Figure 3, including context lengths and the number of subproblems considered. It would also be beneficial to clarify whether the background knowledge provided is sufficient for expert-level performance and to include a dedicated limitations section discussing potential biases and dataset disqualifications. Lastly, we encourage the authors to present few-shot results and include error bars in evaluation results to enhance the robustness of their findings.