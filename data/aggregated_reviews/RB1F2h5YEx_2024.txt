ID: RB1F2h5YEx
Title: Parseval Regularization for Continual Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 4, 4, 7, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 5, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified framework for selecting and designing embedding methods in reinforcement learning (RL), treating them as RL problems. It encompasses various techniques, including VAEs, UMAP, and t-SNE, illustrating their interrelations and enabling hybrid method creation. The authors connect the ELBO approximation in VAEs to the exploration-exploitation trade-off in RL, demonstrating the framework's flexibility in designing novel methods and extending existing ones. They introduce new hybrid methods, such as variational UMAP and a UMAP/t-SNE hybrid, showcasing state-of-the-art performance across multiple datasets. Additionally, a Python package implementing this framework is provided for practical use.

### Strengths and Weaknesses
Strengths:
- The introduction of Parseval regularization is a significant contribution, addressing continual reinforcement learning challenges by maintaining orthogonal weight matrices, which preserves crucial optimization properties.
- Robust empirical evidence supports the effectiveness of Parseval regularization across various RL tasks, enhancing credibility.
- Thorough ablation studies dissect the impact of Parseval regularization, providing insights into its components.
- The paper compares Parseval regularization with alternative algorithms, demonstrating its superiority in specific contexts.
- The exploration of interactions with different activation functions and network architectures suggests versatility.

Weaknesses:
- Parseval regularization may complicate neural network implementation, making integration and tuning challenging for practitioners.
- The method's performance is primarily demonstrated in simpler environments, raising questions about its scalability to complex, high-dimensional tasks.
- A broader range of baseline comparisons could strengthen the evaluation of Parseval regularization's effectiveness.
- The computational overhead of maintaining orthogonality in weight matrices is not thoroughly discussed, particularly regarding performance gains versus costs.
- The theoretical analysis lacks depth; a more rigorous exploration of underlying mechanisms could enhance contributions.
- Empirical results are limited to controlled settings, necessitating additional experiments in varied environments for generalizability.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational complexity and runtime of Parseval regularization, including a detailed comparison with baseline methods. Additionally, addressing the integration and tuning of regularization parameters in practice would be beneficial. We suggest expanding the theoretical analysis to provide stronger guarantees and deeper insights into the effectiveness of Parseval regularization. Furthermore, including a broader range of state-of-the-art methods in comparisons could enhance the evaluation of the proposed approach. Lastly, we encourage the authors to conduct experiments in more complex and realistic environments to confirm the robustness of their findings.