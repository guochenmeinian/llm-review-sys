ID: kR21XsZeAr
Title: Subclass-Dominant Label Noise: A Counterexample for the Success of Early Stopping
Conference: NeurIPS
Year: 2023
Number of Reviews: 11
Original Ratings: 5, 6, 5, 5, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 3, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel type of label noise termed subclass-dominant label noise (SDN) and introduces an algorithm called NoiseCluster that aims to identify and correct this noise. The authors demonstrate that NoiseCluster outperforms existing methods on both synthetic and real-world datasets, particularly the Clothing1M dataset. The findings highlight the limitations of early stopping in the presence of SDN and suggest that later stopping can better capture high-level semantics of noisy examples.

### Strengths and Weaknesses
Strengths:
- The introduction of subclass-dominant label noise (SDN) provides a well-motivated perspective on the challenges of early stopping in noisy label scenarios.
- The paper is well-organized, with clear sections that effectively introduce the problem and methodology.
- Experimental results indicate superior performance of NoiseCluster over existing robust methods on both SDN and Clothing1M datasets.

Weaknesses:
- The justification for the existence of SDN in real-world scenarios is insufficient, particularly regarding its prevalence in datasets like Clothing1M.
- The evaluation is limited to a single synthetic dataset and one real-world dataset; comparisons with instance-dependent label noise robust methods are lacking.
- There is a need for deeper analysis of clustering effects and results, particularly in relation to the CIFAR20-SDN dataset and its alignment with manually created SDN noise.

### Suggestions for Improvement
We recommend that the authors improve the justification for subclass-dominant label noise by investigating its occurrence in real-world datasets beyond Clothing1M. Additionally, we suggest conducting a comprehensive evaluation of NoiseCluster against various types of noisy label models, including symmetric and instance-dependent noise. Further analysis of clustering effects and results should be included to strengthen the argument for the proposed method's effectiveness. Lastly, we encourage the authors to clarify the rationale behind their training modifications and explore the potential benefits of self-supervised learning in conjunction with their approach.