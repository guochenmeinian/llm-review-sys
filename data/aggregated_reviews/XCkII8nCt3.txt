ID: XCkII8nCt3
Title: Non-asymptotic Approximation Error Bounds of Parameterized Quantum Circuits
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 7, 7, 7, 7, -1, -1, -1, -1
Original Confidences: 4, 3, 5, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a theoretical analysis of parameterized quantum circuits (PQC) for approximating HÃ¶lder smooth functions. The authors demonstrate that PQCs can efficiently represent a large class of multivariate polynomials and smooth functions, achieving better approximation results than previous works, particularly in scenarios with large dimensions and small local Taylor expansion regions. The study also explores the advantages of PQCs over classical deep learning networks.

### Strengths and Weaknesses
Strengths:  
- The paper addresses an important problem regarding the power and limitations of PQCs, contributing novel results and highlighting open problems of interest.  
- It provides a different theoretical perspective on PQCs through non-asymptotic approximation error analysis, which is technically sound.  
- The numerical experiments support the theoretical findings, showing promising results in function approximation.

Weaknesses:  
- The theoretical analysis is limited to continuous or smooth target functions, restricting broader applicability to non-smooth cases.  
- The numerical simulations lack validation against practical machine learning datasets, which diminishes their convincingness.  
- The approximation rate for continuous and Lipschitz functions is established only under Universal Approximation Theory, and the authors should clarify how their results compare to those in Lu's paper, particularly regarding the $L^p$ norm.  
- The neural network structure may be challenging to train due to its depth; a shallower and wider architecture could be beneficial.  
- Some tables and comments currently in the appendix could enhance clarity if presented in the main text.

### Suggestions for Improvement
We recommend that the authors improve the theoretical framework to accommodate non-smooth and non-continuous target functions. Additionally, we suggest providing a clearer comparison of their results with those in Lu's paper, particularly concerning the approximation rates and the implications of using the bit extraction technique. It would be advantageous to include bounds on the parameters required for the neural network architecture to facilitate training. Furthermore, we encourage the authors to present certain tables and comments from the appendix in the main body of the paper for better accessibility. Lastly, incorporating a figure early on to compare PQCs with classical deep learning networks could enhance reader understanding.