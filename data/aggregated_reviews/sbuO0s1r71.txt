ID: sbuO0s1r71
Title: Evaluating Cross-Domain Text-to-SQL Models and Benchmarks
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a comprehensive analysis of the limitations of text-to-SQL evaluation metrics and benchmarks, particularly focusing on Spider and its variants. The authors identify systemic semantic issues in SQL queries that contribute to ambiguity and fragility in approximately 10% to 20% of samples. They explore factors such as aggregates, extrema queries, schema matching ambiguity, and incorrect assumptions about database content. The study highlights the impact of these issues on execution match metrics and proposes modifications to the development set to enhance evaluation accuracy.

### Strengths and Weaknesses
Strengths:
- The paper provides a thorough empirical analysis of popular text-to-SQL datasets, revealing significant limitations and calling for caution in interpreting model accuracy.
- It effectively demonstrates the potential noise in the dataset due to systemic problems, supported by human analysis.
- The suggestion to include multiple queries per question to mitigate ambiguity is well-founded and could improve best practices.

Weaknesses:
- The reliance on non-standard SQL implementations, particularly SQLite, undermines the analysis, as it does not enforce many standard SQL semantic constraints, leading to non-deterministic outputs.
- The experimental analysis is presented in a confusing manner, particularly regarding the interpretation of results in Tables 2 and 3, which could mislead readers about the significance of findings.
- The paper does not provide modified benchmarks for community use, which was an anticipated outcome.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the experimental results, particularly in Table 2, by providing examples of the min/max/top transformations. Additionally, please clarify the interpretation of the human evaluation results in Table 3, confirming that these figures are based on a sample of 102 queries. We also suggest that the authors consider sharing modified benchmarks with the community to enhance the utility of their findings. Lastly, a systematic validation of the standard text-to-SQL dataset against standard SQL constraints would strengthen the paper's contributions.