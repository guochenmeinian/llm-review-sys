ID: gMS6FVZvmF
Title: One Fits All: Power General Time Series Analysis by Pretrained LM
Conference: NeurIPS
Year: 2023
Number of Reviews: 29
Original Ratings: 8, 4, 6, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 2, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for time series analysis by leveraging pre-trained large language models, specifically using a frozen Transformer backbone. The authors propose fine-tuning only the input embeddings and output layers, demonstrating through extensive experiments that their approach outperforms established baseline methods across various time series tasks, including forecasting and anomaly detection. Additionally, the paper provides a comprehensive analysis of forecasting performance using both univariate and multivariate approaches, highlighting discrepancies in mean squared error (MSE) values across various datasets. The authors note that the univariate MSE is generally smaller due to differing experimental settings and discuss the challenges of multivariate forecasting, supported by detailed tables of MSE results for various models.

### Strengths and Weaknesses
Strengths:
1. The proposed method is simple yet effective, consistently outperforming more complex techniques without introducing additional hyperparameters.
2. The experiments are robust, covering a wide range of time series analysis tasks and demonstrating superior performance against strong baselines.
3. The correlation between self-attention and Principal Component Analysis (PCA) provides a credible basis for the method's success.
4. The inclusion of detailed tables summarizing MSE values enhances transparency and clarity regarding the forecasting outcomes.
5. The authors provide a thoughtful discussion on the complexities of multivariate forecasting and the performance of various baseline models.

Weaknesses:
1. The paper lacks a detailed discussion on the computational costs associated with the proposed method compared to alternatives, particularly given the size of the pre-trained GPT-2 backbone.
2. The experiments are limited to GPT-2, and expanding the scope to include other autoregressive Transformers like DeBERTa and GPT-J would enhance the findings.
3. Presentation issues detract from readability, including congested tables, small font sizes in figures, and inconsistent spacing, which compromise the overall impact of the findings.
4. The authors' explanations regarding the implications of their PAC argument seem overly reliant on their specific technical design, lacking broader applicability to other settings.
5. Some reviewers express skepticism about the PCA analysis and its relevance to the empirical performance of transformer models on time series tasks.

### Suggestions for Improvement
We recommend that the authors improve the discussion on computational costs to clarify the efficiency of their method relative to alternatives. Additionally, we suggest expanding the experimental scope to include other autoregressive Transformers to validate the generalizability of their findings. Furthermore, enhancing the overall presentation by addressing the readability of tables and figures will significantly improve the paper's impact. We also recommend that the authors improve the clarity of their PAC argument by explicitly discussing its applicability to other tuning scenarios in LLMs. Including the methodology for binary anomaly calls in the supplementary material would enhance the paper's comprehensiveness. Lastly, we encourage the authors to refine their presentation to ensure that the relationship between their tuning design and PCA insights is clearly articulated, addressing any potential confusion for readers.