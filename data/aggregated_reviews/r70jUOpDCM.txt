ID: r70jUOpDCM
Title: Multi-Scale VMamba: Hierarchy in Hierarchy Visual State Space Model
Conference: NeurIPS
Year: 2024
Number of Reviews: 24
Original Ratings: 7, 2, 7, 2, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a Multi-Scale Vision Mamba (MSVMamba) model designed to enhance performance in computer vision tasks by integrating a multi-scale 2D scanning technique and a Convolutional Feed-Forward Network (ConvFFN). The authors claim that MSVMamba achieves significant improvements, including nearly 1.5x speedup in inference and 2.0x speedup in training compared to the baseline VMamba. They analyze the attenuation coefficient between tokens and propose methods to mitigate long-distance forgetting, asserting that their core contribution, the Multi-Scale 2D (MS2D) module, addresses long-range issues in Mamba models. Extensive experiments validate the effectiveness of MSVMamba across various benchmarks, demonstrating its potential in classification and other downstream tasks. The authors justify their comparisons with the first version of VMamba, arguing that the second version was released shortly before the submission deadline and should be considered concurrent.

### Strengths and Weaknesses
Strengths:  
1. The writing is clear and accessible.  
2. The authors effectively address long-distance forgetting and present a viable solution.  
3. Comprehensive ablation studies validate the contributions of the MS2D, SE, and ConvFFN modules.  
4. Claims of speed improvements in inference and training are commendable and align with expectations for efficiency in real-world applications.  
5. The paper includes relevant comparisons with the second version of VMamba, demonstrating the model's competitive performance.

Weaknesses:  
1. The scalability of MSVMamba remains unproven, as training results on larger models (e.g., 50M and 90M parameters) are not provided.  
2. The ablation studies focus solely on classification tasks, lacking validation in more nuanced tasks like detection and segmentation.  
3. The novelty of the approach is questioned, as hierarchical Mamba-based models and components like ConvFFN have been previously introduced.  
4. MSVMamba is reported to be slower than models like ConvNeXt, limiting its practical usage.  
5. The choice to present results from the first version of VMamba raises questions about the novelty and contribution of MSVMamba, as improvements are not significantly backed by results.  
6. The argument for not including the second version of VMamba in comparisons is seen as weak, potentially undermining the evaluation of MSVMamba's contributions.

### Suggestions for Improvement
We recommend that the authors improve the scalability validation by providing training results on larger models, such as those with 50M and 90M parameters. Additionally, we suggest expanding the ablation studies to include more fine-grained tasks like detection and segmentation to better illustrate the roles of each module. We also recommend including the best results from the second version of VMamba to provide a more comprehensive evaluation of their contributions. Furthermore, the authors should address the concerns regarding the speed of MSVMamba relative to other models, particularly ConvNeXt, to enhance the practical applicability of their work. Lastly, we encourage the authors to clarify the novelty of their contributions in light of the performance metrics presented.