ID: EGfYnTyEGv
Title: A Long $N$-step Surrogate Stage Reward for Deep Reinforcement Learning
Conference: NeurIPS
Year: 2023
Number of Reviews: 18
Original Ratings: 5, 4, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel reward estimator called the long N-step surrogate stage (LNSS) reward for deep reinforcement learning (RL), aimed at addressing the high variance problem in continuous control tasks. The authors propose that LNSS improves performance metrics such as average reward, convergence speed, learning success rate, and variance reduction in Q-values. They validate their approach through empirical evaluations on various environments in the DeepMind Control Suite and OpenAI Gym, demonstrating that LNSS consistently outperforms traditional methods. Additionally, the paper provides a detailed analysis of the relationship between original and surrogate rewards, establishing conditions under which evaluations of policies using these rewards are equivalent. The authors discuss the concept of bias in value estimates, highlighting how various algorithms can introduce biases due to differences in reward structures and optimization procedures. While LNSS methods show improved performance metrics, they still exhibit biases similar to other state-of-the-art (SOTA) algorithms.

### Strengths and Weaknesses
Strengths:
- The paper introduces a straightforward and effective method to reduce variance in RL, which can be easily integrated into existing algorithms like DDPG, TD3, and D4PG.
- Strong empirical results support the proposed method, showcasing its robustness across different reward functions and hyperparameters.
- The paper is well-organized and presents results in a clear manner, facilitating interpretation.
- It provides rigorous mathematical proofs and theorems that clarify the conditions under which surrogate and original reward evaluations align.
- The discussion on bias in RL offers valuable insights into how different algorithms can lead to overestimation or underestimation biases.

Weaknesses:
- The improvements are demonstrated on a limited number of tasks and algorithms, raising concerns about potential cherry-picking and the selection criteria for environments.
- The assumptions in Theorem 1 are unclear, and the interpretation of variance reduction may be misleading, as a decrease in the upper bound does not guarantee a decrease in actual variance.
- There is a lack of discussion regarding the potential bias introduced by the LNSS method, particularly in sparse reward scenarios, and whether the proposed method achieves the same solutions as traditional reward methods.
- The paper may lack clarity in explaining the implications of the bias in practical applications, particularly for practitioners unfamiliar with the theoretical background.
- Some sections could benefit from more intuitive explanations or examples to enhance reader understanding.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the assumptions in Theorem 1 and provide a rigorous proof demonstrating that the proposed method is unbiased with respect to \(Q^*\) or \(Q^\pi\). Additionally, the authors should address the potential bias introduced by the LNSS reward estimator, particularly in sparse reward tasks, and consider including experiments that utilize full trajectories for computing LNSS rewards. Furthermore, we suggest that the authors clarify the selection criteria for environments and ensure that the implementations of baseline algorithms are state-of-the-art (SOTA). It would also be beneficial to enhance the readability of figures and provide justifications for the choice of N in the experiments. Lastly, we recommend improving the clarity of the bias discussion by providing more intuitive explanations and practical examples that illustrate the implications of their findings, as well as enhancing the presentation of theorems and lemmas to ensure that readers can easily grasp their significance and application in real-world scenarios.