ID: PPwRa7Wmg1
Title: VIP5: Towards Multimodal Foundation Models for Recommendation
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a multimodal foundation model (MFM) framework that integrates foundational models from Computer Vision (CV), Natural Language Processing (NLP), and Recommender Systems (RecSys) to enhance recommendations through multimodal information. The authors propose a parameter-efficient training method that improves the performance of recommendation models while reducing training time and memory usage. The paper includes experimental results demonstrating the framework's effectiveness across sequential recommendation, direct recommendation, and explanation generation tasks.

### Strengths and Weaknesses
Strengths:
- The framework performs well in various recommendation tasks, showcasing significant enhancements in performance.
- The paper is clearly presented, with a well-illustrated MFM framework and comprehensive performance comparisons.
- Insightful ablation experiments are conducted on the parameter-efficient training method and visual encoder.

Weaknesses:
- The novelty of the proposed work is limited, with similarities to existing methods like LoRA and llama-adapter.
- Experimental evaluations are restricted, relying on small datasets from a single platform (Amazon.com) and lacking comparisons with the latest state-of-the-art methods.
- Some claims are overly broad and not sufficiently supported, particularly regarding the novelty and contributions of the MFM model.
- The experimental results may not fairly represent the baseline P5 model's performance.

### Suggestions for Improvement
We recommend that the authors improve the novelty section by thoroughly comparing the MFM model with existing methods to highlight its unique contributions. Additionally, we suggest expanding the experimental evaluation to include more diverse datasets and state-of-the-art baselines. Clarifying the distinctions among prompts in the Appendix and providing explanations for unexpected performance patterns in figures would enhance the paper's clarity. Finally, we encourage the authors to better articulate the motivation for their parameter-efficient training method and avoid overclaims in the writing.