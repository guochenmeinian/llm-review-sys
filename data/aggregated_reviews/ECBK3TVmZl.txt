ID: ECBK3TVmZl
Title: Zero-Regret Performative Prediction Under Inequality Constraints
Conference: NeurIPS
Year: 2023
Number of Reviews: 15
Original Ratings: 4, 5, 5, 7, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 2, 2, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new algorithm for performative prediction under general inequality constraints, achieving sublinear regret with respect to the performative optimum when the distribution map follows the location family model. The authors propose a robust primal-dual algorithm that requires only approximate gradients and provides regret guarantees supported by simulations. The study emphasizes the importance of incorporating inequality constraints and explores the implications of decision-dependent distributions.

### Strengths and Weaknesses
Strengths:  
- The motivation for introducing constraints in performative prediction is strong and well-justified.  
- The explicit regret guarantees are compelling, and Example 1 is well-motivated and clear.  
- The paper is well-written, with a clear outline of the main method and thorough connections to existing work.  
- The proposed algorithm is practical and demonstrates good theoretical and empirical results.

Weaknesses:  
- The novelty is limited, as the main method combines existing ideas from the literature directly, particularly the zeroth-order estimate of the gradient, which follows the two-stage method of Miller et al.  
- The assumptions are strong and the setting is somewhat restricted, particularly the reliance on location families and accurate parameter estimation.  
- The paper lacks a discussion on lower bounds for sample complexity and does not adequately address the implications of non-linear distributions.

### Suggestions for Improvement
We recommend that the authors improve clarity regarding the meaning of the time horizon \( T \) in their analyses, ensuring it represents the total number of collected samples. Additionally, we suggest that Assumption 1 be stated more directly to clarify its role in ensuring convexity. In Lemma 3, we advise simplifying the notation for delta to enhance readability. It would be beneficial to explicitly outline the contributions of derivative-free methods like Flaxman et al. to broaden the applicability of the results. Furthermore, we encourage the authors to address the implications of their algorithm's assumptions more explicitly, particularly regarding the learner's ability to query distributions. Lastly, including proof sketches in the main paper could enhance understanding of the theoretical results.