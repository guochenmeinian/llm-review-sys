ID: MhWaMOkoN3
Title: Universality in Transfer Learning for Linear Models
Conference: NeurIPS
Year: 2024
Number of Reviews: 12
Original Ratings: 6, 5, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new universality result applicable to transfer learning within linear models, specifically addressing optimization problems through a Gaussian distribution framework. The authors propose that properties of linear models trained on large datasets can be effectively transferred to new tasks with minimal data. They extend the universality concept, traditionally rooted in random matrix theory, to analyze transfer learning in regression and binary classification settings, establishing conditions for performance close to models trained directly on target tasks.

### Strengths and Weaknesses
Strengths:  
- The derivation of a new universality result enhances the understanding of transfer learning in linear regression and classification.  
- The combination of theoretical analysis and empirical validation provides a comprehensive exploration of the proposed methods.  
- The paper is well-written and accessible, making complex concepts easier to follow.

Weaknesses:  
- Certain assumptions, particularly in Assumptions 2 and 3, are perceived as unintuitive and require clarification.  
- The empirical validation is limited in scope, focusing on a few specific tasks and datasets, which may not represent the diversity of real-world applications.  
- The comparison with existing transfer learning methods is insufficient, hindering a full evaluation of the proposed approach's advantages and limitations.  
- The focus on linear models and a specific algorithm (SGD) may restrict the generalizability of the results.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Assumptions 2 and 3, particularly regarding the dependence of covariance on the mean and the conditions for bounded operator norms. Additionally, we suggest expanding the empirical validation to include a broader range of datasets with varying characteristics to better assess the proposed methods. A more comprehensive comparison with existing transfer learning techniques would also strengthen the paper. Finally, we encourage the authors to explore the implications of their findings for deep learning practices, as this connection remains underexplored.