ID: LkTbLQDo4I
Title: Optimizing protein fitness using Bi-level Gibbs sampling with Graph-based Smoothing
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 5, 5, 4, 4, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 4, 3, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel sampling method for directed protein evolution, incorporating Bi-level Gibbs sampling (BiG), graph-based smoothing (GS), and iterative extrapolation (IE). The method aims to enhance sampling efficiency for multi-point mutant sequences, guide sampling towards high fitness sequences, and minimize Taylor approximation errors. Experimental results indicate that the proposed method achieves competitive performance on datasets such as Green Fluorescent Proteins (GFP) and Adeno-Associated Virus (AAV). Additionally, the authors introduce a negative data augmentation technique that doubles the dataset size by sampling random sequences and assigning them the lowest fitness, which they argue is straightforward and beneficial for improving predictor performance. They also discuss the limitations of the Gibbs with Gradients (GWG) sampling technique, which requires GS to effectively model the problem, and propose IE as a procedure to enhance GWG while managing computational costs.

### Strengths and Weaknesses
Strengths:
- The presentation is clear, with well-formulated problems and detailed descriptions of the method components.
- The experiments are comprehensive, including ablation studies that effectively reveal the impact of each module.
- The addressed problem is significant, with potential applications across various protein design scenarios.
- The authors provide a clear rationale for their choice of negative data augmentation, highlighting its simplicity and effectiveness.
- The combination of GWG with GS demonstrates improved performance, supported by ablation studies.
- The proposed IE method offers a computationally efficient way to enhance GWG.

Weaknesses:
- The approach is somewhat heuristic and lacks theoretical guarantees.
- There is insufficient comparison with strong baselines, such as diffusion-based methods, making it difficult to assess the method's effectiveness.
- The motivation for random sampling the entire sequence is unclear, and the writing contains numerous typos and organizational issues.
- There is a concern regarding the ability of other parts of the proposed method to model the problem effectively, as indicated by the performance of prior baselines.
- The reliance on GWG's assumptions about predictor smoothness may limit the method's applicability in certain scenarios.

### Suggestions for Improvement
We recommend that the authors improve the theoretical foundation of the proposed method to provide stronger assurances of its effectiveness. Additionally, the authors should include comparisons with established baselines, particularly deep learning-based methods, to validate their claims of performance. Clarifying the rationale behind random sampling and addressing the identified writing and organizational issues would also enhance the paper's clarity and impact. Furthermore, we suggest improving clarity regarding the necessity of other components in their method to achieve high fitness levels, particularly in relation to the performance of prior baselines. Addressing the concerns about the modeling capabilities of other parts of the proposed method could strengthen the paper's argument. Finally, including a case study that demonstrates the method's application on a real-world dataset would substantiate its practical relevance.