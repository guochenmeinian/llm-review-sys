ID: MbwVNEx9KS
Title: Energy Transformer
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 6, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a new transformer architecture, the Energy Transformer (ET), which minimizes energy through two networks resembling Hopfield networksâ€”one focusing on attention within data points and another on global features. The architecture aims to enhance interpretability and performance, particularly on graph-based benchmarks, and suggests potential applications across various domains, including computer vision and NLP.

### Strengths and Weaknesses
Strengths:  
- The problem addressed is highly relevant, with a well-motivated contribution to transformer architecture.  
- Strong performance improvements on graph benchmarks are noted.  
- The visual interpretation of intermediate token states is commendable.  
- The paper is well-written and presents ideas clearly.  

Weaknesses:  
- Evaluation is limited to graph tasks, lacking exploration in NLP and large-image representation learning, which are crucial for broader acceptance.  
- There is insufficient discussion regarding the computational cost and scalability of the architecture.  
- The evaluation of image completion lacks quantitative comparisons to state-of-the-art algorithms.  
- The theoretical justification for the architecture's effectiveness is not rigorously established.  

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including NLP tasks and large-image representation learning to strengthen the paper's claims. Additionally, a detailed discussion on the computational cost and scalability should be integrated into the main text. The authors should also provide quantitative comparisons for image completion tasks against established benchmarks, such as FID scores, to enhance the empirical validation of their claims. Lastly, a more rigorous theoretical justification for the architecture's effectiveness would bolster the paper's contributions.