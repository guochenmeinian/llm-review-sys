ID: StD4J5ZlI5
Title: Dataset Diffusion: Diffusion-based Synthetic Data Generation for Pixel-Level Semantic Segmentation
Conference: NeurIPS
Year: 2023
Number of Reviews: 26
Original Ratings: 5, 6, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 5, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for generating synthetic training data for semantic segmentation using Stable Diffusion (SD). The authors utilize captions from existing datasets to create text prompts for SD, which generates synthetic images. The segmentation maps are derived by refining cross-attention maps with self-attention maps. The generated segmentation masks, which include uncertainty regions, serve as pseudo-labels for training segmentation models through self-training, evaluated with Test Time Augmentation. The method shows promise in reducing annotation costs while achieving competitive results on VOC and COCO datasets. Additionally, the authors argue that their approach effectively handles various image domains, including less common ones such as human faces and driving scenes, and performs well even in cases of class imbalance and rare object classes. They emphasize that their method does not rely on labor-intensive image collection or pixel-wise annotation, leveraging a pre-trained foundational model like Stable Diffusion. The authors plan to enhance their results by incorporating a new column in Table 5 to demonstrate the impact of self-attention on cross-attention refinement, showing a performance increase of approximately +15 mIoU.

### Strengths and Weaknesses
Strengths:
- The application of a pretrained stable diffusion model effectively generates synthetic datasets, potentially lowering annotation costs while maintaining performance.
- The integration of self-attention and cross-attention maps for mask generation, along with an uncertainty-aware segmentation loss, enhances the quality of the segmentation masks.
- The approach effectively addresses various image domains, including rare ones, and provides evidence of performance superiority over existing methods, such as DiffusionSeg.
- The ablation studies provide valuable insights into the contributions of different components of the method.

Weaknesses:
- The novelty of the approach is limited, as similar strategies have been explored in prior works without sufficient differentiation.
- The paper lacks comprehensive comparisons with existing methods, particularly regarding finetuning on real data and the performance of other generative models.
- The method's reliance on a fixed set of test classes restricts its applicability to unseen classes, and the potential biases in the generated dataset due to the training data are not adequately addressed.
- The efficacy of the method diminishes with certain datasets, like DroneDeploy, indicating limitations in its applicability.
- There is a need for a more thorough discussion of the method's limitations and supporting data in the paper.

### Suggestions for Improvement
We recommend that the authors improve the novelty of their approach by clearly distinguishing it from similar works, such as DiffuMask and others. Additionally, including a comparison with methods like DiffusionSeg and GAN-based approaches would strengthen the evaluation. We suggest conducting experiments to demonstrate the impact of hyperparameters on segmentation mask quality and discussing the potential biases in the generated dataset. Furthermore, we recommend that the authors improve the discussion of the limitations of their method, including supporting numbers, to provide a clearer understanding of its boundaries. Lastly, we suggest incorporating the revised Table 5 and Fig. 1 into the main paper to illustrate the role of self-attention in refining cross-attention more effectively.