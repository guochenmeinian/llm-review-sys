ID: 3qG4r6FGWD
Title: Aligning Predictive Uncertainty with Clarification Questions in Grounded Dialog
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 5
Original Ratings: -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper addresses the challenge of determining when to ask clarifying questions in grounded dialog due to ambiguities in user instructions. The authors propose several unsupervised methods to compute model uncertainty and conduct experiments to assess their alignment with genuinely ambiguous instructions. They demonstrate that their metrics correlate well with uncertainty, although they identify input length as a confounding factor. The paper also explores the relationship between dialog history length and prediction uncertainty.

### Strengths and Weaknesses
Strengths:
- The proposed method offers greater interpretability than separate classifiers for predicting uncertainty, as it derives uncertainty from the generative modelâ€™s own probability distribution.
- It is unsupervised, eliminating the need for labeled examples of uncertainty.
- The method leverages properties that can be computed on most off-the-shelf generative transformers, making it applicable to various existing use cases.
- The paper provides extensive empirical detail to support its claims.

Weaknesses:
- The implementation details of the ensemble with the T5 pretrained model lack clarity, particularly regarding how to ensure the finetuned models differ.
- There is insufficient reference to existing techniques for detecting out-of-domain inputs, complicating comparisons of the proposed metrics' novelty and effectiveness.
- The tasks appear artificial, with a clear distinction between ambiguous and unambiguous instructions, raising concerns about the robustness of the proposed metrics in real-world scenarios.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the ensemble model implementation, specifically addressing how to ensure the finetuned T5 models differ. Additionally, we suggest that the authors provide more context on existing techniques for detecting out-of-domain inputs to better position their contributions. To enhance the applicability of their findings, we encourage the authors to explore scenarios that reflect the ambiguity present in real-life instructions, potentially incorporating normalization techniques to account for dialog history length while maintaining interpretability.