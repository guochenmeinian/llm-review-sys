ID: 9SwKSvaCiP
Title: SING: A Plug-and-Play DNN Learning Technique
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 4, 6, 7, 6, 3, -1, -1, -1
Original Confidences: 4, 3, 4, 4, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents SING, a gradient preprocessing technique aimed at enhancing the stability and generalization of optimizers in deep learning. The authors argue that SING, which combines gradient centralization and normalization, allows for improved convergence properties and the ability to escape local minima. Theoretical analyses and empirical experiments on various datasets, including ImageNet-1K and NLP tasks, are provided to support the claims of SING's effectiveness.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and clear, making it accessible to readers.
2. SING is simple to implement and does not require additional hyperparameters, enhancing its applicability.
3. Empirical results demonstrate significant performance improvements over existing optimizers across multiple tasks.

Weaknesses:
1. The contribution appears incremental, as the method closely resembles existing techniques that normalize gradients, raising questions about its novelty.
2. Experimental validation is limited, with insufficient comparisons against strong baselines like gradient centralization combined with AdamW, which undermines the practical significance of SING.
3. The theoretical analysis lacks depth, focusing solely on gradients without considering momentum or learning rate schedules, which are critical for convergence.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the paper by clearly articulating the unique contributions of SING compared to existing methods. Additionally, we encourage the authors to include a comprehensive ablation study comparing SING against gradient centralization + AdamW across all experiments to validate its effectiveness. The theoretical analysis should be expanded to incorporate momentum and learning rate schedules, and the assumptions in Theorems 3.3 and 3.4 should be relaxed for broader applicability. Finally, we suggest clarifying the definitions and terms used throughout the paper to enhance understanding and consistency.