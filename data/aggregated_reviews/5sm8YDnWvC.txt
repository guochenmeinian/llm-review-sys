ID: 5sm8YDnWvC
Title: Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers
Conference: NeurIPS
Year: 2024
Number of Reviews: 9
Original Ratings: 6, 5, 6, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Bi-Directional Cross-Attention Transformers (BiXT), a novel architecture designed to enhance the efficiency of self-attention in transformers, achieving linear complexity concerning sequence lengths. The authors replace query embeddings with a fixed-length sequence of learned embeddings and demonstrate the model's effectiveness through ablations and evaluations on ImageNet 1k. The BiXT model, inspired by the Perceiver architecture, employs a bi-directional cross-attention module that allows simultaneous attention between input tokens and latent variables, making it suitable for longer sequences across various tasks.

### Strengths and Weaknesses
Strengths:  
- The paper addresses the critical efficiency of Vision Transformers, enhancing the quality/size trade-off in larger models.  
- The presentation is clear, well-written, and maintains appropriate technical depth.  
- The linear scaling of computational cost is a significant advantage, allowing effective handling of larger datasets and longer sequences.  
- The proposed method is a creative solution to a longstanding issue, achieving competitive performance across multiple tasks.  

Weaknesses:  
- The evaluation is limited, focusing primarily on short sequence lengths and visual tasks, which may not fully demonstrate the method's benefits.  
- There is a lack of experiments with longer sequences (10k+) and in the language domain, where longer sequences are common.  
- The presentation could be improved by visualizing comparisons with the Perceiver-IO series and specifying the architectural configuration of BiXT.  
- The analysis of the bi-directional cross-attention mechanism is insufficient, lacking experiments to support its claimed benefits.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including experiments with longer sequences (10k+) and exploring applications in the language domain. Additionally, we suggest enhancing the presentation by visualizing the comparison between BiXT and the Perceiver architecture, as well as clarifying the architectural configuration in tables. Finally, we encourage the authors to provide a more in-depth analysis of the bi-directional cross-attention mechanism to substantiate its effectiveness in refining 'what' and 'where'.