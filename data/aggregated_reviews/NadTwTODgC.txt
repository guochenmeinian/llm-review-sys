ID: NadTwTODgC
Title: Diffusion for World Modeling: Visual Details Matter in Atari
Conference: NeurIPS
Year: 2024
Number of Reviews: 14
Original Ratings: 7, 7, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents DIAMOND (DIffusion As a Model Of eNvironment Dreams), a novel reinforcement learning (RL) agent utilizing a diffusion-based world model. The authors argue that traditional world models relying on discrete latent variables may overlook crucial visual details, which are vital for RL tasks. DIAMOND generates observations in the original pixel space, demonstrating superior performance on the Atari 100k benchmark, achieving a state-of-the-art mean human-normalized score of 1.46. The training process involves data collection, world model training, and RL agent training, with a focus on design choices such as the diffusion framework (EDM over DDPM) and the number of denoising steps.

### Strengths and Weaknesses
Strengths:
- Clear and well-written paper.
- Strong empirical results surpassing prior model-based reinforcement learning (MBRL) baselines on Atari 100K.
- Impressive experiments for visual diffusion world modeling, maintaining consistent visual quality over long horizons with few diffusion steps.
- Thorough qualitative analysis of design choices for the diffusion world model.
- Publicly available code for the experiments.

Weaknesses:
- Lack of quantitative analysis for design choices; a more thorough quantitative assessment across tasks would strengthen the findings.
- Absence of training time comparisons for DIAMOND versus prior baselines, particularly regarding speed differences between diffusion and latent world models.
- Performance on certain tasks (e.g., BankHeist, Frostbite, UpNDown) is significantly worse than baselines, raising questions about the model's robustness.
- The analysis is primarily qualitative, with a need for objective measurements of generated trajectories compared to nearest neighbors in the replay buffer.
- Limited experimental evidence beyond Atari benchmarks, which raises concerns about the generalizability of the approach.

### Suggestions for Improvement
We recommend that the authors improve the paper by including a quantitative analysis of the design choices to provide stronger evidence for their effectiveness. Additionally, we suggest incorporating a comparison of training times for DIAMOND against prior baselines to clarify the efficiency of the diffusion world model. It would also be beneficial to investigate the performance of DIAMOND on a broader range of tasks beyond Atari to demonstrate its applicability in various environments. Finally, we encourage the authors to explore comparisons with other model-based approaches and consider the implications of using different generative models, such as flow matching, to enhance the understanding of their method's significance.