ID: vfju5hjrJw
Title: ComBack: A Versatile Dataset for Enhancing Compiler Backend Development Efficiency
Conference: NeurIPS
Year: 2024
Number of Reviews: 5
Original Ratings: 7, 7, 7, 8, -1
Original Confidences: 4, 4, 4, 3, -1

Aggregated Review:
### Key Points
This paper presents ComBack, the first public dataset aimed at enhancing compiler backend development capabilities for language models. It comprises a large-scale dataset with 178 backends for GCC and LLVM, encompassing over 180,000 functions and 5.7 million lines of code. The dataset supports three tasks: Statement-Level Completion, Next-Statement Suggestion, and Code Generation. The authors demonstrate that fine-tuning six pre-trained language models on ComBack significantly boosts performance across these tasks, with the fine-tuned CodeT5+ model (220M parameters) outperforming traditional methods and larger models. The authors argue that ComBack can improve the efficiency of compiler backend development, addressing the manual effort currently required. Additionally, the dataset is compiled from 317 public GitHub repositories and official GCC and LLVM source code, including 883.7 KLoC for GCC and 4,847.5 KLoC for LLVM. The authors validate the generalization capabilities of the top-performing model, CodeT5+, against new backend targets.

### Strengths and Weaknesses
Strengths:
- Introduces a novel dataset specifically for compiler backend development, filling a critical gap.
- Comprehensive evaluation across multiple pre-trained language models, demonstrating practical improvements.
- High-quality research with a well-structured methodology and thorough analysis.
- The paper is clear and accessible, making it suitable for readers without extensive compiler knowledge.
- Empirical results demonstrate that the dataset enhances LLM performance, providing a solid foundation for future research.
- Potential for extensibility to new target types and iterative expansion.

Weaknesses:
- Limited discussion on the computational resources required for practical implementation.
- The representativeness of the dataset concerning the full range of compiler backends is unclear.
- Lack of exploration regarding the overall quality or correctness of generated code in real compiler settings.
- Insufficient discussion on integration challenges with existing workflows and potential privacy concerns.
- The random sampling strategy for data splitting may lead to inflated results due to potential leakage of common patterns.
- The approach for code generation tasks diverges from other tasks, potentially causing models to overfit to specific architectures.
- Some design decisions lack sufficient clarification, which could enhance the transparency of the methodology.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the computational resources necessary for training and using these models in practical settings. Additionally, addressing how this approach might integrate with existing compiler development workflows would enhance its applicability. We suggest improving the data splitting strategy by evaluating inherited and custom functions separately to avoid potential leakage into test data and adopting a content-aware sampling approach to address the imbalance between LLVM and GCC backends. In section 4.2, please clarify the metrics used and their significance for readers unfamiliar with them. We also recommend explaining the choice of ChatGPT-3.5-Turbo and Code-LLaMA-34B as baselines to provide context for their selection. Furthermore, we advise against using concrete target-specific values in section 3.3.3, as this could lead to bugs in the generated code. Lastly, please ensure that the alignment in table 3 is improved for readability and address the anomaly observed with NVPTX in table 4.