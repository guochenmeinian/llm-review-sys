ID: 20FxHX25aq
Title: The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline
Conference: NeurIPS
Year: 2023
Number of Reviews: 2
Original Ratings: 8, 7
Original Confidences: 3, 4

Aggregated Review:
### Key Points
This paper presents a data poisoning attack against diffusion models, aiming to breach copyright by enabling these models to produce copyrighted images without having been trained on them. The authors propose using ChatGPT to generate descriptive captions and extract key phrases from copyrighted images, followed by employing GroundingDINO to identify corresponding regions in the images. These regions are segmented to create a set of images using an inpainting diffusion model, which, along with their captions, serve as poisoned data. The evaluation is conducted on the Pokemon BLIP captions dataset, demonstrating the attack's effectiveness in retrieving copyrighted images from diffusion models.

### Strengths and Weaknesses
Strengths:
- The method operates within a practical threat model, accessing only training data while remaining stealthy.
- Experiments validate the effectiveness of the attack.
- The paper highlights a property of diffusion models, indicating that stronger models are more susceptible to attacks.

Weaknesses:
- Related works should be integrated into the main paper.
- The construction of samples 11 and 12 in Bul. poisoning data and Cate. poisoning data requires clearer explanation, as the current description in Line 163-164 is ambiguous.
- Quantitative results on the clean performance of diffusion models should be included.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the construction process for samples 11 and 12 in Bul. poisoning data and Cate. poisoning data. Additionally, we suggest integrating related works into the main body of the paper and including quantitative results on the clean performance of diffusion models to enhance the overall rigor of the study.