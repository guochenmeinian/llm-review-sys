ID: nBjmMF2IZU
Title: Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 15
Original Ratings: 6, 7, 7, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an algorithmic framework that fine-tunes large vision-language models (VLMs) using reinforcement learning (RL) to enhance their performance in multi-step, goal-directed decision-making tasks. The authors address the limitations of traditional visual instruction tuning, which relies on pre-collected datasets and may not effectively train VLMs for interactive scenarios. Their framework incorporates task descriptions to prompt VLMs to generate chain-of-thought (CoT) reasoning, which is then translated into executable actions. Empirical results demonstrate that this method significantly improves decision-making capabilities, enabling the VLMs to outperform models like GPT-4V and Gemini.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and clear, making it accessible and easy to follow.
2. The proposal of an alternative to current visual instruction tuning for decision-making is intriguing, with well-justified design choices.
3. The execution is particularly noteworthy, especially the method of balancing the influence of CoT output and action output.

Weaknesses:
1. The current implementation requires fine-tuning for each task individually, despite large VLMs being capable of handling multiple tasks.
2. The trade-off between CoT output and action output is manually set, raising questions about how this parameter can be adjusted for training a multi-task policy.
3. The computational cost remains high, even with Low-Rank Adaptation (LoRA) for fine-tuning.
4. The evaluation tasks do not fully convey the benefits of using a multi-modal large language model (MLLM), particularly regarding the model's reasoning ability and self-correction in critical scenarios.

### Suggestions for Improvement
We recommend that the authors improve the framework by exploring how visual prompting could enhance performance. Additionally, the authors should clarify the source of CoT's effectiveness in improving reasoning, distinguishing between manually labeled key states and the textual thought process. It would also be beneficial to compare the proposed RL algorithm with other RL methods, such as "OFFLINE RL FOR NATURAL LANGUAGE GENERATION WITH IMPLICIT LANGUAGE Q LEARNING" and "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL." Finally, we suggest that the authors demonstrate that RL training improves the accuracy of information extracted by VLMs from images, providing direct evidence of the viability of this RL training method for VLMs.