ID: bTHFrqhASY
Title: InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory
Conference: NeurIPS
Year: 2024
Number of Reviews: 16
Original Ratings: 5, 5, 7, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a training-free memory-based method for long context extension in Transformers, termed InfLLM. The authors propose a block memory retrieval module that utilizes a sliding window attention mechanism, allowing each token to attend only to relevant contexts from memory. InfLLM effectively processes long texts using a limited context window, achieving comparable performance to Llama3-1M with a significantly smaller window size. However, it requires multiple retrieved blocks for optimal performance, indicating a need for improvement in memory unit retrieval. Extensive experiments demonstrate the method's effectiveness on benchmarks like InfiniteBench and LongBench, while also illustrating that increasing the amount of relevant content retrieved can enhance performance but may lead to diminishing returns beyond a certain threshold. The authors emphasize the importance of training-free context extrapolation, although fine-tuning can also facilitate extrapolation in other models.

### Strengths and Weaknesses
Strengths:
- The paper is well written and easy to follow.
- InfLLM offers a novel approach to handling long texts without training, maintaining a limited context window to select crucial information.
- The proposed method is simple yet effective, showing good performance on benchmarks and comparable results to full-attention models.

Weaknesses:
- The paper lacks novelty, as it does not introduce new technical challenges or methods, relying on existing techniques like block-wise attention and retrieval methods.
- Comparisons with stronger baseline models are insufficient, undermining the robustness of the results.
- The necessity of several retrieved blocks for optimal performance suggests limitations in the current retrieval mechanism.
- The datasets used do not adequately reflect the changes in effective context window length, and there is a lack of necessary ablation studies.

### Suggestions for Improvement
We recommend that the authors improve the novelty of the paper by clearly distinguishing their contributions from existing methods and providing comparisons with stronger baseline models, particularly those capable of handling long context windows. Additionally, we suggest improving the precision of memory unit retrieval to enhance InfLLM's performance and including results from other relevant benchmarks, such as ruler and long context code understanding benchmarks, to strengthen the evaluation. Clarifying the relationship between the use of Llama3 and the observed lower accuracy in the RULER experiment would also strengthen the manuscript. We encourage the authors to conduct ablation studies to demonstrate the effectiveness of the block-level design and expand their discussion on the training-free paradigm and its implications in the revised manuscript.