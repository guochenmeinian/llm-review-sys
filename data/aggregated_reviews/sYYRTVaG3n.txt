ID: sYYRTVaG3n
Title: Meta-Learning of Prompt Generation for Lightweight Prompt Engineering on Language-Model-as-a-Service
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents MetaL-Prompt, a meta-learning approach for prompt generation that enables the generation of hints for unseen tasks without additional training. The method utilizes two identical Language Models (LMs), with one serving as a prompt generator and the other as a response generator, achieving superior results on QA datasets with minimal computational cost. The authors propose that their method addresses the challenges of existing language models as a service (LMaaS) and the need for large samples for prompt tuning.

### Strengths and Weaknesses
Strengths:
- The paper is clearly written, with appropriate diagrams that enhance understanding.
- Experimental results are comprehensive and demonstrate the method's effectiveness, outperforming state-of-the-art baselines.
- The approach establishes a connection between meta soft-prompts and discrete demonstrations, which is innovative.
- The method shows promise in low-resource conditions and does not require fine-tuning for comparable results.

Weaknesses:
- The paper lacks a discussion of OpenPrompt, which has similar functionalities, and does not provide case studies to illustrate its contributions.
- Limited innovation and technical depth are evident, with some sections being repetitive and lacking clarity.
- The reliance on smaller models for experimentation diminishes the relevance of results to industry applications, and the absence of code and data hinders reproducibility.

### Suggestions for Improvement
We recommend that the authors improve the clarity and detail of the methodology sections, particularly Sections 3.1 and 3.2, to enhance understanding. Additionally, providing case studies would help illustrate the effectiveness of the method. The authors should also address the absence of a substantial baseline comparison and consider using larger models to validate their approach. Furthermore, we suggest including more experimental evidence regarding the computation cost for prompt generation and clarifying the workings of the hidden states used as prompts.