ID: mpDbWjLzfT
Title: CONTRAST: Continual Multi-source Adaptation to Dynamic Distributions
Conference: NeurIPS
Year: 2024
Number of Reviews: 8
Original Ratings: 6, 5, 6, 7, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel continual multi-source adaptation method aimed at addressing the Test-Time Adaptation (TTA) task involving dynamic distributions. The proposed framework consists of two main steps: learning optimal combination weights for merging multiple source models and identifying which source model parameters to update. The authors provide a thorough theoretical analysis of optimization convergence and test risk bounds, supported by extensive experiments that demonstrate the method's effectiveness.

### Strengths and Weaknesses
Strengths:
- The authors propose an innovative method for tackling the challenging TTA task with dynamic distributions.
- The theoretical analysis is comprehensive and well-supported.
- The writing quality is good, and the paper is well-structured, making it easy to understand.

Weaknesses:
- The experimental results do not adequately support the claims of the method's adaptability to dynamic test data distributions, as the authors did not construct a corresponding test dataset.
- The experiments lack comparisons with recent TTA methods, only including Tent, CoTTA, and EaTA, while omitting others like RoTTA, TRIBE, and ROID.
- Some parts of the writing are unclear, particularly regarding the definitions and calculations of test batches and certain algorithmic descriptions.
- The framework largely follows existing strategies from multi-source domain adaptation methods, lacking clear differentiation from previous work.

### Suggestions for Improvement
We recommend that the authors improve their experimental validation by constructing a dynamic test dataset that reflects the conditions they claim to address. Additionally, incorporating more recent TTA methods in their experiments would strengthen their claims. We suggest clarifying unclear sections in the writing, particularly regarding the t-th test batch and the descriptions in Algorithm 1. Furthermore, the authors should discuss the computational complexity of their learning rate selection method, considering the minimal difference in performance compared to a fixed learning rate. Finally, adding two baselines—one using the original TTA method with multiple source models and another independently updating from the original model—would provide a more robust evaluation of their approach.