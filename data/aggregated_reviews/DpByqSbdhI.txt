ID: DpByqSbdhI
Title: UniMTS: Unified Pre-training for Motion Time Series
Conference: NeurIPS
Year: 2024
Number of Reviews: 26
Original Ratings: 6, 7, 7, 7, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 5, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a unified pre-training framework for human activity recognition from motion time series data, referred to as UniMTS. The framework utilizes simulated data generated from motion skeleton sequences and employs a contrastive learning approach that aligns motion data with text descriptions, enhanced by large language models (LLMs). The authors demonstrate that UniMTS significantly outperforms existing baselines, achieving a 342.3% improvement in zero-shot recognition while maintaining high performance in full-shot settings. Extensive evaluations across 18 datasets highlight the model's efficiency in terms of time and space complexities compared to ImageBind, and its ability to generalize across datasets with varying device placements and activity types.

### Strengths and Weaknesses
Strengths:
- The manuscript is well-written and clearly articulates the methodological gap in motion time series data.
- The simulation framework is a significant contribution, addressing challenges in collecting large-scale datasets.
- The use of graph neural networks (GNNs) for joint embedding is innovative and effectively supports downstream tasks.
- UniMTS shows impressive performance metrics, particularly in zero-shot settings, demonstrating practical applicability with minimal labeled data.
- The theoretical analysis of time and space complexities is comprehensive and highlights the model's efficiency.
- Comprehensive evaluations on various benchmark datasets enhance the validity of the findings.

Weaknesses:
- The paper lacks sufficient discussion of existing pre-trained models for motion time series, missing key references that could contextualize the contributions.
- Concerns arise regarding the quality and generalizability of synthetic data compared to real data, particularly in practical applications.
- The model struggles to generalize to datasets with different device placements due to its reliance on the Ego4D dataset.
- The abstract does not clearly convey the main contributions, and the specific LLM used is not mentioned early on.
- There is a need for further clarification on the zero-shot learning process, particularly regarding the necessity of labeled examples for new categories.
- The performance in zero-shot settings raises questions about practical applicability, especially for datasets with limited classes.

### Suggestions for Improvement
We recommend that the authors improve the discussion on related work by including significant studies on pre-training motion time series, such as those from Yuan et al. (2024) and Spathis et al. (2021). Additionally, it would be beneficial to compare the proposed method with recent benchmarks like Ego4D and Exo to clarify its position relative to state-of-the-art methods. 

We suggest that the authors address the potential limitations of synthetic data by providing a more detailed analysis of its generalizability and the conditions under which it may underperform. Clarifying the specific LLM utilized and enhancing the abstract to reflect the core contributions more clearly would also strengthen the paper. 

Furthermore, we recommend improving the clarity around the zero-shot learning process, specifically addressing the requirement for labeled examples from existing categories to facilitate predictions on new labels. Finally, conducting experiments to evaluate the efficiency of the proposed method in terms of training time compared to baselines, as well as discussing the implications of using synthetic versus real data in different contexts, would enhance the robustness of the findings.