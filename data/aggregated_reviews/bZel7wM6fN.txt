ID: bZel7wM6fN
Title: Understanding the Inner-workings of Language Models Through Representation Dissimilarity
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 6
Original Ratings: -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the application of dissimilarity metrics to investigate the inner workings of language models, specifically focusing on representation dissimilarity as a tool for understanding model behavior. The authors propose three main insights: 1) an asymmetry in internal representations between models using SoLU and GeLU activation functions; 2) dissimilarity measures revealing generalization properties not evident from in-distribution performance; and 3) evaluations of how features in language models vary with changes in model width and depth. The methods discussed include model stitching and centered kernel alignment (CKA).

### Strengths and Weaknesses
Strengths:  
- The paper introduces novel techniques for analyzing language models, utilizing underrepresented methods in NLP/NLU.  
- It effectively addresses relevant concerns regarding model generalization and interpretability.  
- The execution and motivation of the work are well appreciated, with strong support for claims and arguments.

Weaknesses:  
- Insights remain abstract and disconnected from linguistic implications; the authors do not sufficiently discuss how results impact language processing.  
- Clarity and motivation for certain design choices are lacking, such as the rationale for focusing on activation functions and the specifics of experimental setups.  
- Some conclusions drawn from the findings are sparse and require more thorough support.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the implications of their findings for language encoding and processing. Specifically, for each study, elaborate on the potential impact of observed phenomena on model behavior. Additionally, clarify the motivation behind the focus on activation functions and provide more detailed explanations for experimental designs. We suggest extending the paper to include more comprehensive studies that support each conjecture, particularly regarding the influence of SoLU activations on hidden feature layers. Finally, we advise revising the clarity of figures and terminology to enhance reader comprehension.