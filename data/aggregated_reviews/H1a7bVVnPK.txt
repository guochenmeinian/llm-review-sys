ID: H1a7bVVnPK
Title: Accelerated Training via Incrementally Growing Neural Networks using Variance Transfer and Learning Rate Adaptation
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 7, 6, 5, 5, 6, -1, -1, -1, -1
Original Confidences: 4, 4, 2, 4, 4, 3, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method for efficiently growing neural networks by dynamically stabilizing weight, activation, and gradient scaling as the architecture expands. The authors propose several novel techniques, including a new initialization method for growing networks and a learning rate adaptation mechanism that rebalances gradient contributions from different subnetworks. The experimental results demonstrate that the proposed approach achieves comparable or improved accuracy compared to large fixed-size models while reducing training time across various benchmarks.

### Strengths and Weaknesses
Strengths:  
- The paper introduces a novel synergy of methods for growing neural networks, demonstrating originality and practical significance.
- It provides empirical evidence showing that the proposed method can achieve real-time speed-ups and maintain model accuracy, which is crucial for the community.
- The clarity of communication regarding methodologies and results enhances reader understanding.

Weaknesses:  
- Formatting issues are present, such as Table 5 violating the right margin and Table 6 being difficult to read. Many figures contain illegible text when printed.
- The experimental evaluation is limited to small-scale tasks, raising concerns about the applicability of the methods to larger-scale problems.
- The motivation for certain techniques, such as the learning rate adaptation based on weight norm, lacks sufficient justification.

### Suggestions for Improvement
We recommend that the authors improve the formatting of the paper, ensuring that all tables and figures are legible and properly aligned. Clarifying the concept of "symmetry breaking" in relation to the methods discussed would enhance understanding. Additionally, we suggest providing more details about the ImageNet experiments and ensuring that comparisons with other methods are conducted under consistent conditions. Justifying the choice of learning rate adaptation rules and exploring alternative strategies could strengthen the paper. Finally, addressing limitations explicitly, including potential societal impacts, would improve the overall rigor of the work.