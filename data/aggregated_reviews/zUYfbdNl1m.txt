ID: zUYfbdNl1m
Title: $S^3$: Increasing GPU Utilization during Generative Inference for Higher Throughput
Conference: NeurIPS
Year: 2023
Number of Reviews: 16
Original Ratings: 5, 6, 7, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 3, 4, 3, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents S^3, a framework designed to enhance throughput during inference on Transformer large language models (LLMs) by predicting output sequence lengths. The authors utilize a fine-tuned DistilBERT model to forecast sequence lengths, allowing for efficient memory allocation of the KV cache. The framework aims to address GPU underutilization and improve batch size management, reporting up to a 6.49 times increase in throughput. The experimental analysis demonstrates the flexibility of S^3 in balancing latency and throughput.

### Strengths and Weaknesses
Strengths:
- The proposed solution effectively addresses the critical issue of memory and compute inefficiency in LLMs, making it a significant contribution to the field.
- The simplicity of the approach, combined with negligible overhead from the predictor, enhances its practicality.
- The paper is well-written, with clear evaluations across various settings, showcasing substantial improvements in throughput.

Weaknesses:
- The effectiveness of the sequence length predictor is not thoroughly evaluated, with discrepancies in batch size predictions needing clarification.
- The evaluation lacks diversity in request patterns and does not measure throughput in terms of tokens per second, which complicates interpretation.
- The paper does not adequately address how much each technique contributes to overall latency and fails to compare S^3 against a broader range of existing systems.

### Suggestions for Improvement
We recommend that the authors improve the evaluation by including results for scenarios without KV caching and by comparing S^3 against a wider array of existing frameworks beyond ORCA. Additionally, we suggest that the authors clarify the effectiveness of the sequence length predictor and provide an ablation study on the choice of predictor models. To enhance clarity, we encourage the authors to illustrate the length-aware sequence scheduler more intuitively and to evaluate the proposed method on various GPU architectures. Finally, we recommend measuring throughput in terms of tokens per second to provide a clearer understanding of performance metrics.