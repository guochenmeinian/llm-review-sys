ID: GcEIvidYSw
Title: Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning
Conference: NeurIPS
Year: 2023
Number of Reviews: 12
Original Ratings: 5, 5, 6, 6, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 2, 4, 3, 3, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Calibrated Q-learning (Cal-QL), a method for efficient online fine-tuning that learns effective initialization from offline data. Cal-QL underestimates the value of the learned policy while ensuring reasonable Q-value scales. The authors provide a theoretical analysis of cumulative regret during online fine-tuning and demonstrate that Cal-QL outperforms previous methods on 9 out of 11 benchmark tasks.

### Strengths and Weaknesses
Strengths:
- The writing is clear and the paper is well-organized.
- The authors introduce a new definition of calibration and provide theoretical analysis.
- Cal-QL can be implemented with minimal changes to conservative Q-learning (CQL) and shows consistent improvements over baseline methods.
- Empirical investigations effectively justify the calibration approach.

Weaknesses:
- Concerns arise regarding potential overestimation due to selecting values larger than the behavior policy, especially with non-diverse datasets.
- The paper lacks experimental results on locomotion tasks, raising questions about performance on these benchmarks.
- Important baseline methods related to pessimistic RL algorithms are missing from comparisons.
- The assumption that Cal-QL is particularly effective only when the reference policy is close to the expert policy appears overly strong.
- Hyperparameter sensitivity, particularly regarding the coefficient (\(\alpha\)) in equation 3.1, is not adequately explored.

### Suggestions for Improvement
We recommend that the authors improve the clarity of baseline comparisons, particularly with RLPD and HyQ, to avoid confusion. Additionally, conducting experiments on locomotion datasets would enhance the robustness of the findings. We also suggest including hyperparameter ablation studies for \(\alpha\) and the mixing ratio to assess the sensitivity of the method. Finally, addressing the potential overestimation issue in diverse datasets would strengthen the paper's contributions.