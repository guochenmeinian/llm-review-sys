ID: OfOCl3dGcF
Title: ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 9, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel compositional reasoning benchmark called ConMe, which aims to address the inadequacies of existing benchmarks like SugarCrepe by utilizing a more powerful data generation pipeline. The authors propose that ConMe, generated through an iterative process involving strong vision-language models (VLMs), leads to a significant performance drop (up to 33%) for state-of-the-art (SOTA) VLMs compared to their performance on previous benchmarks. The methodology claims to produce a dataset that challenges VLMs more effectively, highlighting ongoing difficulties in visual understanding.

### Strengths and Weaknesses
Strengths:
- The paper tackles a significant issue in compositional reasoning, aiming to enhance visual understanding in SOTA models.
- It proposes an innovative, iterative method for generating more challenging compositional reasoning questions based on existing datasets.
- The benchmark results indicate that even leading VLMs struggle with the new dataset, suggesting potential for advancing the field.

Weaknesses:
- The methodology is overly reliant on GPT-4V, raising concerns about the quality and completeness of the generated questions.
- There is insufficient evidence provided to substantiate claims regarding the irrelevance of negative examples in the SugarCrepe dataset.
- The lack of qualitative examples makes it difficult to understand the nature of the questions in ConMe compared to other datasets.
- Discrepancies in performance results between models raise questions about the validity of the findings.

### Suggestions for Improvement
We recommend that the authors improve the exposition of the paper by providing qualitative examples that illustrate the differences between ConMe and previous datasets. Additionally, the authors should address the reliance on GPT-4V by discussing potential limitations and error modes in the generated questions. It would be beneficial to include a more thorough analysis of the negative examples in SugarCrepe to support claims made in the paper. Furthermore, we suggest clarifying the performance discrepancies observed between different models and providing a more detailed discussion on the implications of the results for the field of compositional reasoning.