ID: Tk4tvmdKVP
Title: Not all quantifiers are equal: Probing Transformer-based language models' understanding of generalised quantifiers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper studies the ability of language models to understand generalized quantifiers, such as "All" and "most," through prompting in zero-shot settings and fine-tuning. The authors find some success, particularly in fine-tuning scenarios, and explore the differences in performance based on prompting methods and model scale. The work contributes to understanding how transformer-based language models (TLMs) handle logical semantics and the complexities of quantifiers, including their implications for negation and conjunction.

### Strengths and Weaknesses
Strengths:
- The paper addresses an important and under-researched topic, demonstrating a sound approach and providing interesting results.
- It includes a comprehensive methodology section and evaluates multiple state-of-the-art models, enhancing reliability and generalizability.
- The use of logical semantics allows for careful control over the data-generation process, facilitating model-checking.

Weaknesses:
- The analysis lacks thoroughness, particularly in examining differences between quantifiers rather than just models.
- Key experimental details, such as the values of k and the nature of fine-tuning, are either missing or difficult to locate.
- The paper does not meet reproducibility criteria due to the absence of dataset and code availability, and the limited number of templates used for sentence generation may affect the results' validity.

### Suggestions for Improvement
We recommend that the authors improve the analysis by focusing more on the differences between quantifiers, as this could provide deeper insights into their effects on model performance. Additionally, we suggest clarifying the experimental setup, particularly regarding the values of k used for "at least k" and the specifics of fine-tuning, such as whether instruction-tuning was employed. To enhance reproducibility, the authors should ensure dataset and code availability and provide detailed information on hyper-parameter tuning and data generation processes.