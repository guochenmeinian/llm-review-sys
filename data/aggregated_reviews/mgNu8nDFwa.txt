ID: mgNu8nDFwa
Title: Beyond Average Return in Markov Decision Processes
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 6, 6, 7, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents an investigation into general functionals of the distribution over returns in reinforcement learning (RL) across three scenarios: (1) proving error bounds for a distributional RL algorithm in policy evaluation, (2) exhibiting an optimizable functional form via dynamic programming for planning, and (3) providing a Q-learning algorithm applicable to linear or exponential functionals. The authors also explore the implications of using distributional Q functions for policy evaluation and planning, particularly focusing on the concept of "Bellman optimizability" and its restrictions to exponential utilities.

### Strengths and Weaknesses
Strengths:  
- The paper addresses an important question relevant to sequential decision-making with sophisticated criteria.  
- The writing is generally clear, and the presentation of methods and results is well-structured.  
- The main result regarding the optimizability of functionals by Distributional RL is significant and clarifies the theoretical limits of the subfield.

Weaknesses:  
- The error bound for policy evaluation appears loose, and the novelty of some results is questionable, particularly regarding the relationship between Bellman closedness and optimizability.  
- The proposed Q-learning algorithm's linear case is trivial, and the exponential case lacks distinction from existing methods.  
- The paper contains several typographical errors and unclear notations that detract from its clarity.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the paper by explicitly stating the main result in the Abstract and Introduction sections. Additionally, please address the loose error bounds for policy evaluation and clarify the relationship between Bellman closedness and optimizability. We suggest providing a proof sketch or explanation for the critical assumption regarding the utility integrand in Theorem 2. Furthermore, please ensure all notations are clearly defined and correct any typographical errors throughout the manuscript. Lastly, we encourage the authors to discuss potential limitations, societal impacts, and future work more thoroughly.