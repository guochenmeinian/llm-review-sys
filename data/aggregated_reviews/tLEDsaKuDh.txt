ID: tLEDsaKuDh
Title: Emergent Communication in Interactive Sketch Question Answering
Conference: NeurIPS
Year: 2023
Number of Reviews: 10
Original Ratings: 7, 6, 4, 4, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 3, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel multi-turn visual communication task called Interactive Sketch Question Answering (ISQA), where a sender sketches to communicate a target image, and a receiver answers questions and provides feedback. The authors propose an architecture based on existing components (MCAN, Fast-RCNN) while introducing innovative ideas such as dynamically restricting channel capacity and using focus boxes for feedback. They also introduce a triangular evaluation method balancing task accuracy, drawing complexity, and human interpretability.

### Strengths and Weaknesses
Strengths:
- The proposed two-turn visual communication game effectively models the necessity for two parties to communicate under partial observability.
- The architecture's design and implementation for the problem are complex and interesting.
- The training framework optimally balances task accuracy, drawing complexity, and human interpretability.

Weaknesses:
- The assessment of human interpretability using the CLIP model is insufficient; a human survey would be more appropriate.
- The interoperability/pragmatism balance relies on a CLIP-based loss, which may not align with the goal of modeling communication emergence.
- Experimental datasets lack detailed descriptions, particularly regarding task complexity and how tasks correspond to datasets.
- Results show inconsistencies, particularly in the Yes-No task, indicating a need for more experiments across diverse datasets.
- Notations and explanations require further clarification for reader comprehension.

### Suggestions for Improvement
We recommend that the authors improve the assessment of human interpretability by conducting an actual human survey rather than relying solely on the CLIP model. Additionally, we suggest providing a clearer explanation of the experimental datasets, including the complexity of tasks and how they relate to the datasets. More comprehensive experiments across various datasets should be conducted to ensure result consistency. We also encourage the authors to enhance notations and explanations throughout the paper to aid reader understanding.