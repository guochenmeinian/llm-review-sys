ID: uaNZvF1VFe
Title: Efficient Sign-Based Optimization: Accelerating Convergence via Variance Reduction
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 7, 7, 6, 7, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 4, 3, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents the Sign-based Stochastic Variance Reduction (SSVR) algorithm, which enhances the convergence rate of the traditional signSGD method by integrating variance reduction techniques. The authors achieve a convergence rate of $O(d^{1/2}T^{-1/3})$ for general stochastic optimization and $O(m^{1/4}d^{1/2}T^{-1/2})$ for finite-sum problems. Additionally, the paper introduces novel algorithms for distributed environments using an unbiased sign operation, resulting in improved convergence rates for heterogeneous data. The effectiveness of these methods is supported by numerical experiments.

### Strengths and Weaknesses
Strengths:
1. The proposed methods significantly improve convergence rates over traditional signSGD methods, enhancing performance for both general non-convex and finite-sum optimization.
2. The SSVR-MV algorithm is communication-efficient and well-suited for distributed settings, achieving superior convergence rates in heterogeneous environments.
3. Numerical experiments validate the proposed methods, demonstrating enhanced convergence speed and accuracy compared to existing sign-based optimization techniques.
4. The paper is well-written, with clear problem settings and contributions, and the proofs are easy to follow.

Weaknesses:
1. The authors should provide more experimental results to demonstrate the dependency on batch size, clarifying whether large batches are necessary for convergence.
2. A more detailed explanation of the differences between the stochastic unbiased sign operation and the traditional sign operation is needed, along with guidance on when to prefer one over the other.
3. The description of Algorithm 2 lacks rigor, particularly regarding the use of the variable $\tau$.
4. There are several typographical errors that need correction.

### Suggestions for Improvement
We recommend that the authors improve the experimental section by including additional results that demonstrate the performance of the proposed SSVR method with varying batch sizes. Furthermore, we suggest providing a more comprehensive discussion on the design of equation (3) and the necessity of the bounded gradient assumption in distributed settings. Clarifying the differences between the stochastic unbiased sign operation and the traditional sign operation would also enhance the paper's clarity. Lastly, we urge the authors to correct the identified typographical errors throughout the manuscript.