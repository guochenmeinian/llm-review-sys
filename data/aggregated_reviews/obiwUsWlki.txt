ID: obiwUsWlki
Title: A Cognitive Framework for Learning Debiased and Interpretable Representations via Debiasing Global Workspace
Conference: NeurIPS
Year: 2024
Number of Reviews: 4
Original Ratings: 9, 6, 6, 4
Original Confidences: 1, 3, 4, 4

Aggregated Review:
### Key Points
This paper presents a novel debiasing framework called Debiasing Global Workspace (DGW), inspired by Global Workspace Theory (GWT) from cognitive science. The authors propose specialized modules for learning disentangled representations of intrinsic and biased attributes, utilizing a shared "global workspace" to broadcast relevant information. The method is empirically validated on datasets such as Colored MNIST and Corrupted CIFAR-10, demonstrating improved interpretability and performance compared to other debiasing methods.

### Strengths and Weaknesses
Strengths:
- The use of GWT for debiasing is innovative, mimicking modular information processing in the human brain.
- The paper is well written, with sound experiments and effective visualizations that enhance qualitative evaluation.
- DGW does not require predefined bias labels, making it versatile across different datasets.

Weaknesses:
- The performance on in-distribution data is not convincing, with marginal improvements on counterfactual examples.
- Clarity in the explanation of the DGW framework could be improved, particularly for readers unfamiliar with cognitive theories.
- The computational complexity of DGW is not quantified, and the introduction of numerous hyperparameters contradicts claims of minimal overhead.
- The introduction lacks engagement with other invariant risk minimization (IRM) methods, and some claims appear detached from current trends in ML and AI.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the introduction, particularly in explaining how GWT is adapted for the DGW framework. Additionally, an ablation study should be included to demonstrate the contributions of individual components to performance changes. We suggest quantifying the computational complexity of DGW and providing a direct comparison of training times with baseline models. Furthermore, addressing the lack of discussion on other IRM methods and ensuring that claims in the introduction align with contemporary practices in ML and AI would strengthen the paper. Lastly, we advise the authors to clarify the GPU specifications used in experiments to avoid confusion.