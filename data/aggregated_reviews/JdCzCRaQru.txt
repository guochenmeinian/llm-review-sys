ID: JdCzCRaQru
Title: Scaling Gaussian Processes for Learning Curve Prediction via Latent Kronecker Structure
Conference: NeurIPS
Year: 2024
Number of Reviews: 2
Original Ratings: 8, 6
Original Confidences: 3, 3

Aggregated Review:
### Key Points
This paper presents a method that leverages a projection of a latent Kronecker structure to maintain the benefits of kernel usage while addressing missing values in the inference of joint models for learning curves and hyperparameters in AutoML tasks. This approach significantly reduces the required time and storage space by approximately half. The authors provide a clear exposition of their results, particularly highlighting the advantages of the latent Kronecker Gaussian process (LKGP) over naive methods.

### Strengths and Weaknesses
Strengths:  
- The proposal of a latent Kronecker structure is both simple and effective.  
- The paper is well-written, with clear explanations and convincing results, particularly in Figures 1 and 3.  

Weaknesses:  
- The exploration of specialized kernels and heteroskedastic noise models is limited.  
- The paper lacks detailed mathematical presentations and discussions on scalability and applications beyond learning curve prediction.  
- The empirical validation could be strengthened with more diverse datasets.  

### Suggestions for Improvement
We recommend that the authors improve the mathematical presentations to enhance clarity. Additionally, the authors should explore the impact of kernel choice on results and provide motivation for their selections of the RBF and Mat√©rn kernels. To enable reproducibility, the authors should make their LKGP code available online. It would also be beneficial to include a discussion on the motivation for illustrating performance per task in Figure 4, rather than reporting the median across all tasks. Finally, we suggest addressing the identified typographical and citation formatting issues for consistency.