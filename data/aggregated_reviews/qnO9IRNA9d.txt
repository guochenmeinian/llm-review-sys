ID: qnO9IRNA9d
Title: Instructed Language Models with Retrievers Are Powerful Entity Linkers
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents InsGenEl, a novel entity linking approach utilizing generative large language models (LLMs) instead of traditional discriminative methods. The authors describe three variations: InsGenEl, InsGenEl-R (which incorporates entity candidate neural retrieval), and InsGenEl-ICL (leveraging in-context learning). They propose a new sequence-to-sequence training objective with instruction tuning and a decoding algorithm that utilizes retrieved information. Evaluation across multiple benchmarks shows InsGenEl achieves over +3.0 F1 points improvement against discriminative baselines and +6.8 F1 points against generative ones, with InsGenEl-R being 4x faster than the baseline without quality loss.

### Strengths and Weaknesses
Strengths:
- Introduces innovative methods using pretrained generative LLMs.
- InsGenEl-R demonstrates significant speed improvements without sacrificing quality.
- Comprehensive evaluation and comparison against both discriminative and generative baselines.

Weaknesses:
- Clarity issues due to omitted details and examples, making comprehension difficult.
- The motivation for choosing generative models over discriminative ones is not clearly articulated.
- Lack of sufficient experimental details, particularly regarding baseline methods and the retriever's effectiveness.

### Suggestions for Improvement
We recommend that the authors improve the paper's clarity by including essential details and examples within the main text rather than relegating them to the Appendix. Additionally, the authors should clearly articulate the motivation for using generative models over discriminative ones. We suggest providing a more thorough ablation study on the retriever's impact and considering other retriever types like BM25 or TF-IDF for a broader analysis. Furthermore, the authors should clarify the differences between instruction tuning and fine-tuning, and address the questions raised regarding model size effects and instruction design.