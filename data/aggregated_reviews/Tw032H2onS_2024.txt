ID: Tw032H2onS
Title: Boosted Conformal Prediction Intervals
Conference: NeurIPS
Year: 2024
Number of Reviews: 11
Original Ratings: 6, 6, 6, 6, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 4, 4, 4, 4, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a gradient boosting-based methodology to enhance conformal prediction intervals by refining a conformal score function post-training. The authors introduce a more general score function family dependent on parameters (e.g., $\mu, \sigma$) and employ a post-hoc boosting stage to minimize property-specific losses, such as conditional coverage deviation and conformal interval length. The refined parameter estimates $\mu^{(\tau)}, \sigma^{(\tau)}$ are then utilized for conformal calibration and prediction interval generation. The authors propose leveraging training data during the boosting process, arguing that this maintains the validity of marginal coverage as long as the calibration and test sets are exchangeable. Empirical results indicate slight improvements over default local conformal scores and CQR across various regression tasks, with significant improvements observed in specific datasets.

### Strengths and Weaknesses
Strengths:
- The method addresses the practical issue of overly conservative conformal sets, offering a lightweight solution that does not alter the model weights of $f$.
- The proposed approach shows substantial improvements in local conformal score functions and experimental validation of the boosting procedure demonstrates notable improvements in coverage rates.
- The originality of learning a conformal score function post-training using the same data is a notable contribution, and the proposal to clarify the relationship between training data and exchangeability is a valuable addition.

Weaknesses:
- Evaluation and practicality concerns arise, particularly regarding the complexity of the boosting loss and its connection to the conditional coverage target. The empirical benefits appear marginal, with results suggesting limited improvement over existing methods.
- The baseline score functions used in experiments are weak and untuned, raising questions about the sensitivity of improvements to baseline choices.
- The connection between the boosting procedure and the overall conformal process lacks clarity, particularly regarding data reuse and its implications for calibration and testing.
- The paper lacks explicit discussion on the limitations of the modeling design, which could confuse readers, and the approximations made for differentiable objectives and their impact on CP outcomes are not clearly addressed.
- The current formulation of Theorem 5.1 is considered weak and requires renaming to better reflect its significance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the connection between the boosting procedure and the conformal prediction process, particularly addressing how the boosting dataset relates to calibration and test data. Additionally, it would be beneficial to provide a comprehensive description of the experimental settings, including the impact of potential distribution shifts. We suggest comparing the proposed method against other conformal algorithms to clarify its empirical benefits and exploring alternative methods for mapping trained model predictions to necessary parameters. Furthermore, we recommend that the authors improve the clarity of the limitations associated with their modeling design to aid reader understanding and explicitly discuss how the approximations affect CP outcomes. We also suggest formalizing the relationship between the boosting stage and the guarantees of split CP in a new proposition. Finally, we encourage the authors to report the effects of optimizing for conditional coverage on interval length and vice versa, and we suggest renaming Theorem 5.1 to a "remark" or "proposition" to better represent its content.