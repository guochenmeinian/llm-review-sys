ID: iATY9W5Xw7
Title: CAST: Cross-Attention in Space and Time for Video Action Recognition
Conference: NeurIPS
Year: 2023
Number of Reviews: 8
Original Ratings: 7, 6, 6, 6, 6, -1, -1, -1
Original Confidences: 4, 3, 5, 5, 4, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel two-stream architecture called Cross-Attention in Space and Time (CAST) for video action recognition, aiming to achieve a balanced spatio-temporal understanding using RGB input. The authors propose a B-CAST module that facilitates the exchange of complementary information between spatial and temporal experts, leveraging large-scale pre-trained models like CLIP and VideoMAE. The results demonstrate strong performance across various benchmarks, although improvements over existing methods are often marginal.

### Strengths and Weaknesses
Strengths:
1. The paper is well-written and structured, clearly explaining its contributions and motivations.
2. The implementation of the cross-attention mechanism is innovative and enhances two-stream fusion.
3. Extensive experiments validate the proposed method, showing good results on standard benchmarks.

Weaknesses:
1. The marginal improvements over existing methods raise questions about the contribution's significance, particularly on datasets like SSv2.
2. The complexity of the B-CAST mechanism may hinder understanding; simplification of its description is encouraged.
3. There is a lack of comprehensive comparisons regarding computational costs, including FLOPs and inference speed, which are crucial for evaluating the method's applicability.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the limited impact of the cross-attention mechanism on motion-dominated datasets and provide an analysis of performance trade-offs between verbs and nouns. Additionally, we suggest including more examples in the supplementary material and addressing the fairness of comparisons with fully fine-tuned models versus those using adapters. It would also be beneficial to report the total number of parameters and runtime latencies for a clearer understanding of the model's efficiency. Lastly, we encourage the authors to clarify discrepancies in figure captions and ensure consistency in terminology throughout the manuscript.