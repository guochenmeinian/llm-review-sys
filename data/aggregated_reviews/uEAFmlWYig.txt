ID: uEAFmlWYig
Title: Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models
Conference: EMNLP/2023/Conference
Year: 2023
Number of Reviews: 6
Original Ratings: -1, -1, -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a pruning strategy that balances accuracy, sparsity, robustness, and pruning cost. The authors propose using weight averaging to create a robust dense model, followed by an adaptive pruning method to achieve sparse models. The experimental results indicate the effectiveness of their approach. Additionally, the authors introduce an adaptive tuning method based on the Hessian matrix to guide the pruning process, which is claimed to preserve model performance. The paper also explores the relationship between robustness and knowledge coverage in language models.

### Strengths and Weaknesses
Strengths:
- The research topic is significant and timely, addressing the balance between robustness and efficiency in model pruning.
- Experimental results demonstrate strong performance, particularly in adversarial robustness assessments.
- The writing is generally clear, and the presentation is well-structured.

Weaknesses:
- Limitations of existing methods are not clearly articulated, making it challenging for readers unfamiliar with pruning to grasp the context.
- The method requires multiple models for weight averaging, potentially increasing computational costs.
- Some arguments lack sufficient support, and the evaluation is not thorough enough, particularly regarding the choice of baseline methods.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the limitations in existing methods to enhance understanding for a broader audience. Additionally, consider providing quantitative results on how the pruned model's weights compare to the dense model, as suggested by equation 4. We also advise choosing a more competitive baseline, such as Movement Pruning, to better contextualize the performance of the proposed method. Furthermore, please clarify the need for additional tuning after pruning and provide a clearer definition of the use of attention scores as a proxy for pretrained knowledge. Lastly, we suggest revising the writing for clarity, particularly in equations and figures, and ensuring that all figures are correctly labeled and referenced in the main text.