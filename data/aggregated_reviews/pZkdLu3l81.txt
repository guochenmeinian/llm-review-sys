ID: pZkdLu3l81
Title: Toward Information Theoretic Active Inverse Reinforcement Learning
Conference: NeurIPS
Year: 2024
Number of Reviews: 3
Original Ratings: 4, 8, 6
Original Confidences: 3, 3, 3

Aggregated Review:
### Key Points
This paper presents an approach for active inverse reinforcement learning (IRL) that utilizes full trajectories instead of single-state queries. The authors propose using expected information gain (EIG) as the acquisition function, estimated via an upper confidence bound (UCB) algorithm, which enhances efficiency. The method is evaluated in structured grid-world environments, demonstrating improvements in posterior entropy and apprentice regret compared to Q Entropy and random sampling.

### Strengths and Weaknesses
Strengths:  
- The motivation for using full trajectories is compelling and represents a novel contribution to the field of IRL.  
- The proposed acquisition function is generally well-explained and innovative.

Weaknesses:  
- The technical sections lack clarity, particularly in Section 3.1, where the computation of $\widehat{EIG}(s)$ is not clearly articulated.  
- Steps 1 and 2 are ambiguous, particularly regarding the estimation of the expert policy $\hat{\pi}^{r_i, \xi}_E$.  
- The experiments are limited in scope, and the paper does not adequately discuss potential scaling of the method to larger problems.  
- Figure captions are insufficiently informative, and certain terms (e.g., "mud," "water," "lava") are not referenced elsewhere in the text.

### Suggestions for Improvement
We recommend that the authors improve the clarity of Section 3.1 by explicitly linking the computation of $\widehat{EIG}(s)$ to the framework discussed earlier. Additionally, the authors should clarify how the expert policy $\hat{\pi}^{r_i, \xi}_E$ is estimated and its relationship to expert demonstrations. It would be beneficial to include a discussion on scaling the method for larger MDPs. Furthermore, we suggest that the authors provide more informative figure captions and ensure that all terms used are defined within the text. Lastly, including results that compare full trajectories to single-state queries in the main body of the paper would strengthen the contribution.