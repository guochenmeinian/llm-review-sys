ID: 1Hfhp9BhI5
Title: Joint Evaluation of Fairness and Relevance in Recommender Systems with Pareto Frontier
Conference: ACM
Year: 2024
Number of Reviews: 4
Original Ratings: -1, -1, -1, -1
Original Confidences: -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a novel approach for jointly evaluating fairness and relevance in recommender systems, termed Distance from Pareto Frontier (DPFR). The authors compute the Pareto frontier for existing relevance and fairness metrics and utilize the distance from this frontier as a standard for measuring jointly achievable fairness and relevance. The paper emphasizes the importance of fairness in recommendation systems and proposes a modular solution that effectively addresses the challenge of balancing fairness and relevance.

### Strengths and Weaknesses
Strengths:
- The research focus on evaluating fairness and performance in recommender systems is significant and valuable.
- The motivation for the proposed approach is clearly articulated.
- The introduction of a versatile metric computation framework compatible with various existing metrics enhances its applicability.
- The paper provides a detailed experimental setup, demonstrating the robustness of the proposed method through extensive experiments.

Weaknesses:
- The authors do not discuss the connection between their work and the web as required by conference guidelines.
- The experimental code is not publicly available, which limits reproducibility.
- Methodological unclarities exist regarding the replacement process and its impact on recommendation performance and fairness metrics.
- The experimental evaluation lacks clarity on certain metrics and results, and some terms are not adequately explained upon first introduction.
- The focus is primarily on individual item fairness, neglecting other dimensions such as group fairness, and the computation of the Pareto frontier may be resource-intensive for large datasets.

### Suggestions for Improvement
We recommend that the authors improve the discussion on the connection between their work and the web, as per conference requirements. Additionally, we encourage the authors to make their experimental code publicly available to enhance reproducibility. To address methodological unclarities, we suggest providing a formal explanation of how the replacement process ensures that the new recommendation list remains on the Pareto frontier. Furthermore, we recommend including numerical values of the distances to the Pareto frontier for each method in the experimental results and clarifying the implications of setting alpha = 0.5 uniformly across datasets. Lastly, we suggest exploring the computational complexity of the method for large datasets and considering a broader perspective on fairness that includes group fairness dimensions.