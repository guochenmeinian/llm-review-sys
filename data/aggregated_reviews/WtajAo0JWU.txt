ID: WtajAo0JWU
Title: Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset
Conference: NeurIPS
Year: 2023
Number of Reviews: 22
Original Ratings: 6, 6, 8, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1
Original Confidences: 5, 4, 5, 4, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents Motion-X, a large-scale 3D human pose dataset featuring comprehensive annotations of body, hand, and facial expressions, along with text descriptions of 3D whole-body motion. The dataset comprises 96,000 motion sequences and 13.7 million precise 3D pose annotations, aiming to facilitate multimodal learning and advance research in human motion generation and mesh recovery. The authors introduce an automatic annotation pipeline designed to enhance the efficiency and accuracy of data processing, which employs a three-step process involving keypoints estimation, local pose optimization, and global translation optimization. Additionally, the authors propose HandScript for detailed hand gesture descriptions and emphasize ethical considerations in data collection and annotation. They also clarify that their hierarchical keypoint estimation method utilizes strong backbones and combines whole-body keypoints with a state-of-the-art hand detector to enhance robustness under occlusion.

### Strengths and Weaknesses
Strengths:  
1. Motion-X is the largest dataset of its kind, offering extensive annotations that include facial expressions and hand gestures, which are often absent in existing datasets.  
2. The automatic annotation pipeline demonstrates superior performance in 2D keypoint and 3D SMPL-X annotation compared to other methods, showcasing its potential for large-scale applications.  
3. The motion annotation pipeline is robust, employing rigorous filtering methods to ensure high-quality video input, which correlates with improved annotation accuracy.  
4. The introduction of HandScript enhances the description of hand gestures, providing both low-level and high-level annotations.  
5. The authors provide a thorough explanation of their annotation process, including the use of 2D and 3D keypoints for enhanced accuracy.  

Weaknesses:  
1. The quality of motion capture data in Motion-X is inconsistent, particularly regarding the accuracy of 3D hand poses and facial expressions, which may not align with body motions.  
2. Annotations derived from online videos exhibit depth ambiguity and occlusion issues, leading to potentially inaccurate pose representations, which the authors acknowledge as pseudo-GT.  
3. The dataset lacks a thorough ablation study to assess the impact of varying annotation methods on data quality, and the robustness of the annotation pipeline under challenging conditions remains unverified.  
4. The reliance on low-quality videos can still impact the accuracy of annotations, particularly for facial expressions and hand motions.  
5. The paper lacks detailed examples of failure cases and their ratios, which could provide insight into the limitations of the proposed methods.  

### Suggestions for Improvement
1. We recommend that the authors improve the quality of mocap datasets in Motion-X by ensuring that 3D hand pose annotations are accurate and consistent with body motions, rather than relying on potentially flawed sources like AMASS.  
2. The authors should clarify the nature of annotations from online videos, explicitly labeling them as pseudo-GT in all comparisons and excluding them from performance metrics against datasets with ground truth.  
3. We suggest that the authors conduct a comparison of their annotation pipeline with existing methods for obtaining pseudo-GT, such as NeuralAnnot, to validate the effectiveness of their approach.  
4. The authors should address the limitations of the annotation pipeline by providing insights into its performance under various conditions, including occlusion and low resolution, and include failure case analyses.  
5. We recommend that the authors improve the annotation accuracy of facial expressions and hand motions by further refining their video quality filtering techniques and exploring additional innovative annotation methods.  
6. We encourage the authors to consider using original facial expressions in sequences where specific facial motions are critical to maintain real-world correspondence.  
7. Lastly, we suggest providing more visual examples of failure cases and their corresponding ratios to better illustrate the limitations of the pipeline, and enhancing the explanation of the dataset's limitations to provide a more balanced view of its strengths and weaknesses.