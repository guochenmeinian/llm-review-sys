ID: GJMYvWzjE1
Title: Language Models as Hierarchy Encoders
Conference: NeurIPS
Year: 2024
Number of Reviews: 10
Original Ratings: 5, 4, 6, 5, -1, -1, -1, -1, -1, -1
Original Confidences: 3, 3, 4, 4, -1, -1, -1, -1, -1, -1

Aggregated Review:
### Key Points
This paper presents a method that utilizes a non-Euclidean representation space to encode hierarchical relationships between entities through a hyperbolic embedding space. The authors propose that items closer to the origin represent higher-level concepts, while those further away represent lower-level concepts, facilitating the translation between geometric distances and inheritance relationships. The authors conduct experiments using sentence-Transformer models to predict taxonomic relationships in a zero-shot manner, demonstrating that their method outperforms naive approaches. Additionally, the paper introduces Hierarchy Transformer encoders (HITs) to retrain transformer-based models, enhancing their ability to encode hierarchical structures.

### Strengths and Weaknesses
Strengths:
- The proposed embedding space effectively categorizes hierarchical relationships, with clear meanings for distances in latent space.
- The method is computationally efficient, making it accessible and scalable.
- The utilization of hyperbolic space for encoding hierarchies is creative and theoretically sound, supported by extensive experimental results showing consistent performance improvements.

Weaknesses:
- The preservation of properties of independent entities in the new latent space is unclear, necessitating evaluations on a broader range of downstream tasks.
- The motivation for the approach may be overstated, as previous works have explored similar ideas, and the distinction from prior approaches is not as significant as suggested.
- The proposed method's reliance on predefined hierarchies may limit generalization, particularly for new entities without established relationships.

### Suggestions for Improvement
We recommend that the authors improve the evaluation of their method by conducting analyses on a wider variety of downstream tasks to establish what semantic information is preserved. Additionally, we suggest providing a more systematic analysis of the results in Table 4, potentially by exploring a larger subset of WordNet. It would also be beneficial to evaluate the method's performance on tasks that require entity classification as a key component, such as counterfactual reasoning tasks. Furthermore, clarifying the impact of entity frequency on the captured hierarchy and addressing the applicability of the method to decoder-only models would enhance the paper's robustness.