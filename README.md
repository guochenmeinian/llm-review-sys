# Automated LLM-Based Peer Review System

This project uses LLM to auto-reviews the research paper.

####  Data Preparation

- We crawled papers from openreview with top venues papers (e.g. ICML, NIPS, ICLR, CVPR) along with their reviews. The dataset is published in Huggingface and made public: 
  - [Dataset (raw)](https://huggingface.co/datasets/guochenmeinian/openreview)
  - [Dataset (ready for training)](https://huggingface.co/datasets/guochenmeinian/openreview_dataset)

- We used [Nougat-OCR](https://github.com/facebookresearch/nougat) to parse the PDF. Here's a [usage](https://github.com/ad17171717/YouTube-Tutorials/blob/main/Machine%20Learning%20with%20Python/Optical_Character_Recognition_(OCR)_with_Meta's_Nougat!.ipynb) guide I found. With the raw dataset, We used GPT-4-mini to format/merge reviews to prepare the dataset for training.

Here's a sample of the structured output we prepared for training:
```
### Key Points
This paper presents an online learning framework for Markov Decision Processes (MDPs) with countably infinite states, utilizing a Bayesian perspective where MDP parameters follow a prior distribution. The authors propose a Thompson-sampling-like approach to solve the MDP, assuming access to an optimal policy oracle. The learning goal is Bayesian regret minimization, achieving a regret bound of \(\sqrt{TA}\) under certain assumptions. The paper contributes to theoretical reinforcement learning by providing near-optimal algorithms for unbounded state spaces and includes empirical simulations demonstrating the algorithm's performance.

### Strengths and Weaknesses
Strengths:
- The model exhibits high generality and contributes significantly to theoretical reinforcement learning.
- The combination of Lyapunov analysis with existing proofs offers valuable insights for future research.
- The empirical simulations provide evidence supporting the algorithm's performance.
Weaknesses:
- The reliance on assumptions, particularly Assumption 3 regarding stability, may limit practical applicability and verification.
- The algorithm's dependence on an oracle for optimal policy solutions poses challenges for general queueing systems.
- The requirement to return to state 0 at the end of each episode could lead to impractical exponential dependencies.
- The paper lacks clarity in presenting constants related to theoretical results, which are crucial for practical performance.

### Suggestions for Improvement
We recommend that the authors improve the clarity of the assumptions, particularly Assumption 3, by discussing its implications for stability in more general systems. It would be beneficial to explore heuristics for designing the parameter and policy spaces to ensure this assumption holds. Additionally, we suggest testing the algorithm in more general systems and clarifying the necessity of the optimal policy oracle, possibly by presenting results in a comparative form against simpler policies like MaxWeight. The authors should also address the dependence of regret on system size and ensure consistent terminology by using either "queueing" or "queuing" throughout the paper. Finally, we advise revising the abstract for conciseness and improving the overall writing quality to enhance readability.

### Rating
Overall Quality: 6.2
Review Confidence: 3.2
```

####  Model Training

- We first used the LLaMA-Factory to fine-tune the LLaMA 3.1-8B-Instruct-8K model using QLoRA. We ran the training on two NVIDIA 4090 GPUs through AutoDL. Along the way, we explored and learned related tools like Hugging Face Transformers, Accelerate, and DeepSpeed, which helped us better understand model loading, distributed training, and memory optimization.

- After supervised fine-tuning (SFT) with QLoRA, we further align the model with DPO (Direct Preference Optimization). Our idea is: while QLoRA teaches the model to reproduce structured reviews, DPO will push the model to prefer higher-quality, better-structured responses by optimizing preferences between "good" and "bad" outputs. This should make the review style even more standardized and human-aligned.

(Ideally) The matching ratings between the reviews generated by our model and the manual reviews would surpass the effect of GPT-4 reviewers, which improves the accuracy of automated review.


---

start with setup the env:
```
conda env create -f environment.yml
conda activate openreview
```
if you changed the env, do this:
`conda env export > environment.yml`

---



